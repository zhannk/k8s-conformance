I0118 22:09:37.598446      23 e2e.go:126] Starting e2e run "5a657bac-996d-4529-8596-8a8788f66c01" on Ginkgo node 1
Jan 18 22:09:37.616: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674079777 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 18 22:09:37.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:09:37.740: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 18 22:09:37.752: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 22:09:37.767: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 22:09:37.767: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 18 22:09:37.767: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 22:09:37.771: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 18 22:09:37.771: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jan 18 22:09:37.771: INFO: e2e test version: v1.26.0
Jan 18 22:09:37.772: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 18 22:09:37.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:09:37.776: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.038 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 18 22:09:37.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:09:37.740: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 18 22:09:37.752: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 18 22:09:37.767: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 18 22:09:37.767: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 18 22:09:37.767: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 18 22:09:37.771: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 18 22:09:37.771: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
    Jan 18 22:09:37.771: INFO: e2e test version: v1.26.0
    Jan 18 22:09:37.772: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 18 22:09:37.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:09:37.776: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:37.804
Jan 18 22:09:37.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 22:09:37.805
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:37.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:37.821
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/18/23 22:09:37.824
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:37.833
STEP: Creating a service in the namespace 01/18/23 22:09:37.836
STEP: Deleting the namespace 01/18/23 22:09:37.843
STEP: Waiting for the namespace to be removed. 01/18/23 22:09:37.848
STEP: Recreating the namespace 01/18/23 22:09:43.852
STEP: Verifying there is no service in the namespace 01/18/23 22:09:43.867
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:09:43.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3970" for this suite. 01/18/23 22:09:43.872
STEP: Destroying namespace "nsdeletetest-9967" for this suite. 01/18/23 22:09:43.877
Jan 18 22:09:43.879: INFO: Namespace nsdeletetest-9967 was already deleted
STEP: Destroying namespace "nsdeletetest-2872" for this suite. 01/18/23 22:09:43.879
------------------------------
â€¢ [SLOW TEST] [6.080 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:37.804
    Jan 18 22:09:37.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:09:37.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:37.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:37.821
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/18/23 22:09:37.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:37.833
    STEP: Creating a service in the namespace 01/18/23 22:09:37.836
    STEP: Deleting the namespace 01/18/23 22:09:37.843
    STEP: Waiting for the namespace to be removed. 01/18/23 22:09:37.848
    STEP: Recreating the namespace 01/18/23 22:09:43.852
    STEP: Verifying there is no service in the namespace 01/18/23 22:09:43.867
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:09:43.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3970" for this suite. 01/18/23 22:09:43.872
    STEP: Destroying namespace "nsdeletetest-9967" for this suite. 01/18/23 22:09:43.877
    Jan 18 22:09:43.879: INFO: Namespace nsdeletetest-9967 was already deleted
    STEP: Destroying namespace "nsdeletetest-2872" for this suite. 01/18/23 22:09:43.879
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:43.885
Jan 18 22:09:43.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 22:09:43.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:43.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:43.898
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/18/23 22:09:43.904
STEP: waiting for Deployment to be created 01/18/23 22:09:43.909
STEP: waiting for all Replicas to be Ready 01/18/23 22:09:43.91
Jan 18 22:09:43.912: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.912: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.921: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.921: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.936: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.936: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.948: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:43.948: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:09:47.061: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 22:09:47.061: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 22:09:47.954: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/18/23 22:09:47.954
W0118 22:09:47.965562      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 22:09:47.967: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/18/23 22:09:47.967
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.970: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.978: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.978: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.994: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:47.994: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:48.005: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:48.005: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:48.013: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:48.013: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:49.083: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:49.083: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:49.100: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
STEP: listing Deployments 01/18/23 22:09:49.1
Jan 18 22:09:49.103: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/18/23 22:09:49.103
Jan 18 22:09:49.124: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/18/23 22:09:49.124
Jan 18 22:09:49.132: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:49.135: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:49.154: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:49.169: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:49.176: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:49.961: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:54.086: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:54.114: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:54.121: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:09:58.000: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/18/23 22:09:58.018
STEP: fetching the DeploymentStatus 01/18/23 22:09:58.025
Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3
Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3
STEP: deleting the Deployment 01/18/23 22:09:58.03
Jan 18 22:09:58.039: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
Jan 18 22:09:58.040: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:09:58.045: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 22:09:58.067: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3158  6678b8de-ceda-4203-b73b-6363018ba1b7 1439 2 2023-01-18 22:09:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e2097 0xc0017e2098}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 22:09:58.070: INFO: pod: "test-deployment-7b7876f9d6-7fhlq":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-7fhlq test-deployment-7b7876f9d6- deployment-3158  1b577f02-af2e-40f6-9da8-143eb3e8e5ae 1402 0 2023-01-18 22:09:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6678b8de-ceda-4203-b73b-6363018ba1b7 0xc0017e2f07 0xc0017e2f08}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6678b8de-ceda-4203-b73b-6363018ba1b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgnzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgnzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.5,StartTime:2023-01-18 22:09:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9a7f95eb50887fecf05e55978b1293ed72268f210873d16c0aede27b36f98a82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:09:58.071: INFO: pod: "test-deployment-7b7876f9d6-lzlk9":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-lzlk9 test-deployment-7b7876f9d6- deployment-3158  635b4044-cb9c-4cb6-9087-9bdd0645b67b 1438 0 2023-01-18 22:09:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6678b8de-ceda-4203-b73b-6363018ba1b7 0xc0017e33a7 0xc0017e33a8}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6678b8de-ceda-4203-b73b-6363018ba1b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twbj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twbj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.2,StartTime:2023-01-18 22:09:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d7e8830a5010a04db0d9195ae141d9be9fbbab1d62e1c1e59570abf55ce6dd70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:09:58.071: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3158  97135067-35ae-4df3-a495-2047e24b66bb 1447 4 2023-01-18 22:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e2197 0xc0017e2198}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2350 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 22:09:58.080: INFO: pod: "test-deployment-7df74c55ff-h7sxz":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-h7sxz test-deployment-7df74c55ff- deployment-3158  966d98d0-9d76-4cae-bb22-fa7e6f361622 1442 0 2023-01-18 22:09:49 +0000 UTC 2023-01-18 22:09:58 +0000 UTC 0xc00007f080 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 97135067-35ae-4df3-a495-2047e24b66bb 0xc00007f0d7 0xc00007f0d8}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97135067-35ae-4df3-a495-2047e24b66bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfxc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfxc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.3,StartTime:2023-01-18 22:09:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://44a56048492b4f339f99d3b3e8e1fa4ee64e6fbb9f4e0c3b94e58c5ab874a411,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:09:58.080: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3158  d533caee-b1a4-4572-aeca-01def52b15a1 1352 3 2023-01-18 22:09:43 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e23b7 0xc0017e23b8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2440 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:09:58.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3158" for this suite. 01/18/23 22:09:58.085
------------------------------
â€¢ [SLOW TEST] [14.355 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:43.885
    Jan 18 22:09:43.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 22:09:43.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:43.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:43.898
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/18/23 22:09:43.904
    STEP: waiting for Deployment to be created 01/18/23 22:09:43.909
    STEP: waiting for all Replicas to be Ready 01/18/23 22:09:43.91
    Jan 18 22:09:43.912: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.912: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.921: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.921: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.936: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.936: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.948: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:43.948: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:09:47.061: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 22:09:47.061: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 22:09:47.954: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/18/23 22:09:47.954
    W0118 22:09:47.965562      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 22:09:47.967: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/18/23 22:09:47.967
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 0
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.969: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.970: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.978: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.978: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.994: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:47.994: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:48.005: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:48.005: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:48.013: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:48.013: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:49.083: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:49.083: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:49.100: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    STEP: listing Deployments 01/18/23 22:09:49.1
    Jan 18 22:09:49.103: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/18/23 22:09:49.103
    Jan 18 22:09:49.124: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/18/23 22:09:49.124
    Jan 18 22:09:49.132: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:49.135: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:49.154: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:49.169: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:49.176: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:49.961: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:54.086: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:54.114: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:54.121: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:09:58.000: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/18/23 22:09:58.018
    STEP: fetching the DeploymentStatus 01/18/23 22:09:58.025
    Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:58.029: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 1
    Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3
    Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 2
    Jan 18 22:09:58.030: INFO: observed Deployment test-deployment in namespace deployment-3158 with ReadyReplicas 3
    STEP: deleting the Deployment 01/18/23 22:09:58.03
    Jan 18 22:09:58.039: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    Jan 18 22:09:58.040: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:09:58.045: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 18 22:09:58.067: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3158  6678b8de-ceda-4203-b73b-6363018ba1b7 1439 2 2023-01-18 22:09:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e2097 0xc0017e2098}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 22:09:58.070: INFO: pod: "test-deployment-7b7876f9d6-7fhlq":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-7fhlq test-deployment-7b7876f9d6- deployment-3158  1b577f02-af2e-40f6-9da8-143eb3e8e5ae 1402 0 2023-01-18 22:09:49 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6678b8de-ceda-4203-b73b-6363018ba1b7 0xc0017e2f07 0xc0017e2f08}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6678b8de-ceda-4203-b73b-6363018ba1b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgnzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgnzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.5,StartTime:2023-01-18 22:09:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9a7f95eb50887fecf05e55978b1293ed72268f210873d16c0aede27b36f98a82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:09:58.071: INFO: pod: "test-deployment-7b7876f9d6-lzlk9":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-lzlk9 test-deployment-7b7876f9d6- deployment-3158  635b4044-cb9c-4cb6-9087-9bdd0645b67b 1438 0 2023-01-18 22:09:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6678b8de-ceda-4203-b73b-6363018ba1b7 0xc0017e33a7 0xc0017e33a8}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6678b8de-ceda-4203-b73b-6363018ba1b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twbj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twbj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.2,StartTime:2023-01-18 22:09:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d7e8830a5010a04db0d9195ae141d9be9fbbab1d62e1c1e59570abf55ce6dd70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:09:58.071: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3158  97135067-35ae-4df3-a495-2047e24b66bb 1447 4 2023-01-18 22:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e2197 0xc0017e2198}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2350 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 22:09:58.080: INFO: pod: "test-deployment-7df74c55ff-h7sxz":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-h7sxz test-deployment-7df74c55ff- deployment-3158  966d98d0-9d76-4cae-bb22-fa7e6f361622 1442 0 2023-01-18 22:09:49 +0000 UTC 2023-01-18 22:09:58 +0000 UTC 0xc00007f080 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 97135067-35ae-4df3-a495-2047e24b66bb 0xc00007f0d7 0xc00007f0d8}] [] [{kube-controller-manager Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97135067-35ae-4df3-a495-2047e24b66bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfxc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfxc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:09:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.3,StartTime:2023-01-18 22:09:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://44a56048492b4f339f99d3b3e8e1fa4ee64e6fbb9f4e0c3b94e58c5ab874a411,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:09:58.080: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3158  d533caee-b1a4-4572-aeca-01def52b15a1 1352 3 2023-01-18 22:09:43 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 81541389-5c70-4ae7-b297-bced6992df64 0xc0017e23b7 0xc0017e23b8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81541389-5c70-4ae7-b297-bced6992df64\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:09:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017e2440 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:09:58.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3158" for this suite. 01/18/23 22:09:58.085
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:58.24
Jan 18 22:09:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:09:58.241
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:58.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:58.296
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-9a85fd3c-3c22-4722-8722-857400f334db 01/18/23 22:09:58.299
STEP: Creating a pod to test consume secrets 01/18/23 22:09:58.313
Jan 18 22:09:58.321: INFO: Waiting up to 5m0s for pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58" in namespace "secrets-5091" to be "Succeeded or Failed"
Jan 18 22:09:58.323: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655012ms
Jan 18 22:10:00.327: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0064671s
Jan 18 22:10:02.328: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007592519s
STEP: Saw pod success 01/18/23 22:10:02.328
Jan 18 22:10:02.328: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58" satisfied condition "Succeeded or Failed"
Jan 18 22:10:02.331: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:10:02.346
Jan 18 22:10:02.377: INFO: Waiting for pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 to disappear
Jan 18 22:10:02.380: INFO: Pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:10:02.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5091" for this suite. 01/18/23 22:10:02.383
------------------------------
â€¢ [4.148 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:58.24
    Jan 18 22:09:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:09:58.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:58.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:58.296
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-9a85fd3c-3c22-4722-8722-857400f334db 01/18/23 22:09:58.299
    STEP: Creating a pod to test consume secrets 01/18/23 22:09:58.313
    Jan 18 22:09:58.321: INFO: Waiting up to 5m0s for pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58" in namespace "secrets-5091" to be "Succeeded or Failed"
    Jan 18 22:09:58.323: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655012ms
    Jan 18 22:10:00.327: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0064671s
    Jan 18 22:10:02.328: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007592519s
    STEP: Saw pod success 01/18/23 22:10:02.328
    Jan 18 22:10:02.328: INFO: Pod "pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58" satisfied condition "Succeeded or Failed"
    Jan 18 22:10:02.331: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:10:02.346
    Jan 18 22:10:02.377: INFO: Waiting for pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 to disappear
    Jan 18 22:10:02.380: INFO: Pod pod-secrets-f2f0161a-0a03-44c4-9335-37c9bf41ef58 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:10:02.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5091" for this suite. 01/18/23 22:10:02.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:10:02.389
Jan 18 22:10:02.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:10:02.39
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:02.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:02.403
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:02.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9519" for this suite. 01/18/23 22:11:02.419
------------------------------
â€¢ [SLOW TEST] [60.035 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:10:02.389
    Jan 18 22:10:02.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:10:02.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:02.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:02.403
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:02.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9519" for this suite. 01/18/23 22:11:02.419
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:02.424
Jan 18 22:11:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:11:02.426
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:02.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:02.439
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:11:02.442
Jan 18 22:11:02.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0" in namespace "downward-api-5348" to be "Succeeded or Failed"
Jan 18 22:11:02.454: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925518ms
Jan 18 22:11:04.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007493458s
Jan 18 22:11:06.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007554286s
STEP: Saw pod success 01/18/23 22:11:06.458
Jan 18 22:11:06.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0" satisfied condition "Succeeded or Failed"
Jan 18 22:11:06.461: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 container client-container: <nil>
STEP: delete the pod 01/18/23 22:11:06.466
Jan 18 22:11:06.477: INFO: Waiting for pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 to disappear
Jan 18 22:11:06.480: INFO: Pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5348" for this suite. 01/18/23 22:11:06.483
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:02.424
    Jan 18 22:11:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:11:02.426
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:02.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:02.439
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:11:02.442
    Jan 18 22:11:02.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0" in namespace "downward-api-5348" to be "Succeeded or Failed"
    Jan 18 22:11:02.454: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925518ms
    Jan 18 22:11:04.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007493458s
    Jan 18 22:11:06.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007554286s
    STEP: Saw pod success 01/18/23 22:11:06.458
    Jan 18 22:11:06.458: INFO: Pod "downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0" satisfied condition "Succeeded or Failed"
    Jan 18 22:11:06.461: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:11:06.466
    Jan 18 22:11:06.477: INFO: Waiting for pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 to disappear
    Jan 18 22:11:06.480: INFO: Pod downwardapi-volume-98e51254-3e3a-4a80-a749-1466682b81b0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5348" for this suite. 01/18/23 22:11:06.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:06.49
Jan 18 22:11:06.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:11:06.491
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:06.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:06.506
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/18/23 22:11:06.509
STEP: submitting the pod to kubernetes 01/18/23 22:11:06.509
Jan 18 22:11:06.518: INFO: Waiting up to 5m0s for pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" in namespace "pods-2440" to be "running and ready"
Jan 18 22:11:06.521: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893672ms
Jan 18 22:11:06.521: INFO: The phase of Pod pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:11:08.525: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006880799s
Jan 18 22:11:08.525: INFO: The phase of Pod pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0 is Running (Ready = true)
Jan 18 22:11:08.525: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 22:11:08.528
STEP: updating the pod 01/18/23 22:11:08.531
Jan 18 22:11:09.043: INFO: Successfully updated pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0"
Jan 18 22:11:09.043: INFO: Waiting up to 5m0s for pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" in namespace "pods-2440" to be "running"
Jan 18 22:11:09.045: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Running", Reason="", readiness=true. Elapsed: 2.511918ms
Jan 18 22:11:09.045: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/18/23 22:11:09.045
Jan 18 22:11:09.048: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:09.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2440" for this suite. 01/18/23 22:11:09.051
------------------------------
â€¢ [2.566 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:06.49
    Jan 18 22:11:06.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:11:06.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:06.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:06.506
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/18/23 22:11:06.509
    STEP: submitting the pod to kubernetes 01/18/23 22:11:06.509
    Jan 18 22:11:06.518: INFO: Waiting up to 5m0s for pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" in namespace "pods-2440" to be "running and ready"
    Jan 18 22:11:06.521: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893672ms
    Jan 18 22:11:06.521: INFO: The phase of Pod pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:11:08.525: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006880799s
    Jan 18 22:11:08.525: INFO: The phase of Pod pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0 is Running (Ready = true)
    Jan 18 22:11:08.525: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 22:11:08.528
    STEP: updating the pod 01/18/23 22:11:08.531
    Jan 18 22:11:09.043: INFO: Successfully updated pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0"
    Jan 18 22:11:09.043: INFO: Waiting up to 5m0s for pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" in namespace "pods-2440" to be "running"
    Jan 18 22:11:09.045: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0": Phase="Running", Reason="", readiness=true. Elapsed: 2.511918ms
    Jan 18 22:11:09.045: INFO: Pod "pod-update-0594a48d-2a18-4f05-970a-4b93478cbbe0" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/18/23 22:11:09.045
    Jan 18 22:11:09.048: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:09.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2440" for this suite. 01/18/23 22:11:09.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:09.057
Jan 18 22:11:09.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename subpath 01/18/23 22:11:09.058
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:09.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:09.071
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:11:09.074
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-z2xm 01/18/23 22:11:09.085
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:11:09.085
Jan 18 22:11:09.095: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z2xm" in namespace "subpath-4899" to be "Succeeded or Failed"
Jan 18 22:11:09.097: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474119ms
Jan 18 22:11:11.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006436438s
Jan 18 22:11:13.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 4.005722396s
Jan 18 22:11:15.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 6.007000767s
Jan 18 22:11:17.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 8.006591259s
Jan 18 22:11:19.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 10.006828563s
Jan 18 22:11:21.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 12.007220192s
Jan 18 22:11:23.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 14.00641592s
Jan 18 22:11:25.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 16.007400137s
Jan 18 22:11:27.103: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 18.007691018s
Jan 18 22:11:29.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 20.006047994s
Jan 18 22:11:31.103: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=false. Elapsed: 22.007716363s
Jan 18 22:11:33.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006334973s
STEP: Saw pod success 01/18/23 22:11:33.101
Jan 18 22:11:33.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm" satisfied condition "Succeeded or Failed"
Jan 18 22:11:33.104: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-downwardapi-z2xm container test-container-subpath-downwardapi-z2xm: <nil>
STEP: delete the pod 01/18/23 22:11:33.109
Jan 18 22:11:33.121: INFO: Waiting for pod pod-subpath-test-downwardapi-z2xm to disappear
Jan 18 22:11:33.124: INFO: Pod pod-subpath-test-downwardapi-z2xm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-z2xm 01/18/23 22:11:33.124
Jan 18 22:11:33.125: INFO: Deleting pod "pod-subpath-test-downwardapi-z2xm" in namespace "subpath-4899"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:33.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4899" for this suite. 01/18/23 22:11:33.13
------------------------------
â€¢ [SLOW TEST] [24.079 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:09.057
    Jan 18 22:11:09.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename subpath 01/18/23 22:11:09.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:09.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:09.071
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:11:09.074
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-z2xm 01/18/23 22:11:09.085
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:11:09.085
    Jan 18 22:11:09.095: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z2xm" in namespace "subpath-4899" to be "Succeeded or Failed"
    Jan 18 22:11:09.097: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474119ms
    Jan 18 22:11:11.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006436438s
    Jan 18 22:11:13.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 4.005722396s
    Jan 18 22:11:15.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 6.007000767s
    Jan 18 22:11:17.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 8.006591259s
    Jan 18 22:11:19.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 10.006828563s
    Jan 18 22:11:21.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 12.007220192s
    Jan 18 22:11:23.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 14.00641592s
    Jan 18 22:11:25.102: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 16.007400137s
    Jan 18 22:11:27.103: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 18.007691018s
    Jan 18 22:11:29.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=true. Elapsed: 20.006047994s
    Jan 18 22:11:31.103: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Running", Reason="", readiness=false. Elapsed: 22.007716363s
    Jan 18 22:11:33.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006334973s
    STEP: Saw pod success 01/18/23 22:11:33.101
    Jan 18 22:11:33.101: INFO: Pod "pod-subpath-test-downwardapi-z2xm" satisfied condition "Succeeded or Failed"
    Jan 18 22:11:33.104: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-downwardapi-z2xm container test-container-subpath-downwardapi-z2xm: <nil>
    STEP: delete the pod 01/18/23 22:11:33.109
    Jan 18 22:11:33.121: INFO: Waiting for pod pod-subpath-test-downwardapi-z2xm to disappear
    Jan 18 22:11:33.124: INFO: Pod pod-subpath-test-downwardapi-z2xm no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-z2xm 01/18/23 22:11:33.124
    Jan 18 22:11:33.125: INFO: Deleting pod "pod-subpath-test-downwardapi-z2xm" in namespace "subpath-4899"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:33.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4899" for this suite. 01/18/23 22:11:33.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:33.137
Jan 18 22:11:33.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:11:33.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:33.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:33.153
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-82 01/18/23 22:11:33.156
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:11:33.166
STEP: creating service externalsvc in namespace services-82 01/18/23 22:11:33.166
STEP: creating replication controller externalsvc in namespace services-82 01/18/23 22:11:33.179
I0118 22:11:33.186677      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-82, replica count: 2
I0118 22:11:36.238704      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:11:36.241
Jan 18 22:11:36.251: INFO: Creating new exec pod
Jan 18 22:11:36.258: INFO: Waiting up to 5m0s for pod "execpodq5lnp" in namespace "services-82" to be "running"
Jan 18 22:11:36.261: INFO: Pod "execpodq5lnp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48658ms
Jan 18 22:11:38.264: INFO: Pod "execpodq5lnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005456424s
Jan 18 22:11:38.264: INFO: Pod "execpodq5lnp" satisfied condition "running"
Jan 18 22:11:38.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-82 exec execpodq5lnp -- /bin/sh -x -c nslookup clusterip-service.services-82.svc.cluster.local'
Jan 18 22:11:38.455: INFO: stderr: "+ nslookup clusterip-service.services-82.svc.cluster.local\n"
Jan 18 22:11:38.455: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-82.svc.cluster.local\tcanonical name = externalsvc.services-82.svc.cluster.local.\nName:\texternalsvc.services-82.svc.cluster.local\nAddress: 10.96.2.136\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-82, will wait for the garbage collector to delete the pods 01/18/23 22:11:38.455
Jan 18 22:11:38.514: INFO: Deleting ReplicationController externalsvc took: 5.682036ms
Jan 18 22:11:38.615: INFO: Terminating ReplicationController externalsvc pods took: 101.008682ms
Jan 18 22:11:40.428: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:40.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-82" for this suite. 01/18/23 22:11:40.439
------------------------------
â€¢ [SLOW TEST] [7.308 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:33.137
    Jan 18 22:11:33.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:11:33.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:33.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:33.153
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-82 01/18/23 22:11:33.156
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:11:33.166
    STEP: creating service externalsvc in namespace services-82 01/18/23 22:11:33.166
    STEP: creating replication controller externalsvc in namespace services-82 01/18/23 22:11:33.179
    I0118 22:11:33.186677      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-82, replica count: 2
    I0118 22:11:36.238704      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:11:36.241
    Jan 18 22:11:36.251: INFO: Creating new exec pod
    Jan 18 22:11:36.258: INFO: Waiting up to 5m0s for pod "execpodq5lnp" in namespace "services-82" to be "running"
    Jan 18 22:11:36.261: INFO: Pod "execpodq5lnp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48658ms
    Jan 18 22:11:38.264: INFO: Pod "execpodq5lnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005456424s
    Jan 18 22:11:38.264: INFO: Pod "execpodq5lnp" satisfied condition "running"
    Jan 18 22:11:38.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-82 exec execpodq5lnp -- /bin/sh -x -c nslookup clusterip-service.services-82.svc.cluster.local'
    Jan 18 22:11:38.455: INFO: stderr: "+ nslookup clusterip-service.services-82.svc.cluster.local\n"
    Jan 18 22:11:38.455: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-82.svc.cluster.local\tcanonical name = externalsvc.services-82.svc.cluster.local.\nName:\texternalsvc.services-82.svc.cluster.local\nAddress: 10.96.2.136\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-82, will wait for the garbage collector to delete the pods 01/18/23 22:11:38.455
    Jan 18 22:11:38.514: INFO: Deleting ReplicationController externalsvc took: 5.682036ms
    Jan 18 22:11:38.615: INFO: Terminating ReplicationController externalsvc pods took: 101.008682ms
    Jan 18 22:11:40.428: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:40.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-82" for this suite. 01/18/23 22:11:40.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:40.447
Jan 18 22:11:40.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:11:40.448
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:40.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:40.461
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5879 01/18/23 22:11:40.465
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[] 01/18/23 22:11:40.475
Jan 18 22:11:40.483: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5879 01/18/23 22:11:40.483
Jan 18 22:11:40.492: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5879" to be "running and ready"
Jan 18 22:11:40.495: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533039ms
Jan 18 22:11:40.495: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:11:42.499: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007217727s
Jan 18 22:11:42.499: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 22:11:42.499: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod1:[100]] 01/18/23 22:11:42.502
Jan 18 22:11:42.512: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5879 01/18/23 22:11:42.512
Jan 18 22:11:42.518: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5879" to be "running and ready"
Jan 18 22:11:42.521: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397121ms
Jan 18 22:11:42.521: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:11:44.525: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007107998s
Jan 18 22:11:44.525: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 22:11:44.525: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 22:11:44.528
Jan 18 22:11:44.539: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/18/23 22:11:44.539
Jan 18 22:11:44.539: INFO: Creating new exec pod
Jan 18 22:11:44.545: INFO: Waiting up to 5m0s for pod "execpodhwwsm" in namespace "services-5879" to be "running"
Jan 18 22:11:44.548: INFO: Pod "execpodhwwsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522967ms
Jan 18 22:11:46.551: INFO: Pod "execpodhwwsm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005663035s
Jan 18 22:11:46.551: INFO: Pod "execpodhwwsm" satisfied condition "running"
Jan 18 22:11:47.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 18 22:11:47.700: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 22:11:47.700: INFO: stdout: ""
Jan 18 22:11:47.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 10.96.0.150 80'
Jan 18 22:11:47.841: INFO: stderr: "+ nc -v -z -w 2 10.96.0.150 80\nConnection to 10.96.0.150 80 port [tcp/http] succeeded!\n"
Jan 18 22:11:47.841: INFO: stdout: ""
Jan 18 22:11:47.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 18 22:11:47.970: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 22:11:47.970: INFO: stdout: ""
Jan 18 22:11:47.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 10.96.0.150 81'
Jan 18 22:11:48.106: INFO: stderr: "+ nc -v -z -w 2 10.96.0.150 81\nConnection to 10.96.0.150 81 port [tcp/*] succeeded!\n"
Jan 18 22:11:48.106: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5879 01/18/23 22:11:48.106
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod2:[101]] 01/18/23 22:11:48.119
Jan 18 22:11:48.131: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5879 01/18/23 22:11:48.131
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[] 01/18/23 22:11:48.148
Jan 18 22:11:48.156: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:48.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5879" for this suite. 01/18/23 22:11:48.175
------------------------------
â€¢ [SLOW TEST] [7.735 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:40.447
    Jan 18 22:11:40.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:11:40.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:40.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:40.461
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5879 01/18/23 22:11:40.465
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[] 01/18/23 22:11:40.475
    Jan 18 22:11:40.483: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5879 01/18/23 22:11:40.483
    Jan 18 22:11:40.492: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5879" to be "running and ready"
    Jan 18 22:11:40.495: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533039ms
    Jan 18 22:11:40.495: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:11:42.499: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007217727s
    Jan 18 22:11:42.499: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 22:11:42.499: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod1:[100]] 01/18/23 22:11:42.502
    Jan 18 22:11:42.512: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5879 01/18/23 22:11:42.512
    Jan 18 22:11:42.518: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5879" to be "running and ready"
    Jan 18 22:11:42.521: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397121ms
    Jan 18 22:11:42.521: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:11:44.525: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007107998s
    Jan 18 22:11:44.525: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 22:11:44.525: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 22:11:44.528
    Jan 18 22:11:44.539: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/18/23 22:11:44.539
    Jan 18 22:11:44.539: INFO: Creating new exec pod
    Jan 18 22:11:44.545: INFO: Waiting up to 5m0s for pod "execpodhwwsm" in namespace "services-5879" to be "running"
    Jan 18 22:11:44.548: INFO: Pod "execpodhwwsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.522967ms
    Jan 18 22:11:46.551: INFO: Pod "execpodhwwsm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005663035s
    Jan 18 22:11:46.551: INFO: Pod "execpodhwwsm" satisfied condition "running"
    Jan 18 22:11:47.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 18 22:11:47.700: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 18 22:11:47.700: INFO: stdout: ""
    Jan 18 22:11:47.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 10.96.0.150 80'
    Jan 18 22:11:47.841: INFO: stderr: "+ nc -v -z -w 2 10.96.0.150 80\nConnection to 10.96.0.150 80 port [tcp/http] succeeded!\n"
    Jan 18 22:11:47.841: INFO: stdout: ""
    Jan 18 22:11:47.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 18 22:11:47.970: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 18 22:11:47.970: INFO: stdout: ""
    Jan 18 22:11:47.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5879 exec execpodhwwsm -- /bin/sh -x -c nc -v -z -w 2 10.96.0.150 81'
    Jan 18 22:11:48.106: INFO: stderr: "+ nc -v -z -w 2 10.96.0.150 81\nConnection to 10.96.0.150 81 port [tcp/*] succeeded!\n"
    Jan 18 22:11:48.106: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5879 01/18/23 22:11:48.106
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[pod2:[101]] 01/18/23 22:11:48.119
    Jan 18 22:11:48.131: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5879 01/18/23 22:11:48.131
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5879 to expose endpoints map[] 01/18/23 22:11:48.148
    Jan 18 22:11:48.156: INFO: successfully validated that service multi-endpoint-test in namespace services-5879 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:48.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5879" for this suite. 01/18/23 22:11:48.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:48.183
Jan 18 22:11:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:11:48.184
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:48.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:48.201
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/18/23 22:11:48.204
STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:11:48.211
STEP: Creating a ResourceQuota with not terminating scope 01/18/23 22:11:50.215
STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:11:50.219
STEP: Creating a long running pod 01/18/23 22:11:52.223
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 22:11:52.235
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 22:11:54.24
STEP: Deleting the pod 01/18/23 22:11:56.244
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:11:56.256
STEP: Creating a terminating pod 01/18/23 22:11:58.261
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 22:11:58.273
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 22:12:00.277
STEP: Deleting the pod 01/18/23 22:12:02.279
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:12:02.293
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:12:04.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3777" for this suite. 01/18/23 22:12:04.3
------------------------------
â€¢ [SLOW TEST] [16.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:48.183
    Jan 18 22:11:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:11:48.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:48.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:48.201
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/18/23 22:11:48.204
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:11:48.211
    STEP: Creating a ResourceQuota with not terminating scope 01/18/23 22:11:50.215
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:11:50.219
    STEP: Creating a long running pod 01/18/23 22:11:52.223
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 22:11:52.235
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 22:11:54.24
    STEP: Deleting the pod 01/18/23 22:11:56.244
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:11:56.256
    STEP: Creating a terminating pod 01/18/23 22:11:58.261
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 22:11:58.273
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 22:12:00.277
    STEP: Deleting the pod 01/18/23 22:12:02.279
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:12:02.293
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:12:04.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3777" for this suite. 01/18/23 22:12:04.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:12:04.306
Jan 18 22:12:04.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 22:12:04.307
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.323
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-7932" 01/18/23 22:12:04.326
Jan 18 22:12:04.332: INFO: Namespace "namespaces-7932" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5a657bac-996d-4529-8596-8a8788f66c01", "kubernetes.io/metadata.name":"namespaces-7932", "namespaces-7932":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:12:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7932" for this suite. 01/18/23 22:12:04.335
------------------------------
â€¢ [0.035 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:12:04.306
    Jan 18 22:12:04.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:12:04.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.323
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-7932" 01/18/23 22:12:04.326
    Jan 18 22:12:04.332: INFO: Namespace "namespaces-7932" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5a657bac-996d-4529-8596-8a8788f66c01", "kubernetes.io/metadata.name":"namespaces-7932", "namespaces-7932":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:12:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7932" for this suite. 01/18/23 22:12:04.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:12:04.342
Jan 18 22:12:04.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename podtemplate 01/18/23 22:12:04.343
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.355
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/18/23 22:12:04.358
STEP: Replace a pod template 01/18/23 22:12:04.364
Jan 18 22:12:04.371: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 22:12:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7184" for this suite. 01/18/23 22:12:04.374
------------------------------
â€¢ [0.036 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:12:04.342
    Jan 18 22:12:04.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 22:12:04.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.355
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/18/23 22:12:04.358
    STEP: Replace a pod template 01/18/23 22:12:04.364
    Jan 18 22:12:04.371: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:12:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7184" for this suite. 01/18/23 22:12:04.374
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:12:04.378
Jan 18 22:12:04.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:12:04.379
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.394
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/18/23 22:12:04.396
Jan 18 22:12:04.402: INFO: Waiting up to 5m0s for pod "var-expansion-94b73233-c504-4374-a21b-277157495a21" in namespace "var-expansion-7456" to be "Succeeded or Failed"
Jan 18 22:12:04.404: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.164504ms
Jan 18 22:12:06.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005326394s
Jan 18 22:12:08.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005585253s
STEP: Saw pod success 01/18/23 22:12:08.408
Jan 18 22:12:08.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21" satisfied condition "Succeeded or Failed"
Jan 18 22:12:08.411: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-94b73233-c504-4374-a21b-277157495a21 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:12:08.415
Jan 18 22:12:08.427: INFO: Waiting for pod var-expansion-94b73233-c504-4374-a21b-277157495a21 to disappear
Jan 18 22:12:08.429: INFO: Pod var-expansion-94b73233-c504-4374-a21b-277157495a21 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:12:08.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7456" for this suite. 01/18/23 22:12:08.432
------------------------------
â€¢ [4.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:12:04.378
    Jan 18 22:12:04.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:12:04.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:04.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:04.394
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/18/23 22:12:04.396
    Jan 18 22:12:04.402: INFO: Waiting up to 5m0s for pod "var-expansion-94b73233-c504-4374-a21b-277157495a21" in namespace "var-expansion-7456" to be "Succeeded or Failed"
    Jan 18 22:12:04.404: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.164504ms
    Jan 18 22:12:06.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005326394s
    Jan 18 22:12:08.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005585253s
    STEP: Saw pod success 01/18/23 22:12:08.408
    Jan 18 22:12:08.408: INFO: Pod "var-expansion-94b73233-c504-4374-a21b-277157495a21" satisfied condition "Succeeded or Failed"
    Jan 18 22:12:08.411: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-94b73233-c504-4374-a21b-277157495a21 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:12:08.415
    Jan 18 22:12:08.427: INFO: Waiting for pod var-expansion-94b73233-c504-4374-a21b-277157495a21 to disappear
    Jan 18 22:12:08.429: INFO: Pod var-expansion-94b73233-c504-4374-a21b-277157495a21 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:12:08.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7456" for this suite. 01/18/23 22:12:08.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:12:08.437
Jan 18 22:12:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:12:08.438
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:08.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:08.454
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/18/23 22:12:08.457
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_tcp@PTR;sleep 1; done
 01/18/23 22:12:08.47
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_tcp@PTR;sleep 1; done
 01/18/23 22:12:08.47
STEP: creating a pod to probe DNS 01/18/23 22:12:08.47
STEP: submitting the pod to kubernetes 01/18/23 22:12:08.47
Jan 18 22:12:08.480: INFO: Waiting up to 15m0s for pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf" in namespace "dns-2461" to be "running"
Jan 18 22:12:08.483: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718319ms
Jan 18 22:12:10.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006355891s
Jan 18 22:12:12.487: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006740755s
Jan 18 22:12:14.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.00648945s
Jan 18 22:12:14.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:12:14.486
STEP: looking for the results for each expected name from probers 01/18/23 22:12:14.489
Jan 18 22:12:14.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.498: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.501: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.504: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.517: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.523: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:14.536: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:19.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.550: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.568: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.574: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:19.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:24.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.552: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.568: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.574: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:24.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:29.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.548: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.566: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.569: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.572: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.575: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:29.591: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:34.542: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.546: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.549: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.552: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.567: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.573: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.575: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:34.588: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:39.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.548: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.567: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.573: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.576: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
Jan 18 22:12:39.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

Jan 18 22:12:44.590: INFO: DNS probes using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf succeeded

STEP: deleting the pod 01/18/23 22:12:44.59
STEP: deleting the test service 01/18/23 22:12:44.61
STEP: deleting the test headless service 01/18/23 22:12:44.629
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:12:44.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2461" for this suite. 01/18/23 22:12:44.64
------------------------------
â€¢ [SLOW TEST] [36.207 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:12:08.437
    Jan 18 22:12:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:12:08.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:08.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:08.454
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/18/23 22:12:08.457
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_tcp@PTR;sleep 1; done
     01/18/23 22:12:08.47
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2461.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2461.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2461.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.1.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.1.178_tcp@PTR;sleep 1; done
     01/18/23 22:12:08.47
    STEP: creating a pod to probe DNS 01/18/23 22:12:08.47
    STEP: submitting the pod to kubernetes 01/18/23 22:12:08.47
    Jan 18 22:12:08.480: INFO: Waiting up to 15m0s for pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf" in namespace "dns-2461" to be "running"
    Jan 18 22:12:08.483: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718319ms
    Jan 18 22:12:10.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006355891s
    Jan 18 22:12:12.487: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006740755s
    Jan 18 22:12:14.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.00648945s
    Jan 18 22:12:14.486: INFO: Pod "dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:12:14.486
    STEP: looking for the results for each expected name from probers 01/18/23 22:12:14.489
    Jan 18 22:12:14.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.498: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.501: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.504: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.517: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.523: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:14.536: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:19.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.550: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.568: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.574: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:19.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:24.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.552: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.568: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.574: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:24.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:29.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.548: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.566: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.569: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.572: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.575: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:29.591: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:34.542: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.546: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.549: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.552: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.567: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.573: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.575: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:34.588: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:39.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.548: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.567: INFO: Unable to read jessie_udp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.573: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.576: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local from pod dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf: the server could not find the requested resource (get pods dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf)
    Jan 18 22:12:39.589: INFO: Lookups using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf failed for: [wheezy_udp@dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@dns-test-service.dns-2461.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_udp@dns-test-service.dns-2461.svc.cluster.local jessie_tcp@dns-test-service.dns-2461.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2461.svc.cluster.local]

    Jan 18 22:12:44.590: INFO: DNS probes using dns-2461/dns-test-905a764f-43cc-411d-84af-574cd6b3f6cf succeeded

    STEP: deleting the pod 01/18/23 22:12:44.59
    STEP: deleting the test service 01/18/23 22:12:44.61
    STEP: deleting the test headless service 01/18/23 22:12:44.629
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:12:44.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2461" for this suite. 01/18/23 22:12:44.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:12:44.645
Jan 18 22:12:44.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename cronjob 01/18/23 22:12:44.646
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:44.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:44.66
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/18/23 22:12:44.663
STEP: Ensuring a job is scheduled 01/18/23 22:12:44.67
STEP: Ensuring exactly one is scheduled 01/18/23 22:13:00.673
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:13:00.676
STEP: Ensuring the job is replaced with a new one 01/18/23 22:13:00.678
STEP: Removing cronjob 01/18/23 22:14:00.682
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:00.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1090" for this suite. 01/18/23 22:14:00.69
------------------------------
â€¢ [SLOW TEST] [76.054 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:12:44.645
    Jan 18 22:12:44.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:12:44.646
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:44.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:44.66
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/18/23 22:12:44.663
    STEP: Ensuring a job is scheduled 01/18/23 22:12:44.67
    STEP: Ensuring exactly one is scheduled 01/18/23 22:13:00.673
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:13:00.676
    STEP: Ensuring the job is replaced with a new one 01/18/23 22:13:00.678
    STEP: Removing cronjob 01/18/23 22:14:00.682
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:00.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1090" for this suite. 01/18/23 22:14:00.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:00.7
Jan 18 22:14:00.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:14:00.701
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:00.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:00.721
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9866 01/18/23 22:14:00.724
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9866 01/18/23 22:14:00.728
Jan 18 22:14:00.739: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:14:10.743: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/18/23 22:14:10.748
STEP: updating a scale subresource 01/18/23 22:14:10.751
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:14:10.756
STEP: Patch a scale subresource 01/18/23 22:14:10.758
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:14:10.765
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:14:10.772: INFO: Deleting all statefulset in ns statefulset-9866
Jan 18 22:14:10.775: INFO: Scaling statefulset ss to 0
Jan 18 22:14:20.789: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:14:20.792: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:20.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9866" for this suite. 01/18/23 22:14:20.804
------------------------------
â€¢ [SLOW TEST] [20.112 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:00.7
    Jan 18 22:14:00.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:14:00.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:00.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:00.721
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9866 01/18/23 22:14:00.724
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9866 01/18/23 22:14:00.728
    Jan 18 22:14:00.739: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:14:10.743: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/18/23 22:14:10.748
    STEP: updating a scale subresource 01/18/23 22:14:10.751
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:14:10.756
    STEP: Patch a scale subresource 01/18/23 22:14:10.758
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:14:10.765
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:14:10.772: INFO: Deleting all statefulset in ns statefulset-9866
    Jan 18 22:14:10.775: INFO: Scaling statefulset ss to 0
    Jan 18 22:14:20.789: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:14:20.792: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:20.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9866" for this suite. 01/18/23 22:14:20.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:20.812
Jan 18 22:14:20.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:14:20.813
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:20.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:20.828
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 18 22:14:20.839: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8" in namespace "security-context-test-5926" to be "Succeeded or Failed"
Jan 18 22:14:20.842: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074854ms
Jan 18 22:14:22.845: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481218s
Jan 18 22:14:24.847: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007884183s
Jan 18 22:14:24.847: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8" satisfied condition "Succeeded or Failed"
Jan 18 22:14:24.860: INFO: Got logs for pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:24.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5926" for this suite. 01/18/23 22:14:24.879
------------------------------
â€¢ [4.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:20.812
    Jan 18 22:14:20.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:14:20.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:20.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:20.828
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 18 22:14:20.839: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8" in namespace "security-context-test-5926" to be "Succeeded or Failed"
    Jan 18 22:14:20.842: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074854ms
    Jan 18 22:14:22.845: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481218s
    Jan 18 22:14:24.847: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007884183s
    Jan 18 22:14:24.847: INFO: Pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8" satisfied condition "Succeeded or Failed"
    Jan 18 22:14:24.860: INFO: Got logs for pod "busybox-privileged-false-c2fc2088-cf64-40bf-9510-fcf365d12fd8": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:24.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5926" for this suite. 01/18/23 22:14:24.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:24.887
Jan 18 22:14:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:14:24.888
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:24.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:24.906
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 18 22:14:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:14:26.353
Jan 18 22:14:26.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 create -f -'
Jan 18 22:14:26.914: INFO: stderr: ""
Jan 18 22:14:26.914: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 22:14:26.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 delete e2e-test-crd-publish-openapi-7930-crds test-cr'
Jan 18 22:14:26.988: INFO: stderr: ""
Jan 18 22:14:26.988: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 22:14:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 apply -f -'
Jan 18 22:14:27.186: INFO: stderr: ""
Jan 18 22:14:27.186: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 22:14:27.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 delete e2e-test-crd-publish-openapi-7930-crds test-cr'
Jan 18 22:14:27.259: INFO: stderr: ""
Jan 18 22:14:27.259: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/18/23 22:14:27.259
Jan 18 22:14:27.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 explain e2e-test-crd-publish-openapi-7930-crds'
Jan 18 22:14:27.452: INFO: stderr: ""
Jan 18 22:14:27.452: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7930-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3852" for this suite. 01/18/23 22:14:28.903
------------------------------
â€¢ [4.023 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:24.887
    Jan 18 22:14:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:14:24.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:24.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:24.906
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 18 22:14:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:14:26.353
    Jan 18 22:14:26.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 create -f -'
    Jan 18 22:14:26.914: INFO: stderr: ""
    Jan 18 22:14:26.914: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 22:14:26.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 delete e2e-test-crd-publish-openapi-7930-crds test-cr'
    Jan 18 22:14:26.988: INFO: stderr: ""
    Jan 18 22:14:26.988: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 18 22:14:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 apply -f -'
    Jan 18 22:14:27.186: INFO: stderr: ""
    Jan 18 22:14:27.186: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 22:14:27.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 --namespace=crd-publish-openapi-3852 delete e2e-test-crd-publish-openapi-7930-crds test-cr'
    Jan 18 22:14:27.259: INFO: stderr: ""
    Jan 18 22:14:27.259: INFO: stdout: "e2e-test-crd-publish-openapi-7930-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/18/23 22:14:27.259
    Jan 18 22:14:27.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3852 explain e2e-test-crd-publish-openapi-7930-crds'
    Jan 18 22:14:27.452: INFO: stderr: ""
    Jan 18 22:14:27.452: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7930-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3852" for this suite. 01/18/23 22:14:28.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:28.911
Jan 18 22:14:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:14:28.912
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:28.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:28.926
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-2ee143ef-dee0-4a81-aa8d-ddc91e05198d 01/18/23 22:14:28.929
STEP: Creating a pod to test consume configMaps 01/18/23 22:14:28.935
Jan 18 22:14:28.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d" in namespace "projected-2009" to be "Succeeded or Failed"
Jan 18 22:14:28.945: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.33659ms
Jan 18 22:14:30.949: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005735973s
Jan 18 22:14:32.949: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006426202s
STEP: Saw pod success 01/18/23 22:14:32.949
Jan 18 22:14:32.950: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d" satisfied condition "Succeeded or Failed"
Jan 18 22:14:32.953: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:14:32.959
Jan 18 22:14:32.968: INFO: Waiting for pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d to disappear
Jan 18 22:14:32.970: INFO: Pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2009" for this suite. 01/18/23 22:14:32.974
------------------------------
â€¢ [4.069 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:28.911
    Jan 18 22:14:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:14:28.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:28.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:28.926
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-2ee143ef-dee0-4a81-aa8d-ddc91e05198d 01/18/23 22:14:28.929
    STEP: Creating a pod to test consume configMaps 01/18/23 22:14:28.935
    Jan 18 22:14:28.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d" in namespace "projected-2009" to be "Succeeded or Failed"
    Jan 18 22:14:28.945: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.33659ms
    Jan 18 22:14:30.949: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005735973s
    Jan 18 22:14:32.949: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006426202s
    STEP: Saw pod success 01/18/23 22:14:32.949
    Jan 18 22:14:32.950: INFO: Pod "pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d" satisfied condition "Succeeded or Failed"
    Jan 18 22:14:32.953: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:14:32.959
    Jan 18 22:14:32.968: INFO: Waiting for pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d to disappear
    Jan 18 22:14:32.970: INFO: Pod pod-projected-configmaps-46a8418e-defc-4e90-9155-996bd45bdc0d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2009" for this suite. 01/18/23 22:14:32.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:32.981
Jan 18 22:14:32.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:14:32.983
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:32.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:32.999
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/18/23 22:14:33.002
STEP: Creating a ResourceQuota 01/18/23 22:14:38.005
STEP: Ensuring resource quota status is calculated 01/18/23 22:14:38.01
STEP: Creating a ReplicaSet 01/18/23 22:14:40.014
STEP: Ensuring resource quota status captures replicaset creation 01/18/23 22:14:40.027
STEP: Deleting a ReplicaSet 01/18/23 22:14:42.031
STEP: Ensuring resource quota status released usage 01/18/23 22:14:42.036
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:44.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9787" for this suite. 01/18/23 22:14:44.044
------------------------------
â€¢ [SLOW TEST] [11.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:32.981
    Jan 18 22:14:32.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:14:32.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:32.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:32.999
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/18/23 22:14:33.002
    STEP: Creating a ResourceQuota 01/18/23 22:14:38.005
    STEP: Ensuring resource quota status is calculated 01/18/23 22:14:38.01
    STEP: Creating a ReplicaSet 01/18/23 22:14:40.014
    STEP: Ensuring resource quota status captures replicaset creation 01/18/23 22:14:40.027
    STEP: Deleting a ReplicaSet 01/18/23 22:14:42.031
    STEP: Ensuring resource quota status released usage 01/18/23 22:14:42.036
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:44.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9787" for this suite. 01/18/23 22:14:44.044
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:44.049
Jan 18 22:14:44.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 22:14:44.05
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:44.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:44.065
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 18 22:14:44.076: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:14:49.080: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:14:49.08
STEP: Scaling up "test-rs" replicaset  01/18/23 22:14:49.08
Jan 18 22:14:49.088: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/18/23 22:14:49.088
W0118 22:14:49.100682      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 22:14:49.102: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:14:49.114: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:14:49.128: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:14:49.133: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:14:50.678: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 22:14:50.749: INFO: observed Replicaset test-rs in namespace replicaset-7147 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7147" for this suite. 01/18/23 22:14:50.752
------------------------------
â€¢ [SLOW TEST] [6.708 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:44.049
    Jan 18 22:14:44.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:14:44.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:44.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:44.065
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 18 22:14:44.076: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 22:14:49.080: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:14:49.08
    STEP: Scaling up "test-rs" replicaset  01/18/23 22:14:49.08
    Jan 18 22:14:49.088: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/18/23 22:14:49.088
    W0118 22:14:49.100682      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 22:14:49.102: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:14:49.114: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:14:49.128: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:14:49.133: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:14:50.678: INFO: observed ReplicaSet test-rs in namespace replicaset-7147 with ReadyReplicas 2, AvailableReplicas 2
    Jan 18 22:14:50.749: INFO: observed Replicaset test-rs in namespace replicaset-7147 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:50.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7147" for this suite. 01/18/23 22:14:50.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:50.758
Jan 18 22:14:50.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:14:50.759
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:50.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:50.773
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 22:14:50.776: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:14:50.782: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:14:50.784: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
Jan 18 22:14:50.789: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:14:50.789: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:14:50.789: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container etcd ready: true, restart count 0
Jan 18 22:14:50.789: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 22:14:50.789: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 22:14:50.789: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:14:50.789: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 22:14:50.789: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container weave ready: true, restart count 1
Jan 18 22:14:50.789: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 22:14:50.789: INFO: test-rs-45wmt from replicaset-7147 started at 2023-01-18 22:14:49 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container httpd ready: true, restart count 0
Jan 18 22:14:50.789: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:14:50.789: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:14:50.789: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
Jan 18 22:14:50.794: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:14:50.794: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container weave ready: true, restart count 0
Jan 18 22:14:50.794: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 22:14:50.794: INFO: test-rs-n9xrw from replicaset-7147 started at 2023-01-18 22:14:49 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container httpd ready: true, restart count 0
Jan 18 22:14:50.794: INFO: 	Container test-rs ready: true, restart count 0
Jan 18 22:14:50.794: INFO: test-rs-v9v54 from replicaset-7147 started at 2023-01-18 22:14:44 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container httpd ready: true, restart count 0
Jan 18 22:14:50.794: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:14:50.794: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:14:50.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:14:50.794: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:14:50.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:14:50.794: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node cncf-conformance-1-26-1 01/18/23 22:14:50.811
STEP: verifying the node has the label node cncf-conformance-1-26-2 01/18/23 22:14:50.823
Jan 18 22:14:50.831: INFO: Pod coredns-787d4945fb-4gpmq requesting resource cpu=100m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod coredns-787d4945fb-4xgtd requesting resource cpu=100m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod etcd-cncf-conformance-1-26-1 requesting resource cpu=100m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod kube-apiserver-cncf-conformance-1-26-1 requesting resource cpu=250m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod kube-controller-manager-cncf-conformance-1-26-1 requesting resource cpu=200m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod kube-proxy-79fqc requesting resource cpu=0m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod kube-proxy-qt8bw requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod kube-scheduler-cncf-conformance-1-26-1 requesting resource cpu=100m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod weave-net-c2v49 requesting resource cpu=100m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod weave-net-wzqsl requesting resource cpu=100m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod test-rs-45wmt requesting resource cpu=0m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.831: INFO: Pod test-rs-n9xrw requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod test-rs-v9v54 requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod sonobuoy-e2e-job-5d5752962aaf4a7e requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq requesting resource cpu=0m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.831: INFO: Pod sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b requesting resource cpu=0m on Node cncf-conformance-1-26-1
STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 22:14:50.831
Jan 18 22:14:50.831: INFO: Creating a pod which consumes cpu=4935m on Node cncf-conformance-1-26-1
Jan 18 22:14:50.841: INFO: Creating a pod which consumes cpu=5530m on Node cncf-conformance-1-26-2
Jan 18 22:14:50.850: INFO: Waiting up to 5m0s for pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45" in namespace "sched-pred-8909" to be "running"
Jan 18 22:14:50.855: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.889946ms
Jan 18 22:14:52.859: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45": Phase="Running", Reason="", readiness=true. Elapsed: 2.009367241s
Jan 18 22:14:52.859: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45" satisfied condition "running"
Jan 18 22:14:52.859: INFO: Waiting up to 5m0s for pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0" in namespace "sched-pred-8909" to be "running"
Jan 18 22:14:52.863: INFO: Pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0": Phase="Running", Reason="", readiness=true. Elapsed: 3.068457ms
Jan 18 22:14:52.863: INFO: Pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 22:14:52.863
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f91465c6a1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8909/filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45 to cncf-conformance-1-26-1] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f935a2ef22], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f936b9dfae], Reason = [Created], Message = [Created container filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f93ca247a8], Reason = [Started], Message = [Started container filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f91507babc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8909/filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0 to cncf-conformance-1-26-2] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f93551b73a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f936faa6e1], Reason = [Created], Message = [Created container filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f93e2225d5], Reason = [Started], Message = [Started container filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0] 01/18/23 22:14:52.866
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b86f98d4d6b9a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/18/23 22:14:52.877
STEP: removing the label node off the node cncf-conformance-1-26-1 01/18/23 22:14:53.878
STEP: verifying the node doesn't have the label node 01/18/23 22:14:53.89
STEP: removing the label node off the node cncf-conformance-1-26-2 01/18/23 22:14:53.893
STEP: verifying the node doesn't have the label node 01/18/23 22:14:53.903
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8909" for this suite. 01/18/23 22:14:53.909
------------------------------
â€¢ [3.156 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:50.758
    Jan 18 22:14:50.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:14:50.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:50.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:50.773
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 22:14:50.776: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:14:50.782: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:14:50.784: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
    Jan 18 22:14:50.789: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container etcd ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container weave ready: true, restart count 1
    Jan 18 22:14:50.789: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: test-rs-45wmt from replicaset-7147 started at 2023-01-18 22:14:49 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container httpd ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:14:50.789: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
    Jan 18 22:14:50.794: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container weave ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: test-rs-n9xrw from replicaset-7147 started at 2023-01-18 22:14:49 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container httpd ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: 	Container test-rs ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: test-rs-v9v54 from replicaset-7147 started at 2023-01-18 22:14:44 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container httpd ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 22:14:50.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:14:50.794: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node cncf-conformance-1-26-1 01/18/23 22:14:50.811
    STEP: verifying the node has the label node cncf-conformance-1-26-2 01/18/23 22:14:50.823
    Jan 18 22:14:50.831: INFO: Pod coredns-787d4945fb-4gpmq requesting resource cpu=100m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod coredns-787d4945fb-4xgtd requesting resource cpu=100m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod etcd-cncf-conformance-1-26-1 requesting resource cpu=100m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod kube-apiserver-cncf-conformance-1-26-1 requesting resource cpu=250m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod kube-controller-manager-cncf-conformance-1-26-1 requesting resource cpu=200m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod kube-proxy-79fqc requesting resource cpu=0m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod kube-proxy-qt8bw requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod kube-scheduler-cncf-conformance-1-26-1 requesting resource cpu=100m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod weave-net-c2v49 requesting resource cpu=100m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod weave-net-wzqsl requesting resource cpu=100m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod test-rs-45wmt requesting resource cpu=0m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.831: INFO: Pod test-rs-n9xrw requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod test-rs-v9v54 requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod sonobuoy-e2e-job-5d5752962aaf4a7e requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq requesting resource cpu=0m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.831: INFO: Pod sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b requesting resource cpu=0m on Node cncf-conformance-1-26-1
    STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 22:14:50.831
    Jan 18 22:14:50.831: INFO: Creating a pod which consumes cpu=4935m on Node cncf-conformance-1-26-1
    Jan 18 22:14:50.841: INFO: Creating a pod which consumes cpu=5530m on Node cncf-conformance-1-26-2
    Jan 18 22:14:50.850: INFO: Waiting up to 5m0s for pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45" in namespace "sched-pred-8909" to be "running"
    Jan 18 22:14:50.855: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.889946ms
    Jan 18 22:14:52.859: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45": Phase="Running", Reason="", readiness=true. Elapsed: 2.009367241s
    Jan 18 22:14:52.859: INFO: Pod "filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45" satisfied condition "running"
    Jan 18 22:14:52.859: INFO: Waiting up to 5m0s for pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0" in namespace "sched-pred-8909" to be "running"
    Jan 18 22:14:52.863: INFO: Pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0": Phase="Running", Reason="", readiness=true. Elapsed: 3.068457ms
    Jan 18 22:14:52.863: INFO: Pod "filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 22:14:52.863
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f91465c6a1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8909/filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45 to cncf-conformance-1-26-1] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f935a2ef22], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f936b9dfae], Reason = [Created], Message = [Created container filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45.173b86f93ca247a8], Reason = [Started], Message = [Started container filler-pod-801f4f21-30d0-482f-be7d-bfd335a0ec45] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f91507babc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8909/filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0 to cncf-conformance-1-26-2] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f93551b73a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f936faa6e1], Reason = [Created], Message = [Created container filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0.173b86f93e2225d5], Reason = [Started], Message = [Started container filler-pod-a1441b57-5e9c-4719-ada1-14525b5cfaf0] 01/18/23 22:14:52.866
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b86f98d4d6b9a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/18/23 22:14:52.877
    STEP: removing the label node off the node cncf-conformance-1-26-1 01/18/23 22:14:53.878
    STEP: verifying the node doesn't have the label node 01/18/23 22:14:53.89
    STEP: removing the label node off the node cncf-conformance-1-26-2 01/18/23 22:14:53.893
    STEP: verifying the node doesn't have the label node 01/18/23 22:14:53.903
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8909" for this suite. 01/18/23 22:14:53.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:53.915
Jan 18 22:14:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:14:53.916
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:53.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:53.93
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:53.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-705" for this suite. 01/18/23 22:14:53.943
------------------------------
â€¢ [0.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:53.915
    Jan 18 22:14:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:14:53.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:53.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:53.93
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:53.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-705" for this suite. 01/18/23 22:14:53.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:53.948
Jan 18 22:14:53.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:14:53.949
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:53.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:53.961
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/18/23 22:14:53.964
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/18/23 22:14:53.968
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/18/23 22:14:53.968
STEP: creating a pod to probe DNS 01/18/23 22:14:53.968
STEP: submitting the pod to kubernetes 01/18/23 22:14:53.968
Jan 18 22:14:53.978: INFO: Waiting up to 15m0s for pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333" in namespace "dns-2890" to be "running"
Jan 18 22:14:53.980: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255091ms
Jan 18 22:14:55.983: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333": Phase="Running", Reason="", readiness=true. Elapsed: 2.005478442s
Jan 18 22:14:55.983: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:14:55.983
STEP: looking for the results for each expected name from probers 01/18/23 22:14:55.986
Jan 18 22:14:56.012: INFO: DNS probes using dns-2890/dns-test-a0a29ea0-a444-473c-9c38-654e29598333 succeeded

STEP: deleting the pod 01/18/23 22:14:56.012
STEP: deleting the test headless service 01/18/23 22:14:56.023
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:56.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2890" for this suite. 01/18/23 22:14:56.042
------------------------------
â€¢ [2.099 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:53.948
    Jan 18 22:14:53.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:14:53.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:53.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:53.961
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/18/23 22:14:53.964
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/18/23 22:14:53.968
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2890.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/18/23 22:14:53.968
    STEP: creating a pod to probe DNS 01/18/23 22:14:53.968
    STEP: submitting the pod to kubernetes 01/18/23 22:14:53.968
    Jan 18 22:14:53.978: INFO: Waiting up to 15m0s for pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333" in namespace "dns-2890" to be "running"
    Jan 18 22:14:53.980: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255091ms
    Jan 18 22:14:55.983: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333": Phase="Running", Reason="", readiness=true. Elapsed: 2.005478442s
    Jan 18 22:14:55.983: INFO: Pod "dns-test-a0a29ea0-a444-473c-9c38-654e29598333" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:14:55.983
    STEP: looking for the results for each expected name from probers 01/18/23 22:14:55.986
    Jan 18 22:14:56.012: INFO: DNS probes using dns-2890/dns-test-a0a29ea0-a444-473c-9c38-654e29598333 succeeded

    STEP: deleting the pod 01/18/23 22:14:56.012
    STEP: deleting the test headless service 01/18/23 22:14:56.023
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:56.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2890" for this suite. 01/18/23 22:14:56.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:56.048
Jan 18 22:14:56.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename podtemplate 01/18/23 22:14:56.049
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:56.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:56.066
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/18/23 22:14:56.069
Jan 18 22:14:56.074: INFO: created test-podtemplate-1
Jan 18 22:14:56.078: INFO: created test-podtemplate-2
Jan 18 22:14:56.083: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/18/23 22:14:56.083
STEP: delete collection of pod templates 01/18/23 22:14:56.085
Jan 18 22:14:56.085: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/18/23 22:14:56.099
Jan 18 22:14:56.099: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 22:14:56.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5445" for this suite. 01/18/23 22:14:56.106
------------------------------
â€¢ [0.062 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:56.048
    Jan 18 22:14:56.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 22:14:56.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:56.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:56.066
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/18/23 22:14:56.069
    Jan 18 22:14:56.074: INFO: created test-podtemplate-1
    Jan 18 22:14:56.078: INFO: created test-podtemplate-2
    Jan 18 22:14:56.083: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/18/23 22:14:56.083
    STEP: delete collection of pod templates 01/18/23 22:14:56.085
    Jan 18 22:14:56.085: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/18/23 22:14:56.099
    Jan 18 22:14:56.099: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:14:56.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5445" for this suite. 01/18/23 22:14:56.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:14:56.111
Jan 18 22:14:56.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:14:56.112
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:56.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:56.126
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 18 22:14:56.136: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6" in namespace "security-context-test-7066" to be "Succeeded or Failed"
Jan 18 22:14:56.138: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243123ms
Jan 18 22:14:58.141: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005684724s
Jan 18 22:15:00.143: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007204921s
Jan 18 22:15:00.143: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7066" for this suite. 01/18/23 22:15:00.151
------------------------------
â€¢ [4.045 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:14:56.111
    Jan 18 22:14:56.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:14:56.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:56.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:56.126
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 18 22:14:56.136: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6" in namespace "security-context-test-7066" to be "Succeeded or Failed"
    Jan 18 22:14:56.138: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243123ms
    Jan 18 22:14:58.141: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005684724s
    Jan 18 22:15:00.143: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007204921s
    Jan 18 22:15:00.143: INFO: Pod "alpine-nnp-false-8662eff5-f4bf-4266-b8eb-13e3687f86d6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7066" for this suite. 01/18/23 22:15:00.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:00.161
Jan 18 22:15:00.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:15:00.162
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:00.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:00.18
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 18 22:15:00.192: INFO: Waiting up to 5m0s for pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1" in namespace "pods-2915" to be "running and ready"
Jan 18 22:15:00.195: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.826281ms
Jan 18 22:15:00.195: INFO: The phase of Pod server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:15:02.199: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0066961s
Jan 18 22:15:02.199: INFO: The phase of Pod server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1 is Running (Ready = true)
Jan 18 22:15:02.199: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1" satisfied condition "running and ready"
Jan 18 22:15:02.217: INFO: Waiting up to 5m0s for pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d" in namespace "pods-2915" to be "Succeeded or Failed"
Jan 18 22:15:02.220: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572873ms
Jan 18 22:15:04.225: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007191421s
Jan 18 22:15:06.223: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005875994s
STEP: Saw pod success 01/18/23 22:15:06.223
Jan 18 22:15:06.224: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d" satisfied condition "Succeeded or Failed"
Jan 18 22:15:06.226: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d container env3cont: <nil>
STEP: delete the pod 01/18/23 22:15:06.233
Jan 18 22:15:06.243: INFO: Waiting for pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d to disappear
Jan 18 22:15:06.245: INFO: Pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:06.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2915" for this suite. 01/18/23 22:15:06.248
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:00.161
    Jan 18 22:15:00.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:15:00.162
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:00.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:00.18
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 18 22:15:00.192: INFO: Waiting up to 5m0s for pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1" in namespace "pods-2915" to be "running and ready"
    Jan 18 22:15:00.195: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.826281ms
    Jan 18 22:15:00.195: INFO: The phase of Pod server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:15:02.199: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0066961s
    Jan 18 22:15:02.199: INFO: The phase of Pod server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1 is Running (Ready = true)
    Jan 18 22:15:02.199: INFO: Pod "server-envvars-d94cbc80-4d95-44b8-b431-e73660d246f1" satisfied condition "running and ready"
    Jan 18 22:15:02.217: INFO: Waiting up to 5m0s for pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d" in namespace "pods-2915" to be "Succeeded or Failed"
    Jan 18 22:15:02.220: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572873ms
    Jan 18 22:15:04.225: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007191421s
    Jan 18 22:15:06.223: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005875994s
    STEP: Saw pod success 01/18/23 22:15:06.223
    Jan 18 22:15:06.224: INFO: Pod "client-envvars-011597de-6982-4afa-920a-ed97b0cea35d" satisfied condition "Succeeded or Failed"
    Jan 18 22:15:06.226: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d container env3cont: <nil>
    STEP: delete the pod 01/18/23 22:15:06.233
    Jan 18 22:15:06.243: INFO: Waiting for pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d to disappear
    Jan 18 22:15:06.245: INFO: Pod client-envvars-011597de-6982-4afa-920a-ed97b0cea35d no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:06.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2915" for this suite. 01/18/23 22:15:06.248
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:06.254
Jan 18 22:15:06.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 22:15:06.255
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:06.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:06.27
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 18 22:15:06.273: INFO: Creating simple deployment test-new-deployment
Jan 18 22:15:06.285: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/18/23 22:15:08.295
STEP: updating a scale subresource 01/18/23 22:15:08.298
STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:15:08.305
STEP: Patch a scale subresource 01/18/23 22:15:08.307
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:15:08.323: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9677  ca5b8d0a-1eee-4998-a190-3cbeec61c864 2932 3 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:15:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:15:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004807938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:15:07 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-18 22:15:07 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 22:15:08.328: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9677  a43e34c2-29f9-4eb2-96f5-fbad46f688c3 2937 2 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ca5b8d0a-1eee-4998-a190-3cbeec61c864 0xc004807d70 0xc004807d71}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5b8d0a-1eee-4998-a190-3cbeec61c864\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004807df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:15:08.332: INFO: Pod "test-new-deployment-7f5969cbc7-8x8l9" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8x8l9 test-new-deployment-7f5969cbc7- deployment-9677  8695971a-9aa6-47f1-9aea-92a4954d0b76 2935 0 2023-01-18 22:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 a43e34c2-29f9-4eb2-96f5-fbad46f688c3 0xc00487c1e0 0xc00487c1e1}] [] [{kube-controller-manager Update v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43e34c2-29f9-4eb2-96f5-fbad46f688c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbtv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbtv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:15:08.332: INFO: Pod "test-new-deployment-7f5969cbc7-nz85b" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nz85b test-new-deployment-7f5969cbc7- deployment-9677  f6330746-ac26-45fc-bfae-6620e0e40e11 2925 0 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 a43e34c2-29f9-4eb2-96f5-fbad46f688c3 0xc00487c330 0xc00487c331}] [] [{kube-controller-manager Update v1 2023-01-18 22:15:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43e34c2-29f9-4eb2-96f5-fbad46f688c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:15:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w28vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w28vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.4,StartTime:2023-01-18 22:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:15:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a72455f84818d6162aa9cdcabfa5f8af68d026a4c6c9237948212172f61fda7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:08.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9677" for this suite. 01/18/23 22:15:08.335
------------------------------
â€¢ [2.088 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:06.254
    Jan 18 22:15:06.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 22:15:06.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:06.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:06.27
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 18 22:15:06.273: INFO: Creating simple deployment test-new-deployment
    Jan 18 22:15:06.285: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/18/23 22:15:08.295
    STEP: updating a scale subresource 01/18/23 22:15:08.298
    STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:15:08.305
    STEP: Patch a scale subresource 01/18/23 22:15:08.307
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:15:08.323: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9677  ca5b8d0a-1eee-4998-a190-3cbeec61c864 2932 3 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:15:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:15:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004807938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:15:07 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-18 22:15:07 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 22:15:08.328: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9677  a43e34c2-29f9-4eb2-96f5-fbad46f688c3 2937 2 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ca5b8d0a-1eee-4998-a190-3cbeec61c864 0xc004807d70 0xc004807d71}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5b8d0a-1eee-4998-a190-3cbeec61c864\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004807df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:15:08.332: INFO: Pod "test-new-deployment-7f5969cbc7-8x8l9" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8x8l9 test-new-deployment-7f5969cbc7- deployment-9677  8695971a-9aa6-47f1-9aea-92a4954d0b76 2935 0 2023-01-18 22:15:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 a43e34c2-29f9-4eb2-96f5-fbad46f688c3 0xc00487c1e0 0xc00487c1e1}] [] [{kube-controller-manager Update v1 2023-01-18 22:15:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43e34c2-29f9-4eb2-96f5-fbad46f688c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbtv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbtv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:15:08.332: INFO: Pod "test-new-deployment-7f5969cbc7-nz85b" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nz85b test-new-deployment-7f5969cbc7- deployment-9677  f6330746-ac26-45fc-bfae-6620e0e40e11 2925 0 2023-01-18 22:15:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 a43e34c2-29f9-4eb2-96f5-fbad46f688c3 0xc00487c330 0xc00487c331}] [] [{kube-controller-manager Update v1 2023-01-18 22:15:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43e34c2-29f9-4eb2-96f5-fbad46f688c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:15:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w28vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w28vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:15:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.4,StartTime:2023-01-18 22:15:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:15:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a72455f84818d6162aa9cdcabfa5f8af68d026a4c6c9237948212172f61fda7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:08.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9677" for this suite. 01/18/23 22:15:08.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:08.343
Jan 18 22:15:08.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:15:08.344
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:08.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:08.358
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7238 01/18/23 22:15:08.36
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7238 01/18/23 22:15:08.365
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7238 01/18/23 22:15:08.374
Jan 18 22:15:08.376: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:15:18.380: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:15:18.38
Jan 18 22:15:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:18.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:18.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:18.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:18.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 22:15:28.524: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:28.524: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:15:28.538: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Jan 18 22:15:28.538: INFO: ss-0  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  }]
Jan 18 22:15:28.538: INFO: 
Jan 18 22:15:28.538: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 18 22:15:29.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997159634s
Jan 18 22:15:30.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993712085s
Jan 18 22:15:31.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990053902s
Jan 18 22:15:32.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986502721s
Jan 18 22:15:33.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982095467s
Jan 18 22:15:34.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978552544s
Jan 18 22:15:35.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974407894s
Jan 18 22:15:36.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97058123s
Jan 18 22:15:37.572: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.853941ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7238 01/18/23 22:15:38.572
Jan 18 22:15:38.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:38.715: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:15:38.715: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:38.715: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:38.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:15:38.858: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:38.858: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:38.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:38.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:15:38.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:38.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:39.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:39.001: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:39.001: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:15:39.001
Jan 18 22:15:39.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.408: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:15:39.410: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 22:15:49.419: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.419: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.419: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.432: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Jan 18 22:15:49.432: INFO: ss-0  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  }]
Jan 18 22:15:49.432: INFO: ss-1  cncf-conformance-1-26-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  }]
Jan 18 22:15:49.432: INFO: ss-2  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  }]
Jan 18 22:15:49.432: INFO: 
Jan 18 22:15:49.432: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:15:50.436: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.99586705s
Jan 18 22:15:51.438: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992722474s
Jan 18 22:15:52.442: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989864485s
Jan 18 22:15:53.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986655919s
Jan 18 22:15:54.448: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983195038s
Jan 18 22:15:55.451: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980011194s
Jan 18 22:15:56.455: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.976893717s
Jan 18 22:15:57.458: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973570316s
Jan 18 22:15:58.461: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.070077ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7238 01/18/23 22:15:59.462
Jan 18 22:15:59.465: INFO: Scaling statefulset ss to 0
Jan 18 22:15:59.473: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:15:59.476: INFO: Deleting all statefulset in ns statefulset-7238
Jan 18 22:15:59.478: INFO: Scaling statefulset ss to 0
Jan 18 22:15:59.486: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:15:59.488: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:59.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7238" for this suite. 01/18/23 22:15:59.501
------------------------------
â€¢ [SLOW TEST] [51.163 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:08.343
    Jan 18 22:15:08.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:15:08.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:08.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:08.358
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7238 01/18/23 22:15:08.36
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7238 01/18/23 22:15:08.365
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7238 01/18/23 22:15:08.374
    Jan 18 22:15:08.376: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:15:18.380: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:15:18.38
    Jan 18 22:15:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:15:18.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:15:18.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:15:18.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:15:18.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 22:15:28.524: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:15:28.524: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:15:28.538: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
    Jan 18 22:15:28.538: INFO: ss-0  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  }]
    Jan 18 22:15:28.538: INFO: 
    Jan 18 22:15:28.538: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 18 22:15:29.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997159634s
    Jan 18 22:15:30.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993712085s
    Jan 18 22:15:31.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990053902s
    Jan 18 22:15:32.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986502721s
    Jan 18 22:15:33.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982095467s
    Jan 18 22:15:34.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978552544s
    Jan 18 22:15:35.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974407894s
    Jan 18 22:15:36.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97058123s
    Jan 18 22:15:37.572: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.853941ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7238 01/18/23 22:15:38.572
    Jan 18 22:15:38.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:15:38.715: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:15:38.715: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:15:38.715: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:15:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:15:38.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:15:38.858: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:15:38.858: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:15:38.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:15:38.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:15:38.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:15:38.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:15:39.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:15:39.001: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:15:39.001: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:15:39.001
    Jan 18 22:15:39.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:15:39.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:15:39.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:15:39.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:15:39.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:15:39.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:15:39.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:15:39.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:15:39.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-7238 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:15:39.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:15:39.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:15:39.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:15:39.408: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:15:39.410: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 22:15:49.419: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:15:49.419: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:15:49.419: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:15:49.432: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
    Jan 18 22:15:49.432: INFO: ss-0  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:08 +0000 UTC  }]
    Jan 18 22:15:49.432: INFO: ss-1  cncf-conformance-1-26-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  }]
    Jan 18 22:15:49.432: INFO: ss-2  cncf-conformance-1-26-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:15:28 +0000 UTC  }]
    Jan 18 22:15:49.432: INFO: 
    Jan 18 22:15:49.432: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 22:15:50.436: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.99586705s
    Jan 18 22:15:51.438: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992722474s
    Jan 18 22:15:52.442: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989864485s
    Jan 18 22:15:53.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986655919s
    Jan 18 22:15:54.448: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983195038s
    Jan 18 22:15:55.451: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980011194s
    Jan 18 22:15:56.455: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.976893717s
    Jan 18 22:15:57.458: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973570316s
    Jan 18 22:15:58.461: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.070077ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7238 01/18/23 22:15:59.462
    Jan 18 22:15:59.465: INFO: Scaling statefulset ss to 0
    Jan 18 22:15:59.473: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:15:59.476: INFO: Deleting all statefulset in ns statefulset-7238
    Jan 18 22:15:59.478: INFO: Scaling statefulset ss to 0
    Jan 18 22:15:59.486: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:15:59.488: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:59.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7238" for this suite. 01/18/23 22:15:59.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:59.506
Jan 18 22:15:59.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:15:59.507
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:59.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:59.519
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:15:59.534
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:16:00.115
STEP: Deploying the webhook pod 01/18/23 22:16:00.122
STEP: Wait for the deployment to be ready 01/18/23 22:16:00.132
Jan 18 22:16:00.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:16:02.149
STEP: Verifying the service has paired with the endpoint 01/18/23 22:16:02.157
Jan 18 22:16:03.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 22:16:03.161
STEP: create a configmap that should be updated by the webhook 01/18/23 22:16:03.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-274" for this suite. 01/18/23 22:16:03.236
STEP: Destroying namespace "webhook-274-markers" for this suite. 01/18/23 22:16:03.24
------------------------------
â€¢ [3.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:59.506
    Jan 18 22:15:59.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:15:59.507
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:59.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:59.519
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:15:59.534
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:16:00.115
    STEP: Deploying the webhook pod 01/18/23 22:16:00.122
    STEP: Wait for the deployment to be ready 01/18/23 22:16:00.132
    Jan 18 22:16:00.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:16:02.149
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:16:02.157
    Jan 18 22:16:03.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 22:16:03.161
    STEP: create a configmap that should be updated by the webhook 01/18/23 22:16:03.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-274" for this suite. 01/18/23 22:16:03.236
    STEP: Destroying namespace "webhook-274-markers" for this suite. 01/18/23 22:16:03.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:03.248
Jan 18 22:16:03.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:16:03.249
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:03.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:03.264
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/18/23 22:16:03.267
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 22:16:03.268
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:16:03.269
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 22:16:03.269
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 22:16:03.27
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 22:16:03.27
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 22:16:03.271
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:03.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4481" for this suite. 01/18/23 22:16:03.274
------------------------------
â€¢ [0.033 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:03.248
    Jan 18 22:16:03.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:16:03.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:03.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:03.264
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/18/23 22:16:03.267
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 22:16:03.268
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:16:03.269
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 22:16:03.269
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 22:16:03.27
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 22:16:03.27
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 22:16:03.271
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:03.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4481" for this suite. 01/18/23 22:16:03.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:03.283
Jan 18 22:16:03.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename cronjob 01/18/23 22:16:03.284
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:03.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:03.299
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/18/23 22:16:03.302
STEP: Ensuring no jobs are scheduled 01/18/23 22:16:03.309
STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:21:03.316
STEP: Removing cronjob 01/18/23 22:21:03.319
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:03.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6515" for this suite. 01/18/23 22:21:03.327
------------------------------
â€¢ [SLOW TEST] [300.052 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:03.283
    Jan 18 22:16:03.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:16:03.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:03.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:03.299
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/18/23 22:16:03.302
    STEP: Ensuring no jobs are scheduled 01/18/23 22:16:03.309
    STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:21:03.316
    STEP: Removing cronjob 01/18/23 22:21:03.319
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:03.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6515" for this suite. 01/18/23 22:21:03.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:03.335
Jan 18 22:21:03.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename init-container 01/18/23 22:21:03.336
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:03.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:03.35
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/18/23 22:21:03.353
Jan 18 22:21:03.353: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 22:21:48.630: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-827258b3-8a76-4ab5-9a7d-402ea45e2626", GenerateName:"", Namespace:"init-container-2255", SelfLink:"", UID:"c087ff54-3cd6-445f-a649-ff0ad469301e", ResourceVersion:"3848", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"353811010"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064c3bd8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 21, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064c3c08), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8pv9x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0017ff3a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003cfca18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-conformance-1-26-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000aea230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003cfcab0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003cfcad0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003cfcad8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003cfcadc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001567540), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.15.199", PodIP:"10.32.12.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.32.12.3"}}, StartTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aea310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aea380)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://60fdefc477336b3e774746885e26cf971182726b1e8cbfc7bea85069c8ae7b6d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017ff420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017ff400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003cfcb5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:48.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2255" for this suite. 01/18/23 22:21:48.634
------------------------------
â€¢ [SLOW TEST] [45.303 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:03.335
    Jan 18 22:21:03.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename init-container 01/18/23 22:21:03.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:03.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:03.35
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/18/23 22:21:03.353
    Jan 18 22:21:03.353: INFO: PodSpec: initContainers in spec.initContainers
    Jan 18 22:21:48.630: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-827258b3-8a76-4ab5-9a7d-402ea45e2626", GenerateName:"", Namespace:"init-container-2255", SelfLink:"", UID:"c087ff54-3cd6-445f-a649-ff0ad469301e", ResourceVersion:"3848", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"353811010"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064c3bd8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 21, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064c3c08), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8pv9x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0017ff3a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8pv9x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003cfca18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-conformance-1-26-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000aea230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003cfcab0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003cfcad0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003cfcad8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003cfcadc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001567540), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.15.199", PodIP:"10.32.12.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.32.12.3"}}, StartTime:time.Date(2023, time.January, 18, 22, 21, 3, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aea310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aea380)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://60fdefc477336b3e774746885e26cf971182726b1e8cbfc7bea85069c8ae7b6d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017ff420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017ff400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003cfcb5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:48.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2255" for this suite. 01/18/23 22:21:48.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:48.639
Jan 18 22:21:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:21:48.64
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:48.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:48.654
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 18 22:21:48.663: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33" in namespace "kubelet-test-7656" to be "running and ready"
Jan 18 22:21:48.665: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238499ms
Jan 18 22:21:48.665: INFO: The phase of Pod busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:21:50.669: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33": Phase="Running", Reason="", readiness=true. Elapsed: 2.006111143s
Jan 18 22:21:50.669: INFO: The phase of Pod busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33 is Running (Ready = true)
Jan 18 22:21:50.669: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:50.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7656" for this suite. 01/18/23 22:21:50.688
------------------------------
â€¢ [2.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:48.639
    Jan 18 22:21:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:21:48.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:48.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:48.654
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 18 22:21:48.663: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33" in namespace "kubelet-test-7656" to be "running and ready"
    Jan 18 22:21:48.665: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238499ms
    Jan 18 22:21:48.665: INFO: The phase of Pod busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:21:50.669: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33": Phase="Running", Reason="", readiness=true. Elapsed: 2.006111143s
    Jan 18 22:21:50.669: INFO: The phase of Pod busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33 is Running (Ready = true)
    Jan 18 22:21:50.669: INFO: Pod "busybox-scheduling-c37a96df-8d2a-4bc5-a010-c77ce824bd33" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:50.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7656" for this suite. 01/18/23 22:21:50.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:50.694
Jan 18 22:21:50.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:21:50.695
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:50.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:50.708
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:21:50.71
Jan 18 22:21:50.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 22:21:50.782: INFO: stderr: ""
Jan 18 22:21:50.782: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/18/23 22:21:50.782
Jan 18 22:21:50.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 18 22:21:51.342: INFO: stderr: ""
Jan 18 22:21:51.342: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:21:51.342
Jan 18 22:21:51.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 delete pods e2e-test-httpd-pod'
Jan 18 22:21:52.656: INFO: stderr: ""
Jan 18 22:21:52.656: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:52.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8766" for this suite. 01/18/23 22:21:52.659
------------------------------
â€¢ [1.969 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:50.694
    Jan 18 22:21:50.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:21:50.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:50.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:50.708
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:21:50.71
    Jan 18 22:21:50.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 22:21:50.782: INFO: stderr: ""
    Jan 18 22:21:50.782: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/18/23 22:21:50.782
    Jan 18 22:21:50.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 18 22:21:51.342: INFO: stderr: ""
    Jan 18 22:21:51.342: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:21:51.342
    Jan 18 22:21:51.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8766 delete pods e2e-test-httpd-pod'
    Jan 18 22:21:52.656: INFO: stderr: ""
    Jan 18 22:21:52.656: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:52.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8766" for this suite. 01/18/23 22:21:52.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:52.665
Jan 18 22:21:52.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:21:52.666
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:52.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:52.682
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 18 22:21:52.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: creating the pod 01/18/23 22:21:52.685
STEP: submitting the pod to kubernetes 01/18/23 22:21:52.686
Jan 18 22:21:52.694: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e" in namespace "pods-1022" to be "running and ready"
Jan 18 22:21:52.697: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.43808ms
Jan 18 22:21:52.697: INFO: The phase of Pod pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:21:54.701: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006429829s
Jan 18 22:21:54.701: INFO: The phase of Pod pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e is Running (Ready = true)
Jan 18 22:21:54.701: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:54.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1022" for this suite. 01/18/23 22:21:54.716
------------------------------
â€¢ [2.056 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:52.665
    Jan 18 22:21:52.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:21:52.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:52.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:52.682
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 18 22:21:52.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: creating the pod 01/18/23 22:21:52.685
    STEP: submitting the pod to kubernetes 01/18/23 22:21:52.686
    Jan 18 22:21:52.694: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e" in namespace "pods-1022" to be "running and ready"
    Jan 18 22:21:52.697: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.43808ms
    Jan 18 22:21:52.697: INFO: The phase of Pod pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:21:54.701: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006429829s
    Jan 18 22:21:54.701: INFO: The phase of Pod pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e is Running (Ready = true)
    Jan 18 22:21:54.701: INFO: Pod "pod-logs-websocket-ecee10e1-65b5-48af-a2ac-1a9acf92057e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:54.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1022" for this suite. 01/18/23 22:21:54.716
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:54.722
Jan 18 22:21:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:21:54.723
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.735
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/18/23 22:21:54.738
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 22:21:54.744
STEP: patching the secret 01/18/23 22:21:54.747
STEP: deleting the secret using a LabelSelector 01/18/23 22:21:54.756
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 22:21:54.761
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1805" for this suite. 01/18/23 22:21:54.767
------------------------------
â€¢ [0.050 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:54.722
    Jan 18 22:21:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:21:54.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.735
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/18/23 22:21:54.738
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 22:21:54.744
    STEP: patching the secret 01/18/23 22:21:54.747
    STEP: deleting the secret using a LabelSelector 01/18/23 22:21:54.756
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 22:21:54.761
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1805" for this suite. 01/18/23 22:21:54.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:54.773
Jan 18 22:21:54.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:21:54.773
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.786
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/18/23 22:21:54.789
STEP: fetching the ConfigMap 01/18/23 22:21:54.794
STEP: patching the ConfigMap 01/18/23 22:21:54.796
STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 22:21:54.802
STEP: deleting the ConfigMap by collection with a label selector 01/18/23 22:21:54.805
STEP: listing all ConfigMaps in test namespace 01/18/23 22:21:54.813
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9468" for this suite. 01/18/23 22:21:54.818
------------------------------
â€¢ [0.051 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:54.773
    Jan 18 22:21:54.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:21:54.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.786
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/18/23 22:21:54.789
    STEP: fetching the ConfigMap 01/18/23 22:21:54.794
    STEP: patching the ConfigMap 01/18/23 22:21:54.796
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 22:21:54.802
    STEP: deleting the ConfigMap by collection with a label selector 01/18/23 22:21:54.805
    STEP: listing all ConfigMaps in test namespace 01/18/23 22:21:54.813
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9468" for this suite. 01/18/23 22:21:54.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:54.824
Jan 18 22:21:54.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:21:54.824
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.842
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 18 22:21:54.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:21:55.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6612" for this suite. 01/18/23 22:21:55.871
------------------------------
â€¢ [1.056 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:54.824
    Jan 18 22:21:54.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:21:54.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:54.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:54.842
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 18 22:21:54.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:21:55.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6612" for this suite. 01/18/23 22:21:55.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:21:55.88
Jan 18 22:21:55.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 22:21:55.881
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:55.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:55.894
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/18/23 22:21:55.897
STEP: Ensuring active pods == parallelism 01/18/23 22:21:55.905
STEP: delete a job 01/18/23 22:21:57.909
STEP: deleting Job.batch foo in namespace job-3593, will wait for the garbage collector to delete the pods 01/18/23 22:21:57.909
Jan 18 22:21:57.968: INFO: Deleting Job.batch foo took: 5.624816ms
Jan 18 22:21:58.069: INFO: Terminating Job.batch foo pods took: 100.521824ms
STEP: Ensuring job was deleted 01/18/23 22:22:30.87
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:22:30.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3593" for this suite. 01/18/23 22:22:30.876
------------------------------
â€¢ [SLOW TEST] [35.001 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:21:55.88
    Jan 18 22:21:55.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 22:21:55.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:21:55.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:21:55.894
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/18/23 22:21:55.897
    STEP: Ensuring active pods == parallelism 01/18/23 22:21:55.905
    STEP: delete a job 01/18/23 22:21:57.909
    STEP: deleting Job.batch foo in namespace job-3593, will wait for the garbage collector to delete the pods 01/18/23 22:21:57.909
    Jan 18 22:21:57.968: INFO: Deleting Job.batch foo took: 5.624816ms
    Jan 18 22:21:58.069: INFO: Terminating Job.batch foo pods took: 100.521824ms
    STEP: Ensuring job was deleted 01/18/23 22:22:30.87
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:22:30.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3593" for this suite. 01/18/23 22:22:30.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:22:30.882
Jan 18 22:22:30.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:22:30.883
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:30.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:30.9
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 18 22:22:30.915: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 22:22:30.92
Jan 18 22:22:30.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:30.922: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 22:22:30.922
Jan 18 22:22:30.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:30.943: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:22:31.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:31.946: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:22:32.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:22:32.947: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 22:22:32.95
Jan 18 22:22:32.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:22:32.965: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 18 22:22:33.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:33.969: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 22:22:33.969
Jan 18 22:22:33.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:33.982: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:22:34.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:34.986: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:22:35.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:35.986: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:22:36.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:22:36.986: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:22:36.991
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9206, will wait for the garbage collector to delete the pods 01/18/23 22:22:36.991
Jan 18 22:22:37.049: INFO: Deleting DaemonSet.extensions daemon-set took: 5.252427ms
Jan 18 22:22:37.150: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.313958ms
Jan 18 22:22:39.853: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:22:39.853: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:22:39.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4153"},"items":null}

Jan 18 22:22:39.860: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4153"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:22:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9206" for this suite. 01/18/23 22:22:39.88
------------------------------
â€¢ [SLOW TEST] [9.003 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:22:30.882
    Jan 18 22:22:30.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:22:30.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:30.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:30.9
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 18 22:22:30.915: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 22:22:30.92
    Jan 18 22:22:30.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:30.922: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 22:22:30.922
    Jan 18 22:22:30.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:30.943: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:22:31.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:31.946: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:22:32.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:22:32.947: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 22:22:32.95
    Jan 18 22:22:32.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:22:32.965: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 18 22:22:33.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:33.969: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 22:22:33.969
    Jan 18 22:22:33.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:33.982: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:22:34.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:34.986: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:22:35.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:35.986: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:22:36.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:22:36.986: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:22:36.991
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9206, will wait for the garbage collector to delete the pods 01/18/23 22:22:36.991
    Jan 18 22:22:37.049: INFO: Deleting DaemonSet.extensions daemon-set took: 5.252427ms
    Jan 18 22:22:37.150: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.313958ms
    Jan 18 22:22:39.853: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:22:39.853: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:22:39.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4153"},"items":null}

    Jan 18 22:22:39.860: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4153"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:22:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9206" for this suite. 01/18/23 22:22:39.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:22:39.886
Jan 18 22:22:39.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 22:22:39.887
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.9
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/18/23 22:22:39.903
Jan 18 22:22:39.906: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/18/23 22:22:39.906
Jan 18 22:22:39.914: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/18/23 22:22:39.914
Jan 18 22:22:39.922: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:22:39.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1006" for this suite. 01/18/23 22:22:39.926
------------------------------
â€¢ [0.044 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:22:39.886
    Jan 18 22:22:39.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:22:39.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.9
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/18/23 22:22:39.903
    Jan 18 22:22:39.906: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/18/23 22:22:39.906
    Jan 18 22:22:39.914: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/18/23 22:22:39.914
    Jan 18 22:22:39.922: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:22:39.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1006" for this suite. 01/18/23 22:22:39.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:22:39.932
Jan 18 22:22:39.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:22:39.933
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.947
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:22:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5958" for this suite. 01/18/23 22:22:39.959
------------------------------
â€¢ [0.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:22:39.932
    Jan 18 22:22:39.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:22:39.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.947
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:22:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5958" for this suite. 01/18/23 22:22:39.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:22:39.965
Jan 18 22:22:39.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:22:39.966
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.98
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 18 22:22:39.993: INFO: Waiting up to 5m0s for pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7" in namespace "svcaccounts-22" to be "running"
Jan 18 22:22:39.996: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793333ms
Jan 18 22:22:42.001: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007460758s
Jan 18 22:22:42.001: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7" satisfied condition "running"
STEP: reading a file in the container 01/18/23 22:22:42.001
Jan 18 22:22:42.001: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/18/23 22:22:42.144
Jan 18 22:22:42.145: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/18/23 22:22:42.281
Jan 18 22:22:42.281: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 22:22:42.415: INFO: Got root ca configmap in namespace "svcaccounts-22"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:22:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-22" for this suite. 01/18/23 22:22:42.421
------------------------------
â€¢ [2.460 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:22:39.965
    Jan 18 22:22:39.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:22:39.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:39.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:39.98
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 18 22:22:39.993: INFO: Waiting up to 5m0s for pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7" in namespace "svcaccounts-22" to be "running"
    Jan 18 22:22:39.996: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793333ms
    Jan 18 22:22:42.001: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007460758s
    Jan 18 22:22:42.001: INFO: Pod "pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7" satisfied condition "running"
    STEP: reading a file in the container 01/18/23 22:22:42.001
    Jan 18 22:22:42.001: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/18/23 22:22:42.144
    Jan 18 22:22:42.145: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/18/23 22:22:42.281
    Jan 18 22:22:42.281: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-22 pod-service-account-95806e0d-d8d0-4044-beba-77e9dcf0cfa7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 18 22:22:42.415: INFO: Got root ca configmap in namespace "svcaccounts-22"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:22:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-22" for this suite. 01/18/23 22:22:42.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:22:42.426
Jan 18 22:22:42.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename cronjob 01/18/23 22:22:42.427
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:42.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:42.44
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/18/23 22:22:42.442
STEP: Ensuring more than one job is running at a time 01/18/23 22:22:42.447
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 22:24:00.451
STEP: Removing cronjob 01/18/23 22:24:00.453
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:00.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4408" for this suite. 01/18/23 22:24:00.461
------------------------------
â€¢ [SLOW TEST] [78.040 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:22:42.426
    Jan 18 22:22:42.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:22:42.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:22:42.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:22:42.44
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/18/23 22:22:42.442
    STEP: Ensuring more than one job is running at a time 01/18/23 22:22:42.447
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 22:24:00.451
    STEP: Removing cronjob 01/18/23 22:24:00.453
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:00.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4408" for this suite. 01/18/23 22:24:00.461
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:00.466
Jan 18 22:24:00.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 22:24:00.467
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:00.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:00.483
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 18 22:24:00.496: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 22:24:05.501: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:24:05.501
Jan 18 22:24:05.502: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 22:24:07.506: INFO: Creating deployment "test-rollover-deployment"
Jan 18 22:24:07.516: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 22:24:09.525: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 22:24:09.529: INFO: Ensure that both replica sets have 1 created replica
Jan 18 22:24:09.533: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 22:24:09.542: INFO: Updating deployment test-rollover-deployment
Jan 18 22:24:09.542: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 22:24:11.549: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 22:24:11.554: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 22:24:11.559: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 22:24:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:24:13.566: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 22:24:13.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:24:15.566: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 22:24:15.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:24:17.567: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 22:24:17.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:24:19.566: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 22:24:19.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:24:21.565: INFO: 
Jan 18 22:24:21.565: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:24:21.572: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8473  49748d2e-74eb-4fb7-a38d-8c50617fbc8a 4482 2 2023-01-18 22:24:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047ba798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-18 22:24:21 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 22:24:21.575: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8473  f5bc4213-75a9-4919-99f6-73b9328fb256 4472 2 2023-01-18 22:24:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bac77 0xc0047bac78}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047bad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:24:21.575: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 22:24:21.575: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8473  eaeec6dd-fb2f-446f-a345-82ce1ba2ff6c 4481 2 2023-01-18 22:24:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bab4f 0xc0047bab60}] [] [{e2e.test Update apps/v1 2023-01-18 22:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047bac18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:24:21.575: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8473  0bf6fb13-5782-4c79-bb9e-c474bbd07d84 4441 2 2023-01-18 22:24:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bad97 0xc0047bad98}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047bae68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:24:21.578: INFO: Pod "test-rollover-deployment-6c6df9974f-rqml9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rqml9 test-rollover-deployment-6c6df9974f- deployment-8473  f93cd123-f9d9-46f3-9818-9fc978175032 4455 0 2023-01-18 22:24:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f f5bc4213-75a9-4919-99f6-73b9328fb256 0xc0047bb467 0xc0047bb468}] [] [{kube-controller-manager Update v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5bc4213-75a9-4919-99f6-73b9328fb256\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:24:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r45w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r45w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.7,StartTime:2023-01-18 22:24:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:24:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://013cab1c4f44e75b105866a75e74a10877688dd4f5f2f54abc151aa22abfd3d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8473" for this suite. 01/18/23 22:24:21.581
------------------------------
â€¢ [SLOW TEST] [21.122 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:00.466
    Jan 18 22:24:00.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 22:24:00.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:00.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:00.483
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 18 22:24:00.496: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 18 22:24:05.501: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:24:05.501
    Jan 18 22:24:05.502: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 18 22:24:07.506: INFO: Creating deployment "test-rollover-deployment"
    Jan 18 22:24:07.516: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 18 22:24:09.525: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 18 22:24:09.529: INFO: Ensure that both replica sets have 1 created replica
    Jan 18 22:24:09.533: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 18 22:24:09.542: INFO: Updating deployment test-rollover-deployment
    Jan 18 22:24:09.542: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 18 22:24:11.549: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 18 22:24:11.554: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 18 22:24:11.559: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 22:24:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:24:13.566: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 22:24:13.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:24:15.566: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 22:24:15.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:24:17.567: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 22:24:17.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:24:19.566: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 22:24:19.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:24:21.565: INFO: 
    Jan 18 22:24:21.565: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:24:21.572: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8473  49748d2e-74eb-4fb7-a38d-8c50617fbc8a 4482 2 2023-01-18 22:24:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047ba798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-18 22:24:21 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 22:24:21.575: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8473  f5bc4213-75a9-4919-99f6-73b9328fb256 4472 2 2023-01-18 22:24:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bac77 0xc0047bac78}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047bad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:24:21.575: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 18 22:24:21.575: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8473  eaeec6dd-fb2f-446f-a345-82ce1ba2ff6c 4481 2 2023-01-18 22:24:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bab4f 0xc0047bab60}] [] [{e2e.test Update apps/v1 2023-01-18 22:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047bac18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:24:21.575: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8473  0bf6fb13-5782-4c79-bb9e-c474bbd07d84 4441 2 2023-01-18 22:24:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 49748d2e-74eb-4fb7-a38d-8c50617fbc8a 0xc0047bad97 0xc0047bad98}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49748d2e-74eb-4fb7-a38d-8c50617fbc8a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047bae68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:24:21.578: INFO: Pod "test-rollover-deployment-6c6df9974f-rqml9" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rqml9 test-rollover-deployment-6c6df9974f- deployment-8473  f93cd123-f9d9-46f3-9818-9fc978175032 4455 0 2023-01-18 22:24:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f f5bc4213-75a9-4919-99f6-73b9328fb256 0xc0047bb467 0xc0047bb468}] [] [{kube-controller-manager Update v1 2023-01-18 22:24:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5bc4213-75a9-4919-99f6-73b9328fb256\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:24:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r45w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r45w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.7,StartTime:2023-01-18 22:24:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:24:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://013cab1c4f44e75b105866a75e74a10877688dd4f5f2f54abc151aa22abfd3d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8473" for this suite. 01/18/23 22:24:21.581
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:21.588
Jan 18 22:24:21.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:24:21.59
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:21.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:21.604
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:24:21.607
Jan 18 22:24:21.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 22:24:21.684: INFO: stderr: ""
Jan 18 22:24:21.684: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 22:24:21.684
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:24:26.738
Jan 18 22:24:26.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 get pod e2e-test-httpd-pod -o json'
Jan 18 22:24:26.809: INFO: stderr: ""
Jan 18 22:24:26.809: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-18T22:24:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5938\",\n        \"resourceVersion\": \"4497\",\n        \"uid\": \"206815f5-b624-4b37-947b-255f08d4d4cf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f7g6j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-conformance-1-26-2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f7g6j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7c0e05e4ce461722e6370c5716f4a83e29b6e9ced4ddc6483514155bcf226cb7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T22:24:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.15.199\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.12.5\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.32.12.5\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T22:24:21Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/18/23 22:24:26.809
Jan 18 22:24:26.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 replace -f -'
Jan 18 22:24:27.519: INFO: stderr: ""
Jan 18 22:24:27.519: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/18/23 22:24:27.519
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 18 22:24:27.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 delete pods e2e-test-httpd-pod'
Jan 18 22:24:29.025: INFO: stderr: ""
Jan 18 22:24:29.025: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:29.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5938" for this suite. 01/18/23 22:24:29.028
------------------------------
â€¢ [SLOW TEST] [7.445 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:21.588
    Jan 18 22:24:21.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:24:21.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:21.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:21.604
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:24:21.607
    Jan 18 22:24:21.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 22:24:21.684: INFO: stderr: ""
    Jan 18 22:24:21.684: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 22:24:21.684
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:24:26.738
    Jan 18 22:24:26.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 get pod e2e-test-httpd-pod -o json'
    Jan 18 22:24:26.809: INFO: stderr: ""
    Jan 18 22:24:26.809: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-18T22:24:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5938\",\n        \"resourceVersion\": \"4497\",\n        \"uid\": \"206815f5-b624-4b37-947b-255f08d4d4cf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f7g6j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-conformance-1-26-2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f7g6j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:24:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7c0e05e4ce461722e6370c5716f4a83e29b6e9ced4ddc6483514155bcf226cb7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T22:24:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.15.199\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.12.5\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.32.12.5\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T22:24:21Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/18/23 22:24:26.809
    Jan 18 22:24:26.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 replace -f -'
    Jan 18 22:24:27.519: INFO: stderr: ""
    Jan 18 22:24:27.519: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/18/23 22:24:27.519
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 18 22:24:27.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-5938 delete pods e2e-test-httpd-pod'
    Jan 18 22:24:29.025: INFO: stderr: ""
    Jan 18 22:24:29.025: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:29.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5938" for this suite. 01/18/23 22:24:29.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:29.035
Jan 18 22:24:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:24:29.036
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:29.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:29.049
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:24:29.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:24:29.25
STEP: Deploying the webhook pod 01/18/23 22:24:29.257
STEP: Wait for the deployment to be ready 01/18/23 22:24:29.268
Jan 18 22:24:29.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:24:31.285
STEP: Verifying the service has paired with the endpoint 01/18/23 22:24:31.292
Jan 18 22:24:32.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/18/23 22:24:32.36
STEP: Creating a configMap that should be mutated 01/18/23 22:24:32.377
STEP: Deleting the collection of validation webhooks 01/18/23 22:24:32.408
STEP: Creating a configMap that should not be mutated 01/18/23 22:24:32.445
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:32.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6839" for this suite. 01/18/23 22:24:32.483
STEP: Destroying namespace "webhook-6839-markers" for this suite. 01/18/23 22:24:32.49
------------------------------
â€¢ [3.462 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:29.035
    Jan 18 22:24:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:24:29.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:29.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:29.049
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:24:29.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:24:29.25
    STEP: Deploying the webhook pod 01/18/23 22:24:29.257
    STEP: Wait for the deployment to be ready 01/18/23 22:24:29.268
    Jan 18 22:24:29.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:24:31.285
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:24:31.292
    Jan 18 22:24:32.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/18/23 22:24:32.36
    STEP: Creating a configMap that should be mutated 01/18/23 22:24:32.377
    STEP: Deleting the collection of validation webhooks 01/18/23 22:24:32.408
    STEP: Creating a configMap that should not be mutated 01/18/23 22:24:32.445
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:32.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6839" for this suite. 01/18/23 22:24:32.483
    STEP: Destroying namespace "webhook-6839-markers" for this suite. 01/18/23 22:24:32.49
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:32.497
Jan 18 22:24:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:24:32.498
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:32.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:32.512
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/18/23 22:24:32.515
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:24:32.521
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:24:32.521
STEP: creating a pod to probe DNS 01/18/23 22:24:32.521
STEP: submitting the pod to kubernetes 01/18/23 22:24:32.522
Jan 18 22:24:32.530: INFO: Waiting up to 15m0s for pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780" in namespace "dns-618" to be "running"
Jan 18 22:24:32.534: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838736ms
Jan 18 22:24:34.538: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780": Phase="Running", Reason="", readiness=true. Elapsed: 2.008532697s
Jan 18 22:24:34.538: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:24:34.538
STEP: looking for the results for each expected name from probers 01/18/23 22:24:34.541
Jan 18 22:24:34.549: INFO: DNS probes using dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780 succeeded

STEP: deleting the pod 01/18/23 22:24:34.549
STEP: changing the externalName to bar.example.com 01/18/23 22:24:34.56
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:24:34.568
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:24:34.568
STEP: creating a second pod to probe DNS 01/18/23 22:24:34.568
STEP: submitting the pod to kubernetes 01/18/23 22:24:34.568
Jan 18 22:24:34.575: INFO: Waiting up to 15m0s for pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f" in namespace "dns-618" to be "running"
Jan 18 22:24:34.577: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440081ms
Jan 18 22:24:36.581: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005549793s
Jan 18 22:24:36.581: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:24:36.581
STEP: looking for the results for each expected name from probers 01/18/23 22:24:36.583
Jan 18 22:24:36.588: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:36.593: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:36.593: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:24:41.599: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:41.602: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:41.602: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:24:46.600: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:46.603: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:46.603: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:24:51.601: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:51.605: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:51.605: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:24:56.600: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:24:56.603: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains '' instead of 'bar.example.com.'
Jan 18 22:24:56.603: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:25:01.601: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:25:01.605: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 22:25:01.605: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

Jan 18 22:25:06.606: INFO: DNS probes using dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f succeeded

STEP: deleting the pod 01/18/23 22:25:06.606
STEP: changing the service to type=ClusterIP 01/18/23 22:25:06.618
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:25:06.632
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
 01/18/23 22:25:06.632
STEP: creating a third pod to probe DNS 01/18/23 22:25:06.632
STEP: submitting the pod to kubernetes 01/18/23 22:25:06.635
Jan 18 22:25:06.643: INFO: Waiting up to 15m0s for pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07" in namespace "dns-618" to be "running"
Jan 18 22:25:06.645: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.532392ms
Jan 18 22:25:08.649: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07": Phase="Running", Reason="", readiness=true. Elapsed: 2.006058897s
Jan 18 22:25:08.649: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:25:08.649
STEP: looking for the results for each expected name from probers 01/18/23 22:25:08.652
Jan 18 22:25:08.661: INFO: DNS probes using dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07 succeeded

STEP: deleting the pod 01/18/23 22:25:08.661
STEP: deleting the test externalName service 01/18/23 22:25:08.673
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-618" for this suite. 01/18/23 22:25:08.687
------------------------------
â€¢ [SLOW TEST] [36.197 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:32.497
    Jan 18 22:24:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:24:32.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:32.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:32.512
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/18/23 22:24:32.515
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:24:32.521
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:24:32.521
    STEP: creating a pod to probe DNS 01/18/23 22:24:32.521
    STEP: submitting the pod to kubernetes 01/18/23 22:24:32.522
    Jan 18 22:24:32.530: INFO: Waiting up to 15m0s for pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780" in namespace "dns-618" to be "running"
    Jan 18 22:24:32.534: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838736ms
    Jan 18 22:24:34.538: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780": Phase="Running", Reason="", readiness=true. Elapsed: 2.008532697s
    Jan 18 22:24:34.538: INFO: Pod "dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:24:34.538
    STEP: looking for the results for each expected name from probers 01/18/23 22:24:34.541
    Jan 18 22:24:34.549: INFO: DNS probes using dns-test-b3fb3d19-592b-40c1-8410-6d72b05ad780 succeeded

    STEP: deleting the pod 01/18/23 22:24:34.549
    STEP: changing the externalName to bar.example.com 01/18/23 22:24:34.56
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:24:34.568
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:24:34.568
    STEP: creating a second pod to probe DNS 01/18/23 22:24:34.568
    STEP: submitting the pod to kubernetes 01/18/23 22:24:34.568
    Jan 18 22:24:34.575: INFO: Waiting up to 15m0s for pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f" in namespace "dns-618" to be "running"
    Jan 18 22:24:34.577: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440081ms
    Jan 18 22:24:36.581: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005549793s
    Jan 18 22:24:36.581: INFO: Pod "dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:24:36.581
    STEP: looking for the results for each expected name from probers 01/18/23 22:24:36.583
    Jan 18 22:24:36.588: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:36.593: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:36.593: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:24:41.599: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:41.602: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:41.602: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:24:46.600: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:46.603: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:46.603: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:24:51.601: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:51.605: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:51.605: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:24:56.600: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:24:56.603: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains '' instead of 'bar.example.com.'
    Jan 18 22:24:56.603: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:25:01.601: INFO: File wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:25:01.605: INFO: File jessie_udp@dns-test-service-3.dns-618.svc.cluster.local from pod  dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 22:25:01.605: INFO: Lookups using dns-618/dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f failed for: [wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local jessie_udp@dns-test-service-3.dns-618.svc.cluster.local]

    Jan 18 22:25:06.606: INFO: DNS probes using dns-test-cae24e46-19e2-4280-beb6-00cbdf157b0f succeeded

    STEP: deleting the pod 01/18/23 22:25:06.606
    STEP: changing the service to type=ClusterIP 01/18/23 22:25:06.618
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:25:06.632
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-618.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-618.svc.cluster.local; sleep 1; done
     01/18/23 22:25:06.632
    STEP: creating a third pod to probe DNS 01/18/23 22:25:06.632
    STEP: submitting the pod to kubernetes 01/18/23 22:25:06.635
    Jan 18 22:25:06.643: INFO: Waiting up to 15m0s for pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07" in namespace "dns-618" to be "running"
    Jan 18 22:25:06.645: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.532392ms
    Jan 18 22:25:08.649: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07": Phase="Running", Reason="", readiness=true. Elapsed: 2.006058897s
    Jan 18 22:25:08.649: INFO: Pod "dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:25:08.649
    STEP: looking for the results for each expected name from probers 01/18/23 22:25:08.652
    Jan 18 22:25:08.661: INFO: DNS probes using dns-test-23fe4e98-ae88-4ef6-9ccd-0c6a93470c07 succeeded

    STEP: deleting the pod 01/18/23 22:25:08.661
    STEP: deleting the test externalName service 01/18/23 22:25:08.673
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-618" for this suite. 01/18/23 22:25:08.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:08.694
Jan 18 22:25:08.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 22:25:08.695
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:08.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:08.708
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 18 22:25:08.711: INFO: Creating deployment "test-recreate-deployment"
Jan 18 22:25:08.718: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 22:25:08.723: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 18 22:25:10.730: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 22:25:10.732: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 22:25:10.743: INFO: Updating deployment test-recreate-deployment
Jan 18 22:25:10.743: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:25:10.824: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9626  c0e84753-71f8-41f1-89ed-b30e02431752 4829 2 2023-01-18 22:25:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006517858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:25:10 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-18 22:25:10 +0000 UTC,LastTransitionTime:2023-01-18 22:25:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 22:25:10.828: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9626  510eb852-5f99-4ad7-a065-ce4c8eea13c8 4827 1 2023-01-18 22:25:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c0e84753-71f8-41f1-89ed-b30e02431752 0xc004770cd0 0xc004770cd1}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0e84753-71f8-41f1-89ed-b30e02431752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004770d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:25:10.828: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 22:25:10.828: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9626  ee2c5a54-68b4-4132-aece-874451f42945 4817 2 2023-01-18 22:25:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c0e84753-71f8-41f1-89ed-b30e02431752 0xc004770bc7 0xc004770bc8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0e84753-71f8-41f1-89ed-b30e02431752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004770c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:25:10.832: INFO: Pod "test-recreate-deployment-cff6dc657-kckp6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kckp6 test-recreate-deployment-cff6dc657- deployment-9626  88079238-12bd-4ba0-964c-bbef1edfb711 4828 0 2023-01-18 22:25:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 510eb852-5f99-4ad7-a065-ce4c8eea13c8 0xc004771690 0xc004771691}] [] [{kube-controller-manager Update v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"510eb852-5f99-4ad7-a065-ce4c8eea13c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42wc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42wc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 22:25:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:10.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9626" for this suite. 01/18/23 22:25:10.836
------------------------------
â€¢ [2.153 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:08.694
    Jan 18 22:25:08.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 22:25:08.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:08.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:08.708
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 18 22:25:08.711: INFO: Creating deployment "test-recreate-deployment"
    Jan 18 22:25:08.718: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 18 22:25:08.723: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 18 22:25:10.730: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 18 22:25:10.732: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 18 22:25:10.743: INFO: Updating deployment test-recreate-deployment
    Jan 18 22:25:10.743: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:25:10.824: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9626  c0e84753-71f8-41f1-89ed-b30e02431752 4829 2 2023-01-18 22:25:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006517858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:25:10 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-18 22:25:10 +0000 UTC,LastTransitionTime:2023-01-18 22:25:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 22:25:10.828: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9626  510eb852-5f99-4ad7-a065-ce4c8eea13c8 4827 1 2023-01-18 22:25:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c0e84753-71f8-41f1-89ed-b30e02431752 0xc004770cd0 0xc004770cd1}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0e84753-71f8-41f1-89ed-b30e02431752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004770d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:25:10.828: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 18 22:25:10.828: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9626  ee2c5a54-68b4-4132-aece-874451f42945 4817 2 2023-01-18 22:25:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c0e84753-71f8-41f1-89ed-b30e02431752 0xc004770bc7 0xc004770bc8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0e84753-71f8-41f1-89ed-b30e02431752\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004770c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:25:10.832: INFO: Pod "test-recreate-deployment-cff6dc657-kckp6" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kckp6 test-recreate-deployment-cff6dc657- deployment-9626  88079238-12bd-4ba0-964c-bbef1edfb711 4828 0 2023-01-18 22:25:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 510eb852-5f99-4ad7-a065-ce4c8eea13c8 0xc004771690 0xc004771691}] [] [{kube-controller-manager Update v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"510eb852-5f99-4ad7-a065-ce4c8eea13c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:25:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42wc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42wc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:25:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 22:25:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:10.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9626" for this suite. 01/18/23 22:25:10.836
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:10.848
Jan 18 22:25:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:25:10.849
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:10.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:10.863
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-87a00330-a557-4b8d-948d-dcbd792c2ae4 01/18/23 22:25:10.867
STEP: Creating a pod to test consume configMaps 01/18/23 22:25:10.874
Jan 18 22:25:10.882: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412" in namespace "projected-3589" to be "Succeeded or Failed"
Jan 18 22:25:10.885: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625426ms
Jan 18 22:25:12.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006733398s
Jan 18 22:25:14.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007233703s
STEP: Saw pod success 01/18/23 22:25:14.889
Jan 18 22:25:14.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412" satisfied condition "Succeeded or Failed"
Jan 18 22:25:14.892: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:25:14.905
Jan 18 22:25:14.918: INFO: Waiting for pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 to disappear
Jan 18 22:25:14.920: INFO: Pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:14.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3589" for this suite. 01/18/23 22:25:14.924
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:10.848
    Jan 18 22:25:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:25:10.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:10.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:10.863
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-87a00330-a557-4b8d-948d-dcbd792c2ae4 01/18/23 22:25:10.867
    STEP: Creating a pod to test consume configMaps 01/18/23 22:25:10.874
    Jan 18 22:25:10.882: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412" in namespace "projected-3589" to be "Succeeded or Failed"
    Jan 18 22:25:10.885: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625426ms
    Jan 18 22:25:12.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006733398s
    Jan 18 22:25:14.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007233703s
    STEP: Saw pod success 01/18/23 22:25:14.889
    Jan 18 22:25:14.889: INFO: Pod "pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412" satisfied condition "Succeeded or Failed"
    Jan 18 22:25:14.892: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:25:14.905
    Jan 18 22:25:14.918: INFO: Waiting for pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 to disappear
    Jan 18 22:25:14.920: INFO: Pod pod-projected-configmaps-02bfd5a1-9f1e-4be6-a592-0d5e456d8412 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:14.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3589" for this suite. 01/18/23 22:25:14.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:14.93
Jan 18 22:25:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 22:25:14.931
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:14.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:14.946
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/18/23 22:25:14.949
STEP: getting /apis/storage.k8s.io 01/18/23 22:25:14.951
STEP: getting /apis/storage.k8s.io/v1 01/18/23 22:25:14.953
STEP: creating 01/18/23 22:25:14.954
STEP: watching 01/18/23 22:25:14.967
Jan 18 22:25:14.967: INFO: starting watch
STEP: getting 01/18/23 22:25:14.973
STEP: listing in namespace 01/18/23 22:25:14.975
STEP: listing across namespaces 01/18/23 22:25:14.977
STEP: patching 01/18/23 22:25:14.98
STEP: updating 01/18/23 22:25:14.984
Jan 18 22:25:14.990: INFO: waiting for watch events with expected annotations in namespace
Jan 18 22:25:14.990: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/18/23 22:25:14.99
STEP: deleting a collection 01/18/23 22:25:14.999
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:15.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-3169" for this suite. 01/18/23 22:25:15.016
------------------------------
â€¢ [0.094 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:14.93
    Jan 18 22:25:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 22:25:14.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:14.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:14.946
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/18/23 22:25:14.949
    STEP: getting /apis/storage.k8s.io 01/18/23 22:25:14.951
    STEP: getting /apis/storage.k8s.io/v1 01/18/23 22:25:14.953
    STEP: creating 01/18/23 22:25:14.954
    STEP: watching 01/18/23 22:25:14.967
    Jan 18 22:25:14.967: INFO: starting watch
    STEP: getting 01/18/23 22:25:14.973
    STEP: listing in namespace 01/18/23 22:25:14.975
    STEP: listing across namespaces 01/18/23 22:25:14.977
    STEP: patching 01/18/23 22:25:14.98
    STEP: updating 01/18/23 22:25:14.984
    Jan 18 22:25:14.990: INFO: waiting for watch events with expected annotations in namespace
    Jan 18 22:25:14.990: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/18/23 22:25:14.99
    STEP: deleting a collection 01/18/23 22:25:14.999
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:15.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-3169" for this suite. 01/18/23 22:25:15.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:15.025
Jan 18 22:25:15.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:25:15.026
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:15.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:15.043
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:25:15.045
Jan 18 22:25:15.053: INFO: Waiting up to 5m0s for pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306" in namespace "emptydir-4820" to be "Succeeded or Failed"
Jan 18 22:25:15.055: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385943ms
Jan 18 22:25:17.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006883336s
Jan 18 22:25:19.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006487973s
STEP: Saw pod success 01/18/23 22:25:19.06
Jan 18 22:25:19.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306" satisfied condition "Succeeded or Failed"
Jan 18 22:25:19.062: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 container test-container: <nil>
STEP: delete the pod 01/18/23 22:25:19.067
Jan 18 22:25:19.076: INFO: Waiting for pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 to disappear
Jan 18 22:25:19.078: INFO: Pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:19.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4820" for this suite. 01/18/23 22:25:19.081
------------------------------
â€¢ [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:15.025
    Jan 18 22:25:15.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:25:15.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:15.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:15.043
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:25:15.045
    Jan 18 22:25:15.053: INFO: Waiting up to 5m0s for pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306" in namespace "emptydir-4820" to be "Succeeded or Failed"
    Jan 18 22:25:15.055: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385943ms
    Jan 18 22:25:17.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006883336s
    Jan 18 22:25:19.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006487973s
    STEP: Saw pod success 01/18/23 22:25:19.06
    Jan 18 22:25:19.060: INFO: Pod "pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306" satisfied condition "Succeeded or Failed"
    Jan 18 22:25:19.062: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:25:19.067
    Jan 18 22:25:19.076: INFO: Waiting for pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 to disappear
    Jan 18 22:25:19.078: INFO: Pod pod-d67eaa1a-b239-4add-b0e4-651a2a6dd306 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:19.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4820" for this suite. 01/18/23 22:25:19.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:19.086
Jan 18 22:25:19.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:25:19.087
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:19.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:19.1
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-1cafba36-7c95-4ca7-ad48-0587e4cb77ff 01/18/23 22:25:19.106
STEP: Creating secret with name s-test-opt-upd-47d7affa-5127-4f7c-8ec2-655fc9fc9f1d 01/18/23 22:25:19.11
STEP: Creating the pod 01/18/23 22:25:19.114
Jan 18 22:25:19.124: INFO: Waiting up to 5m0s for pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf" in namespace "secrets-577" to be "running and ready"
Jan 18 22:25:19.127: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281128ms
Jan 18 22:25:19.127: INFO: The phase of Pod pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:25:21.130: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005537742s
Jan 18 22:25:21.130: INFO: The phase of Pod pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf is Running (Ready = true)
Jan 18 22:25:21.130: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1cafba36-7c95-4ca7-ad48-0587e4cb77ff 01/18/23 22:25:21.148
STEP: Updating secret s-test-opt-upd-47d7affa-5127-4f7c-8ec2-655fc9fc9f1d 01/18/23 22:25:21.153
STEP: Creating secret with name s-test-opt-create-09754d6b-612a-4133-b53b-dbe042499501 01/18/23 22:25:21.157
STEP: waiting to observe update in volume 01/18/23 22:25:21.161
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:23.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-577" for this suite. 01/18/23 22:25:23.185
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:19.086
    Jan 18 22:25:19.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:25:19.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:19.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:19.1
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-1cafba36-7c95-4ca7-ad48-0587e4cb77ff 01/18/23 22:25:19.106
    STEP: Creating secret with name s-test-opt-upd-47d7affa-5127-4f7c-8ec2-655fc9fc9f1d 01/18/23 22:25:19.11
    STEP: Creating the pod 01/18/23 22:25:19.114
    Jan 18 22:25:19.124: INFO: Waiting up to 5m0s for pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf" in namespace "secrets-577" to be "running and ready"
    Jan 18 22:25:19.127: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281128ms
    Jan 18 22:25:19.127: INFO: The phase of Pod pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:25:21.130: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005537742s
    Jan 18 22:25:21.130: INFO: The phase of Pod pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf is Running (Ready = true)
    Jan 18 22:25:21.130: INFO: Pod "pod-secrets-8816d4c0-1c6f-45e9-ad3c-8e7158ad6abf" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1cafba36-7c95-4ca7-ad48-0587e4cb77ff 01/18/23 22:25:21.148
    STEP: Updating secret s-test-opt-upd-47d7affa-5127-4f7c-8ec2-655fc9fc9f1d 01/18/23 22:25:21.153
    STEP: Creating secret with name s-test-opt-create-09754d6b-612a-4133-b53b-dbe042499501 01/18/23 22:25:21.157
    STEP: waiting to observe update in volume 01/18/23 22:25:21.161
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:23.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-577" for this suite. 01/18/23 22:25:23.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:23.192
Jan 18 22:25:23.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:25:23.193
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:23.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:23.208
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/18/23 22:25:23.211
Jan 18 22:25:23.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9409 create -f -'
Jan 18 22:25:23.407: INFO: stderr: ""
Jan 18 22:25:23.407: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:25:23.407
Jan 18 22:25:24.410: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:24.410: INFO: Found 1 / 1
Jan 18 22:25:24.410: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/18/23 22:25:24.41
Jan 18 22:25:24.413: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:24.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:25:24.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9409 patch pod agnhost-primary-pxthd -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 22:25:24.489: INFO: stderr: ""
Jan 18 22:25:24.489: INFO: stdout: "pod/agnhost-primary-pxthd patched\n"
STEP: checking annotations 01/18/23 22:25:24.489
Jan 18 22:25:24.492: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:24.492: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:24.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9409" for this suite. 01/18/23 22:25:24.495
------------------------------
â€¢ [1.311 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:23.192
    Jan 18 22:25:23.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:25:23.193
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:23.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:23.208
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/18/23 22:25:23.211
    Jan 18 22:25:23.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9409 create -f -'
    Jan 18 22:25:23.407: INFO: stderr: ""
    Jan 18 22:25:23.407: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:25:23.407
    Jan 18 22:25:24.410: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:24.410: INFO: Found 1 / 1
    Jan 18 22:25:24.410: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/18/23 22:25:24.41
    Jan 18 22:25:24.413: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:24.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:25:24.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9409 patch pod agnhost-primary-pxthd -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 18 22:25:24.489: INFO: stderr: ""
    Jan 18 22:25:24.489: INFO: stdout: "pod/agnhost-primary-pxthd patched\n"
    STEP: checking annotations 01/18/23 22:25:24.489
    Jan 18 22:25:24.492: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:24.492: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:24.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9409" for this suite. 01/18/23 22:25:24.495
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:24.503
Jan 18 22:25:24.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:25:24.504
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:24.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:24.519
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 18 22:25:24.540: INFO: Create a RollingUpdate DaemonSet
Jan 18 22:25:24.544: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 22:25:24.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:25:24.551: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:25:25.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:25:25.558: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:25:26.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:25:26.559: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 18 22:25:26.559: INFO: Update the DaemonSet to trigger a rollout
Jan 18 22:25:26.569: INFO: Updating DaemonSet daemon-set
Jan 18 22:25:29.580: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 22:25:29.591: INFO: Updating DaemonSet daemon-set
Jan 18 22:25:29.591: INFO: Make sure DaemonSet rollback is complete
Jan 18 22:25:29.595: INFO: Wrong image for pod: daemon-set-kcpn9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 18 22:25:29.595: INFO: Pod daemon-set-kcpn9 is not available
Jan 18 22:25:31.604: INFO: Pod daemon-set-ch6wv is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:25:31.612
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9900, will wait for the garbage collector to delete the pods 01/18/23 22:25:31.613
Jan 18 22:25:31.672: INFO: Deleting DaemonSet.extensions daemon-set took: 5.692508ms
Jan 18 22:25:31.773: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.140618ms
Jan 18 22:25:33.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:25:33.176: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:25:33.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5141"},"items":null}

Jan 18 22:25:33.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:33.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9900" for this suite. 01/18/23 22:25:33.193
------------------------------
â€¢ [SLOW TEST] [8.695 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:24.503
    Jan 18 22:25:24.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:25:24.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:24.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:24.519
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 18 22:25:24.540: INFO: Create a RollingUpdate DaemonSet
    Jan 18 22:25:24.544: INFO: Check that daemon pods launch on every node of the cluster
    Jan 18 22:25:24.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:25:24.551: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:25:25.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:25:25.558: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:25:26.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:25:26.559: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 18 22:25:26.559: INFO: Update the DaemonSet to trigger a rollout
    Jan 18 22:25:26.569: INFO: Updating DaemonSet daemon-set
    Jan 18 22:25:29.580: INFO: Roll back the DaemonSet before rollout is complete
    Jan 18 22:25:29.591: INFO: Updating DaemonSet daemon-set
    Jan 18 22:25:29.591: INFO: Make sure DaemonSet rollback is complete
    Jan 18 22:25:29.595: INFO: Wrong image for pod: daemon-set-kcpn9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 18 22:25:29.595: INFO: Pod daemon-set-kcpn9 is not available
    Jan 18 22:25:31.604: INFO: Pod daemon-set-ch6wv is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:25:31.612
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9900, will wait for the garbage collector to delete the pods 01/18/23 22:25:31.613
    Jan 18 22:25:31.672: INFO: Deleting DaemonSet.extensions daemon-set took: 5.692508ms
    Jan 18 22:25:31.773: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.140618ms
    Jan 18 22:25:33.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:25:33.176: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:25:33.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5141"},"items":null}

    Jan 18 22:25:33.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5141"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:33.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9900" for this suite. 01/18/23 22:25:33.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:33.2
Jan 18 22:25:33.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:25:33.2
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:33.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:33.216
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-a4b91b1c-fbfe-4f0e-b7d3-7c3f4e2a68f8 01/18/23 22:25:33.219
STEP: Creating a pod to test consume secrets 01/18/23 22:25:33.223
Jan 18 22:25:33.230: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436" in namespace "projected-1913" to be "Succeeded or Failed"
Jan 18 22:25:33.233: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572601ms
Jan 18 22:25:35.237: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00646467s
Jan 18 22:25:37.240: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010043061s
STEP: Saw pod success 01/18/23 22:25:37.24
Jan 18 22:25:37.241: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436" satisfied condition "Succeeded or Failed"
Jan 18 22:25:37.243: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:25:37.248
Jan 18 22:25:37.261: INFO: Waiting for pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 to disappear
Jan 18 22:25:37.263: INFO: Pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:37.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1913" for this suite. 01/18/23 22:25:37.267
------------------------------
â€¢ [4.072 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:33.2
    Jan 18 22:25:33.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:25:33.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:33.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:33.216
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-a4b91b1c-fbfe-4f0e-b7d3-7c3f4e2a68f8 01/18/23 22:25:33.219
    STEP: Creating a pod to test consume secrets 01/18/23 22:25:33.223
    Jan 18 22:25:33.230: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436" in namespace "projected-1913" to be "Succeeded or Failed"
    Jan 18 22:25:33.233: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572601ms
    Jan 18 22:25:35.237: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00646467s
    Jan 18 22:25:37.240: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010043061s
    STEP: Saw pod success 01/18/23 22:25:37.24
    Jan 18 22:25:37.241: INFO: Pod "pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436" satisfied condition "Succeeded or Failed"
    Jan 18 22:25:37.243: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:25:37.248
    Jan 18 22:25:37.261: INFO: Waiting for pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 to disappear
    Jan 18 22:25:37.263: INFO: Pod pod-projected-secrets-e86cb66a-c223-4202-9dca-70e2be94f436 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:37.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1913" for this suite. 01/18/23 22:25:37.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:37.272
Jan 18 22:25:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:25:37.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:37.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:37.288
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 22:25:37.305: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:26:37.324: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/18/23 22:26:37.327
Jan 18 22:26:37.350: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:26:37.356: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:26:37.371: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:26:37.377: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 22:26:37.377
Jan 18 22:26:37.377: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5343" to be "running"
Jan 18 22:26:37.380: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535324ms
Jan 18 22:26:39.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007109076s
Jan 18 22:26:41.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.007185032s
Jan 18 22:26:41.384: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 22:26:41.384: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
Jan 18 22:26:41.387: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.489597ms
Jan 18 22:26:41.387: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:26:41.387: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
Jan 18 22:26:41.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.538717ms
Jan 18 22:26:41.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:26:41.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
Jan 18 22:26:41.392: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.402826ms
Jan 18 22:26:41.392: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 22:26:41.392
Jan 18 22:26:41.399: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5343" to be "running"
Jan 18 22:26:41.401: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248324ms
Jan 18 22:26:43.405: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006275702s
Jan 18 22:26:45.406: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006766766s
Jan 18 22:26:47.406: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006590497s
Jan 18 22:26:47.406: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:47.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5343" for this suite. 01/18/23 22:26:47.445
------------------------------
â€¢ [SLOW TEST] [70.177 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:37.272
    Jan 18 22:25:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:25:37.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:37.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:37.288
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 22:25:37.305: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:26:37.324: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/18/23 22:26:37.327
    Jan 18 22:26:37.350: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 22:26:37.356: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 22:26:37.371: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 22:26:37.377: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 22:26:37.377
    Jan 18 22:26:37.377: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5343" to be "running"
    Jan 18 22:26:37.380: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535324ms
    Jan 18 22:26:39.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007109076s
    Jan 18 22:26:41.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.007185032s
    Jan 18 22:26:41.384: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 22:26:41.384: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
    Jan 18 22:26:41.387: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.489597ms
    Jan 18 22:26:41.387: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:26:41.387: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
    Jan 18 22:26:41.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.538717ms
    Jan 18 22:26:41.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:26:41.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5343" to be "running"
    Jan 18 22:26:41.392: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.402826ms
    Jan 18 22:26:41.392: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 22:26:41.392
    Jan 18 22:26:41.399: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5343" to be "running"
    Jan 18 22:26:41.401: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248324ms
    Jan 18 22:26:43.405: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006275702s
    Jan 18 22:26:45.406: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006766766s
    Jan 18 22:26:47.406: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006590497s
    Jan 18 22:26:47.406: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:47.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5343" for this suite. 01/18/23 22:26:47.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:47.45
Jan 18 22:26:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:47.451
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:47.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:47.465
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 18 22:26:47.475: INFO: Waiting up to 2m0s for pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" in namespace "var-expansion-7396" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 22:26:47.478: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052308ms
Jan 18 22:26:49.482: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006679979s
Jan 18 22:26:49.482: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 22:26:49.482: INFO: Deleting pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" in namespace "var-expansion-7396"
Jan 18 22:26:49.489: INFO: Wait up to 5m0s for pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:51.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7396" for this suite. 01/18/23 22:26:51.498
------------------------------
â€¢ [4.053 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:47.45
    Jan 18 22:26:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:47.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:47.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:47.465
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 18 22:26:47.475: INFO: Waiting up to 2m0s for pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" in namespace "var-expansion-7396" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 22:26:47.478: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052308ms
    Jan 18 22:26:49.482: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006679979s
    Jan 18 22:26:49.482: INFO: Pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 22:26:49.482: INFO: Deleting pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" in namespace "var-expansion-7396"
    Jan 18 22:26:49.489: INFO: Wait up to 5m0s for pod "var-expansion-c5d29a62-ef6c-4b01-b7be-fb0705187b1e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:51.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7396" for this suite. 01/18/23 22:26:51.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:51.504
Jan 18 22:26:51.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:26:51.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:51.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:51.521
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:26:51.524
Jan 18 22:26:51.531: INFO: Waiting up to 5m0s for pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb" in namespace "emptydir-2235" to be "Succeeded or Failed"
Jan 18 22:26:51.534: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32861ms
Jan 18 22:26:53.537: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005601142s
Jan 18 22:26:55.538: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006582806s
STEP: Saw pod success 01/18/23 22:26:55.538
Jan 18 22:26:55.538: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb" satisfied condition "Succeeded or Failed"
Jan 18 22:26:55.540: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb container test-container: <nil>
STEP: delete the pod 01/18/23 22:26:55.547
Jan 18 22:26:55.556: INFO: Waiting for pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb to disappear
Jan 18 22:26:55.560: INFO: Pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2235" for this suite. 01/18/23 22:26:55.563
------------------------------
â€¢ [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:51.504
    Jan 18 22:26:51.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:26:51.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:51.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:51.521
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:26:51.524
    Jan 18 22:26:51.531: INFO: Waiting up to 5m0s for pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb" in namespace "emptydir-2235" to be "Succeeded or Failed"
    Jan 18 22:26:51.534: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32861ms
    Jan 18 22:26:53.537: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005601142s
    Jan 18 22:26:55.538: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006582806s
    STEP: Saw pod success 01/18/23 22:26:55.538
    Jan 18 22:26:55.538: INFO: Pod "pod-b5bb969f-a015-432b-958d-5d150cfd15bb" satisfied condition "Succeeded or Failed"
    Jan 18 22:26:55.540: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb container test-container: <nil>
    STEP: delete the pod 01/18/23 22:26:55.547
    Jan 18 22:26:55.556: INFO: Waiting for pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb to disappear
    Jan 18 22:26:55.560: INFO: Pod pod-b5bb969f-a015-432b-958d-5d150cfd15bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2235" for this suite. 01/18/23 22:26:55.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:55.568
Jan 18 22:26:55.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:26:55.569
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:55.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:55.586
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 18 22:26:55.589: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 22:26:56.598
STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 22:26:56.603
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 22:26:57.61
Jan 18 22:26:57.617: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/18/23 22:26:57.617
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:58.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9451" for this suite. 01/18/23 22:26:58.626
------------------------------
â€¢ [3.063 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:55.568
    Jan 18 22:26:55.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:26:55.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:55.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:55.586
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 18 22:26:55.589: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 22:26:56.598
    STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 22:26:56.603
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 22:26:57.61
    Jan 18 22:26:57.617: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/18/23 22:26:57.617
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:58.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9451" for this suite. 01/18/23 22:26:58.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:58.634
Jan 18 22:26:58.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:58.634
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:58.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:58.648
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:58.651
Jan 18 22:26:58.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402" in namespace "downward-api-4294" to be "Succeeded or Failed"
Jan 18 22:26:58.660: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456785ms
Jan 18 22:27:00.665: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Running", Reason="", readiness=false. Elapsed: 2.007551788s
Jan 18 22:27:02.663: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005629017s
STEP: Saw pod success 01/18/23 22:27:02.663
Jan 18 22:27:02.664: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402" satisfied condition "Succeeded or Failed"
Jan 18 22:27:02.666: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 container client-container: <nil>
STEP: delete the pod 01/18/23 22:27:02.671
Jan 18 22:27:02.681: INFO: Waiting for pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 to disappear
Jan 18 22:27:02.683: INFO: Pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4294" for this suite. 01/18/23 22:27:02.686
------------------------------
â€¢ [4.057 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:58.634
    Jan 18 22:26:58.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:58.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:58.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:58.648
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:58.651
    Jan 18 22:26:58.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402" in namespace "downward-api-4294" to be "Succeeded or Failed"
    Jan 18 22:26:58.660: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456785ms
    Jan 18 22:27:00.665: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Running", Reason="", readiness=false. Elapsed: 2.007551788s
    Jan 18 22:27:02.663: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005629017s
    STEP: Saw pod success 01/18/23 22:27:02.663
    Jan 18 22:27:02.664: INFO: Pod "downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:02.666: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:27:02.671
    Jan 18 22:27:02.681: INFO: Waiting for pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 to disappear
    Jan 18 22:27:02.683: INFO: Pod downwardapi-volume-0eadc744-dc4f-467d-9b66-8d37bbd89402 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4294" for this suite. 01/18/23 22:27:02.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:02.691
Jan 18 22:27:02.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:27:02.692
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:02.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:02.706
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 18 22:27:02.721: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5983 to be scheduled
Jan 18 22:27:02.723: INFO: 1 pods are not scheduled: [runtimeclass-5983/test-runtimeclass-runtimeclass-5983-preconfigured-handler-66cpl(6b0e946c-e774-440c-87d3-1a9ff82218e4)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:04.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5983" for this suite. 01/18/23 22:27:04.735
------------------------------
â€¢ [2.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:02.691
    Jan 18 22:27:02.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:27:02.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:02.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:02.706
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 18 22:27:02.721: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5983 to be scheduled
    Jan 18 22:27:02.723: INFO: 1 pods are not scheduled: [runtimeclass-5983/test-runtimeclass-runtimeclass-5983-preconfigured-handler-66cpl(6b0e946c-e774-440c-87d3-1a9ff82218e4)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:04.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5983" for this suite. 01/18/23 22:27:04.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:04.745
Jan 18 22:27:04.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:27:04.745
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:04.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:04.757
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/18/23 22:27:04.76
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local;sleep 1; done
 01/18/23 22:27:04.765
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local;sleep 1; done
 01/18/23 22:27:04.766
STEP: creating a pod to probe DNS 01/18/23 22:27:04.766
STEP: submitting the pod to kubernetes 01/18/23 22:27:04.766
Jan 18 22:27:04.775: INFO: Waiting up to 15m0s for pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b" in namespace "dns-7776" to be "running"
Jan 18 22:27:04.778: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676581ms
Jan 18 22:27:06.781: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005905265s
Jan 18 22:27:06.781: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:27:06.781
STEP: looking for the results for each expected name from probers 01/18/23 22:27:06.784
Jan 18 22:27:06.790: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.793: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.796: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.800: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.803: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.806: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.809: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.812: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:06.812: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:11.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.821: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.824: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.827: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.829: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.832: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.835: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.837: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:11.837: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:16.819: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.822: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.825: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.828: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.830: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:16.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:21.820: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.826: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.830: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.833: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.836: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.854: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.862: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:21.862: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:26.820: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.826: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.829: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.832: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:26.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:31.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.822: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.825: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.828: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.831: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
Jan 18 22:27:31.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

Jan 18 22:27:36.841: INFO: DNS probes using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b succeeded

STEP: deleting the pod 01/18/23 22:27:36.841
STEP: deleting the test headless service 01/18/23 22:27:36.852
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:36.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7776" for this suite. 01/18/23 22:27:36.865
------------------------------
â€¢ [SLOW TEST] [32.125 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:04.745
    Jan 18 22:27:04.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:27:04.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:04.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:04.757
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/18/23 22:27:04.76
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local;sleep 1; done
     01/18/23 22:27:04.765
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7776.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local;sleep 1; done
     01/18/23 22:27:04.766
    STEP: creating a pod to probe DNS 01/18/23 22:27:04.766
    STEP: submitting the pod to kubernetes 01/18/23 22:27:04.766
    Jan 18 22:27:04.775: INFO: Waiting up to 15m0s for pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b" in namespace "dns-7776" to be "running"
    Jan 18 22:27:04.778: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676581ms
    Jan 18 22:27:06.781: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005905265s
    Jan 18 22:27:06.781: INFO: Pod "dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:27:06.781
    STEP: looking for the results for each expected name from probers 01/18/23 22:27:06.784
    Jan 18 22:27:06.790: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.793: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.796: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.800: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.803: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.806: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.809: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.812: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:06.812: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:11.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.821: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.824: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.827: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.829: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.832: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.835: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.837: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:11.837: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:16.819: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.822: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.825: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.828: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.830: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:16.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:21.820: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.826: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.830: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.833: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.836: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.854: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.862: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:21.862: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:26.820: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.826: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.829: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.832: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:26.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:31.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.822: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.825: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.828: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.831: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.837: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.840: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local from pod dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b: the server could not find the requested resource (get pods dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b)
    Jan 18 22:27:31.840: INFO: Lookups using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7776.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7776.svc.cluster.local jessie_udp@dns-test-service-2.dns-7776.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7776.svc.cluster.local]

    Jan 18 22:27:36.841: INFO: DNS probes using dns-7776/dns-test-4f9c7ab2-f52d-4b80-9e4d-769e518df97b succeeded

    STEP: deleting the pod 01/18/23 22:27:36.841
    STEP: deleting the test headless service 01/18/23 22:27:36.852
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:36.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7776" for this suite. 01/18/23 22:27:36.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:36.87
Jan 18 22:27:36.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:27:36.871
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:36.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:36.885
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:27:36.888
Jan 18 22:27:36.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9" in namespace "projected-9575" to be "Succeeded or Failed"
Jan 18 22:27:36.899: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567505ms
Jan 18 22:27:38.903: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089833s
Jan 18 22:27:40.904: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007348659s
STEP: Saw pod success 01/18/23 22:27:40.904
Jan 18 22:27:40.904: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9" satisfied condition "Succeeded or Failed"
Jan 18 22:27:40.907: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 container client-container: <nil>
STEP: delete the pod 01/18/23 22:27:40.911
Jan 18 22:27:40.923: INFO: Waiting for pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 to disappear
Jan 18 22:27:40.928: INFO: Pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:40.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9575" for this suite. 01/18/23 22:27:40.931
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:36.87
    Jan 18 22:27:36.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:27:36.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:36.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:36.885
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:27:36.888
    Jan 18 22:27:36.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9" in namespace "projected-9575" to be "Succeeded or Failed"
    Jan 18 22:27:36.899: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567505ms
    Jan 18 22:27:38.903: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089833s
    Jan 18 22:27:40.904: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007348659s
    STEP: Saw pod success 01/18/23 22:27:40.904
    Jan 18 22:27:40.904: INFO: Pod "downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:40.907: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:27:40.911
    Jan 18 22:27:40.923: INFO: Waiting for pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 to disappear
    Jan 18 22:27:40.928: INFO: Pod downwardapi-volume-1463a7d5-2bc1-422d-afa3-1e34b4e41dc9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:40.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9575" for this suite. 01/18/23 22:27:40.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:40.937
Jan 18 22:27:40.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 22:27:40.938
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:40.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:40.954
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/18/23 22:27:40.957
STEP: Verify that the required pods have come up 01/18/23 22:27:40.962
Jan 18 22:27:40.964: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 22:27:45.971: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/18/23 22:27:45.971
Jan 18 22:27:45.973: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/18/23 22:27:45.973
STEP: DeleteCollection of the ReplicaSets 01/18/23 22:27:45.976
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 22:27:45.981
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:45.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2957" for this suite. 01/18/23 22:27:45.986
------------------------------
â€¢ [SLOW TEST] [5.063 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:40.937
    Jan 18 22:27:40.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:27:40.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:40.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:40.954
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/18/23 22:27:40.957
    STEP: Verify that the required pods have come up 01/18/23 22:27:40.962
    Jan 18 22:27:40.964: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 18 22:27:45.971: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/18/23 22:27:45.971
    Jan 18 22:27:45.973: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/18/23 22:27:45.973
    STEP: DeleteCollection of the ReplicaSets 01/18/23 22:27:45.976
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 22:27:45.981
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:45.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2957" for this suite. 01/18/23 22:27:45.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:46.002
Jan 18 22:27:46.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:27:46.003
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:46.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:46.018
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-914f48f7-d39b-4821-a0f2-b50870766a70 01/18/23 22:27:46.021
STEP: Creating a pod to test consume configMaps 01/18/23 22:27:46.025
Jan 18 22:27:46.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a" in namespace "configmap-3001" to be "Succeeded or Failed"
Jan 18 22:27:46.038: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45355ms
Jan 18 22:27:48.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007910275s
Jan 18 22:27:50.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865896s
STEP: Saw pod success 01/18/23 22:27:50.041
Jan 18 22:27:50.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a" satisfied condition "Succeeded or Failed"
Jan 18 22:27:50.044: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:27:50.049
Jan 18 22:27:50.061: INFO: Waiting for pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a to disappear
Jan 18 22:27:50.063: INFO: Pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3001" for this suite. 01/18/23 22:27:50.066
------------------------------
â€¢ [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:46.002
    Jan 18 22:27:46.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:27:46.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:46.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:46.018
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-914f48f7-d39b-4821-a0f2-b50870766a70 01/18/23 22:27:46.021
    STEP: Creating a pod to test consume configMaps 01/18/23 22:27:46.025
    Jan 18 22:27:46.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a" in namespace "configmap-3001" to be "Succeeded or Failed"
    Jan 18 22:27:46.038: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45355ms
    Jan 18 22:27:48.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007910275s
    Jan 18 22:27:50.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865896s
    STEP: Saw pod success 01/18/23 22:27:50.041
    Jan 18 22:27:50.041: INFO: Pod "pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:50.044: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:27:50.049
    Jan 18 22:27:50.061: INFO: Waiting for pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a to disappear
    Jan 18 22:27:50.063: INFO: Pod pod-configmaps-d8c41617-0045-4cef-8048-7c4368c9225a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3001" for this suite. 01/18/23 22:27:50.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:50.072
Jan 18 22:27:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:27:50.073
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:50.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:50.087
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-fc40b46b-ac90-4011-adff-ffde7b06d696 01/18/23 22:27:50.09
STEP: Creating a pod to test consume secrets 01/18/23 22:27:50.094
Jan 18 22:27:50.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d" in namespace "projected-342" to be "Succeeded or Failed"
Jan 18 22:27:50.105: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341164ms
Jan 18 22:27:52.108: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006110288s
Jan 18 22:27:54.109: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006930339s
STEP: Saw pod success 01/18/23 22:27:54.109
Jan 18 22:27:54.109: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d" satisfied condition "Succeeded or Failed"
Jan 18 22:27:54.112: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:27:54.117
Jan 18 22:27:54.127: INFO: Waiting for pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d to disappear
Jan 18 22:27:54.129: INFO: Pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:54.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-342" for this suite. 01/18/23 22:27:54.132
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:50.072
    Jan 18 22:27:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:27:50.073
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:50.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:50.087
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-fc40b46b-ac90-4011-adff-ffde7b06d696 01/18/23 22:27:50.09
    STEP: Creating a pod to test consume secrets 01/18/23 22:27:50.094
    Jan 18 22:27:50.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d" in namespace "projected-342" to be "Succeeded or Failed"
    Jan 18 22:27:50.105: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341164ms
    Jan 18 22:27:52.108: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006110288s
    Jan 18 22:27:54.109: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006930339s
    STEP: Saw pod success 01/18/23 22:27:54.109
    Jan 18 22:27:54.109: INFO: Pod "pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:54.112: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:27:54.117
    Jan 18 22:27:54.127: INFO: Waiting for pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d to disappear
    Jan 18 22:27:54.129: INFO: Pod pod-projected-secrets-29f2ee92-462a-4c17-ae75-368cfa2fee0d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:54.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-342" for this suite. 01/18/23 22:27:54.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:54.137
Jan 18 22:27:54.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:27:54.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:54.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:54.153
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 22:27:54.156
Jan 18 22:27:54.164: INFO: Waiting up to 5m0s for pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90" in namespace "emptydir-3456" to be "Succeeded or Failed"
Jan 18 22:27:54.166: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242231ms
Jan 18 22:27:56.170: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887926s
Jan 18 22:27:58.169: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005532801s
STEP: Saw pod success 01/18/23 22:27:58.169
Jan 18 22:27:58.169: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90" satisfied condition "Succeeded or Failed"
Jan 18 22:27:58.172: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 container test-container: <nil>
STEP: delete the pod 01/18/23 22:27:58.176
Jan 18 22:27:58.187: INFO: Waiting for pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 to disappear
Jan 18 22:27:58.189: INFO: Pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:58.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3456" for this suite. 01/18/23 22:27:58.192
------------------------------
â€¢ [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:54.137
    Jan 18 22:27:54.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:27:54.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:54.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:54.153
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 22:27:54.156
    Jan 18 22:27:54.164: INFO: Waiting up to 5m0s for pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90" in namespace "emptydir-3456" to be "Succeeded or Failed"
    Jan 18 22:27:54.166: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242231ms
    Jan 18 22:27:56.170: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887926s
    Jan 18 22:27:58.169: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005532801s
    STEP: Saw pod success 01/18/23 22:27:58.169
    Jan 18 22:27:58.169: INFO: Pod "pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:58.172: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:27:58.176
    Jan 18 22:27:58.187: INFO: Waiting for pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 to disappear
    Jan 18 22:27:58.189: INFO: Pod pod-39a0b52d-307a-4e7b-8419-2d9bb219cd90 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:58.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3456" for this suite. 01/18/23 22:27:58.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:58.198
Jan 18 22:27:58.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 22:27:58.199
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:58.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:58.215
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 18 22:27:58.250: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"89f449a8-0a6a-422f-8390-fc50f2689daf", Controller:(*bool)(0xc004165fde), BlockOwnerDeletion:(*bool)(0xc004165fdf)}}
Jan 18 22:27:58.261: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a4a1f4f1-de78-4bbf-a66f-089ddbd9b801", Controller:(*bool)(0xc00435625e), BlockOwnerDeletion:(*bool)(0xc00435625f)}}
Jan 18 22:27:58.280: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f67969ca-af09-4fcd-9cdf-ba240984fc03", Controller:(*bool)(0xc0041ff026), BlockOwnerDeletion:(*bool)(0xc0041ff027)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5991" for this suite. 01/18/23 22:28:03.292
------------------------------
â€¢ [SLOW TEST] [5.101 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:58.198
    Jan 18 22:27:58.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 22:27:58.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:58.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:58.215
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 18 22:27:58.250: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"89f449a8-0a6a-422f-8390-fc50f2689daf", Controller:(*bool)(0xc004165fde), BlockOwnerDeletion:(*bool)(0xc004165fdf)}}
    Jan 18 22:27:58.261: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a4a1f4f1-de78-4bbf-a66f-089ddbd9b801", Controller:(*bool)(0xc00435625e), BlockOwnerDeletion:(*bool)(0xc00435625f)}}
    Jan 18 22:27:58.280: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f67969ca-af09-4fcd-9cdf-ba240984fc03", Controller:(*bool)(0xc0041ff026), BlockOwnerDeletion:(*bool)(0xc0041ff027)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:03.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5991" for this suite. 01/18/23 22:28:03.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:03.299
Jan 18 22:28:03.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 22:28:03.3
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.313
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-crzl7" 01/18/23 22:28:03.315
Jan 18 22:28:03.328: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-crzl7-1770" 01/18/23 22:28:03.328
Jan 18 22:28:03.334: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-crzl7-1770" 01/18/23 22:28:03.334
Jan 18 22:28:03.339: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:03.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6660" for this suite. 01/18/23 22:28:03.342
STEP: Destroying namespace "e2e-ns-crzl7-1770" for this suite. 01/18/23 22:28:03.346
------------------------------
â€¢ [0.051 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:03.299
    Jan 18 22:28:03.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:28:03.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.313
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-crzl7" 01/18/23 22:28:03.315
    Jan 18 22:28:03.328: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-crzl7-1770" 01/18/23 22:28:03.328
    Jan 18 22:28:03.334: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-crzl7-1770" 01/18/23 22:28:03.334
    Jan 18 22:28:03.339: INFO: Namespace "e2e-ns-crzl7-1770" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:03.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6660" for this suite. 01/18/23 22:28:03.342
    STEP: Destroying namespace "e2e-ns-crzl7-1770" for this suite. 01/18/23 22:28:03.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:03.351
Jan 18 22:28:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:28:03.352
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.364
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:03.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-56" for this suite. 01/18/23 22:28:03.458
------------------------------
â€¢ [0.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:03.351
    Jan 18 22:28:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:28:03.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.364
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:03.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-56" for this suite. 01/18/23 22:28:03.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:03.463
Jan 18 22:28:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:28:03.464
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.48
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 22:28:03.483
Jan 18 22:28:03.490: INFO: Waiting up to 5m0s for pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b" in namespace "emptydir-4750" to be "Succeeded or Failed"
Jan 18 22:28:03.492: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402918ms
Jan 18 22:28:05.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006846132s
Jan 18 22:28:07.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006923471s
STEP: Saw pod success 01/18/23 22:28:07.497
Jan 18 22:28:07.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b" satisfied condition "Succeeded or Failed"
Jan 18 22:28:07.500: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b container test-container: <nil>
STEP: delete the pod 01/18/23 22:28:07.504
Jan 18 22:28:07.515: INFO: Waiting for pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b to disappear
Jan 18 22:28:07.517: INFO: Pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4750" for this suite. 01/18/23 22:28:07.52
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:03.463
    Jan 18 22:28:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:28:03.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:03.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:03.48
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 22:28:03.483
    Jan 18 22:28:03.490: INFO: Waiting up to 5m0s for pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b" in namespace "emptydir-4750" to be "Succeeded or Failed"
    Jan 18 22:28:03.492: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402918ms
    Jan 18 22:28:05.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006846132s
    Jan 18 22:28:07.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006923471s
    STEP: Saw pod success 01/18/23 22:28:07.497
    Jan 18 22:28:07.497: INFO: Pod "pod-d97f5775-84b8-44f5-9638-7eca5418e15b" satisfied condition "Succeeded or Failed"
    Jan 18 22:28:07.500: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b container test-container: <nil>
    STEP: delete the pod 01/18/23 22:28:07.504
    Jan 18 22:28:07.515: INFO: Waiting for pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b to disappear
    Jan 18 22:28:07.517: INFO: Pod pod-d97f5775-84b8-44f5-9638-7eca5418e15b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4750" for this suite. 01/18/23 22:28:07.52
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:07.525
Jan 18 22:28:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:28:07.526
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:07.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:07.539
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 22:28:07.542
Jan 18 22:28:07.548: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3022  14a59b5b-777d-482a-bf40-9925019391c1 6017 0 2023-01-18 22:28:07 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:07 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85hf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85hf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:28:07.548: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3022" to be "running and ready"
Jan 18 22:28:07.551: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089051ms
Jan 18 22:28:07.551: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:28:09.554: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005404879s
Jan 18 22:28:09.554: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 18 22:28:09.554: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 22:28:09.554
Jan 18 22:28:09.554: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3022 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:28:09.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:28:09.555: INFO: ExecWithOptions: Clientset creation
Jan 18 22:28:09.555: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3022/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/18/23 22:28:09.65
Jan 18 22:28:09.650: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3022 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:28:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:28:09.650: INFO: ExecWithOptions: Clientset creation
Jan 18 22:28:09.651: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3022/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:28:09.733: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:09.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3022" for this suite. 01/18/23 22:28:09.746
------------------------------
â€¢ [2.229 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:07.525
    Jan 18 22:28:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:28:07.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:07.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:07.539
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 22:28:07.542
    Jan 18 22:28:07.548: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3022  14a59b5b-777d-482a-bf40-9925019391c1 6017 0 2023-01-18 22:28:07 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:07 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85hf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85hf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:28:07.548: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3022" to be "running and ready"
    Jan 18 22:28:07.551: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089051ms
    Jan 18 22:28:07.551: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:28:09.554: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005404879s
    Jan 18 22:28:09.554: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 18 22:28:09.554: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 22:28:09.554
    Jan 18 22:28:09.554: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3022 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:28:09.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:28:09.555: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:28:09.555: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3022/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/18/23 22:28:09.65
    Jan 18 22:28:09.650: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3022 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:28:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:28:09.650: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:28:09.651: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3022/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:28:09.733: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:09.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3022" for this suite. 01/18/23 22:28:09.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:09.754
Jan 18 22:28:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:28:09.755
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:09.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:09.773
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:28:09.79
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:28:10.228
STEP: Deploying the webhook pod 01/18/23 22:28:10.235
STEP: Wait for the deployment to be ready 01/18/23 22:28:10.247
Jan 18 22:28:10.257: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:28:12.265
STEP: Verifying the service has paired with the endpoint 01/18/23 22:28:12.272
Jan 18 22:28:13.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/18/23 22:28:13.275
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:28:13.277
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:28:13.277
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:28:13.277
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:28:13.278
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:28:13.278
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:28:13.279
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6397" for this suite. 01/18/23 22:28:13.313
STEP: Destroying namespace "webhook-6397-markers" for this suite. 01/18/23 22:28:13.321
------------------------------
â€¢ [3.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:09.754
    Jan 18 22:28:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:28:09.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:09.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:09.773
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:28:09.79
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:28:10.228
    STEP: Deploying the webhook pod 01/18/23 22:28:10.235
    STEP: Wait for the deployment to be ready 01/18/23 22:28:10.247
    Jan 18 22:28:10.257: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:28:12.265
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:28:12.272
    Jan 18 22:28:13.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/18/23 22:28:13.275
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:28:13.277
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:28:13.277
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:28:13.277
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:28:13.278
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:28:13.278
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:28:13.279
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6397" for this suite. 01/18/23 22:28:13.313
    STEP: Destroying namespace "webhook-6397-markers" for this suite. 01/18/23 22:28:13.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:13.327
Jan 18 22:28:13.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename watch 01/18/23 22:28:13.328
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:13.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:13.344
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/18/23 22:28:13.347
STEP: modifying the configmap once 01/18/23 22:28:13.352
STEP: modifying the configmap a second time 01/18/23 22:28:13.36
STEP: deleting the configmap 01/18/23 22:28:13.368
STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 22:28:13.373
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 22:28:13.375
Jan 18 22:28:13.375: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6365  ac5740f5-1038-4430-9699-dd0362c6b444 6116 0 2023-01-18 22:28:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:28:13.375: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6365  ac5740f5-1038-4430-9699-dd0362c6b444 6117 0 2023-01-18 22:28:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:13.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6365" for this suite. 01/18/23 22:28:13.378
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:13.327
    Jan 18 22:28:13.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename watch 01/18/23 22:28:13.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:13.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:13.344
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/18/23 22:28:13.347
    STEP: modifying the configmap once 01/18/23 22:28:13.352
    STEP: modifying the configmap a second time 01/18/23 22:28:13.36
    STEP: deleting the configmap 01/18/23 22:28:13.368
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 22:28:13.373
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 22:28:13.375
    Jan 18 22:28:13.375: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6365  ac5740f5-1038-4430-9699-dd0362c6b444 6116 0 2023-01-18 22:28:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:28:13.375: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6365  ac5740f5-1038-4430-9699-dd0362c6b444 6117 0 2023-01-18 22:28:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:13.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6365" for this suite. 01/18/23 22:28:13.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:13.384
Jan 18 22:28:13.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:28:13.385
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:13.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:13.402
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-ae3de95a-37c4-417d-8765-cbe5cfe38fe7 01/18/23 22:28:13.408
STEP: Creating the pod 01/18/23 22:28:13.412
Jan 18 22:28:13.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579" in namespace "configmap-1893" to be "running and ready"
Jan 18 22:28:13.421: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413659ms
Jan 18 22:28:13.421: INFO: The phase of Pod pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:28:15.424: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579": Phase="Running", Reason="", readiness=true. Elapsed: 2.005957131s
Jan 18 22:28:15.424: INFO: The phase of Pod pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579 is Running (Ready = true)
Jan 18 22:28:15.424: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-ae3de95a-37c4-417d-8765-cbe5cfe38fe7 01/18/23 22:28:15.432
STEP: waiting to observe update in volume 01/18/23 22:28:15.436
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:17.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1893" for this suite. 01/18/23 22:28:17.451
------------------------------
â€¢ [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:13.384
    Jan 18 22:28:13.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:28:13.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:13.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:13.402
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-ae3de95a-37c4-417d-8765-cbe5cfe38fe7 01/18/23 22:28:13.408
    STEP: Creating the pod 01/18/23 22:28:13.412
    Jan 18 22:28:13.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579" in namespace "configmap-1893" to be "running and ready"
    Jan 18 22:28:13.421: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413659ms
    Jan 18 22:28:13.421: INFO: The phase of Pod pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:28:15.424: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579": Phase="Running", Reason="", readiness=true. Elapsed: 2.005957131s
    Jan 18 22:28:15.424: INFO: The phase of Pod pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579 is Running (Ready = true)
    Jan 18 22:28:15.424: INFO: Pod "pod-configmaps-f3aeb48e-6d7d-407c-8895-64ad97cb3579" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-ae3de95a-37c4-417d-8765-cbe5cfe38fe7 01/18/23 22:28:15.432
    STEP: waiting to observe update in volume 01/18/23 22:28:15.436
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:17.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1893" for this suite. 01/18/23 22:28:17.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:17.456
Jan 18 22:28:17.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:28:17.457
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:17.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:17.471
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 18 22:28:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 create -f -'
Jan 18 22:28:17.660: INFO: stderr: ""
Jan 18 22:28:17.660: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 22:28:17.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 create -f -'
Jan 18 22:28:17.851: INFO: stderr: ""
Jan 18 22:28:17.851: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:28:17.851
Jan 18 22:28:18.854: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:28:18.854: INFO: Found 1 / 1
Jan 18 22:28:18.854: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 22:28:18.857: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:28:18.857: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:28:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe pod agnhost-primary-jxt9w'
Jan 18 22:28:18.932: INFO: stderr: ""
Jan 18 22:28:18.932: INFO: stdout: "Name:             agnhost-primary-jxt9w\nNamespace:        kubectl-288\nPriority:         0\nService Account:  default\nNode:             cncf-conformance-1-26-2/10.128.15.199\nStart Time:       Wed, 18 Jan 2023 22:28:17 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.32.12.4\nIPs:\n  IP:           10.32.12.4\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://14aa2508431d81c3056d070d16b3d56329e378dcb5dfb15f4dc26cd25998fd36\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 22:28:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vflbk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vflbk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-288/agnhost-primary-jxt9w to cncf-conformance-1-26-2\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Jan 18 22:28:18.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe rc agnhost-primary'
Jan 18 22:28:19.013: INFO: stderr: ""
Jan 18 22:28:19.013: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-288\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jxt9w\n"
Jan 18 22:28:19.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe service agnhost-primary'
Jan 18 22:28:19.094: INFO: stderr: ""
Jan 18 22:28:19.094: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-288\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.1.211\nIPs:               10.96.1.211\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.32.12.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 22:28:19.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe node cncf-conformance-1-26-1'
Jan 18 22:28:19.189: INFO: stderr: ""
Jan 18 22:28:19.189: INFO: stdout: "Name:               cncf-conformance-1-26-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-conformance-1-26-1\n                    kubernetes.io/os=linux\n                    kurl.sh/cluster=true\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 22:01:44 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-conformance-1-26-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 22:28:10 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 22:02:09 +0000   Wed, 18 Jan 2023 22:02:09 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:02:06 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.15.198\n  Hostname:    cncf-conformance-1-26-1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      203056560Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 30807380Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      187136925387\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 30704980Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 3f493728de48c51d495eccc3f940ba2b\n  System UUID:                3f493728-de48-c51d-495e-ccc3f940ba2b\n  Boot ID:                    d1cf5dfe-e91c-4c64-a346-236f863a5f7f\n  Kernel Version:             5.15.0-1027-gcp\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-4gpmq                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     26m\n  kube-system                 coredns-787d4945fb-4xgtd                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     26m\n  kube-system                 etcd-cncf-conformance-1-26-1                               100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         26m\n  kube-system                 kube-apiserver-cncf-conformance-1-26-1                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-controller-manager-cncf-conformance-1-26-1            200m (2%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-proxy-79fqc                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-scheduler-cncf-conformance-1-26-1                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 weave-net-wzqsl                                            100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    950m (11%)  0 (0%)\n  memory                 240Mi (0%)  340Mi (1%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 26m                kube-proxy       \n  Normal   NodeHasSufficientMemory  26m (x4 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    26m (x4 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     26m (x3 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 26m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      26m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           26m                node-controller  Node cncf-conformance-1-26-1 event: Registered Node cncf-conformance-1-26-1 in Controller\n  Normal   NodeReady                26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeReady\n"
Jan 18 22:28:19.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe namespace kubectl-288'
Jan 18 22:28:19.265: INFO: stderr: ""
Jan 18 22:28:19.265: INFO: stdout: "Name:         kubectl-288\nLabels:       e2e-framework=kubectl\n              e2e-run=5a657bac-996d-4529-8596-8a8788f66c01\n              kubernetes.io/metadata.name=kubectl-288\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-288" for this suite. 01/18/23 22:28:19.269
------------------------------
â€¢ [1.818 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:17.456
    Jan 18 22:28:17.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:28:17.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:17.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:17.471
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 18 22:28:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 create -f -'
    Jan 18 22:28:17.660: INFO: stderr: ""
    Jan 18 22:28:17.660: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 18 22:28:17.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 create -f -'
    Jan 18 22:28:17.851: INFO: stderr: ""
    Jan 18 22:28:17.851: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:28:17.851
    Jan 18 22:28:18.854: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:28:18.854: INFO: Found 1 / 1
    Jan 18 22:28:18.854: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 22:28:18.857: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:28:18.857: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:28:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe pod agnhost-primary-jxt9w'
    Jan 18 22:28:18.932: INFO: stderr: ""
    Jan 18 22:28:18.932: INFO: stdout: "Name:             agnhost-primary-jxt9w\nNamespace:        kubectl-288\nPriority:         0\nService Account:  default\nNode:             cncf-conformance-1-26-2/10.128.15.199\nStart Time:       Wed, 18 Jan 2023 22:28:17 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.32.12.4\nIPs:\n  IP:           10.32.12.4\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://14aa2508431d81c3056d070d16b3d56329e378dcb5dfb15f4dc26cd25998fd36\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 22:28:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vflbk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vflbk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-288/agnhost-primary-jxt9w to cncf-conformance-1-26-2\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Jan 18 22:28:18.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe rc agnhost-primary'
    Jan 18 22:28:19.013: INFO: stderr: ""
    Jan 18 22:28:19.013: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-288\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jxt9w\n"
    Jan 18 22:28:19.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe service agnhost-primary'
    Jan 18 22:28:19.094: INFO: stderr: ""
    Jan 18 22:28:19.094: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-288\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.1.211\nIPs:               10.96.1.211\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.32.12.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 18 22:28:19.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe node cncf-conformance-1-26-1'
    Jan 18 22:28:19.189: INFO: stderr: ""
    Jan 18 22:28:19.189: INFO: stdout: "Name:               cncf-conformance-1-26-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-conformance-1-26-1\n                    kubernetes.io/os=linux\n                    kurl.sh/cluster=true\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 22:01:44 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-conformance-1-26-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 22:28:10 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 22:02:09 +0000   Wed, 18 Jan 2023 22:02:09 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:01:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 22:26:39 +0000   Wed, 18 Jan 2023 22:02:06 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.15.198\n  Hostname:    cncf-conformance-1-26-1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      203056560Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 30807380Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      187136925387\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 30704980Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 3f493728de48c51d495eccc3f940ba2b\n  System UUID:                3f493728-de48-c51d-495e-ccc3f940ba2b\n  Boot ID:                    d1cf5dfe-e91c-4c64-a346-236f863a5f7f\n  Kernel Version:             5.15.0-1027-gcp\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-4gpmq                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     26m\n  kube-system                 coredns-787d4945fb-4xgtd                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     26m\n  kube-system                 etcd-cncf-conformance-1-26-1                               100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         26m\n  kube-system                 kube-apiserver-cncf-conformance-1-26-1                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-controller-manager-cncf-conformance-1-26-1            200m (2%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-proxy-79fqc                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-scheduler-cncf-conformance-1-26-1                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 weave-net-wzqsl                                            100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    950m (11%)  0 (0%)\n  memory                 240Mi (0%)  340Mi (1%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 26m                kube-proxy       \n  Normal   NodeHasSufficientMemory  26m (x4 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    26m (x4 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     26m (x3 over 26m)  kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 26m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      26m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           26m                node-controller  Node cncf-conformance-1-26-1 event: Registered Node cncf-conformance-1-26-1 in Controller\n  Normal   NodeReady                26m                kubelet          Node cncf-conformance-1-26-1 status is now: NodeReady\n"
    Jan 18 22:28:19.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-288 describe namespace kubectl-288'
    Jan 18 22:28:19.265: INFO: stderr: ""
    Jan 18 22:28:19.265: INFO: stdout: "Name:         kubectl-288\nLabels:       e2e-framework=kubectl\n              e2e-run=5a657bac-996d-4529-8596-8a8788f66c01\n              kubernetes.io/metadata.name=kubectl-288\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-288" for this suite. 01/18/23 22:28:19.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:19.275
Jan 18 22:28:19.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:28:19.276
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:19.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:19.291
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 18 22:28:19.307: INFO: created pod pod-service-account-defaultsa
Jan 18 22:28:19.307: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 22:28:19.314: INFO: created pod pod-service-account-mountsa
Jan 18 22:28:19.314: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 22:28:19.320: INFO: created pod pod-service-account-nomountsa
Jan 18 22:28:19.320: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 22:28:19.329: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 22:28:19.329: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 22:28:19.335: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 22:28:19.335: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 22:28:19.342: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 22:28:19.342: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 22:28:19.349: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 22:28:19.349: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 22:28:19.354: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 22:28:19.354: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 22:28:19.358: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 22:28:19.358: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4936" for this suite. 01/18/23 22:28:19.364
------------------------------
â€¢ [0.104 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:19.275
    Jan 18 22:28:19.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:28:19.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:19.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:19.291
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 18 22:28:19.307: INFO: created pod pod-service-account-defaultsa
    Jan 18 22:28:19.307: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 18 22:28:19.314: INFO: created pod pod-service-account-mountsa
    Jan 18 22:28:19.314: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 18 22:28:19.320: INFO: created pod pod-service-account-nomountsa
    Jan 18 22:28:19.320: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 18 22:28:19.329: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 18 22:28:19.329: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 18 22:28:19.335: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 18 22:28:19.335: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 18 22:28:19.342: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 18 22:28:19.342: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 18 22:28:19.349: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 18 22:28:19.349: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 18 22:28:19.354: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 18 22:28:19.354: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 18 22:28:19.358: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 18 22:28:19.358: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4936" for this suite. 01/18/23 22:28:19.364
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:19.379
Jan 18 22:28:19.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:28:19.38
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:19.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:19.407
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-5083 01/18/23 22:28:19.41
STEP: creating service affinity-nodeport-transition in namespace services-5083 01/18/23 22:28:19.41
STEP: creating replication controller affinity-nodeport-transition in namespace services-5083 01/18/23 22:28:19.43
I0118 22:28:19.436781      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5083, replica count: 3
I0118 22:28:22.488365      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:28:25.488634      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:28:28.489657      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:28:28.496: INFO: Creating new exec pod
Jan 18 22:28:28.502: INFO: Waiting up to 5m0s for pod "execpod-affinity46wqh" in namespace "services-5083" to be "running"
Jan 18 22:28:28.505: INFO: Pod "execpod-affinity46wqh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25636ms
Jan 18 22:28:30.508: INFO: Pod "execpod-affinity46wqh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005485478s
Jan 18 22:28:32.508: INFO: Pod "execpod-affinity46wqh": Phase="Running", Reason="", readiness=true. Elapsed: 4.005118167s
Jan 18 22:28:32.508: INFO: Pod "execpod-affinity46wqh" satisfied condition "running"
Jan 18 22:28:33.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 18 22:28:33.651: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 22:28:33.652: INFO: stdout: ""
Jan 18 22:28:33.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.96.3.17 80'
Jan 18 22:28:33.788: INFO: stderr: "+ nc -v -z -w 2 10.96.3.17 80\nConnection to 10.96.3.17 80 port [tcp/http] succeeded!\n"
Jan 18 22:28:33.788: INFO: stdout: ""
Jan 18 22:28:33.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 31309'
Jan 18 22:28:33.926: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 31309\nConnection to 10.128.15.198 31309 port [tcp/*] succeeded!\n"
Jan 18 22:28:33.927: INFO: stdout: ""
Jan 18 22:28:33.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 31309'
Jan 18 22:28:34.059: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 31309\nConnection to 10.128.15.199 31309 port [tcp/*] succeeded!\n"
Jan 18 22:28:34.059: INFO: stdout: ""
Jan 18 22:28:34.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:31309/ ; done'
Jan 18 22:28:34.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n"
Jan 18 22:28:34.284: INFO: stdout: "\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s"
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
Jan 18 22:28:34.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:31309/ ; done'
Jan 18 22:28:34.517: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n"
Jan 18 22:28:34.517: INFO: stdout: "\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw"
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.518: INFO: Received response from host: affinity-nodeport-transition-swmtw
Jan 18 22:28:34.518: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5083, will wait for the garbage collector to delete the pods 01/18/23 22:28:34.527
Jan 18 22:28:34.585: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.081416ms
Jan 18 22:28:34.686: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.320539ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:36.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5083" for this suite. 01/18/23 22:28:36.809
------------------------------
â€¢ [SLOW TEST] [17.444 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:19.379
    Jan 18 22:28:19.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:28:19.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:19.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:19.407
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-5083 01/18/23 22:28:19.41
    STEP: creating service affinity-nodeport-transition in namespace services-5083 01/18/23 22:28:19.41
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5083 01/18/23 22:28:19.43
    I0118 22:28:19.436781      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5083, replica count: 3
    I0118 22:28:22.488365      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 22:28:25.488634      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 22:28:28.489657      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:28:28.496: INFO: Creating new exec pod
    Jan 18 22:28:28.502: INFO: Waiting up to 5m0s for pod "execpod-affinity46wqh" in namespace "services-5083" to be "running"
    Jan 18 22:28:28.505: INFO: Pod "execpod-affinity46wqh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25636ms
    Jan 18 22:28:30.508: INFO: Pod "execpod-affinity46wqh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005485478s
    Jan 18 22:28:32.508: INFO: Pod "execpod-affinity46wqh": Phase="Running", Reason="", readiness=true. Elapsed: 4.005118167s
    Jan 18 22:28:32.508: INFO: Pod "execpod-affinity46wqh" satisfied condition "running"
    Jan 18 22:28:33.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 18 22:28:33.651: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 22:28:33.652: INFO: stdout: ""
    Jan 18 22:28:33.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.96.3.17 80'
    Jan 18 22:28:33.788: INFO: stderr: "+ nc -v -z -w 2 10.96.3.17 80\nConnection to 10.96.3.17 80 port [tcp/http] succeeded!\n"
    Jan 18 22:28:33.788: INFO: stdout: ""
    Jan 18 22:28:33.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 31309'
    Jan 18 22:28:33.926: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 31309\nConnection to 10.128.15.198 31309 port [tcp/*] succeeded!\n"
    Jan 18 22:28:33.927: INFO: stdout: ""
    Jan 18 22:28:33.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 31309'
    Jan 18 22:28:34.059: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 31309\nConnection to 10.128.15.199 31309 port [tcp/*] succeeded!\n"
    Jan 18 22:28:34.059: INFO: stdout: ""
    Jan 18 22:28:34.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:31309/ ; done'
    Jan 18 22:28:34.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n"
    Jan 18 22:28:34.284: INFO: stdout: "\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s\naffinity-nodeport-transition-p9f4b\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-bzt8s"
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-p9f4b
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.284: INFO: Received response from host: affinity-nodeport-transition-bzt8s
    Jan 18 22:28:34.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5083 exec execpod-affinity46wqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:31309/ ; done'
    Jan 18 22:28:34.517: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:31309/\n"
    Jan 18 22:28:34.517: INFO: stdout: "\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw\naffinity-nodeport-transition-swmtw"
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.517: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.518: INFO: Received response from host: affinity-nodeport-transition-swmtw
    Jan 18 22:28:34.518: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5083, will wait for the garbage collector to delete the pods 01/18/23 22:28:34.527
    Jan 18 22:28:34.585: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.081416ms
    Jan 18 22:28:34.686: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.320539ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:36.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5083" for this suite. 01/18/23 22:28:36.809
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:36.823
Jan 18 22:28:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:28:36.824
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:36.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:36.841
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 18 22:28:36.847: INFO: Got root ca configmap in namespace "svcaccounts-3318"
Jan 18 22:28:36.851: INFO: Deleted root ca configmap in namespace "svcaccounts-3318"
STEP: waiting for a new root ca configmap created 01/18/23 22:28:37.352
Jan 18 22:28:37.355: INFO: Recreated root ca configmap in namespace "svcaccounts-3318"
Jan 18 22:28:37.360: INFO: Updated root ca configmap in namespace "svcaccounts-3318"
STEP: waiting for the root ca configmap reconciled 01/18/23 22:28:37.86
Jan 18 22:28:37.863: INFO: Reconciled root ca configmap in namespace "svcaccounts-3318"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3318" for this suite. 01/18/23 22:28:37.866
------------------------------
â€¢ [1.049 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:36.823
    Jan 18 22:28:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:28:36.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:36.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:36.841
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 18 22:28:36.847: INFO: Got root ca configmap in namespace "svcaccounts-3318"
    Jan 18 22:28:36.851: INFO: Deleted root ca configmap in namespace "svcaccounts-3318"
    STEP: waiting for a new root ca configmap created 01/18/23 22:28:37.352
    Jan 18 22:28:37.355: INFO: Recreated root ca configmap in namespace "svcaccounts-3318"
    Jan 18 22:28:37.360: INFO: Updated root ca configmap in namespace "svcaccounts-3318"
    STEP: waiting for the root ca configmap reconciled 01/18/23 22:28:37.86
    Jan 18 22:28:37.863: INFO: Reconciled root ca configmap in namespace "svcaccounts-3318"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3318" for this suite. 01/18/23 22:28:37.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:37.872
Jan 18 22:28:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:28:37.873
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:37.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:37.886
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/18/23 22:28:37.888
Jan 18 22:28:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 create -f -'
Jan 18 22:28:38.078: INFO: stderr: ""
Jan 18 22:28:38.078: INFO: stdout: "pod/pause created\n"
Jan 18 22:28:38.078: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 22:28:38.078: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9931" to be "running and ready"
Jan 18 22:28:38.081: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598664ms
Jan 18 22:28:38.081: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-conformance-1-26-2' to be 'Running' but was 'Pending'
Jan 18 22:28:40.085: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006472646s
Jan 18 22:28:40.085: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 22:28:40.085: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 22:28:40.085
Jan 18 22:28:40.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 label pods pause testing-label=testing-label-value'
Jan 18 22:28:40.164: INFO: stderr: ""
Jan 18 22:28:40.164: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 22:28:40.164
Jan 18 22:28:40.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pod pause -L testing-label'
Jan 18 22:28:40.230: INFO: stderr: ""
Jan 18 22:28:40.230: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/18/23 22:28:40.23
Jan 18 22:28:40.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 label pods pause testing-label-'
Jan 18 22:28:40.310: INFO: stderr: ""
Jan 18 22:28:40.310: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/18/23 22:28:40.31
Jan 18 22:28:40.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pod pause -L testing-label'
Jan 18 22:28:40.379: INFO: stderr: ""
Jan 18 22:28:40.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/18/23 22:28:40.379
Jan 18 22:28:40.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 delete --grace-period=0 --force -f -'
Jan 18 22:28:40.457: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:28:40.457: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 22:28:40.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get rc,svc -l name=pause --no-headers'
Jan 18 22:28:40.533: INFO: stderr: "No resources found in kubectl-9931 namespace.\n"
Jan 18 22:28:40.533: INFO: stdout: ""
Jan 18 22:28:40.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 22:28:40.605: INFO: stderr: ""
Jan 18 22:28:40.605: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:40.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9931" for this suite. 01/18/23 22:28:40.609
------------------------------
â€¢ [2.741 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:37.872
    Jan 18 22:28:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:28:37.873
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:37.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:37.886
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/18/23 22:28:37.888
    Jan 18 22:28:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 create -f -'
    Jan 18 22:28:38.078: INFO: stderr: ""
    Jan 18 22:28:38.078: INFO: stdout: "pod/pause created\n"
    Jan 18 22:28:38.078: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 18 22:28:38.078: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9931" to be "running and ready"
    Jan 18 22:28:38.081: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598664ms
    Jan 18 22:28:38.081: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-conformance-1-26-2' to be 'Running' but was 'Pending'
    Jan 18 22:28:40.085: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006472646s
    Jan 18 22:28:40.085: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 18 22:28:40.085: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 22:28:40.085
    Jan 18 22:28:40.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 label pods pause testing-label=testing-label-value'
    Jan 18 22:28:40.164: INFO: stderr: ""
    Jan 18 22:28:40.164: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 22:28:40.164
    Jan 18 22:28:40.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pod pause -L testing-label'
    Jan 18 22:28:40.230: INFO: stderr: ""
    Jan 18 22:28:40.230: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/18/23 22:28:40.23
    Jan 18 22:28:40.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 label pods pause testing-label-'
    Jan 18 22:28:40.310: INFO: stderr: ""
    Jan 18 22:28:40.310: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/18/23 22:28:40.31
    Jan 18 22:28:40.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pod pause -L testing-label'
    Jan 18 22:28:40.379: INFO: stderr: ""
    Jan 18 22:28:40.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/18/23 22:28:40.379
    Jan 18 22:28:40.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 delete --grace-period=0 --force -f -'
    Jan 18 22:28:40.457: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:28:40.457: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 18 22:28:40.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get rc,svc -l name=pause --no-headers'
    Jan 18 22:28:40.533: INFO: stderr: "No resources found in kubectl-9931 namespace.\n"
    Jan 18 22:28:40.533: INFO: stdout: ""
    Jan 18 22:28:40.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9931 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 22:28:40.605: INFO: stderr: ""
    Jan 18 22:28:40.605: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:40.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9931" for this suite. 01/18/23 22:28:40.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:40.615
Jan 18 22:28:40.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:28:40.616
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:40.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:40.63
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:28:40.633
Jan 18 22:28:40.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d" in namespace "downward-api-6593" to be "Succeeded or Failed"
Jan 18 22:28:40.645: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.801473ms
Jan 18 22:28:42.649: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006260527s
Jan 18 22:28:44.650: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007614918s
STEP: Saw pod success 01/18/23 22:28:44.65
Jan 18 22:28:44.650: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d" satisfied condition "Succeeded or Failed"
Jan 18 22:28:44.653: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d container client-container: <nil>
STEP: delete the pod 01/18/23 22:28:44.658
Jan 18 22:28:44.668: INFO: Waiting for pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d to disappear
Jan 18 22:28:44.671: INFO: Pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:28:44.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6593" for this suite. 01/18/23 22:28:44.674
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:40.615
    Jan 18 22:28:40.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:28:40.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:40.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:40.63
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:28:40.633
    Jan 18 22:28:40.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d" in namespace "downward-api-6593" to be "Succeeded or Failed"
    Jan 18 22:28:40.645: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.801473ms
    Jan 18 22:28:42.649: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006260527s
    Jan 18 22:28:44.650: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007614918s
    STEP: Saw pod success 01/18/23 22:28:44.65
    Jan 18 22:28:44.650: INFO: Pod "downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d" satisfied condition "Succeeded or Failed"
    Jan 18 22:28:44.653: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d container client-container: <nil>
    STEP: delete the pod 01/18/23 22:28:44.658
    Jan 18 22:28:44.668: INFO: Waiting for pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d to disappear
    Jan 18 22:28:44.671: INFO: Pod downwardapi-volume-d9c7b247-d839-47a9-a98c-7f15e36e179d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:28:44.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6593" for this suite. 01/18/23 22:28:44.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:28:44.682
Jan 18 22:28:44.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:28:44.683
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:44.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:44.696
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/18/23 22:28:44.699
STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:28:44.705
STEP: Creating a ResourceQuota with not best effort scope 01/18/23 22:28:46.709
STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:28:46.714
STEP: Creating a best-effort pod 01/18/23 22:28:48.718
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 22:28:48.729
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 22:28:50.732
STEP: Deleting the pod 01/18/23 22:28:52.735
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:28:52.748
STEP: Creating a not best-effort pod 01/18/23 22:28:54.751
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 22:28:54.76
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 22:28:56.764
STEP: Deleting the pod 01/18/23 22:28:58.767
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:28:58.775
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:29:00.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8455" for this suite. 01/18/23 22:29:00.781
------------------------------
â€¢ [SLOW TEST] [16.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:28:44.682
    Jan 18 22:28:44.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:28:44.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:44.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:44.696
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/18/23 22:28:44.699
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:28:44.705
    STEP: Creating a ResourceQuota with not best effort scope 01/18/23 22:28:46.709
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 22:28:46.714
    STEP: Creating a best-effort pod 01/18/23 22:28:48.718
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 22:28:48.729
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 22:28:50.732
    STEP: Deleting the pod 01/18/23 22:28:52.735
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:28:52.748
    STEP: Creating a not best-effort pod 01/18/23 22:28:54.751
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 22:28:54.76
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 22:28:56.764
    STEP: Deleting the pod 01/18/23 22:28:58.767
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:28:58.775
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:29:00.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8455" for this suite. 01/18/23 22:29:00.781
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:29:00.787
Jan 18 22:29:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename aggregator 01/18/23 22:29:00.788
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:00.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:00.804
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 18 22:29:00.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/18/23 22:29:00.807
Jan 18 22:29:01.468: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 18 22:29:03.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:05.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:07.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:09.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:11.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:13.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:15.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:17.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:19.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:21.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:23.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:29:25.635: INFO: Waited 119.496949ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 22:29:25.679
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 22:29:25.683
STEP: List APIServices 01/18/23 22:29:25.688
Jan 18 22:29:25.693: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 18 22:29:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7202" for this suite. 01/18/23 22:29:25.823
------------------------------
â€¢ [SLOW TEST] [25.089 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:29:00.787
    Jan 18 22:29:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename aggregator 01/18/23 22:29:00.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:00.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:00.804
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 18 22:29:00.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/18/23 22:29:00.807
    Jan 18 22:29:01.468: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 18 22:29:03.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:05.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:07.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:09.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:11.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:13.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:15.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:17.510: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:19.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:21.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:23.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:29:25.635: INFO: Waited 119.496949ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 22:29:25.679
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 22:29:25.683
    STEP: List APIServices 01/18/23 22:29:25.688
    Jan 18 22:29:25.693: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:29:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7202" for this suite. 01/18/23 22:29:25.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:29:25.876
Jan 18 22:29:25.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:29:25.877
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:25.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:25.892
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 22:29:25.907: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:30:25.925: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:30:25.928
Jan 18 22:30:25.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:30:25.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:25.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:25.941
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/18/23 22:30:25.944
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:30:25.944
Jan 18 22:30:25.953: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8516" to be "running"
Jan 18 22:30:25.956: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706235ms
Jan 18 22:30:27.959: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006196944s
Jan 18 22:30:27.959: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:30:27.962
Jan 18 22:30:27.972: INFO: found a healthy node: cncf-conformance-1-26-2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan 18 22:30:38.038: INFO: pods created so far: [1 1 1]
Jan 18 22:30:38.039: INFO: length of pods created so far: 3
Jan 18 22:30:40.047: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:30:47.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:30:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8516" for this suite. 01/18/23 22:30:47.118
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1114" for this suite. 01/18/23 22:30:47.124
------------------------------
â€¢ [SLOW TEST] [81.254 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:29:25.876
    Jan 18 22:29:25.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:29:25.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:25.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:25.892
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 22:29:25.907: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:30:25.925: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:30:25.928
    Jan 18 22:30:25.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:30:25.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:25.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:25.941
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/18/23 22:30:25.944
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:30:25.944
    Jan 18 22:30:25.953: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8516" to be "running"
    Jan 18 22:30:25.956: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706235ms
    Jan 18 22:30:27.959: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006196944s
    Jan 18 22:30:27.959: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:30:27.962
    Jan 18 22:30:27.972: INFO: found a healthy node: cncf-conformance-1-26-2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan 18 22:30:38.038: INFO: pods created so far: [1 1 1]
    Jan 18 22:30:38.039: INFO: length of pods created so far: 3
    Jan 18 22:30:40.047: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:30:47.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:30:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8516" for this suite. 01/18/23 22:30:47.118
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1114" for this suite. 01/18/23 22:30:47.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:30:47.131
Jan 18 22:30:47.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 22:30:47.132
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:47.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:47.146
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 18 22:30:47.149: INFO: Creating ReplicaSet my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91
Jan 18 22:30:47.158: INFO: Pod name my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Found 0 pods out of 1
Jan 18 22:30:52.165: INFO: Pod name my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Found 1 pods out of 1
Jan 18 22:30:52.165: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91" is running
Jan 18 22:30:52.165: INFO: Waiting up to 5m0s for pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" in namespace "replicaset-1875" to be "running"
Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6": Phase="Running", Reason="", readiness=true. Elapsed: 2.296908ms
Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" satisfied condition "running"
Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:}])
Jan 18 22:30:52.167: INFO: Trying to dial the pod
Jan 18 22:30:57.178: INFO: Controller my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Got expected result from replica 1 [my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6]: "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:30:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1875" for this suite. 01/18/23 22:30:57.182
------------------------------
â€¢ [SLOW TEST] [10.055 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:30:47.131
    Jan 18 22:30:47.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:30:47.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:47.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:47.146
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 18 22:30:47.149: INFO: Creating ReplicaSet my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91
    Jan 18 22:30:47.158: INFO: Pod name my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Found 0 pods out of 1
    Jan 18 22:30:52.165: INFO: Pod name my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Found 1 pods out of 1
    Jan 18 22:30:52.165: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91" is running
    Jan 18 22:30:52.165: INFO: Waiting up to 5m0s for pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" in namespace "replicaset-1875" to be "running"
    Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6": Phase="Running", Reason="", readiness=true. Elapsed: 2.296908ms
    Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" satisfied condition "running"
    Jan 18 22:30:52.167: INFO: Pod "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:30:47 +0000 UTC Reason: Message:}])
    Jan 18 22:30:52.167: INFO: Trying to dial the pod
    Jan 18 22:30:57.178: INFO: Controller my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91: Got expected result from replica 1 [my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6]: "my-hostname-basic-11cdfef4-6b76-4f12-a78d-0b0bd4a62e91-jl7l6", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:30:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1875" for this suite. 01/18/23 22:30:57.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:30:57.188
Jan 18 22:30:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:30:57.189
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:57.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:57.205
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 22:30:57.208
Jan 18 22:30:57.217: INFO: Waiting up to 5m0s for pod "pod-fbb1008a-58fe-418a-a905-810330f962ec" in namespace "emptydir-60" to be "Succeeded or Failed"
Jan 18 22:30:57.219: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514ms
Jan 18 22:30:59.223: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730638s
Jan 18 22:31:01.223: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006928563s
STEP: Saw pod success 01/18/23 22:31:01.224
Jan 18 22:31:01.224: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec" satisfied condition "Succeeded or Failed"
Jan 18 22:31:01.226: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-fbb1008a-58fe-418a-a905-810330f962ec container test-container: <nil>
STEP: delete the pod 01/18/23 22:31:01.238
Jan 18 22:31:01.247: INFO: Waiting for pod pod-fbb1008a-58fe-418a-a905-810330f962ec to disappear
Jan 18 22:31:01.249: INFO: Pod pod-fbb1008a-58fe-418a-a905-810330f962ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:01.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-60" for this suite. 01/18/23 22:31:01.252
------------------------------
â€¢ [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:30:57.188
    Jan 18 22:30:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:30:57.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:57.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:57.205
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 22:30:57.208
    Jan 18 22:30:57.217: INFO: Waiting up to 5m0s for pod "pod-fbb1008a-58fe-418a-a905-810330f962ec" in namespace "emptydir-60" to be "Succeeded or Failed"
    Jan 18 22:30:57.219: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514ms
    Jan 18 22:30:59.223: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730638s
    Jan 18 22:31:01.223: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006928563s
    STEP: Saw pod success 01/18/23 22:31:01.224
    Jan 18 22:31:01.224: INFO: Pod "pod-fbb1008a-58fe-418a-a905-810330f962ec" satisfied condition "Succeeded or Failed"
    Jan 18 22:31:01.226: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-fbb1008a-58fe-418a-a905-810330f962ec container test-container: <nil>
    STEP: delete the pod 01/18/23 22:31:01.238
    Jan 18 22:31:01.247: INFO: Waiting for pod pod-fbb1008a-58fe-418a-a905-810330f962ec to disappear
    Jan 18 22:31:01.249: INFO: Pod pod-fbb1008a-58fe-418a-a905-810330f962ec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:01.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-60" for this suite. 01/18/23 22:31:01.252
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:01.257
Jan 18 22:31:01.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename subpath 01/18/23 22:31:01.258
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:01.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:01.27
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:31:01.273
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-2gbb 01/18/23 22:31:01.281
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:31:01.281
Jan 18 22:31:01.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2gbb" in namespace "subpath-6528" to be "Succeeded or Failed"
Jan 18 22:31:01.293: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466927ms
Jan 18 22:31:03.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005592243s
Jan 18 22:31:05.298: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007733209s
Jan 18 22:31:07.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 6.006434572s
Jan 18 22:31:09.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 8.00722359s
Jan 18 22:31:11.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 10.006543741s
Jan 18 22:31:13.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.006032261s
Jan 18 22:31:15.298: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 14.007376569s
Jan 18 22:31:17.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 16.007228173s
Jan 18 22:31:19.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 18.006646829s
Jan 18 22:31:21.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 20.006723393s
Jan 18 22:31:23.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=false. Elapsed: 22.005846517s
Jan 18 22:31:25.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007296918s
STEP: Saw pod success 01/18/23 22:31:25.298
Jan 18 22:31:25.298: INFO: Pod "pod-subpath-test-configmap-2gbb" satisfied condition "Succeeded or Failed"
Jan 18 22:31:25.301: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-configmap-2gbb container test-container-subpath-configmap-2gbb: <nil>
STEP: delete the pod 01/18/23 22:31:25.306
Jan 18 22:31:25.318: INFO: Waiting for pod pod-subpath-test-configmap-2gbb to disappear
Jan 18 22:31:25.320: INFO: Pod pod-subpath-test-configmap-2gbb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2gbb 01/18/23 22:31:25.32
Jan 18 22:31:25.320: INFO: Deleting pod "pod-subpath-test-configmap-2gbb" in namespace "subpath-6528"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:25.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6528" for this suite. 01/18/23 22:31:25.325
------------------------------
â€¢ [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:01.257
    Jan 18 22:31:01.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename subpath 01/18/23 22:31:01.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:01.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:01.27
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:31:01.273
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-2gbb 01/18/23 22:31:01.281
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:31:01.281
    Jan 18 22:31:01.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2gbb" in namespace "subpath-6528" to be "Succeeded or Failed"
    Jan 18 22:31:01.293: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466927ms
    Jan 18 22:31:03.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005592243s
    Jan 18 22:31:05.298: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007733209s
    Jan 18 22:31:07.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 6.006434572s
    Jan 18 22:31:09.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 8.00722359s
    Jan 18 22:31:11.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 10.006543741s
    Jan 18 22:31:13.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.006032261s
    Jan 18 22:31:15.298: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 14.007376569s
    Jan 18 22:31:17.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 16.007228173s
    Jan 18 22:31:19.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 18.006646829s
    Jan 18 22:31:21.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=true. Elapsed: 20.006723393s
    Jan 18 22:31:23.296: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Running", Reason="", readiness=false. Elapsed: 22.005846517s
    Jan 18 22:31:25.297: INFO: Pod "pod-subpath-test-configmap-2gbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007296918s
    STEP: Saw pod success 01/18/23 22:31:25.298
    Jan 18 22:31:25.298: INFO: Pod "pod-subpath-test-configmap-2gbb" satisfied condition "Succeeded or Failed"
    Jan 18 22:31:25.301: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-configmap-2gbb container test-container-subpath-configmap-2gbb: <nil>
    STEP: delete the pod 01/18/23 22:31:25.306
    Jan 18 22:31:25.318: INFO: Waiting for pod pod-subpath-test-configmap-2gbb to disappear
    Jan 18 22:31:25.320: INFO: Pod pod-subpath-test-configmap-2gbb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-2gbb 01/18/23 22:31:25.32
    Jan 18 22:31:25.320: INFO: Deleting pod "pod-subpath-test-configmap-2gbb" in namespace "subpath-6528"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:25.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6528" for this suite. 01/18/23 22:31:25.325
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:25.331
Jan 18 22:31:25.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 22:31:25.332
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:25.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:25.348
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:31:25.351
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:31:25.351
STEP: creating a pod to probe DNS 01/18/23 22:31:25.351
STEP: submitting the pod to kubernetes 01/18/23 22:31:25.351
Jan 18 22:31:25.363: INFO: Waiting up to 15m0s for pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022" in namespace "dns-7752" to be "running"
Jan 18 22:31:25.366: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928893ms
Jan 18 22:31:27.369: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022": Phase="Running", Reason="", readiness=true. Elapsed: 2.006340784s
Jan 18 22:31:27.369: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:31:27.369
STEP: looking for the results for each expected name from probers 01/18/23 22:31:27.372
Jan 18 22:31:27.386: INFO: DNS probes using dns-7752/dns-test-fccbe809-85a8-419c-8553-f3854c953022 succeeded

STEP: deleting the pod 01/18/23 22:31:27.386
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:27.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7752" for this suite. 01/18/23 22:31:27.402
------------------------------
â€¢ [2.076 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:25.331
    Jan 18 22:31:25.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 22:31:25.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:25.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:25.348
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:31:25.351
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:31:25.351
    STEP: creating a pod to probe DNS 01/18/23 22:31:25.351
    STEP: submitting the pod to kubernetes 01/18/23 22:31:25.351
    Jan 18 22:31:25.363: INFO: Waiting up to 15m0s for pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022" in namespace "dns-7752" to be "running"
    Jan 18 22:31:25.366: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928893ms
    Jan 18 22:31:27.369: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022": Phase="Running", Reason="", readiness=true. Elapsed: 2.006340784s
    Jan 18 22:31:27.369: INFO: Pod "dns-test-fccbe809-85a8-419c-8553-f3854c953022" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:31:27.369
    STEP: looking for the results for each expected name from probers 01/18/23 22:31:27.372
    Jan 18 22:31:27.386: INFO: DNS probes using dns-7752/dns-test-fccbe809-85a8-419c-8553-f3854c953022 succeeded

    STEP: deleting the pod 01/18/23 22:31:27.386
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:27.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7752" for this suite. 01/18/23 22:31:27.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:27.409
Jan 18 22:31:27.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:31:27.41
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:27.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:27.423
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/18/23 22:31:27.425
STEP: wait for the container to reach Failed 01/18/23 22:31:27.434
STEP: get the container status 01/18/23 22:31:31.451
STEP: the container should be terminated 01/18/23 22:31:31.453
STEP: the termination message should be set 01/18/23 22:31:31.453
Jan 18 22:31:31.453: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 22:31:31.453
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8977" for this suite. 01/18/23 22:31:31.469
------------------------------
â€¢ [4.065 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:27.409
    Jan 18 22:31:27.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:31:27.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:27.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:27.423
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/18/23 22:31:27.425
    STEP: wait for the container to reach Failed 01/18/23 22:31:27.434
    STEP: get the container status 01/18/23 22:31:31.451
    STEP: the container should be terminated 01/18/23 22:31:31.453
    STEP: the termination message should be set 01/18/23 22:31:31.453
    Jan 18 22:31:31.453: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 22:31:31.453
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8977" for this suite. 01/18/23 22:31:31.469
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:31.474
Jan 18 22:31:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:31:31.476
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:31.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:31.492
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2240 01/18/23 22:31:31.495
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/18/23 22:31:31.499
Jan 18 22:31:31.509: INFO: Found 0 stateful pods, waiting for 3
Jan 18 22:31:41.513: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:31:41.513: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:31:41.513: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:31:41.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:31:41.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:31:41.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:31:41.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 22:31:51.681
Jan 18 22:31:51.701: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 22:31:51.701
STEP: Updating Pods in reverse ordinal order 01/18/23 22:32:01.718
Jan 18 22:32:01.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:32:01.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:32:01.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:32:01.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/18/23 22:32:21.877
Jan 18 22:32:21.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:32:22.023: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:32:22.023: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:32:22.023: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:32:32.060: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/18/23 22:32:42.076
Jan 18 22:32:42.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:32:42.209: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:32:42.209: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:32:42.209: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:32:52.229: INFO: Deleting all statefulset in ns statefulset-2240
Jan 18 22:32:52.231: INFO: Scaling statefulset ss2 to 0
Jan 18 22:33:02.245: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:33:02.248: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:02.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2240" for this suite. 01/18/23 22:33:02.263
------------------------------
â€¢ [SLOW TEST] [90.793 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:31.474
    Jan 18 22:31:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:31:31.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:31.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:31.492
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2240 01/18/23 22:31:31.495
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/18/23 22:31:31.499
    Jan 18 22:31:31.509: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 22:31:41.513: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:31:41.513: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:31:41.513: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:31:41.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:31:41.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:31:41.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:31:41.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 22:31:51.681
    Jan 18 22:31:51.701: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 22:31:51.701
    STEP: Updating Pods in reverse ordinal order 01/18/23 22:32:01.718
    Jan 18 22:32:01.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:32:01.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:32:01.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:32:01.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/18/23 22:32:21.877
    Jan 18 22:32:21.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:32:22.023: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:32:22.023: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:32:22.023: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:32:32.060: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/18/23 22:32:42.076
    Jan 18 22:32:42.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-2240 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:32:42.209: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:32:42.209: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:32:42.209: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:32:52.229: INFO: Deleting all statefulset in ns statefulset-2240
    Jan 18 22:32:52.231: INFO: Scaling statefulset ss2 to 0
    Jan 18 22:33:02.245: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:33:02.248: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:02.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2240" for this suite. 01/18/23 22:33:02.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:02.271
Jan 18 22:33:02.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:33:02.272
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:02.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:02.285
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2255-delete-me 01/18/23 22:33:02.292
STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:33:02.296
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:02.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2255" for this suite. 01/18/23 22:33:02.307
------------------------------
â€¢ [0.041 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:02.271
    Jan 18 22:33:02.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:33:02.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:02.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:02.285
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2255-delete-me 01/18/23 22:33:02.292
    STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:33:02.296
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:02.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2255" for this suite. 01/18/23 22:33:02.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:02.313
Jan 18 22:33:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:33:02.314
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:02.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:02.327
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-f870deef-f18e-4708-b748-625adb899480 01/18/23 22:33:02.33
STEP: Creating a pod to test consume configMaps 01/18/23 22:33:02.334
Jan 18 22:33:02.343: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319" in namespace "projected-4979" to be "Succeeded or Failed"
Jan 18 22:33:02.345: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602852ms
Jan 18 22:33:04.349: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00661003s
Jan 18 22:33:06.350: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007827574s
STEP: Saw pod success 01/18/23 22:33:06.35
Jan 18 22:33:06.351: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319" satisfied condition "Succeeded or Failed"
Jan 18 22:33:06.353: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 22:33:06.367
Jan 18 22:33:06.379: INFO: Waiting for pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 to disappear
Jan 18 22:33:06.382: INFO: Pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:06.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4979" for this suite. 01/18/23 22:33:06.385
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:02.313
    Jan 18 22:33:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:33:02.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:02.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:02.327
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-f870deef-f18e-4708-b748-625adb899480 01/18/23 22:33:02.33
    STEP: Creating a pod to test consume configMaps 01/18/23 22:33:02.334
    Jan 18 22:33:02.343: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319" in namespace "projected-4979" to be "Succeeded or Failed"
    Jan 18 22:33:02.345: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602852ms
    Jan 18 22:33:04.349: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00661003s
    Jan 18 22:33:06.350: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007827574s
    STEP: Saw pod success 01/18/23 22:33:06.35
    Jan 18 22:33:06.351: INFO: Pod "pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319" satisfied condition "Succeeded or Failed"
    Jan 18 22:33:06.353: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:33:06.367
    Jan 18 22:33:06.379: INFO: Waiting for pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 to disappear
    Jan 18 22:33:06.382: INFO: Pod pod-projected-configmaps-494e69d5-f898-4586-87d6-12bbe954c319 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:06.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4979" for this suite. 01/18/23 22:33:06.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:06.391
Jan 18 22:33:06.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:33:06.392
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.409
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 in namespace container-probe-7952 01/18/23 22:33:06.413
Jan 18 22:33:06.419: INFO: Waiting up to 5m0s for pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53" in namespace "container-probe-7952" to be "not pending"
Jan 18 22:33:06.422: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434467ms
Jan 18 22:33:08.426: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53": Phase="Running", Reason="", readiness=true. Elapsed: 2.006642204s
Jan 18 22:33:08.426: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53" satisfied condition "not pending"
Jan 18 22:33:08.426: INFO: Started pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 in namespace container-probe-7952
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:33:08.426
Jan 18 22:33:08.429: INFO: Initial restart count of pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 is 0
STEP: deleting the pod 01/18/23 22:37:08.908
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:08.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7952" for this suite. 01/18/23 22:37:08.923
------------------------------
â€¢ [SLOW TEST] [242.539 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:06.391
    Jan 18 22:33:06.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:33:06.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.409
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 in namespace container-probe-7952 01/18/23 22:33:06.413
    Jan 18 22:33:06.419: INFO: Waiting up to 5m0s for pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53" in namespace "container-probe-7952" to be "not pending"
    Jan 18 22:33:06.422: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434467ms
    Jan 18 22:33:08.426: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53": Phase="Running", Reason="", readiness=true. Elapsed: 2.006642204s
    Jan 18 22:33:08.426: INFO: Pod "test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53" satisfied condition "not pending"
    Jan 18 22:33:08.426: INFO: Started pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 in namespace container-probe-7952
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:33:08.426
    Jan 18 22:33:08.429: INFO: Initial restart count of pod test-webserver-5b206c7f-9300-4df9-b662-1cfaea09ac53 is 0
    STEP: deleting the pod 01/18/23 22:37:08.908
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:08.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7952" for this suite. 01/18/23 22:37:08.923
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:08.93
Jan 18 22:37:08.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename discovery 01/18/23 22:37:08.931
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:08.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:08.945
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/18/23 22:37:08.949
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 18 22:37:09.668: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 22:37:09.669: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 22:37:09.669: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 22:37:09.669: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 22:37:09.669: INFO: Checking APIGroup: apps
Jan 18 22:37:09.670: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 22:37:09.670: INFO: Versions found [{apps/v1 v1}]
Jan 18 22:37:09.670: INFO: apps/v1 matches apps/v1
Jan 18 22:37:09.670: INFO: Checking APIGroup: events.k8s.io
Jan 18 22:37:09.671: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 22:37:09.671: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 18 22:37:09.671: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 22:37:09.671: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 22:37:09.672: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 22:37:09.672: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 22:37:09.672: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 22:37:09.672: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 22:37:09.673: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 22:37:09.673: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 22:37:09.673: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 22:37:09.673: INFO: Checking APIGroup: autoscaling
Jan 18 22:37:09.674: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 22:37:09.674: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 18 22:37:09.674: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 22:37:09.674: INFO: Checking APIGroup: batch
Jan 18 22:37:09.675: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 22:37:09.675: INFO: Versions found [{batch/v1 v1}]
Jan 18 22:37:09.675: INFO: batch/v1 matches batch/v1
Jan 18 22:37:09.675: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 22:37:09.676: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 22:37:09.676: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 22:37:09.676: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 22:37:09.676: INFO: Checking APIGroup: networking.k8s.io
Jan 18 22:37:09.677: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 22:37:09.677: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 18 22:37:09.677: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 22:37:09.677: INFO: Checking APIGroup: policy
Jan 18 22:37:09.678: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 22:37:09.678: INFO: Versions found [{policy/v1 v1}]
Jan 18 22:37:09.678: INFO: policy/v1 matches policy/v1
Jan 18 22:37:09.678: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 22:37:09.679: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 22:37:09.679: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 22:37:09.679: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 22:37:09.679: INFO: Checking APIGroup: storage.k8s.io
Jan 18 22:37:09.680: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 22:37:09.680: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 22:37:09.680: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 22:37:09.680: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 22:37:09.681: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 22:37:09.681: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 22:37:09.681: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 22:37:09.681: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 22:37:09.682: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 22:37:09.682: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 22:37:09.682: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 22:37:09.682: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 22:37:09.683: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 22:37:09.683: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 22:37:09.683: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 22:37:09.683: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 22:37:09.684: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 22:37:09.684: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 22:37:09.684: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 22:37:09.684: INFO: Checking APIGroup: node.k8s.io
Jan 18 22:37:09.685: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 22:37:09.685: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 18 22:37:09.685: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 22:37:09.685: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 22:37:09.686: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 22:37:09.686: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 18 22:37:09.686: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 22:37:09.686: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 22:37:09.687: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 18 22:37:09.687: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 18 22:37:09.687: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 18 22:37:09.687: INFO: Checking APIGroup: cluster.kurl.sh
Jan 18 22:37:09.689: INFO: PreferredVersion.GroupVersion: cluster.kurl.sh/v1beta1
Jan 18 22:37:09.689: INFO: Versions found [{cluster.kurl.sh/v1beta1 v1beta1}]
Jan 18 22:37:09.689: INFO: cluster.kurl.sh/v1beta1 matches cluster.kurl.sh/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:09.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2858" for this suite. 01/18/23 22:37:09.692
------------------------------
â€¢ [0.766 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:08.93
    Jan 18 22:37:08.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename discovery 01/18/23 22:37:08.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:08.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:08.945
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/18/23 22:37:08.949
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 18 22:37:09.668: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 18 22:37:09.669: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 18 22:37:09.669: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 18 22:37:09.669: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 18 22:37:09.669: INFO: Checking APIGroup: apps
    Jan 18 22:37:09.670: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 18 22:37:09.670: INFO: Versions found [{apps/v1 v1}]
    Jan 18 22:37:09.670: INFO: apps/v1 matches apps/v1
    Jan 18 22:37:09.670: INFO: Checking APIGroup: events.k8s.io
    Jan 18 22:37:09.671: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 18 22:37:09.671: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 18 22:37:09.671: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 18 22:37:09.671: INFO: Checking APIGroup: authentication.k8s.io
    Jan 18 22:37:09.672: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 18 22:37:09.672: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 18 22:37:09.672: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 18 22:37:09.672: INFO: Checking APIGroup: authorization.k8s.io
    Jan 18 22:37:09.673: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 18 22:37:09.673: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 18 22:37:09.673: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 18 22:37:09.673: INFO: Checking APIGroup: autoscaling
    Jan 18 22:37:09.674: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 18 22:37:09.674: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 18 22:37:09.674: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 18 22:37:09.674: INFO: Checking APIGroup: batch
    Jan 18 22:37:09.675: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 18 22:37:09.675: INFO: Versions found [{batch/v1 v1}]
    Jan 18 22:37:09.675: INFO: batch/v1 matches batch/v1
    Jan 18 22:37:09.675: INFO: Checking APIGroup: certificates.k8s.io
    Jan 18 22:37:09.676: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 18 22:37:09.676: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 18 22:37:09.676: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 18 22:37:09.676: INFO: Checking APIGroup: networking.k8s.io
    Jan 18 22:37:09.677: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 18 22:37:09.677: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 18 22:37:09.677: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 18 22:37:09.677: INFO: Checking APIGroup: policy
    Jan 18 22:37:09.678: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 18 22:37:09.678: INFO: Versions found [{policy/v1 v1}]
    Jan 18 22:37:09.678: INFO: policy/v1 matches policy/v1
    Jan 18 22:37:09.678: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 18 22:37:09.679: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 18 22:37:09.679: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 18 22:37:09.679: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 18 22:37:09.679: INFO: Checking APIGroup: storage.k8s.io
    Jan 18 22:37:09.680: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 18 22:37:09.680: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 22:37:09.680: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 18 22:37:09.680: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 18 22:37:09.681: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 18 22:37:09.681: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 18 22:37:09.681: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 18 22:37:09.681: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 18 22:37:09.682: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 18 22:37:09.682: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 18 22:37:09.682: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 18 22:37:09.682: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 18 22:37:09.683: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 18 22:37:09.683: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 18 22:37:09.683: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 18 22:37:09.683: INFO: Checking APIGroup: coordination.k8s.io
    Jan 18 22:37:09.684: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 18 22:37:09.684: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 18 22:37:09.684: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 18 22:37:09.684: INFO: Checking APIGroup: node.k8s.io
    Jan 18 22:37:09.685: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 18 22:37:09.685: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 18 22:37:09.685: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 18 22:37:09.685: INFO: Checking APIGroup: discovery.k8s.io
    Jan 18 22:37:09.686: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 18 22:37:09.686: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 18 22:37:09.686: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 18 22:37:09.686: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 18 22:37:09.687: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 18 22:37:09.687: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 18 22:37:09.687: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 18 22:37:09.687: INFO: Checking APIGroup: cluster.kurl.sh
    Jan 18 22:37:09.689: INFO: PreferredVersion.GroupVersion: cluster.kurl.sh/v1beta1
    Jan 18 22:37:09.689: INFO: Versions found [{cluster.kurl.sh/v1beta1 v1beta1}]
    Jan 18 22:37:09.689: INFO: cluster.kurl.sh/v1beta1 matches cluster.kurl.sh/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:09.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2858" for this suite. 01/18/23 22:37:09.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:09.697
Jan 18 22:37:09.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:37:09.698
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:09.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:09.712
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/18/23 22:37:09.715
Jan 18 22:37:09.722: INFO: Waiting up to 5m0s for pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59" in namespace "pods-4572" to be "running and ready"
Jan 18 22:37:09.724: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371628ms
Jan 18 22:37:09.724: INFO: The phase of Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:37:11.728: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006358145s
Jan 18 22:37:11.728: INFO: The phase of Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 is Running (Ready = true)
Jan 18 22:37:11.728: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59" satisfied condition "running and ready"
Jan 18 22:37:11.733: INFO: Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 has hostIP: 10.128.15.199
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:11.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4572" for this suite. 01/18/23 22:37:11.736
------------------------------
â€¢ [2.046 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:09.697
    Jan 18 22:37:09.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:37:09.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:09.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:09.712
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/18/23 22:37:09.715
    Jan 18 22:37:09.722: INFO: Waiting up to 5m0s for pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59" in namespace "pods-4572" to be "running and ready"
    Jan 18 22:37:09.724: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371628ms
    Jan 18 22:37:09.724: INFO: The phase of Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:37:11.728: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006358145s
    Jan 18 22:37:11.728: INFO: The phase of Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 is Running (Ready = true)
    Jan 18 22:37:11.728: INFO: Pod "pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59" satisfied condition "running and ready"
    Jan 18 22:37:11.733: INFO: Pod pod-hostip-5ca256b6-b238-4591-8fa7-8bbd985e9b59 has hostIP: 10.128.15.199
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:11.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4572" for this suite. 01/18/23 22:37:11.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:11.744
Jan 18 22:37:11.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename events 01/18/23 22:37:11.745
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.759
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/18/23 22:37:11.761
STEP: get a list of Events with a label in the current namespace 01/18/23 22:37:11.777
STEP: delete a list of events 01/18/23 22:37:11.779
Jan 18 22:37:11.779: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 22:37:11.796
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:11.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7390" for this suite. 01/18/23 22:37:11.801
------------------------------
â€¢ [0.063 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:11.744
    Jan 18 22:37:11.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename events 01/18/23 22:37:11.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.759
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/18/23 22:37:11.761
    STEP: get a list of Events with a label in the current namespace 01/18/23 22:37:11.777
    STEP: delete a list of events 01/18/23 22:37:11.779
    Jan 18 22:37:11.779: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 22:37:11.796
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:11.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7390" for this suite. 01/18/23 22:37:11.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:11.809
Jan 18 22:37:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename events 01/18/23 22:37:11.81
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.826
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/18/23 22:37:11.828
STEP: listing events in all namespaces 01/18/23 22:37:11.832
STEP: listing events in test namespace 01/18/23 22:37:11.837
STEP: listing events with field selection filtering on source 01/18/23 22:37:11.839
STEP: listing events with field selection filtering on reportingController 01/18/23 22:37:11.841
STEP: getting the test event 01/18/23 22:37:11.843
STEP: patching the test event 01/18/23 22:37:11.845
STEP: getting the test event 01/18/23 22:37:11.853
STEP: updating the test event 01/18/23 22:37:11.855
STEP: getting the test event 01/18/23 22:37:11.862
STEP: deleting the test event 01/18/23 22:37:11.864
STEP: listing events in all namespaces 01/18/23 22:37:11.868
STEP: listing events in test namespace 01/18/23 22:37:11.872
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:11.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6278" for this suite. 01/18/23 22:37:11.877
------------------------------
â€¢ [0.073 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:11.809
    Jan 18 22:37:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename events 01/18/23 22:37:11.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.826
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/18/23 22:37:11.828
    STEP: listing events in all namespaces 01/18/23 22:37:11.832
    STEP: listing events in test namespace 01/18/23 22:37:11.837
    STEP: listing events with field selection filtering on source 01/18/23 22:37:11.839
    STEP: listing events with field selection filtering on reportingController 01/18/23 22:37:11.841
    STEP: getting the test event 01/18/23 22:37:11.843
    STEP: patching the test event 01/18/23 22:37:11.845
    STEP: getting the test event 01/18/23 22:37:11.853
    STEP: updating the test event 01/18/23 22:37:11.855
    STEP: getting the test event 01/18/23 22:37:11.862
    STEP: deleting the test event 01/18/23 22:37:11.864
    STEP: listing events in all namespaces 01/18/23 22:37:11.868
    STEP: listing events in test namespace 01/18/23 22:37:11.872
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:11.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6278" for this suite. 01/18/23 22:37:11.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:11.883
Jan 18 22:37:11.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:37:11.884
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.896
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:37:11.899
Jan 18 22:37:11.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170" in namespace "projected-2510" to be "Succeeded or Failed"
Jan 18 22:37:11.908: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.303645ms
Jan 18 22:37:13.912: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006835151s
Jan 18 22:37:15.912: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007004952s
STEP: Saw pod success 01/18/23 22:37:15.912
Jan 18 22:37:15.913: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170" satisfied condition "Succeeded or Failed"
Jan 18 22:37:15.915: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 container client-container: <nil>
STEP: delete the pod 01/18/23 22:37:15.928
Jan 18 22:37:15.937: INFO: Waiting for pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 to disappear
Jan 18 22:37:15.940: INFO: Pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:15.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2510" for this suite. 01/18/23 22:37:15.943
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:11.883
    Jan 18 22:37:11.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:37:11.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:11.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:11.896
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:37:11.899
    Jan 18 22:37:11.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170" in namespace "projected-2510" to be "Succeeded or Failed"
    Jan 18 22:37:11.908: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.303645ms
    Jan 18 22:37:13.912: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006835151s
    Jan 18 22:37:15.912: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007004952s
    STEP: Saw pod success 01/18/23 22:37:15.912
    Jan 18 22:37:15.913: INFO: Pod "downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:15.915: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:37:15.928
    Jan 18 22:37:15.937: INFO: Waiting for pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 to disappear
    Jan 18 22:37:15.940: INFO: Pod downwardapi-volume-8de0d503-c7ad-4491-bd46-fe8642fed170 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:15.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2510" for this suite. 01/18/23 22:37:15.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:15.949
Jan 18 22:37:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename events 01/18/23 22:37:15.95
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:15.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:15.964
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/18/23 22:37:15.967
STEP: listing all events in all namespaces 01/18/23 22:37:15.972
STEP: patching the test event 01/18/23 22:37:15.976
STEP: fetching the test event 01/18/23 22:37:15.981
STEP: updating the test event 01/18/23 22:37:15.983
STEP: getting the test event 01/18/23 22:37:15.992
STEP: deleting the test event 01/18/23 22:37:15.994
STEP: listing all events in all namespaces 01/18/23 22:37:15.999
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:16.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1384" for this suite. 01/18/23 22:37:16.006
------------------------------
â€¢ [0.061 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:15.949
    Jan 18 22:37:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename events 01/18/23 22:37:15.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:15.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:15.964
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/18/23 22:37:15.967
    STEP: listing all events in all namespaces 01/18/23 22:37:15.972
    STEP: patching the test event 01/18/23 22:37:15.976
    STEP: fetching the test event 01/18/23 22:37:15.981
    STEP: updating the test event 01/18/23 22:37:15.983
    STEP: getting the test event 01/18/23 22:37:15.992
    STEP: deleting the test event 01/18/23 22:37:15.994
    STEP: listing all events in all namespaces 01/18/23 22:37:15.999
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:16.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1384" for this suite. 01/18/23 22:37:16.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:16.011
Jan 18 22:37:16.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:37:16.012
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:16.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:16.024
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:37:16.026
Jan 18 22:37:16.035: INFO: Waiting up to 5m0s for pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9" in namespace "emptydir-8329" to be "Succeeded or Failed"
Jan 18 22:37:16.038: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497749ms
Jan 18 22:37:18.041: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0060288s
Jan 18 22:37:20.042: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006814994s
STEP: Saw pod success 01/18/23 22:37:20.042
Jan 18 22:37:20.042: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9" satisfied condition "Succeeded or Failed"
Jan 18 22:37:20.045: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 container test-container: <nil>
STEP: delete the pod 01/18/23 22:37:20.05
Jan 18 22:37:20.062: INFO: Waiting for pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 to disappear
Jan 18 22:37:20.065: INFO: Pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:20.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8329" for this suite. 01/18/23 22:37:20.068
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:16.011
    Jan 18 22:37:16.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:37:16.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:16.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:16.024
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:37:16.026
    Jan 18 22:37:16.035: INFO: Waiting up to 5m0s for pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9" in namespace "emptydir-8329" to be "Succeeded or Failed"
    Jan 18 22:37:16.038: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497749ms
    Jan 18 22:37:18.041: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0060288s
    Jan 18 22:37:20.042: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006814994s
    STEP: Saw pod success 01/18/23 22:37:20.042
    Jan 18 22:37:20.042: INFO: Pod "pod-a4e99187-6841-4945-8183-d2e5c5526ec9" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:20.045: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:37:20.05
    Jan 18 22:37:20.062: INFO: Waiting for pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 to disappear
    Jan 18 22:37:20.065: INFO: Pod pod-a4e99187-6841-4945-8183-d2e5c5526ec9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:20.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8329" for this suite. 01/18/23 22:37:20.068
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:20.073
Jan 18 22:37:20.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sysctl 01/18/23 22:37:20.074
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:20.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:20.088
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 22:37:20.091
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1858" for this suite. 01/18/23 22:37:20.098
------------------------------
â€¢ [0.032 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:20.073
    Jan 18 22:37:20.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sysctl 01/18/23 22:37:20.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:20.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:20.088
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 22:37:20.091
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1858" for this suite. 01/18/23 22:37:20.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:20.106
Jan 18 22:37:20.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:37:20.107
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:20.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:20.121
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-2cdecc6a-f113-432d-a3fc-c26be9175307 01/18/23 22:37:20.128
STEP: Creating configMap with name cm-test-opt-upd-0e21eddf-9c22-43c5-a123-7ed37bb31b39 01/18/23 22:37:20.133
STEP: Creating the pod 01/18/23 22:37:20.137
Jan 18 22:37:20.146: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2" in namespace "configmap-1748" to be "running and ready"
Jan 18 22:37:20.148: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695938ms
Jan 18 22:37:20.148: INFO: The phase of Pod pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:37:22.152: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433633s
Jan 18 22:37:22.152: INFO: The phase of Pod pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2 is Running (Ready = true)
Jan 18 22:37:22.152: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2cdecc6a-f113-432d-a3fc-c26be9175307 01/18/23 22:37:22.168
STEP: Updating configmap cm-test-opt-upd-0e21eddf-9c22-43c5-a123-7ed37bb31b39 01/18/23 22:37:22.173
STEP: Creating configMap with name cm-test-opt-create-b6d6bf0a-9916-474a-a8da-aee4d4be893a 01/18/23 22:37:22.177
STEP: waiting to observe update in volume 01/18/23 22:37:22.18
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:26.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1748" for this suite. 01/18/23 22:37:26.21
------------------------------
â€¢ [SLOW TEST] [6.110 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:20.106
    Jan 18 22:37:20.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:37:20.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:20.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:20.121
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-2cdecc6a-f113-432d-a3fc-c26be9175307 01/18/23 22:37:20.128
    STEP: Creating configMap with name cm-test-opt-upd-0e21eddf-9c22-43c5-a123-7ed37bb31b39 01/18/23 22:37:20.133
    STEP: Creating the pod 01/18/23 22:37:20.137
    Jan 18 22:37:20.146: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2" in namespace "configmap-1748" to be "running and ready"
    Jan 18 22:37:20.148: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695938ms
    Jan 18 22:37:20.148: INFO: The phase of Pod pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:37:22.152: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433633s
    Jan 18 22:37:22.152: INFO: The phase of Pod pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2 is Running (Ready = true)
    Jan 18 22:37:22.152: INFO: Pod "pod-configmaps-f7816286-5fe3-41d1-842c-5fe73800a6a2" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2cdecc6a-f113-432d-a3fc-c26be9175307 01/18/23 22:37:22.168
    STEP: Updating configmap cm-test-opt-upd-0e21eddf-9c22-43c5-a123-7ed37bb31b39 01/18/23 22:37:22.173
    STEP: Creating configMap with name cm-test-opt-create-b6d6bf0a-9916-474a-a8da-aee4d4be893a 01/18/23 22:37:22.177
    STEP: waiting to observe update in volume 01/18/23 22:37:22.18
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:26.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1748" for this suite. 01/18/23 22:37:26.21
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:26.216
Jan 18 22:37:26.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 22:37:26.217
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:26.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:26.229
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/18/23 22:37:26.232
Jan 18 22:37:26.238: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2812" to be "running and ready"
Jan 18 22:37:26.241: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096172ms
Jan 18 22:37:26.241: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:37:28.244: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005604163s
Jan 18 22:37:28.244: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 18 22:37:28.244: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/18/23 22:37:28.247
Jan 18 22:37:28.261: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2812" to be "container debugger running"
Jan 18 22:37:28.263: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.378935ms
Jan 18 22:37:30.267: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396959s
Jan 18 22:37:32.269: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00757628s
Jan 18 22:37:32.269: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/18/23 22:37:32.269
Jan 18 22:37:32.269: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2812 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:37:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:37:32.270: INFO: ExecWithOptions: Clientset creation
Jan 18 22:37:32.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2812/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 18 22:37:32.343: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:32.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2812" for this suite. 01/18/23 22:37:32.351
------------------------------
â€¢ [SLOW TEST] [6.141 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:26.216
    Jan 18 22:37:26.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 22:37:26.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:26.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:26.229
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/18/23 22:37:26.232
    Jan 18 22:37:26.238: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2812" to be "running and ready"
    Jan 18 22:37:26.241: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096172ms
    Jan 18 22:37:26.241: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:37:28.244: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005604163s
    Jan 18 22:37:28.244: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 18 22:37:28.244: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/18/23 22:37:28.247
    Jan 18 22:37:28.261: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2812" to be "container debugger running"
    Jan 18 22:37:28.263: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.378935ms
    Jan 18 22:37:30.267: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396959s
    Jan 18 22:37:32.269: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00757628s
    Jan 18 22:37:32.269: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/18/23 22:37:32.269
    Jan 18 22:37:32.269: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2812 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:37:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:37:32.270: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:37:32.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2812/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 18 22:37:32.343: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:32.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2812" for this suite. 01/18/23 22:37:32.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:32.357
Jan 18 22:37:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:37:32.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:32.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:32.376
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 18 22:37:32.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 22:37:33.792
Jan 18 22:37:33.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
Jan 18 22:37:34.385: INFO: stderr: ""
Jan 18 22:37:34.385: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 22:37:34.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 delete e2e-test-crd-publish-openapi-2582-crds test-foo'
Jan 18 22:37:34.460: INFO: stderr: ""
Jan 18 22:37:34.460: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 22:37:34.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
Jan 18 22:37:34.647: INFO: stderr: ""
Jan 18 22:37:34.647: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 22:37:34.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 delete e2e-test-crd-publish-openapi-2582-crds test-foo'
Jan 18 22:37:34.722: INFO: stderr: ""
Jan 18 22:37:34.722: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 22:37:34.722
Jan 18 22:37:34.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
Jan 18 22:37:34.908: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 22:37:34.908
Jan 18 22:37:34.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
Jan 18 22:37:35.090: INFO: rc: 1
Jan 18 22:37:35.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
Jan 18 22:37:35.299: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 22:37:35.299
Jan 18 22:37:35.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
Jan 18 22:37:35.479: INFO: rc: 1
Jan 18 22:37:35.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
Jan 18 22:37:35.669: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/18/23 22:37:35.669
Jan 18 22:37:35.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds'
Jan 18 22:37:35.855: INFO: stderr: ""
Jan 18 22:37:35.855: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/18/23 22:37:35.856
Jan 18 22:37:35.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.metadata'
Jan 18 22:37:36.039: INFO: stderr: ""
Jan 18 22:37:36.039: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 22:37:36.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec'
Jan 18 22:37:36.233: INFO: stderr: ""
Jan 18 22:37:36.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 22:37:36.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec.bars'
Jan 18 22:37:36.430: INFO: stderr: ""
Jan 18 22:37:36.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 22:37:36.431
Jan 18 22:37:36.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec.bars2'
Jan 18 22:37:36.615: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3947" for this suite. 01/18/23 22:37:38.048
------------------------------
â€¢ [SLOW TEST] [5.695 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:32.357
    Jan 18 22:37:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:37:32.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:32.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:32.376
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 18 22:37:32.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 22:37:33.792
    Jan 18 22:37:33.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
    Jan 18 22:37:34.385: INFO: stderr: ""
    Jan 18 22:37:34.385: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 22:37:34.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 delete e2e-test-crd-publish-openapi-2582-crds test-foo'
    Jan 18 22:37:34.460: INFO: stderr: ""
    Jan 18 22:37:34.460: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 18 22:37:34.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
    Jan 18 22:37:34.647: INFO: stderr: ""
    Jan 18 22:37:34.647: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 22:37:34.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 delete e2e-test-crd-publish-openapi-2582-crds test-foo'
    Jan 18 22:37:34.722: INFO: stderr: ""
    Jan 18 22:37:34.722: INFO: stdout: "e2e-test-crd-publish-openapi-2582-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 22:37:34.722
    Jan 18 22:37:34.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
    Jan 18 22:37:34.908: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 22:37:34.908
    Jan 18 22:37:34.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
    Jan 18 22:37:35.090: INFO: rc: 1
    Jan 18 22:37:35.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
    Jan 18 22:37:35.299: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 22:37:35.299
    Jan 18 22:37:35.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 create -f -'
    Jan 18 22:37:35.479: INFO: rc: 1
    Jan 18 22:37:35.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 --namespace=crd-publish-openapi-3947 apply -f -'
    Jan 18 22:37:35.669: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/18/23 22:37:35.669
    Jan 18 22:37:35.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds'
    Jan 18 22:37:35.855: INFO: stderr: ""
    Jan 18 22:37:35.855: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/18/23 22:37:35.856
    Jan 18 22:37:35.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.metadata'
    Jan 18 22:37:36.039: INFO: stderr: ""
    Jan 18 22:37:36.039: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 18 22:37:36.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec'
    Jan 18 22:37:36.233: INFO: stderr: ""
    Jan 18 22:37:36.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 18 22:37:36.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec.bars'
    Jan 18 22:37:36.430: INFO: stderr: ""
    Jan 18 22:37:36.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2582-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 22:37:36.431
    Jan 18 22:37:36.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3947 explain e2e-test-crd-publish-openapi-2582-crds.spec.bars2'
    Jan 18 22:37:36.615: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3947" for this suite. 01/18/23 22:37:38.048
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:38.053
Jan 18 22:37:38.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:37:38.054
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:38.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:38.068
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4164 01/18/23 22:37:38.071
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:37:38.082
STEP: creating service externalsvc in namespace services-4164 01/18/23 22:37:38.082
STEP: creating replication controller externalsvc in namespace services-4164 01/18/23 22:37:38.091
I0118 22:37:38.096880      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4164, replica count: 2
I0118 22:37:41.149008      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/18/23 22:37:41.151
Jan 18 22:37:41.169: INFO: Creating new exec pod
Jan 18 22:37:41.174: INFO: Waiting up to 5m0s for pod "execpodttbrw" in namespace "services-4164" to be "running"
Jan 18 22:37:41.177: INFO: Pod "execpodttbrw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.792586ms
Jan 18 22:37:43.180: INFO: Pod "execpodttbrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005374034s
Jan 18 22:37:43.180: INFO: Pod "execpodttbrw" satisfied condition "running"
Jan 18 22:37:43.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4164 exec execpodttbrw -- /bin/sh -x -c nslookup nodeport-service.services-4164.svc.cluster.local'
Jan 18 22:37:43.347: INFO: stderr: "+ nslookup nodeport-service.services-4164.svc.cluster.local\n"
Jan 18 22:37:43.347: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4164.svc.cluster.local\tcanonical name = externalsvc.services-4164.svc.cluster.local.\nName:\texternalsvc.services-4164.svc.cluster.local\nAddress: 10.96.3.223\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4164, will wait for the garbage collector to delete the pods 01/18/23 22:37:43.347
Jan 18 22:37:43.405: INFO: Deleting ReplicationController externalsvc took: 5.063875ms
Jan 18 22:37:43.506: INFO: Terminating ReplicationController externalsvc pods took: 100.57209ms
Jan 18 22:37:44.917: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:44.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4164" for this suite. 01/18/23 22:37:44.926
------------------------------
â€¢ [SLOW TEST] [6.880 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:38.053
    Jan 18 22:37:38.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:37:38.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:38.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:38.068
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4164 01/18/23 22:37:38.071
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:37:38.082
    STEP: creating service externalsvc in namespace services-4164 01/18/23 22:37:38.082
    STEP: creating replication controller externalsvc in namespace services-4164 01/18/23 22:37:38.091
    I0118 22:37:38.096880      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4164, replica count: 2
    I0118 22:37:41.149008      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/18/23 22:37:41.151
    Jan 18 22:37:41.169: INFO: Creating new exec pod
    Jan 18 22:37:41.174: INFO: Waiting up to 5m0s for pod "execpodttbrw" in namespace "services-4164" to be "running"
    Jan 18 22:37:41.177: INFO: Pod "execpodttbrw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.792586ms
    Jan 18 22:37:43.180: INFO: Pod "execpodttbrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005374034s
    Jan 18 22:37:43.180: INFO: Pod "execpodttbrw" satisfied condition "running"
    Jan 18 22:37:43.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4164 exec execpodttbrw -- /bin/sh -x -c nslookup nodeport-service.services-4164.svc.cluster.local'
    Jan 18 22:37:43.347: INFO: stderr: "+ nslookup nodeport-service.services-4164.svc.cluster.local\n"
    Jan 18 22:37:43.347: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4164.svc.cluster.local\tcanonical name = externalsvc.services-4164.svc.cluster.local.\nName:\texternalsvc.services-4164.svc.cluster.local\nAddress: 10.96.3.223\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4164, will wait for the garbage collector to delete the pods 01/18/23 22:37:43.347
    Jan 18 22:37:43.405: INFO: Deleting ReplicationController externalsvc took: 5.063875ms
    Jan 18 22:37:43.506: INFO: Terminating ReplicationController externalsvc pods took: 100.57209ms
    Jan 18 22:37:44.917: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:44.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4164" for this suite. 01/18/23 22:37:44.926
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:44.933
Jan 18 22:37:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:37:44.934
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:44.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:44.95
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:37:44.953
Jan 18 22:37:44.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e" in namespace "downward-api-5971" to be "Succeeded or Failed"
Jan 18 22:37:44.963: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.878119ms
Jan 18 22:37:46.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996066s
Jan 18 22:37:48.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006829948s
STEP: Saw pod success 01/18/23 22:37:48.967
Jan 18 22:37:48.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e" satisfied condition "Succeeded or Failed"
Jan 18 22:37:48.970: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e container client-container: <nil>
STEP: delete the pod 01/18/23 22:37:48.975
Jan 18 22:37:48.987: INFO: Waiting for pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e to disappear
Jan 18 22:37:48.990: INFO: Pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5971" for this suite. 01/18/23 22:37:48.993
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:44.933
    Jan 18 22:37:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:37:44.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:44.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:44.95
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:37:44.953
    Jan 18 22:37:44.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e" in namespace "downward-api-5971" to be "Succeeded or Failed"
    Jan 18 22:37:44.963: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.878119ms
    Jan 18 22:37:46.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996066s
    Jan 18 22:37:48.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006829948s
    STEP: Saw pod success 01/18/23 22:37:48.967
    Jan 18 22:37:48.967: INFO: Pod "downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:48.970: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e container client-container: <nil>
    STEP: delete the pod 01/18/23 22:37:48.975
    Jan 18 22:37:48.987: INFO: Waiting for pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e to disappear
    Jan 18 22:37:48.990: INFO: Pod downwardapi-volume-8535912e-58d9-4264-9558-ef972f52a84e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5971" for this suite. 01/18/23 22:37:48.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:49.001
Jan 18 22:37:49.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:37:49.002
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:49.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:49.016
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-b8d0e282-af71-4f0a-bfd7-67b79d6ad79c 01/18/23 22:37:49.019
STEP: Creating a pod to test consume secrets 01/18/23 22:37:49.024
Jan 18 22:37:49.033: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888" in namespace "projected-3773" to be "Succeeded or Failed"
Jan 18 22:37:49.036: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843182ms
Jan 18 22:37:51.040: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018052s
Jan 18 22:37:53.039: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005929157s
STEP: Saw pod success 01/18/23 22:37:53.039
Jan 18 22:37:53.039: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888" satisfied condition "Succeeded or Failed"
Jan 18 22:37:53.042: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:37:53.047
Jan 18 22:37:53.057: INFO: Waiting for pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 to disappear
Jan 18 22:37:53.059: INFO: Pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:53.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3773" for this suite. 01/18/23 22:37:53.062
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:49.001
    Jan 18 22:37:49.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:37:49.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:49.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:49.016
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-b8d0e282-af71-4f0a-bfd7-67b79d6ad79c 01/18/23 22:37:49.019
    STEP: Creating a pod to test consume secrets 01/18/23 22:37:49.024
    Jan 18 22:37:49.033: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888" in namespace "projected-3773" to be "Succeeded or Failed"
    Jan 18 22:37:49.036: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843182ms
    Jan 18 22:37:51.040: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018052s
    Jan 18 22:37:53.039: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005929157s
    STEP: Saw pod success 01/18/23 22:37:53.039
    Jan 18 22:37:53.039: INFO: Pod "pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:53.042: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:37:53.047
    Jan 18 22:37:53.057: INFO: Waiting for pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 to disappear
    Jan 18 22:37:53.059: INFO: Pod pod-projected-secrets-a4c7eca6-ada7-4896-99e3-84de2bbad888 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:53.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3773" for this suite. 01/18/23 22:37:53.062
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:53.068
Jan 18 22:37:53.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:37:53.069
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:53.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:53.087
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:37:53.09
Jan 18 22:37:53.097: INFO: Waiting up to 5m0s for pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e" in namespace "emptydir-7079" to be "Succeeded or Failed"
Jan 18 22:37:53.100: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536501ms
Jan 18 22:37:55.103: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074123s
Jan 18 22:37:57.104: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007062265s
STEP: Saw pod success 01/18/23 22:37:57.104
Jan 18 22:37:57.105: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e" satisfied condition "Succeeded or Failed"
Jan 18 22:37:57.107: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e container test-container: <nil>
STEP: delete the pod 01/18/23 22:37:57.113
Jan 18 22:37:57.124: INFO: Waiting for pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e to disappear
Jan 18 22:37:57.126: INFO: Pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:37:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7079" for this suite. 01/18/23 22:37:57.129
------------------------------
â€¢ [4.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:53.068
    Jan 18 22:37:53.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:37:53.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:53.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:53.087
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:37:53.09
    Jan 18 22:37:53.097: INFO: Waiting up to 5m0s for pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e" in namespace "emptydir-7079" to be "Succeeded or Failed"
    Jan 18 22:37:53.100: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536501ms
    Jan 18 22:37:55.103: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074123s
    Jan 18 22:37:57.104: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007062265s
    STEP: Saw pod success 01/18/23 22:37:57.104
    Jan 18 22:37:57.105: INFO: Pod "pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:57.107: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e container test-container: <nil>
    STEP: delete the pod 01/18/23 22:37:57.113
    Jan 18 22:37:57.124: INFO: Waiting for pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e to disappear
    Jan 18 22:37:57.126: INFO: Pod pod-54a1bb18-e1ee-4f36-99fe-df8feb8c6e7e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:37:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7079" for this suite. 01/18/23 22:37:57.129
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:37:57.136
Jan 18 22:37:57.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:37:57.137
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:57.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:57.15
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-004ee838-62a5-4cb4-9c68-4037d158170f 01/18/23 22:37:57.153
STEP: Creating a pod to test consume configMaps 01/18/23 22:37:57.159
Jan 18 22:37:57.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216" in namespace "configmap-1446" to be "Succeeded or Failed"
Jan 18 22:37:57.169: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.593047ms
Jan 18 22:37:59.172: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006178806s
Jan 18 22:38:01.174: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007343585s
STEP: Saw pod success 01/18/23 22:38:01.174
Jan 18 22:38:01.174: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216" satisfied condition "Succeeded or Failed"
Jan 18 22:38:01.176: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 container configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 22:38:01.182
Jan 18 22:38:01.194: INFO: Waiting for pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 to disappear
Jan 18 22:38:01.196: INFO: Pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:01.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1446" for this suite. 01/18/23 22:38:01.199
------------------------------
â€¢ [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:37:57.136
    Jan 18 22:37:57.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:37:57.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:57.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:57.15
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-004ee838-62a5-4cb4-9c68-4037d158170f 01/18/23 22:37:57.153
    STEP: Creating a pod to test consume configMaps 01/18/23 22:37:57.159
    Jan 18 22:37:57.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216" in namespace "configmap-1446" to be "Succeeded or Failed"
    Jan 18 22:37:57.169: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.593047ms
    Jan 18 22:37:59.172: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006178806s
    Jan 18 22:38:01.174: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007343585s
    STEP: Saw pod success 01/18/23 22:38:01.174
    Jan 18 22:38:01.174: INFO: Pod "pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:01.176: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 container configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:38:01.182
    Jan 18 22:38:01.194: INFO: Waiting for pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 to disappear
    Jan 18 22:38:01.196: INFO: Pod pod-configmaps-9b4a9ef0-7be9-4908-91c9-1f8996a74216 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:01.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1446" for this suite. 01/18/23 22:38:01.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:01.208
Jan 18 22:38:01.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption 01/18/23 22:38:01.208
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:01.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:01.221
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/18/23 22:38:01.229
STEP: Waiting for all pods to be running 01/18/23 22:38:03.256
Jan 18 22:38:03.259: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:05.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8720" for this suite. 01/18/23 22:38:05.27
------------------------------
â€¢ [4.067 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:01.208
    Jan 18 22:38:01.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption 01/18/23 22:38:01.208
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:01.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:01.221
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/18/23 22:38:01.229
    STEP: Waiting for all pods to be running 01/18/23 22:38:03.256
    Jan 18 22:38:03.259: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:05.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8720" for this suite. 01/18/23 22:38:05.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:05.275
Jan 18 22:38:05.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:38:05.276
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:05.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:05.292
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 18 22:38:05.302: INFO: Waiting up to 5m0s for pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f" in namespace "container-probe-23" to be "running and ready"
Jan 18 22:38:05.304: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379229ms
Jan 18 22:38:05.304: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:38:07.307: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005464681s
Jan 18 22:38:07.307: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:09.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 4.00691983s
Jan 18 22:38:09.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:11.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 6.006620064s
Jan 18 22:38:11.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:13.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 8.006596415s
Jan 18 22:38:13.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:15.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 10.0062476s
Jan 18 22:38:15.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:17.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 12.007423817s
Jan 18 22:38:17.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:19.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 14.006963198s
Jan 18 22:38:19.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:21.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 16.006976334s
Jan 18 22:38:21.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:23.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 18.00584487s
Jan 18 22:38:23.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:25.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 20.007357762s
Jan 18 22:38:25.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
Jan 18 22:38:27.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=true. Elapsed: 22.007555801s
Jan 18 22:38:27.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = true)
Jan 18 22:38:27.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f" satisfied condition "running and ready"
Jan 18 22:38:27.312: INFO: Container started at 2023-01-18 22:38:05 +0000 UTC, pod became ready at 2023-01-18 22:38:25 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-23" for this suite. 01/18/23 22:38:27.315
------------------------------
â€¢ [SLOW TEST] [22.045 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:05.275
    Jan 18 22:38:05.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:38:05.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:05.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:05.292
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 18 22:38:05.302: INFO: Waiting up to 5m0s for pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f" in namespace "container-probe-23" to be "running and ready"
    Jan 18 22:38:05.304: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379229ms
    Jan 18 22:38:05.304: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:38:07.307: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005464681s
    Jan 18 22:38:07.307: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:09.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 4.00691983s
    Jan 18 22:38:09.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:11.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 6.006620064s
    Jan 18 22:38:11.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:13.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 8.006596415s
    Jan 18 22:38:13.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:15.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 10.0062476s
    Jan 18 22:38:15.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:17.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 12.007423817s
    Jan 18 22:38:17.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:19.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 14.006963198s
    Jan 18 22:38:19.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:21.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 16.006976334s
    Jan 18 22:38:21.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:23.308: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 18.00584487s
    Jan 18 22:38:23.308: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:25.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=false. Elapsed: 20.007357762s
    Jan 18 22:38:25.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = false)
    Jan 18 22:38:27.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f": Phase="Running", Reason="", readiness=true. Elapsed: 22.007555801s
    Jan 18 22:38:27.309: INFO: The phase of Pod test-webserver-7f1be46f-9e92-4805-a613-f877638f875f is Running (Ready = true)
    Jan 18 22:38:27.309: INFO: Pod "test-webserver-7f1be46f-9e92-4805-a613-f877638f875f" satisfied condition "running and ready"
    Jan 18 22:38:27.312: INFO: Container started at 2023-01-18 22:38:05 +0000 UTC, pod became ready at 2023-01-18 22:38:25 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-23" for this suite. 01/18/23 22:38:27.315
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:27.32
Jan 18 22:38:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:38:27.321
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:27.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:27.337
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/18/23 22:38:27.344
STEP: watching for Pod to be ready 01/18/23 22:38:27.354
Jan 18 22:38:27.356: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 22:38:27.358: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
Jan 18 22:38:27.369: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
Jan 18 22:38:29.023: INFO: Found Pod pod-test in namespace pods-6000 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/18/23 22:38:29.026
STEP: getting the Pod and ensuring that it's patched 01/18/23 22:38:29.034
STEP: replacing the Pod's status Ready condition to False 01/18/23 22:38:29.036
STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 22:38:29.048
STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 22:38:29.048
STEP: watching for the Pod to be deleted 01/18/23 22:38:29.058
Jan 18 22:38:29.061: INFO: observed event type MODIFIED
Jan 18 22:38:29.548: INFO: observed event type MODIFIED
Jan 18 22:38:32.031: INFO: observed event type MODIFIED
Jan 18 22:38:32.038: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:32.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6000" for this suite. 01/18/23 22:38:32.046
------------------------------
â€¢ [4.731 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:27.32
    Jan 18 22:38:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:38:27.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:27.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:27.337
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/18/23 22:38:27.344
    STEP: watching for Pod to be ready 01/18/23 22:38:27.354
    Jan 18 22:38:27.356: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 18 22:38:27.358: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
    Jan 18 22:38:27.369: INFO: observed Pod pod-test in namespace pods-6000 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
    Jan 18 22:38:29.023: INFO: Found Pod pod-test in namespace pods-6000 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:38:27 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/18/23 22:38:29.026
    STEP: getting the Pod and ensuring that it's patched 01/18/23 22:38:29.034
    STEP: replacing the Pod's status Ready condition to False 01/18/23 22:38:29.036
    STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 22:38:29.048
    STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 22:38:29.048
    STEP: watching for the Pod to be deleted 01/18/23 22:38:29.058
    Jan 18 22:38:29.061: INFO: observed event type MODIFIED
    Jan 18 22:38:29.548: INFO: observed event type MODIFIED
    Jan 18 22:38:32.031: INFO: observed event type MODIFIED
    Jan 18 22:38:32.038: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:32.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6000" for this suite. 01/18/23 22:38:32.046
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:32.052
Jan 18 22:38:32.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename taint-single-pod 01/18/23 22:38:32.053
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:32.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:32.07
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 18 22:38:32.073: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:39:32.089: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 18 22:39:32.092: INFO: Starting informer...
STEP: Starting pod... 01/18/23 22:39:32.092
Jan 18 22:39:32.307: INFO: Pod is running on cncf-conformance-1-26-2. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 22:39:32.307
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:39:32.341
STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 22:39:32.344
Jan 18 22:39:32.344: INFO: Pod wasn't evicted. Proceeding
Jan 18 22:39:32.344: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:39:32.356
STEP: Waiting some time to make sure that toleration time passed. 01/18/23 22:39:32.359
Jan 18 22:40:47.360: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:40:47.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3338" for this suite. 01/18/23 22:40:47.363
------------------------------
â€¢ [SLOW TEST] [135.318 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:32.052
    Jan 18 22:38:32.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename taint-single-pod 01/18/23 22:38:32.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:32.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:32.07
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 18 22:38:32.073: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:39:32.089: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 18 22:39:32.092: INFO: Starting informer...
    STEP: Starting pod... 01/18/23 22:39:32.092
    Jan 18 22:39:32.307: INFO: Pod is running on cncf-conformance-1-26-2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 22:39:32.307
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:39:32.341
    STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 22:39:32.344
    Jan 18 22:39:32.344: INFO: Pod wasn't evicted. Proceeding
    Jan 18 22:39:32.344: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:39:32.356
    STEP: Waiting some time to make sure that toleration time passed. 01/18/23 22:39:32.359
    Jan 18 22:40:47.360: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:40:47.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3338" for this suite. 01/18/23 22:40:47.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:40:47.37
Jan 18 22:40:47.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:40:47.371
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:47.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:47.383
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 18 22:40:47.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: creating the pod 01/18/23 22:40:47.387
STEP: submitting the pod to kubernetes 01/18/23 22:40:47.387
Jan 18 22:40:47.397: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39" in namespace "pods-815" to be "running and ready"
Jan 18 22:40:47.399: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343852ms
Jan 18 22:40:47.399: INFO: The phase of Pod pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:40:49.403: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39": Phase="Running", Reason="", readiness=true. Elapsed: 2.005561779s
Jan 18 22:40:49.403: INFO: The phase of Pod pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39 is Running (Ready = true)
Jan 18 22:40:49.403: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:40:49.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-815" for this suite. 01/18/23 22:40:49.481
------------------------------
â€¢ [2.116 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:40:47.37
    Jan 18 22:40:47.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:40:47.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:47.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:47.383
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 18 22:40:47.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: creating the pod 01/18/23 22:40:47.387
    STEP: submitting the pod to kubernetes 01/18/23 22:40:47.387
    Jan 18 22:40:47.397: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39" in namespace "pods-815" to be "running and ready"
    Jan 18 22:40:47.399: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343852ms
    Jan 18 22:40:47.399: INFO: The phase of Pod pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:40:49.403: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39": Phase="Running", Reason="", readiness=true. Elapsed: 2.005561779s
    Jan 18 22:40:49.403: INFO: The phase of Pod pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39 is Running (Ready = true)
    Jan 18 22:40:49.403: INFO: Pod "pod-exec-websocket-715541f8-e518-4c59-b12c-745f66c7ca39" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:40:49.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-815" for this suite. 01/18/23 22:40:49.481
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:40:49.486
Jan 18 22:40:49.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 22:40:49.487
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:49.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:49.5
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 18 22:40:49.510: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 22:40:54.513: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:40:54.513
Jan 18 22:40:54.513: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 22:40:54.521
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:40:54.533: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2174  38719b77-43f4-46ee-b7b8-c0aeb75f829b 9055 1 2023-01-18 22:40:54 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 22:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046ae0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 22:40:54.535: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 18 22:40:54.535: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 18 22:40:54.535: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2174  8d19661e-45e7-4e4b-9320-f5166d7876e0 9056 1 2023-01-18 22:40:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 38719b77-43f4-46ee-b7b8-c0aeb75f829b 0xc0046ae41f 0xc0046ae430}] [] [{e2e.test Update apps/v1 2023-01-18 22:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:40:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 22:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"38719b77-43f4-46ee-b7b8-c0aeb75f829b\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046ae4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:40:54.538: INFO: Pod "test-cleanup-controller-lc2k8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lc2k8 test-cleanup-controller- deployment-2174  d26beb3d-87fa-4c2f-bc4e-a1ca1137b21e 9036 0 2023-01-18 22:40:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 8d19661e-45e7-4e4b-9320-f5166d7876e0 0xc0046ae97f 0xc0046ae990}] [] [{kube-controller-manager Update v1 2023-01-18 22:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d19661e-45e7-4e4b-9320-f5166d7876e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:40:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njshh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njshh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.5,StartTime:2023-01-18 22:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:40:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1c0315fee33620348176ef481d52fb445e4b56e6142fa97d28164496abb43b0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:40:54.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2174" for this suite. 01/18/23 22:40:54.541
------------------------------
â€¢ [SLOW TEST] [5.063 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:40:49.486
    Jan 18 22:40:49.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 22:40:49.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:49.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:49.5
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 18 22:40:49.510: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 18 22:40:54.513: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:40:54.513
    Jan 18 22:40:54.513: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 22:40:54.521
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:40:54.533: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2174  38719b77-43f4-46ee-b7b8-c0aeb75f829b 9055 1 2023-01-18 22:40:54 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 22:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046ae0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 22:40:54.535: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 18 22:40:54.535: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 18 22:40:54.535: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2174  8d19661e-45e7-4e4b-9320-f5166d7876e0 9056 1 2023-01-18 22:40:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 38719b77-43f4-46ee-b7b8-c0aeb75f829b 0xc0046ae41f 0xc0046ae430}] [] [{e2e.test Update apps/v1 2023-01-18 22:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:40:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 22:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"38719b77-43f4-46ee-b7b8-c0aeb75f829b\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046ae4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:40:54.538: INFO: Pod "test-cleanup-controller-lc2k8" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-lc2k8 test-cleanup-controller- deployment-2174  d26beb3d-87fa-4c2f-bc4e-a1ca1137b21e 9036 0 2023-01-18 22:40:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 8d19661e-45e7-4e4b-9320-f5166d7876e0 0xc0046ae97f 0xc0046ae990}] [] [{kube-controller-manager Update v1 2023-01-18 22:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d19661e-45e7-4e4b-9320-f5166d7876e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:40:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njshh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njshh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.5,StartTime:2023-01-18 22:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:40:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1c0315fee33620348176ef481d52fb445e4b56e6142fa97d28164496abb43b0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:40:54.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2174" for this suite. 01/18/23 22:40:54.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:40:54.55
Jan 18 22:40:54.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:40:54.551
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:54.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:54.571
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 in namespace container-probe-5246 01/18/23 22:40:54.574
Jan 18 22:40:54.582: INFO: Waiting up to 5m0s for pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500" in namespace "container-probe-5246" to be "not pending"
Jan 18 22:40:54.584: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230118ms
Jan 18 22:40:56.588: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500": Phase="Running", Reason="", readiness=true. Elapsed: 2.005647769s
Jan 18 22:40:56.588: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500" satisfied condition "not pending"
Jan 18 22:40:56.588: INFO: Started pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 in namespace container-probe-5246
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:40:56.588
Jan 18 22:40:56.590: INFO: Initial restart count of pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is 0
Jan 18 22:41:16.630: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 1 (20.040017876s elapsed)
Jan 18 22:41:36.669: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 2 (40.078135787s elapsed)
Jan 18 22:41:56.708: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 3 (1m0.117565914s elapsed)
Jan 18 22:42:16.748: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 4 (1m20.157629388s elapsed)
Jan 18 22:43:28.889: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 5 (2m32.298866071s elapsed)
STEP: deleting the pod 01/18/23 22:43:28.889
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:43:28.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5246" for this suite. 01/18/23 22:43:28.905
------------------------------
â€¢ [SLOW TEST] [154.360 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:40:54.55
    Jan 18 22:40:54.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:40:54.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:40:54.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:40:54.571
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 in namespace container-probe-5246 01/18/23 22:40:54.574
    Jan 18 22:40:54.582: INFO: Waiting up to 5m0s for pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500" in namespace "container-probe-5246" to be "not pending"
    Jan 18 22:40:54.584: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230118ms
    Jan 18 22:40:56.588: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500": Phase="Running", Reason="", readiness=true. Elapsed: 2.005647769s
    Jan 18 22:40:56.588: INFO: Pod "liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500" satisfied condition "not pending"
    Jan 18 22:40:56.588: INFO: Started pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 in namespace container-probe-5246
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:40:56.588
    Jan 18 22:40:56.590: INFO: Initial restart count of pod liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is 0
    Jan 18 22:41:16.630: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 1 (20.040017876s elapsed)
    Jan 18 22:41:36.669: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 2 (40.078135787s elapsed)
    Jan 18 22:41:56.708: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 3 (1m0.117565914s elapsed)
    Jan 18 22:42:16.748: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 4 (1m20.157629388s elapsed)
    Jan 18 22:43:28.889: INFO: Restart count of pod container-probe-5246/liveness-68cd6ce9-8c94-4f19-81a5-4df6d9df0500 is now 5 (2m32.298866071s elapsed)
    STEP: deleting the pod 01/18/23 22:43:28.889
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:43:28.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5246" for this suite. 01/18/23 22:43:28.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:43:28.911
Jan 18 22:43:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:43:28.912
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:28.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:28.929
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5591 01/18/23 22:43:28.931
STEP: changing the ExternalName service to type=NodePort 01/18/23 22:43:28.935
STEP: creating replication controller externalname-service in namespace services-5591 01/18/23 22:43:28.951
I0118 22:43:28.956290      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5591, replica count: 2
I0118 22:43:32.008456      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:43:32.008: INFO: Creating new exec pod
Jan 18 22:43:32.015: INFO: Waiting up to 5m0s for pod "execpodnr4dx" in namespace "services-5591" to be "running"
Jan 18 22:43:32.017: INFO: Pod "execpodnr4dx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250816ms
Jan 18 22:43:34.020: INFO: Pod "execpodnr4dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.004716155s
Jan 18 22:43:34.020: INFO: Pod "execpodnr4dx" satisfied condition "running"
Jan 18 22:43:35.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 18 22:43:35.159: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 22:43:35.159: INFO: stdout: ""
Jan 18 22:43:35.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.96.1.46 80'
Jan 18 22:43:35.307: INFO: stderr: "+ nc -v -z -w 2 10.96.1.46 80\nConnection to 10.96.1.46 80 port [tcp/http] succeeded!\n"
Jan 18 22:43:35.307: INFO: stdout: ""
Jan 18 22:43:35.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 31071'
Jan 18 22:43:35.456: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 31071\nConnection to 10.128.15.198 31071 port [tcp/*] succeeded!\n"
Jan 18 22:43:35.456: INFO: stdout: ""
Jan 18 22:43:35.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 31071'
Jan 18 22:43:35.595: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 31071\nConnection to 10.128.15.199 31071 port [tcp/*] succeeded!\n"
Jan 18 22:43:35.595: INFO: stdout: ""
Jan 18 22:43:35.595: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:43:35.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5591" for this suite. 01/18/23 22:43:35.619
------------------------------
â€¢ [SLOW TEST] [6.713 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:43:28.911
    Jan 18 22:43:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:43:28.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:28.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:28.929
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5591 01/18/23 22:43:28.931
    STEP: changing the ExternalName service to type=NodePort 01/18/23 22:43:28.935
    STEP: creating replication controller externalname-service in namespace services-5591 01/18/23 22:43:28.951
    I0118 22:43:28.956290      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5591, replica count: 2
    I0118 22:43:32.008456      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:43:32.008: INFO: Creating new exec pod
    Jan 18 22:43:32.015: INFO: Waiting up to 5m0s for pod "execpodnr4dx" in namespace "services-5591" to be "running"
    Jan 18 22:43:32.017: INFO: Pod "execpodnr4dx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250816ms
    Jan 18 22:43:34.020: INFO: Pod "execpodnr4dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.004716155s
    Jan 18 22:43:34.020: INFO: Pod "execpodnr4dx" satisfied condition "running"
    Jan 18 22:43:35.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 18 22:43:35.159: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 22:43:35.159: INFO: stdout: ""
    Jan 18 22:43:35.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.96.1.46 80'
    Jan 18 22:43:35.307: INFO: stderr: "+ nc -v -z -w 2 10.96.1.46 80\nConnection to 10.96.1.46 80 port [tcp/http] succeeded!\n"
    Jan 18 22:43:35.307: INFO: stdout: ""
    Jan 18 22:43:35.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 31071'
    Jan 18 22:43:35.456: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 31071\nConnection to 10.128.15.198 31071 port [tcp/*] succeeded!\n"
    Jan 18 22:43:35.456: INFO: stdout: ""
    Jan 18 22:43:35.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5591 exec execpodnr4dx -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 31071'
    Jan 18 22:43:35.595: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 31071\nConnection to 10.128.15.199 31071 port [tcp/*] succeeded!\n"
    Jan 18 22:43:35.595: INFO: stdout: ""
    Jan 18 22:43:35.595: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:43:35.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5591" for this suite. 01/18/23 22:43:35.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:43:35.625
Jan 18 22:43:35.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:43:35.626
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:35.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:35.639
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 22:43:35.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5067" for this suite. 01/18/23 22:43:35.675
------------------------------
â€¢ [0.054 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:43:35.625
    Jan 18 22:43:35.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:43:35.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:35.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:35.639
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:43:35.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5067" for this suite. 01/18/23 22:43:35.675
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:43:35.68
Jan 18 22:43:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:43:35.681
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:35.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:35.694
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 18 22:43:35.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:43:41.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7147" for this suite. 01/18/23 22:43:41.904
------------------------------
â€¢ [SLOW TEST] [6.229 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:43:35.68
    Jan 18 22:43:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:43:35.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:35.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:35.694
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 18 22:43:35.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:43:41.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7147" for this suite. 01/18/23 22:43:41.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:43:41.909
Jan 18 22:43:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 22:43:41.91
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:41.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:41.927
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/18/23 22:43:41.93
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:43:41.935
STEP: delete the deployment 01/18/23 22:43:42.441
STEP: wait for all rs to be garbage collected 01/18/23 22:43:42.446
STEP: expected 0 rs, got 1 rs 01/18/23 22:43:42.45
STEP: expected 0 pods, got 2 pods 01/18/23 22:43:42.454
STEP: Gathering metrics 01/18/23 22:43:42.963
Jan 18 22:43:42.985: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 22:43:42.988: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.115349ms
Jan 18 22:43:42.988: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 22:43:42.988: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 22:43:43.056: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 22:43:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1033" for this suite. 01/18/23 22:43:43.06
------------------------------
â€¢ [1.159 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:43:41.909
    Jan 18 22:43:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 22:43:41.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:41.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:41.927
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/18/23 22:43:41.93
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:43:41.935
    STEP: delete the deployment 01/18/23 22:43:42.441
    STEP: wait for all rs to be garbage collected 01/18/23 22:43:42.446
    STEP: expected 0 rs, got 1 rs 01/18/23 22:43:42.45
    STEP: expected 0 pods, got 2 pods 01/18/23 22:43:42.454
    STEP: Gathering metrics 01/18/23 22:43:42.963
    Jan 18 22:43:42.985: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 22:43:42.988: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.115349ms
    Jan 18 22:43:42.988: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 22:43:42.988: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 22:43:43.056: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:43:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1033" for this suite. 01/18/23 22:43:43.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:43:43.069
Jan 18 22:43:43.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename subpath 01/18/23 22:43:43.07
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:43.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:43.084
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:43:43.087
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-f4l5 01/18/23 22:43:43.098
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:43:43.098
Jan 18 22:43:43.106: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f4l5" in namespace "subpath-3773" to be "Succeeded or Failed"
Jan 18 22:43:43.108: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781075ms
Jan 18 22:43:45.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006852248s
Jan 18 22:43:47.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 4.007388669s
Jan 18 22:43:49.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 6.007077546s
Jan 18 22:43:51.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 8.007643201s
Jan 18 22:43:53.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 10.006503573s
Jan 18 22:43:55.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 12.006534384s
Jan 18 22:43:57.114: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 14.008033496s
Jan 18 22:43:59.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 16.007358892s
Jan 18 22:44:01.114: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 18.008086601s
Jan 18 22:44:03.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 20.006235889s
Jan 18 22:44:05.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=false. Elapsed: 22.007625801s
Jan 18 22:44:07.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00788207s
STEP: Saw pod success 01/18/23 22:44:07.114
Jan 18 22:44:07.114: INFO: Pod "pod-subpath-test-configmap-f4l5" satisfied condition "Succeeded or Failed"
Jan 18 22:44:07.116: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-configmap-f4l5 container test-container-subpath-configmap-f4l5: <nil>
STEP: delete the pod 01/18/23 22:44:07.129
Jan 18 22:44:07.141: INFO: Waiting for pod pod-subpath-test-configmap-f4l5 to disappear
Jan 18 22:44:07.143: INFO: Pod pod-subpath-test-configmap-f4l5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f4l5 01/18/23 22:44:07.143
Jan 18 22:44:07.143: INFO: Deleting pod "pod-subpath-test-configmap-f4l5" in namespace "subpath-3773"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:44:07.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3773" for this suite. 01/18/23 22:44:07.149
------------------------------
â€¢ [SLOW TEST] [24.087 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:43:43.069
    Jan 18 22:43:43.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename subpath 01/18/23 22:43:43.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:43.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:43.084
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:43:43.087
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-f4l5 01/18/23 22:43:43.098
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:43:43.098
    Jan 18 22:43:43.106: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f4l5" in namespace "subpath-3773" to be "Succeeded or Failed"
    Jan 18 22:43:43.108: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781075ms
    Jan 18 22:43:45.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006852248s
    Jan 18 22:43:47.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 4.007388669s
    Jan 18 22:43:49.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 6.007077546s
    Jan 18 22:43:51.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 8.007643201s
    Jan 18 22:43:53.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 10.006503573s
    Jan 18 22:43:55.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 12.006534384s
    Jan 18 22:43:57.114: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 14.008033496s
    Jan 18 22:43:59.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 16.007358892s
    Jan 18 22:44:01.114: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 18.008086601s
    Jan 18 22:44:03.112: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=true. Elapsed: 20.006235889s
    Jan 18 22:44:05.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Running", Reason="", readiness=false. Elapsed: 22.007625801s
    Jan 18 22:44:07.113: INFO: Pod "pod-subpath-test-configmap-f4l5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00788207s
    STEP: Saw pod success 01/18/23 22:44:07.114
    Jan 18 22:44:07.114: INFO: Pod "pod-subpath-test-configmap-f4l5" satisfied condition "Succeeded or Failed"
    Jan 18 22:44:07.116: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-configmap-f4l5 container test-container-subpath-configmap-f4l5: <nil>
    STEP: delete the pod 01/18/23 22:44:07.129
    Jan 18 22:44:07.141: INFO: Waiting for pod pod-subpath-test-configmap-f4l5 to disappear
    Jan 18 22:44:07.143: INFO: Pod pod-subpath-test-configmap-f4l5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-f4l5 01/18/23 22:44:07.143
    Jan 18 22:44:07.143: INFO: Deleting pod "pod-subpath-test-configmap-f4l5" in namespace "subpath-3773"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:44:07.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3773" for this suite. 01/18/23 22:44:07.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:44:07.157
Jan 18 22:44:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:44:07.158
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:07.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:07.171
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7569 01/18/23 22:44:07.175
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 18 22:44:07.191: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:44:17.195: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/18/23 22:44:17.2
W0118 22:44:17.223109      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 22:44:17.230: INFO: Found 1 stateful pods, waiting for 2
Jan 18 22:44:27.235: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:44:27.235: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/18/23 22:44:27.24
STEP: Delete all of the StatefulSets 01/18/23 22:44:27.242
STEP: Verify that StatefulSets have been deleted 01/18/23 22:44:27.249
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:44:27.251: INFO: Deleting all statefulset in ns statefulset-7569
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:44:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7569" for this suite. 01/18/23 22:44:27.26
------------------------------
â€¢ [SLOW TEST] [20.113 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:44:07.157
    Jan 18 22:44:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:44:07.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:07.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:07.171
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7569 01/18/23 22:44:07.175
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 18 22:44:07.191: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:44:17.195: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/18/23 22:44:17.2
    W0118 22:44:17.223109      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 22:44:17.230: INFO: Found 1 stateful pods, waiting for 2
    Jan 18 22:44:27.235: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:44:27.235: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/18/23 22:44:27.24
    STEP: Delete all of the StatefulSets 01/18/23 22:44:27.242
    STEP: Verify that StatefulSets have been deleted 01/18/23 22:44:27.249
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:44:27.251: INFO: Deleting all statefulset in ns statefulset-7569
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:44:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7569" for this suite. 01/18/23 22:44:27.26
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:44:27.27
Jan 18 22:44:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 22:44:27.271
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:27.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:27.289
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/18/23 22:44:27.295
STEP: Patching the Job 01/18/23 22:44:27.3
STEP: Watching for Job to be patched 01/18/23 22:44:27.324
Jan 18 22:44:27.326: INFO: Event ADDED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 22:44:27.326: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 22:44:27.326: INFO: Event MODIFIED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/18/23 22:44:27.326
STEP: Watching for Job to be updated 01/18/23 22:44:27.335
Jan 18 22:44:27.337: INFO: Event MODIFIED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:27.337: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/18/23 22:44:27.337
Jan 18 22:44:27.339: INFO: Job: e2e-zqj2g as labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched]
STEP: Waiting for job to complete 01/18/23 22:44:27.339
STEP: Delete a job collection with a labelselector 01/18/23 22:44:37.344
STEP: Watching for Job to be deleted 01/18/23 22:44:37.35
Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:44:37.352: INFO: Event DELETED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/18/23 22:44:37.352
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:44:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7403" for this suite. 01/18/23 22:44:37.362
------------------------------
â€¢ [SLOW TEST] [10.100 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:44:27.27
    Jan 18 22:44:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 22:44:27.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:27.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:27.289
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/18/23 22:44:27.295
    STEP: Patching the Job 01/18/23 22:44:27.3
    STEP: Watching for Job to be patched 01/18/23 22:44:27.324
    Jan 18 22:44:27.326: INFO: Event ADDED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 22:44:27.326: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 22:44:27.326: INFO: Event MODIFIED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/18/23 22:44:27.326
    STEP: Watching for Job to be updated 01/18/23 22:44:27.335
    Jan 18 22:44:27.337: INFO: Event MODIFIED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:27.337: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/18/23 22:44:27.337
    Jan 18 22:44:27.339: INFO: Job: e2e-zqj2g as labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched]
    STEP: Waiting for job to complete 01/18/23 22:44:27.339
    STEP: Delete a job collection with a labelselector 01/18/23 22:44:37.344
    STEP: Watching for Job to be deleted 01/18/23 22:44:37.35
    Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:37.352: INFO: Event MODIFIED observed for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:44:37.352: INFO: Event DELETED found for Job e2e-zqj2g in namespace job-7403 with labels: map[e2e-job-label:e2e-zqj2g e2e-zqj2g:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/18/23 22:44:37.352
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:44:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7403" for this suite. 01/18/23 22:44:37.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:44:37.371
Jan 18 22:44:37.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:44:37.372
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:37.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:37.385
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 18 22:44:37.406: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:44:37.411
Jan 18 22:44:37.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:44:37.416: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:44:38.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:44:38.423: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:44:39.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:44:39.423: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/18/23 22:44:39.433
STEP: Check that daemon pods images are updated. 01/18/23 22:44:39.442
Jan 18 22:44:39.446: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 22:44:39.446: INFO: Wrong image for pod: daemon-set-vkdxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 22:44:40.453: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 22:44:41.452: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 22:44:42.452: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 22:44:42.452: INFO: Pod daemon-set-fv5p9 is not available
Jan 18 22:44:44.452: INFO: Pod daemon-set-p22fb is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 22:44:44.455
Jan 18 22:44:44.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:44:44.460: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:44:45.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:44:45.466: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:44:45.48
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5986, will wait for the garbage collector to delete the pods 01/18/23 22:44:45.48
Jan 18 22:44:45.538: INFO: Deleting DaemonSet.extensions daemon-set took: 5.593568ms
Jan 18 22:44:45.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.474017ms
Jan 18 22:44:47.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:44:47.942: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:44:47.945: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9989"},"items":null}

Jan 18 22:44:47.947: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9989"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:44:47.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5986" for this suite. 01/18/23 22:44:47.957
------------------------------
â€¢ [SLOW TEST] [10.591 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:44:37.371
    Jan 18 22:44:37.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:44:37.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:37.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:37.385
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 18 22:44:37.406: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:44:37.411
    Jan 18 22:44:37.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:44:37.416: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:44:38.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:44:38.423: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:44:39.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:44:39.423: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/18/23 22:44:39.433
    STEP: Check that daemon pods images are updated. 01/18/23 22:44:39.442
    Jan 18 22:44:39.446: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 22:44:39.446: INFO: Wrong image for pod: daemon-set-vkdxt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 22:44:40.453: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 22:44:41.452: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 22:44:42.452: INFO: Wrong image for pod: daemon-set-f2tt7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 22:44:42.452: INFO: Pod daemon-set-fv5p9 is not available
    Jan 18 22:44:44.452: INFO: Pod daemon-set-p22fb is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 22:44:44.455
    Jan 18 22:44:44.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:44:44.460: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:44:45.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:44:45.466: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:44:45.48
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5986, will wait for the garbage collector to delete the pods 01/18/23 22:44:45.48
    Jan 18 22:44:45.538: INFO: Deleting DaemonSet.extensions daemon-set took: 5.593568ms
    Jan 18 22:44:45.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.474017ms
    Jan 18 22:44:47.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:44:47.942: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:44:47.945: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9989"},"items":null}

    Jan 18 22:44:47.947: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9989"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:44:47.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5986" for this suite. 01/18/23 22:44:47.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:44:47.962
Jan 18 22:44:47.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename prestop 01/18/23 22:44:47.963
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:47.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:47.976
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-709 01/18/23 22:44:47.979
STEP: Waiting for pods to come up. 01/18/23 22:44:47.986
Jan 18 22:44:47.986: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-709" to be "running"
Jan 18 22:44:47.989: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506163ms
Jan 18 22:44:49.993: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.006347638s
Jan 18 22:44:49.993: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-709 01/18/23 22:44:49.995
Jan 18 22:44:50.002: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-709" to be "running"
Jan 18 22:44:50.005: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933108ms
Jan 18 22:44:52.008: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006036451s
Jan 18 22:44:52.008: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/18/23 22:44:52.008
Jan 18 22:44:57.022: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/18/23 22:44:57.022
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 18 22:44:57.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-709" for this suite. 01/18/23 22:44:57.037
------------------------------
â€¢ [SLOW TEST] [9.081 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:44:47.962
    Jan 18 22:44:47.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename prestop 01/18/23 22:44:47.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:47.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:47.976
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-709 01/18/23 22:44:47.979
    STEP: Waiting for pods to come up. 01/18/23 22:44:47.986
    Jan 18 22:44:47.986: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-709" to be "running"
    Jan 18 22:44:47.989: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506163ms
    Jan 18 22:44:49.993: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.006347638s
    Jan 18 22:44:49.993: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-709 01/18/23 22:44:49.995
    Jan 18 22:44:50.002: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-709" to be "running"
    Jan 18 22:44:50.005: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933108ms
    Jan 18 22:44:52.008: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006036451s
    Jan 18 22:44:52.008: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/18/23 22:44:52.008
    Jan 18 22:44:57.022: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/18/23 22:44:57.022
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:44:57.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-709" for this suite. 01/18/23 22:44:57.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:44:57.044
Jan 18 22:44:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:44:57.045
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:57.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:57.06
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:44:57.063
Jan 18 22:44:57.071: INFO: Waiting up to 5m0s for pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d" in namespace "emptydir-3284" to be "Succeeded or Failed"
Jan 18 22:44:57.074: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577114ms
Jan 18 22:44:59.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006931371s
Jan 18 22:45:01.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006993028s
STEP: Saw pod success 01/18/23 22:45:01.078
Jan 18 22:45:01.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d" satisfied condition "Succeeded or Failed"
Jan 18 22:45:01.081: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d container test-container: <nil>
STEP: delete the pod 01/18/23 22:45:01.086
Jan 18 22:45:01.098: INFO: Waiting for pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d to disappear
Jan 18 22:45:01.101: INFO: Pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3284" for this suite. 01/18/23 22:45:01.104
------------------------------
â€¢ [4.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:44:57.044
    Jan 18 22:44:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:44:57.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:44:57.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:44:57.06
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:44:57.063
    Jan 18 22:44:57.071: INFO: Waiting up to 5m0s for pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d" in namespace "emptydir-3284" to be "Succeeded or Failed"
    Jan 18 22:44:57.074: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577114ms
    Jan 18 22:44:59.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006931371s
    Jan 18 22:45:01.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006993028s
    STEP: Saw pod success 01/18/23 22:45:01.078
    Jan 18 22:45:01.078: INFO: Pod "pod-c21b7ed5-adc2-4926-a08a-f97956195c3d" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:01.081: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d container test-container: <nil>
    STEP: delete the pod 01/18/23 22:45:01.086
    Jan 18 22:45:01.098: INFO: Waiting for pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d to disappear
    Jan 18 22:45:01.101: INFO: Pod pod-c21b7ed5-adc2-4926-a08a-f97956195c3d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3284" for this suite. 01/18/23 22:45:01.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:01.113
Jan 18 22:45:01.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename cronjob 01/18/23 22:45:01.114
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:01.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:01.129
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/18/23 22:45:01.133
STEP: creating 01/18/23 22:45:01.133
STEP: getting 01/18/23 22:45:01.141
STEP: listing 01/18/23 22:45:01.143
STEP: watching 01/18/23 22:45:01.146
Jan 18 22:45:01.146: INFO: starting watch
STEP: cluster-wide listing 01/18/23 22:45:01.147
STEP: cluster-wide watching 01/18/23 22:45:01.151
Jan 18 22:45:01.151: INFO: starting watch
STEP: patching 01/18/23 22:45:01.152
STEP: updating 01/18/23 22:45:01.159
Jan 18 22:45:01.167: INFO: waiting for watch events with expected annotations
Jan 18 22:45:01.167: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 22:45:01.167
STEP: updating /status 01/18/23 22:45:01.174
STEP: get /status 01/18/23 22:45:01.181
STEP: deleting 01/18/23 22:45:01.184
STEP: deleting a collection 01/18/23 22:45:01.196
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:01.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7152" for this suite. 01/18/23 22:45:01.207
------------------------------
â€¢ [0.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:01.113
    Jan 18 22:45:01.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:45:01.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:01.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:01.129
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/18/23 22:45:01.133
    STEP: creating 01/18/23 22:45:01.133
    STEP: getting 01/18/23 22:45:01.141
    STEP: listing 01/18/23 22:45:01.143
    STEP: watching 01/18/23 22:45:01.146
    Jan 18 22:45:01.146: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 22:45:01.147
    STEP: cluster-wide watching 01/18/23 22:45:01.151
    Jan 18 22:45:01.151: INFO: starting watch
    STEP: patching 01/18/23 22:45:01.152
    STEP: updating 01/18/23 22:45:01.159
    Jan 18 22:45:01.167: INFO: waiting for watch events with expected annotations
    Jan 18 22:45:01.167: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 22:45:01.167
    STEP: updating /status 01/18/23 22:45:01.174
    STEP: get /status 01/18/23 22:45:01.181
    STEP: deleting 01/18/23 22:45:01.184
    STEP: deleting a collection 01/18/23 22:45:01.196
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:01.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7152" for this suite. 01/18/23 22:45:01.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:01.215
Jan 18 22:45:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:45:01.216
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:01.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:01.23
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:45:01.234
Jan 18 22:45:01.258: INFO: Waiting up to 5m0s for pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f" in namespace "emptydir-670" to be "Succeeded or Failed"
Jan 18 22:45:01.263: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.68323ms
Jan 18 22:45:03.266: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008094912s
Jan 18 22:45:05.267: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009344128s
STEP: Saw pod success 01/18/23 22:45:05.267
Jan 18 22:45:05.267: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f" satisfied condition "Succeeded or Failed"
Jan 18 22:45:05.270: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f container test-container: <nil>
STEP: delete the pod 01/18/23 22:45:05.276
Jan 18 22:45:05.286: INFO: Waiting for pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f to disappear
Jan 18 22:45:05.289: INFO: Pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:05.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-670" for this suite. 01/18/23 22:45:05.292
------------------------------
â€¢ [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:01.215
    Jan 18 22:45:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:45:01.216
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:01.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:01.23
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:45:01.234
    Jan 18 22:45:01.258: INFO: Waiting up to 5m0s for pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f" in namespace "emptydir-670" to be "Succeeded or Failed"
    Jan 18 22:45:01.263: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.68323ms
    Jan 18 22:45:03.266: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008094912s
    Jan 18 22:45:05.267: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009344128s
    STEP: Saw pod success 01/18/23 22:45:05.267
    Jan 18 22:45:05.267: INFO: Pod "pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:05.270: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f container test-container: <nil>
    STEP: delete the pod 01/18/23 22:45:05.276
    Jan 18 22:45:05.286: INFO: Waiting for pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f to disappear
    Jan 18 22:45:05.289: INFO: Pod pod-b75b9b3a-3cf0-4d2b-b8b2-13d65b82136f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:05.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-670" for this suite. 01/18/23 22:45:05.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:05.299
Jan 18 22:45:05.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:45:05.3
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:05.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:05.318
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-e665bb11-d352-40a5-aa9c-7fa96b173526 01/18/23 22:45:05.321
STEP: Creating a pod to test consume secrets 01/18/23 22:45:05.326
Jan 18 22:45:05.335: INFO: Waiting up to 5m0s for pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9" in namespace "secrets-5569" to be "Succeeded or Failed"
Jan 18 22:45:05.338: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881329ms
Jan 18 22:45:07.341: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005994393s
Jan 18 22:45:09.342: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0070999s
STEP: Saw pod success 01/18/23 22:45:09.342
Jan 18 22:45:09.342: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9" satisfied condition "Succeeded or Failed"
Jan 18 22:45:09.345: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:45:09.35
Jan 18 22:45:09.360: INFO: Waiting for pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 to disappear
Jan 18 22:45:09.364: INFO: Pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5569" for this suite. 01/18/23 22:45:09.367
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:05.299
    Jan 18 22:45:05.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:45:05.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:05.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:05.318
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-e665bb11-d352-40a5-aa9c-7fa96b173526 01/18/23 22:45:05.321
    STEP: Creating a pod to test consume secrets 01/18/23 22:45:05.326
    Jan 18 22:45:05.335: INFO: Waiting up to 5m0s for pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9" in namespace "secrets-5569" to be "Succeeded or Failed"
    Jan 18 22:45:05.338: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881329ms
    Jan 18 22:45:07.341: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005994393s
    Jan 18 22:45:09.342: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0070999s
    STEP: Saw pod success 01/18/23 22:45:09.342
    Jan 18 22:45:09.342: INFO: Pod "pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:09.345: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:45:09.35
    Jan 18 22:45:09.360: INFO: Waiting for pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 to disappear
    Jan 18 22:45:09.364: INFO: Pod pod-secrets-19368acb-5769-4dbd-850b-c16eaa9f8dd9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5569" for this suite. 01/18/23 22:45:09.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:09.374
Jan 18 22:45:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:45:09.375
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:09.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:09.391
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/18/23 22:45:09.394
Jan 18 22:45:09.401: INFO: Waiting up to 5m0s for pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385" in namespace "var-expansion-7866" to be "Succeeded or Failed"
Jan 18 22:45:09.403: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.216583ms
Jan 18 22:45:11.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005628654s
Jan 18 22:45:13.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006185975s
STEP: Saw pod success 01/18/23 22:45:13.407
Jan 18 22:45:13.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385" satisfied condition "Succeeded or Failed"
Jan 18 22:45:13.410: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:45:13.415
Jan 18 22:45:13.425: INFO: Waiting for pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 to disappear
Jan 18 22:45:13.427: INFO: Pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:13.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7866" for this suite. 01/18/23 22:45:13.43
------------------------------
â€¢ [4.060 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:09.374
    Jan 18 22:45:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:45:09.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:09.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:09.391
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/18/23 22:45:09.394
    Jan 18 22:45:09.401: INFO: Waiting up to 5m0s for pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385" in namespace "var-expansion-7866" to be "Succeeded or Failed"
    Jan 18 22:45:09.403: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.216583ms
    Jan 18 22:45:11.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005628654s
    Jan 18 22:45:13.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006185975s
    STEP: Saw pod success 01/18/23 22:45:13.407
    Jan 18 22:45:13.407: INFO: Pod "var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:13.410: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:45:13.415
    Jan 18 22:45:13.425: INFO: Waiting for pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 to disappear
    Jan 18 22:45:13.427: INFO: Pod var-expansion-d0fe5282-1ef9-41ce-aeee-d196d226e385 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:13.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7866" for this suite. 01/18/23 22:45:13.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:13.439
Jan 18 22:45:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:45:13.44
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:13.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:13.455
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-e44bd230-3b3e-449c-8c4d-081d8fdd6284 01/18/23 22:45:13.458
STEP: Creating a pod to test consume configMaps 01/18/23 22:45:13.463
Jan 18 22:45:13.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e" in namespace "configmap-9766" to be "Succeeded or Failed"
Jan 18 22:45:13.474: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351386ms
Jan 18 22:45:15.478: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006175419s
Jan 18 22:45:17.479: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007217885s
STEP: Saw pod success 01/18/23 22:45:17.479
Jan 18 22:45:17.479: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e" satisfied condition "Succeeded or Failed"
Jan 18 22:45:17.482: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:45:17.488
Jan 18 22:45:17.498: INFO: Waiting for pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e to disappear
Jan 18 22:45:17.501: INFO: Pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:17.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9766" for this suite. 01/18/23 22:45:17.504
------------------------------
â€¢ [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:13.439
    Jan 18 22:45:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:45:13.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:13.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:13.455
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-e44bd230-3b3e-449c-8c4d-081d8fdd6284 01/18/23 22:45:13.458
    STEP: Creating a pod to test consume configMaps 01/18/23 22:45:13.463
    Jan 18 22:45:13.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e" in namespace "configmap-9766" to be "Succeeded or Failed"
    Jan 18 22:45:13.474: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351386ms
    Jan 18 22:45:15.478: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006175419s
    Jan 18 22:45:17.479: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007217885s
    STEP: Saw pod success 01/18/23 22:45:17.479
    Jan 18 22:45:17.479: INFO: Pod "pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:17.482: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:45:17.488
    Jan 18 22:45:17.498: INFO: Waiting for pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e to disappear
    Jan 18 22:45:17.501: INFO: Pod pod-configmaps-cd547a3d-9570-40c5-ba1f-98b0644c411e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:17.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9766" for this suite. 01/18/23 22:45:17.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:17.511
Jan 18 22:45:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:45:17.512
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:17.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:17.528
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:45:17.543
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:45:18.074
STEP: Deploying the webhook pod 01/18/23 22:45:18.081
STEP: Wait for the deployment to be ready 01/18/23 22:45:18.097
Jan 18 22:45:18.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:45:20.113
STEP: Verifying the service has paired with the endpoint 01/18/23 22:45:20.123
Jan 18 22:45:21.123: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 18 22:45:21.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 22:45:21.637
STEP: Creating a custom resource that should be denied by the webhook 01/18/23 22:45:21.657
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 22:45:23.694
STEP: Updating the custom resource with disallowed data should be denied 01/18/23 22:45:23.701
STEP: Deleting the custom resource should be denied 01/18/23 22:45:23.71
STEP: Remove the offending key and value from the custom resource data 01/18/23 22:45:23.717
STEP: Deleting the updated custom resource should be successful 01/18/23 22:45:23.727
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8595" for this suite. 01/18/23 22:45:24.28
STEP: Destroying namespace "webhook-8595-markers" for this suite. 01/18/23 22:45:24.291
------------------------------
â€¢ [SLOW TEST] [6.787 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:17.511
    Jan 18 22:45:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:45:17.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:17.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:17.528
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:45:17.543
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:45:18.074
    STEP: Deploying the webhook pod 01/18/23 22:45:18.081
    STEP: Wait for the deployment to be ready 01/18/23 22:45:18.097
    Jan 18 22:45:18.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:45:20.113
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:45:20.123
    Jan 18 22:45:21.123: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 18 22:45:21.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 22:45:21.637
    STEP: Creating a custom resource that should be denied by the webhook 01/18/23 22:45:21.657
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 22:45:23.694
    STEP: Updating the custom resource with disallowed data should be denied 01/18/23 22:45:23.701
    STEP: Deleting the custom resource should be denied 01/18/23 22:45:23.71
    STEP: Remove the offending key and value from the custom resource data 01/18/23 22:45:23.717
    STEP: Deleting the updated custom resource should be successful 01/18/23 22:45:23.727
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8595" for this suite. 01/18/23 22:45:24.28
    STEP: Destroying namespace "webhook-8595-markers" for this suite. 01/18/23 22:45:24.291
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:24.298
Jan 18 22:45:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:45:24.299
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:24.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:24.318
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/18/23 22:45:24.322
STEP: submitting the pod to kubernetes 01/18/23 22:45:24.322
STEP: verifying QOS class is set on the pod 01/18/23 22:45:24.33
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:24.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-462" for this suite. 01/18/23 22:45:24.337
------------------------------
â€¢ [0.046 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:24.298
    Jan 18 22:45:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:45:24.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:24.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:24.318
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/18/23 22:45:24.322
    STEP: submitting the pod to kubernetes 01/18/23 22:45:24.322
    STEP: verifying QOS class is set on the pod 01/18/23 22:45:24.33
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:24.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-462" for this suite. 01/18/23 22:45:24.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:24.345
Jan 18 22:45:24.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:45:24.346
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:24.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:24.362
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-75cbfe45-d3a1-4209-8b41-ffb6655595c1 01/18/23 22:45:24.366
STEP: Creating a pod to test consume secrets 01/18/23 22:45:24.371
Jan 18 22:45:24.381: INFO: Waiting up to 5m0s for pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793" in namespace "secrets-374" to be "Succeeded or Failed"
Jan 18 22:45:24.384: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19682ms
Jan 18 22:45:26.389: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007637801s
Jan 18 22:45:28.388: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007279856s
STEP: Saw pod success 01/18/23 22:45:28.388
Jan 18 22:45:28.388: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793" satisfied condition "Succeeded or Failed"
Jan 18 22:45:28.391: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:45:28.401
Jan 18 22:45:28.412: INFO: Waiting for pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 to disappear
Jan 18 22:45:28.417: INFO: Pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:28.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-374" for this suite. 01/18/23 22:45:28.42
------------------------------
â€¢ [4.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:24.345
    Jan 18 22:45:24.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:45:24.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:24.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:24.362
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-75cbfe45-d3a1-4209-8b41-ffb6655595c1 01/18/23 22:45:24.366
    STEP: Creating a pod to test consume secrets 01/18/23 22:45:24.371
    Jan 18 22:45:24.381: INFO: Waiting up to 5m0s for pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793" in namespace "secrets-374" to be "Succeeded or Failed"
    Jan 18 22:45:24.384: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19682ms
    Jan 18 22:45:26.389: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007637801s
    Jan 18 22:45:28.388: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007279856s
    STEP: Saw pod success 01/18/23 22:45:28.388
    Jan 18 22:45:28.388: INFO: Pod "pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:28.391: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:45:28.401
    Jan 18 22:45:28.412: INFO: Waiting for pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 to disappear
    Jan 18 22:45:28.417: INFO: Pod pod-secrets-7b4a166a-81f3-4d59-8cff-fc73c012a793 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:28.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-374" for this suite. 01/18/23 22:45:28.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:28.427
Jan 18 22:45:28.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:45:28.427
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:28.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:28.44
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1bd4c8f0-ed58-4f8b-8655-dfedc59e2f04 01/18/23 22:45:28.447
STEP: Creating the pod 01/18/23 22:45:28.451
Jan 18 22:45:28.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6" in namespace "projected-7331" to be "running and ready"
Jan 18 22:45:28.460: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.605252ms
Jan 18 22:45:28.460: INFO: The phase of Pod pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:30.465: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007387982s
Jan 18 22:45:30.465: INFO: The phase of Pod pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6 is Running (Ready = true)
Jan 18 22:45:30.465: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-1bd4c8f0-ed58-4f8b-8655-dfedc59e2f04 01/18/23 22:45:30.473
STEP: waiting to observe update in volume 01/18/23 22:45:30.479
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:45:32.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7331" for this suite. 01/18/23 22:45:32.495
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:28.427
    Jan 18 22:45:28.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:45:28.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:28.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:28.44
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-1bd4c8f0-ed58-4f8b-8655-dfedc59e2f04 01/18/23 22:45:28.447
    STEP: Creating the pod 01/18/23 22:45:28.451
    Jan 18 22:45:28.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6" in namespace "projected-7331" to be "running and ready"
    Jan 18 22:45:28.460: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.605252ms
    Jan 18 22:45:28.460: INFO: The phase of Pod pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:30.465: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007387982s
    Jan 18 22:45:30.465: INFO: The phase of Pod pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6 is Running (Ready = true)
    Jan 18 22:45:30.465: INFO: Pod "pod-projected-configmaps-c1b20847-86af-49c1-a759-ebd767de57d6" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-1bd4c8f0-ed58-4f8b-8655-dfedc59e2f04 01/18/23 22:45:30.473
    STEP: waiting to observe update in volume 01/18/23 22:45:30.479
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:45:32.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7331" for this suite. 01/18/23 22:45:32.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:45:32.501
Jan 18 22:45:32.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:45:32.502
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:32.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:32.517
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/18/23 22:45:49.523
STEP: Creating a ResourceQuota 01/18/23 22:45:54.527
STEP: Ensuring resource quota status is calculated 01/18/23 22:45:54.532
STEP: Creating a ConfigMap 01/18/23 22:45:56.537
STEP: Ensuring resource quota status captures configMap creation 01/18/23 22:45:56.548
STEP: Deleting a ConfigMap 01/18/23 22:45:58.552
STEP: Ensuring resource quota status released usage 01/18/23 22:45:58.557
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:46:00.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1990" for this suite. 01/18/23 22:46:00.564
------------------------------
â€¢ [SLOW TEST] [28.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:45:32.501
    Jan 18 22:45:32.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:45:32.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:32.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:32.517
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/18/23 22:45:49.523
    STEP: Creating a ResourceQuota 01/18/23 22:45:54.527
    STEP: Ensuring resource quota status is calculated 01/18/23 22:45:54.532
    STEP: Creating a ConfigMap 01/18/23 22:45:56.537
    STEP: Ensuring resource quota status captures configMap creation 01/18/23 22:45:56.548
    STEP: Deleting a ConfigMap 01/18/23 22:45:58.552
    STEP: Ensuring resource quota status released usage 01/18/23 22:45:58.557
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:46:00.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1990" for this suite. 01/18/23 22:46:00.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:46:00.57
Jan 18 22:46:00.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename init-container 01/18/23 22:46:00.571
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:00.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:00.585
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/18/23 22:46:00.588
Jan 18 22:46:00.588: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:46:06.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-823" for this suite. 01/18/23 22:46:06.069
------------------------------
â€¢ [SLOW TEST] [5.505 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:46:00.57
    Jan 18 22:46:00.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename init-container 01/18/23 22:46:00.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:00.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:00.585
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/18/23 22:46:00.588
    Jan 18 22:46:00.588: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:46:06.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-823" for this suite. 01/18/23 22:46:06.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:46:06.076
Jan 18 22:46:06.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:46:06.078
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:06.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:06.104
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-5615 01/18/23 22:46:06.107
STEP: creating replication controller nodeport-test in namespace services-5615 01/18/23 22:46:06.118
I0118 22:46:06.126157      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5615, replica count: 2
I0118 22:46:09.178125      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:46:09.178: INFO: Creating new exec pod
Jan 18 22:46:09.185: INFO: Waiting up to 5m0s for pod "execpoddjdxv" in namespace "services-5615" to be "running"
Jan 18 22:46:09.187: INFO: Pod "execpoddjdxv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446159ms
Jan 18 22:46:11.191: INFO: Pod "execpoddjdxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005859954s
Jan 18 22:46:11.191: INFO: Pod "execpoddjdxv" satisfied condition "running"
Jan 18 22:46:12.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 18 22:46:12.329: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 22:46:12.329: INFO: stdout: ""
Jan 18 22:46:12.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.96.3.102 80'
Jan 18 22:46:12.488: INFO: stderr: "+ nc -v -z -w 2 10.96.3.102 80\nConnection to 10.96.3.102 80 port [tcp/http] succeeded!\n"
Jan 18 22:46:12.488: INFO: stdout: ""
Jan 18 22:46:12.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 32389'
Jan 18 22:46:12.620: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 32389\nConnection to 10.128.15.198 32389 port [tcp/*] succeeded!\n"
Jan 18 22:46:12.620: INFO: stdout: ""
Jan 18 22:46:12.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 32389'
Jan 18 22:46:12.767: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 32389\nConnection to 10.128.15.199 32389 port [tcp/*] succeeded!\n"
Jan 18 22:46:12.767: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:46:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5615" for this suite. 01/18/23 22:46:12.77
------------------------------
â€¢ [SLOW TEST] [6.699 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:46:06.076
    Jan 18 22:46:06.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:46:06.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:06.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:06.104
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-5615 01/18/23 22:46:06.107
    STEP: creating replication controller nodeport-test in namespace services-5615 01/18/23 22:46:06.118
    I0118 22:46:06.126157      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5615, replica count: 2
    I0118 22:46:09.178125      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:46:09.178: INFO: Creating new exec pod
    Jan 18 22:46:09.185: INFO: Waiting up to 5m0s for pod "execpoddjdxv" in namespace "services-5615" to be "running"
    Jan 18 22:46:09.187: INFO: Pod "execpoddjdxv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446159ms
    Jan 18 22:46:11.191: INFO: Pod "execpoddjdxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005859954s
    Jan 18 22:46:11.191: INFO: Pod "execpoddjdxv" satisfied condition "running"
    Jan 18 22:46:12.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 18 22:46:12.329: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 22:46:12.329: INFO: stdout: ""
    Jan 18 22:46:12.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.96.3.102 80'
    Jan 18 22:46:12.488: INFO: stderr: "+ nc -v -z -w 2 10.96.3.102 80\nConnection to 10.96.3.102 80 port [tcp/http] succeeded!\n"
    Jan 18 22:46:12.488: INFO: stdout: ""
    Jan 18 22:46:12.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 32389'
    Jan 18 22:46:12.620: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 32389\nConnection to 10.128.15.198 32389 port [tcp/*] succeeded!\n"
    Jan 18 22:46:12.620: INFO: stdout: ""
    Jan 18 22:46:12.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-5615 exec execpoddjdxv -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 32389'
    Jan 18 22:46:12.767: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 32389\nConnection to 10.128.15.199 32389 port [tcp/*] succeeded!\n"
    Jan 18 22:46:12.767: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:46:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5615" for this suite. 01/18/23 22:46:12.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:46:12.777
Jan 18 22:46:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 22:46:12.779
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:12.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:12.793
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/18/23 22:46:12.796
STEP: getting 01/18/23 22:46:12.812
STEP: listing in namespace 01/18/23 22:46:12.815
STEP: patching 01/18/23 22:46:12.817
STEP: deleting 01/18/23 22:46:12.833
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:46:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5549" for this suite. 01/18/23 22:46:12.845
------------------------------
â€¢ [0.073 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:46:12.777
    Jan 18 22:46:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 22:46:12.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:12.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:12.793
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/18/23 22:46:12.796
    STEP: getting 01/18/23 22:46:12.812
    STEP: listing in namespace 01/18/23 22:46:12.815
    STEP: patching 01/18/23 22:46:12.817
    STEP: deleting 01/18/23 22:46:12.833
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:46:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5549" for this suite. 01/18/23 22:46:12.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:46:12.851
Jan 18 22:46:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:46:12.852
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:12.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:12.869
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/18/23 22:46:12.872
Jan 18 22:46:12.881: INFO: Waiting up to 5m0s for pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33" in namespace "projected-6355" to be "running and ready"
Jan 18 22:46:12.884: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587002ms
Jan 18 22:46:12.884: INFO: The phase of Pod labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:46:14.888: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33": Phase="Running", Reason="", readiness=true. Elapsed: 2.006749678s
Jan 18 22:46:14.888: INFO: The phase of Pod labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33 is Running (Ready = true)
Jan 18 22:46:14.888: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33" satisfied condition "running and ready"
Jan 18 22:46:15.409: INFO: Successfully updated pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:46:19.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6355" for this suite. 01/18/23 22:46:19.431
------------------------------
â€¢ [SLOW TEST] [6.586 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:46:12.851
    Jan 18 22:46:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:46:12.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:12.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:12.869
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/18/23 22:46:12.872
    Jan 18 22:46:12.881: INFO: Waiting up to 5m0s for pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33" in namespace "projected-6355" to be "running and ready"
    Jan 18 22:46:12.884: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587002ms
    Jan 18 22:46:12.884: INFO: The phase of Pod labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:46:14.888: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33": Phase="Running", Reason="", readiness=true. Elapsed: 2.006749678s
    Jan 18 22:46:14.888: INFO: The phase of Pod labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33 is Running (Ready = true)
    Jan 18 22:46:14.888: INFO: Pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33" satisfied condition "running and ready"
    Jan 18 22:46:15.409: INFO: Successfully updated pod "labelsupdatee9c68fdc-95f0-44db-bb2f-f181cc0f3c33"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:46:19.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6355" for this suite. 01/18/23 22:46:19.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:46:19.438
Jan 18 22:46:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 22:46:19.439
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:19.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:19.454
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/18/23 22:46:19.457
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:19.693
Jan 18 22:46:19.802: INFO: Pod name wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:46:19.802
Jan 18 22:46:19.802: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:19.844: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.34596ms
Jan 18 22:46:21.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047358754s
Jan 18 22:46:23.848: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04662444s
Jan 18 22:46:25.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048010179s
Jan 18 22:46:27.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046999494s
Jan 18 22:46:29.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047321249s
Jan 18 22:46:31.851: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048898255s
Jan 18 22:46:33.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.047228863s
Jan 18 22:46:35.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.04800923s
Jan 18 22:46:35.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8" satisfied condition "running"
Jan 18 22:46:35.850: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:35.853: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt": Phase="Running", Reason="", readiness=true. Elapsed: 3.555239ms
Jan 18 22:46:35.853: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt" satisfied condition "running"
Jan 18 22:46:35.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:35.857: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h": Phase="Running", Reason="", readiness=true. Elapsed: 3.242164ms
Jan 18 22:46:35.857: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h" satisfied condition "running"
Jan 18 22:46:35.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:35.859: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.724463ms
Jan 18 22:46:35.859: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh" satisfied condition "running"
Jan 18 22:46:35.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:35.863: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9": Phase="Running", Reason="", readiness=true. Elapsed: 3.260184ms
Jan 18 22:46:35.863: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:46:35.863
Jan 18 22:46:35.923: INFO: Deleting ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 took: 6.366802ms
Jan 18 22:46:36.024: INFO: Terminating ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 pods took: 100.394596ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:39.628
Jan 18 22:46:39.641: INFO: Pod name wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838: Found 0 pods out of 5
Jan 18 22:46:44.648: INFO: Pod name wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:46:44.648
Jan 18 22:46:44.648: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:44.652: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347323ms
Jan 18 22:46:46.656: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007257098s
Jan 18 22:46:48.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00693937s
Jan 18 22:46:50.657: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008780279s
Jan 18 22:46:52.656: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007969449s
Jan 18 22:46:54.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Running", Reason="", readiness=true. Elapsed: 10.007083932s
Jan 18 22:46:54.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt" satisfied condition "running"
Jan 18 22:46:54.655: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:54.658: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x": Phase="Running", Reason="", readiness=true. Elapsed: 2.732929ms
Jan 18 22:46:54.658: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x" satisfied condition "running"
Jan 18 22:46:54.658: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:54.661: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw": Phase="Running", Reason="", readiness=true. Elapsed: 2.747209ms
Jan 18 22:46:54.661: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw" satisfied condition "running"
Jan 18 22:46:54.661: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:54.664: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc": Phase="Running", Reason="", readiness=true. Elapsed: 2.550711ms
Jan 18 22:46:54.664: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc" satisfied condition "running"
Jan 18 22:46:54.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:46:54.667: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92": Phase="Running", Reason="", readiness=true. Elapsed: 2.893198ms
Jan 18 22:46:54.667: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:46:54.667
Jan 18 22:46:54.726: INFO: Deleting ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 took: 6.240348ms
Jan 18 22:46:54.827: INFO: Terminating ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 pods took: 100.709287ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:57.232
Jan 18 22:46:57.246: INFO: Pod name wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5: Found 0 pods out of 5
Jan 18 22:47:02.256: INFO: Pod name wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:47:02.256
Jan 18 22:47:02.256: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:47:02.259: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183574ms
Jan 18 22:47:04.264: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00823148s
Jan 18 22:47:06.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008969046s
Jan 18 22:47:08.264: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007985211s
Jan 18 22:47:10.263: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007423111s
Jan 18 22:47:12.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Running", Reason="", readiness=true. Elapsed: 10.009275551s
Jan 18 22:47:12.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2" satisfied condition "running"
Jan 18 22:47:12.265: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:47:12.268: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025411ms
Jan 18 22:47:14.274: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z": Phase="Running", Reason="", readiness=true. Elapsed: 2.008774414s
Jan 18 22:47:14.274: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z" satisfied condition "running"
Jan 18 22:47:14.274: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:47:14.278: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz": Phase="Running", Reason="", readiness=true. Elapsed: 3.65266ms
Jan 18 22:47:14.278: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz" satisfied condition "running"
Jan 18 22:47:14.278: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:47:14.281: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs": Phase="Running", Reason="", readiness=true. Elapsed: 3.440051ms
Jan 18 22:47:14.281: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs" satisfied condition "running"
Jan 18 22:47:14.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f" in namespace "emptydir-wrapper-2882" to be "running"
Jan 18 22:47:14.284: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f": Phase="Running", Reason="", readiness=true. Elapsed: 3.045448ms
Jan 18 22:47:14.285: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:47:14.285
Jan 18 22:47:14.345: INFO: Deleting ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 took: 6.162497ms
Jan 18 22:47:14.446: INFO: Terminating ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 pods took: 100.44895ms
STEP: Cleaning up the configMaps 01/18/23 22:47:17.046
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2882" for this suite. 01/18/23 22:47:17.313
------------------------------
â€¢ [SLOW TEST] [57.880 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:46:19.438
    Jan 18 22:46:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 22:46:19.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:19.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:19.454
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/18/23 22:46:19.457
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:19.693
    Jan 18 22:46:19.802: INFO: Pod name wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:46:19.802
    Jan 18 22:46:19.802: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:19.844: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.34596ms
    Jan 18 22:46:21.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047358754s
    Jan 18 22:46:23.848: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04662444s
    Jan 18 22:46:25.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048010179s
    Jan 18 22:46:27.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046999494s
    Jan 18 22:46:29.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047321249s
    Jan 18 22:46:31.851: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048898255s
    Jan 18 22:46:33.849: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.047228863s
    Jan 18 22:46:35.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.04800923s
    Jan 18 22:46:35.850: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-5jrl8" satisfied condition "running"
    Jan 18 22:46:35.850: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:35.853: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt": Phase="Running", Reason="", readiness=true. Elapsed: 3.555239ms
    Jan 18 22:46:35.853: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-6gjbt" satisfied condition "running"
    Jan 18 22:46:35.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:35.857: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h": Phase="Running", Reason="", readiness=true. Elapsed: 3.242164ms
    Jan 18 22:46:35.857: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-ghl9h" satisfied condition "running"
    Jan 18 22:46:35.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:35.859: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.724463ms
    Jan 18 22:46:35.859: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-vwcrh" satisfied condition "running"
    Jan 18 22:46:35.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:35.863: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9": Phase="Running", Reason="", readiness=true. Elapsed: 3.260184ms
    Jan 18 22:46:35.863: INFO: Pod "wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7-z8kp9" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:46:35.863
    Jan 18 22:46:35.923: INFO: Deleting ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 took: 6.366802ms
    Jan 18 22:46:36.024: INFO: Terminating ReplicationController wrapped-volume-race-889da41f-e4e3-4178-964c-b24065d641f7 pods took: 100.394596ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:39.628
    Jan 18 22:46:39.641: INFO: Pod name wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838: Found 0 pods out of 5
    Jan 18 22:46:44.648: INFO: Pod name wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:46:44.648
    Jan 18 22:46:44.648: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:44.652: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347323ms
    Jan 18 22:46:46.656: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007257098s
    Jan 18 22:46:48.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00693937s
    Jan 18 22:46:50.657: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008780279s
    Jan 18 22:46:52.656: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007969449s
    Jan 18 22:46:54.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt": Phase="Running", Reason="", readiness=true. Elapsed: 10.007083932s
    Jan 18 22:46:54.655: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-6jgpt" satisfied condition "running"
    Jan 18 22:46:54.655: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:54.658: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x": Phase="Running", Reason="", readiness=true. Elapsed: 2.732929ms
    Jan 18 22:46:54.658: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-m492x" satisfied condition "running"
    Jan 18 22:46:54.658: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:54.661: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw": Phase="Running", Reason="", readiness=true. Elapsed: 2.747209ms
    Jan 18 22:46:54.661: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-s5zsw" satisfied condition "running"
    Jan 18 22:46:54.661: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:54.664: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc": Phase="Running", Reason="", readiness=true. Elapsed: 2.550711ms
    Jan 18 22:46:54.664: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-stspc" satisfied condition "running"
    Jan 18 22:46:54.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:46:54.667: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92": Phase="Running", Reason="", readiness=true. Elapsed: 2.893198ms
    Jan 18 22:46:54.667: INFO: Pod "wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838-zrw92" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:46:54.667
    Jan 18 22:46:54.726: INFO: Deleting ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 took: 6.240348ms
    Jan 18 22:46:54.827: INFO: Terminating ReplicationController wrapped-volume-race-1ec00b1a-9dba-458e-9bb4-764564a12838 pods took: 100.709287ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:46:57.232
    Jan 18 22:46:57.246: INFO: Pod name wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5: Found 0 pods out of 5
    Jan 18 22:47:02.256: INFO: Pod name wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:47:02.256
    Jan 18 22:47:02.256: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:47:02.259: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183574ms
    Jan 18 22:47:04.264: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00823148s
    Jan 18 22:47:06.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008969046s
    Jan 18 22:47:08.264: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007985211s
    Jan 18 22:47:10.263: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007423111s
    Jan 18 22:47:12.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2": Phase="Running", Reason="", readiness=true. Elapsed: 10.009275551s
    Jan 18 22:47:12.265: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-8vvs2" satisfied condition "running"
    Jan 18 22:47:12.265: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:47:12.268: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025411ms
    Jan 18 22:47:14.274: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z": Phase="Running", Reason="", readiness=true. Elapsed: 2.008774414s
    Jan 18 22:47:14.274: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-d275z" satisfied condition "running"
    Jan 18 22:47:14.274: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:47:14.278: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz": Phase="Running", Reason="", readiness=true. Elapsed: 3.65266ms
    Jan 18 22:47:14.278: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-lvfvz" satisfied condition "running"
    Jan 18 22:47:14.278: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:47:14.281: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs": Phase="Running", Reason="", readiness=true. Elapsed: 3.440051ms
    Jan 18 22:47:14.281: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-pw4cs" satisfied condition "running"
    Jan 18 22:47:14.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f" in namespace "emptydir-wrapper-2882" to be "running"
    Jan 18 22:47:14.284: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f": Phase="Running", Reason="", readiness=true. Elapsed: 3.045448ms
    Jan 18 22:47:14.285: INFO: Pod "wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5-s5b7f" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 in namespace emptydir-wrapper-2882, will wait for the garbage collector to delete the pods 01/18/23 22:47:14.285
    Jan 18 22:47:14.345: INFO: Deleting ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 took: 6.162497ms
    Jan 18 22:47:14.446: INFO: Terminating ReplicationController wrapped-volume-race-be565027-1873-41b0-891b-bf0d23fe17a5 pods took: 100.44895ms
    STEP: Cleaning up the configMaps 01/18/23 22:47:17.046
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2882" for this suite. 01/18/23 22:47:17.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:17.319
Jan 18 22:47:17.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename ingressclass 01/18/23 22:47:17.32
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:17.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:17.333
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/18/23 22:47:17.336
STEP: getting /apis/networking.k8s.io 01/18/23 22:47:17.339
STEP: getting /apis/networking.k8s.iov1 01/18/23 22:47:17.34
STEP: creating 01/18/23 22:47:17.341
STEP: getting 01/18/23 22:47:17.353
STEP: listing 01/18/23 22:47:17.356
STEP: watching 01/18/23 22:47:17.36
Jan 18 22:47:17.360: INFO: starting watch
STEP: patching 01/18/23 22:47:17.361
STEP: updating 01/18/23 22:47:17.365
Jan 18 22:47:17.370: INFO: waiting for watch events with expected annotations
Jan 18 22:47:17.370: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 22:47:17.37
STEP: deleting a collection 01/18/23 22:47:17.379
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:17.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3106" for this suite. 01/18/23 22:47:17.393
------------------------------
â€¢ [0.079 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:17.319
    Jan 18 22:47:17.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename ingressclass 01/18/23 22:47:17.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:17.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:17.333
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/18/23 22:47:17.336
    STEP: getting /apis/networking.k8s.io 01/18/23 22:47:17.339
    STEP: getting /apis/networking.k8s.iov1 01/18/23 22:47:17.34
    STEP: creating 01/18/23 22:47:17.341
    STEP: getting 01/18/23 22:47:17.353
    STEP: listing 01/18/23 22:47:17.356
    STEP: watching 01/18/23 22:47:17.36
    Jan 18 22:47:17.360: INFO: starting watch
    STEP: patching 01/18/23 22:47:17.361
    STEP: updating 01/18/23 22:47:17.365
    Jan 18 22:47:17.370: INFO: waiting for watch events with expected annotations
    Jan 18 22:47:17.370: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 22:47:17.37
    STEP: deleting a collection 01/18/23 22:47:17.379
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:17.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3106" for this suite. 01/18/23 22:47:17.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:17.4
Jan 18 22:47:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svc-latency 01/18/23 22:47:17.4
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:17.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:17.417
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 18 22:47:17.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9200 01/18/23 22:47:17.42
I0118 22:47:17.426091      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9200, replica count: 1
I0118 22:47:18.476951      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:47:19.478164      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:47:19.587: INFO: Created: latency-svc-zjj7t
Jan 18 22:47:19.592: INFO: Got endpoints: latency-svc-zjj7t [13.178615ms]
Jan 18 22:47:19.601: INFO: Created: latency-svc-mnxcv
Jan 18 22:47:19.605: INFO: Created: latency-svc-g9bxp
Jan 18 22:47:19.605: INFO: Got endpoints: latency-svc-mnxcv [13.509542ms]
Jan 18 22:47:19.609: INFO: Got endpoints: latency-svc-g9bxp [17.360683ms]
Jan 18 22:47:19.612: INFO: Created: latency-svc-bv6z7
Jan 18 22:47:19.615: INFO: Got endpoints: latency-svc-bv6z7 [23.249864ms]
Jan 18 22:47:19.618: INFO: Created: latency-svc-pbtc2
Jan 18 22:47:19.622: INFO: Got endpoints: latency-svc-pbtc2 [30.303073ms]
Jan 18 22:47:19.623: INFO: Created: latency-svc-wmzzn
Jan 18 22:47:19.626: INFO: Created: latency-svc-2w22k
Jan 18 22:47:19.630: INFO: Got endpoints: latency-svc-wmzzn [37.770443ms]
Jan 18 22:47:19.632: INFO: Got endpoints: latency-svc-2w22k [40.346304ms]
Jan 18 22:47:19.636: INFO: Created: latency-svc-7pxh7
Jan 18 22:47:19.639: INFO: Created: latency-svc-gtj5g
Jan 18 22:47:19.643: INFO: Got endpoints: latency-svc-7pxh7 [51.399967ms]
Jan 18 22:47:19.645: INFO: Created: latency-svc-5ct79
Jan 18 22:47:19.647: INFO: Got endpoints: latency-svc-gtj5g [54.684385ms]
Jan 18 22:47:19.650: INFO: Created: latency-svc-bv75f
Jan 18 22:47:19.652: INFO: Got endpoints: latency-svc-5ct79 [59.705048ms]
Jan 18 22:47:19.657: INFO: Created: latency-svc-88sbm
Jan 18 22:47:19.658: INFO: Got endpoints: latency-svc-bv75f [65.531086ms]
Jan 18 22:47:19.661: INFO: Created: latency-svc-4xsd2
Jan 18 22:47:19.661: INFO: Got endpoints: latency-svc-88sbm [69.360698ms]
Jan 18 22:47:19.665: INFO: Got endpoints: latency-svc-4xsd2 [72.979287ms]
Jan 18 22:47:19.669: INFO: Created: latency-svc-b922r
Jan 18 22:47:19.673: INFO: Created: latency-svc-gcblq
Jan 18 22:47:19.674: INFO: Got endpoints: latency-svc-b922r [82.122242ms]
Jan 18 22:47:19.676: INFO: Got endpoints: latency-svc-gcblq [84.334215ms]
Jan 18 22:47:19.680: INFO: Created: latency-svc-lqpvd
Jan 18 22:47:19.688: INFO: Got endpoints: latency-svc-lqpvd [95.683074ms]
Jan 18 22:47:19.688: INFO: Created: latency-svc-frzwh
Jan 18 22:47:19.696: INFO: Got endpoints: latency-svc-frzwh [90.725964ms]
Jan 18 22:47:19.698: INFO: Created: latency-svc-mbppj
Jan 18 22:47:19.698: INFO: Created: latency-svc-86drf
Jan 18 22:47:19.708: INFO: Got endpoints: latency-svc-86drf [98.396808ms]
Jan 18 22:47:19.708: INFO: Got endpoints: latency-svc-mbppj [92.397646ms]
Jan 18 22:47:19.708: INFO: Created: latency-svc-4px5j
Jan 18 22:47:19.712: INFO: Got endpoints: latency-svc-4px5j [89.720193ms]
Jan 18 22:47:19.714: INFO: Created: latency-svc-f6rqq
Jan 18 22:47:19.719: INFO: Created: latency-svc-7zm5f
Jan 18 22:47:19.720: INFO: Got endpoints: latency-svc-f6rqq [90.086516ms]
Jan 18 22:47:19.724: INFO: Created: latency-svc-6hbrq
Jan 18 22:47:19.725: INFO: Got endpoints: latency-svc-7zm5f [92.348614ms]
Jan 18 22:47:19.726: INFO: Got endpoints: latency-svc-6hbrq [82.941053ms]
Jan 18 22:47:19.732: INFO: Created: latency-svc-2h27b
Jan 18 22:47:19.735: INFO: Created: latency-svc-c69zt
Jan 18 22:47:19.737: INFO: Got endpoints: latency-svc-2h27b [90.275577ms]
Jan 18 22:47:19.740: INFO: Created: latency-svc-qdbhf
Jan 18 22:47:19.740: INFO: Got endpoints: latency-svc-c69zt [88.421418ms]
Jan 18 22:47:19.743: INFO: Created: latency-svc-22ln8
Jan 18 22:47:19.745: INFO: Got endpoints: latency-svc-qdbhf [87.571674ms]
Jan 18 22:47:19.750: INFO: Got endpoints: latency-svc-22ln8 [89.041054ms]
Jan 18 22:47:19.792: INFO: Created: latency-svc-s8lc4
Jan 18 22:47:19.795: INFO: Created: latency-svc-8pqzj
Jan 18 22:47:19.797: INFO: Got endpoints: latency-svc-s8lc4 [100.63932ms]
Jan 18 22:47:19.798: INFO: Created: latency-svc-vdjtf
Jan 18 22:47:19.799: INFO: Created: latency-svc-qhtbt
Jan 18 22:47:19.799: INFO: Created: latency-svc-z9jr8
Jan 18 22:47:19.799: INFO: Created: latency-svc-c9mc8
Jan 18 22:47:19.799: INFO: Created: latency-svc-f6svw
Jan 18 22:47:19.799: INFO: Created: latency-svc-q7db5
Jan 18 22:47:19.799: INFO: Created: latency-svc-szxsj
Jan 18 22:47:19.799: INFO: Created: latency-svc-gjxzv
Jan 18 22:47:19.799: INFO: Created: latency-svc-bxz9k
Jan 18 22:47:19.799: INFO: Created: latency-svc-w4hjg
Jan 18 22:47:19.799: INFO: Created: latency-svc-zdcqf
Jan 18 22:47:19.799: INFO: Created: latency-svc-68qx2
Jan 18 22:47:19.801: INFO: Created: latency-svc-g8fgx
Jan 18 22:47:19.806: INFO: Got endpoints: latency-svc-68qx2 [129.709078ms]
Jan 18 22:47:19.807: INFO: Got endpoints: latency-svc-zdcqf [99.572621ms]
Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-g8fgx [99.762946ms]
Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-8pqzj [142.412113ms]
Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-w4hjg [81.065089ms]
Jan 18 22:47:19.813: INFO: Got endpoints: latency-svc-bxz9k [138.53337ms]
Jan 18 22:47:19.816: INFO: Created: latency-svc-hxzzb
Jan 18 22:47:19.818: INFO: Created: latency-svc-8s7bb
Jan 18 22:47:19.823: INFO: Created: latency-svc-5zzmg
Jan 18 22:47:19.826: INFO: Created: latency-svc-7b7h9
Jan 18 22:47:19.828: INFO: Created: latency-svc-5vztr
Jan 18 22:47:19.832: INFO: Created: latency-svc-pzpj9
Jan 18 22:47:19.837: INFO: Created: latency-svc-tzvz7
Jan 18 22:47:19.844: INFO: Got endpoints: latency-svc-vdjtf [131.93585ms]
Jan 18 22:47:19.854: INFO: Created: latency-svc-rvbll
Jan 18 22:47:19.894: INFO: Got endpoints: latency-svc-gjxzv [148.477056ms]
Jan 18 22:47:19.905: INFO: Created: latency-svc-v2svn
Jan 18 22:47:19.944: INFO: Got endpoints: latency-svc-q7db5 [203.625019ms]
Jan 18 22:47:19.953: INFO: Created: latency-svc-k9rjk
Jan 18 22:47:19.994: INFO: Got endpoints: latency-svc-f6svw [306.663652ms]
Jan 18 22:47:20.007: INFO: Created: latency-svc-xpddb
Jan 18 22:47:20.043: INFO: Got endpoints: latency-svc-c9mc8 [322.730634ms]
Jan 18 22:47:20.051: INFO: Created: latency-svc-f5zg5
Jan 18 22:47:20.093: INFO: Got endpoints: latency-svc-z9jr8 [355.39054ms]
Jan 18 22:47:20.102: INFO: Created: latency-svc-vxdgl
Jan 18 22:47:20.143: INFO: Got endpoints: latency-svc-szxsj [417.940766ms]
Jan 18 22:47:20.152: INFO: Created: latency-svc-tfng6
Jan 18 22:47:20.192: INFO: Got endpoints: latency-svc-qhtbt [441.980272ms]
Jan 18 22:47:20.203: INFO: Created: latency-svc-xw8fs
Jan 18 22:47:20.243: INFO: Got endpoints: latency-svc-hxzzb [446.257747ms]
Jan 18 22:47:20.253: INFO: Created: latency-svc-sq2r4
Jan 18 22:47:20.293: INFO: Got endpoints: latency-svc-8s7bb [486.560209ms]
Jan 18 22:47:20.303: INFO: Created: latency-svc-h8jdg
Jan 18 22:47:20.351: INFO: Got endpoints: latency-svc-5zzmg [543.993863ms]
Jan 18 22:47:20.360: INFO: Created: latency-svc-fcv6f
Jan 18 22:47:20.395: INFO: Got endpoints: latency-svc-7b7h9 [587.06499ms]
Jan 18 22:47:20.402: INFO: Created: latency-svc-zrzmt
Jan 18 22:47:20.445: INFO: Got endpoints: latency-svc-5vztr [637.634447ms]
Jan 18 22:47:20.454: INFO: Created: latency-svc-dvjdk
Jan 18 22:47:20.493: INFO: Got endpoints: latency-svc-pzpj9 [685.633358ms]
Jan 18 22:47:20.504: INFO: Created: latency-svc-7hrcq
Jan 18 22:47:20.543: INFO: Got endpoints: latency-svc-tzvz7 [729.864936ms]
Jan 18 22:47:20.553: INFO: Created: latency-svc-72rxl
Jan 18 22:47:20.593: INFO: Got endpoints: latency-svc-rvbll [748.821098ms]
Jan 18 22:47:20.601: INFO: Created: latency-svc-xgcxv
Jan 18 22:47:20.645: INFO: Got endpoints: latency-svc-v2svn [750.955297ms]
Jan 18 22:47:20.653: INFO: Created: latency-svc-bd4f6
Jan 18 22:47:20.692: INFO: Got endpoints: latency-svc-k9rjk [748.509678ms]
Jan 18 22:47:20.700: INFO: Created: latency-svc-2c254
Jan 18 22:47:20.745: INFO: Got endpoints: latency-svc-xpddb [750.544466ms]
Jan 18 22:47:20.753: INFO: Created: latency-svc-8qrts
Jan 18 22:47:20.793: INFO: Got endpoints: latency-svc-f5zg5 [749.849537ms]
Jan 18 22:47:20.803: INFO: Created: latency-svc-7cmmh
Jan 18 22:47:20.843: INFO: Got endpoints: latency-svc-vxdgl [750.679119ms]
Jan 18 22:47:20.853: INFO: Created: latency-svc-d7cq9
Jan 18 22:47:20.893: INFO: Got endpoints: latency-svc-tfng6 [749.788678ms]
Jan 18 22:47:20.901: INFO: Created: latency-svc-vnn9j
Jan 18 22:47:20.946: INFO: Got endpoints: latency-svc-xw8fs [753.554737ms]
Jan 18 22:47:20.954: INFO: Created: latency-svc-g6h69
Jan 18 22:47:20.995: INFO: Got endpoints: latency-svc-sq2r4 [752.201853ms]
Jan 18 22:47:21.004: INFO: Created: latency-svc-5chgn
Jan 18 22:47:21.042: INFO: Got endpoints: latency-svc-h8jdg [749.71253ms]
Jan 18 22:47:21.052: INFO: Created: latency-svc-5zfz6
Jan 18 22:47:21.092: INFO: Got endpoints: latency-svc-fcv6f [740.961411ms]
Jan 18 22:47:21.102: INFO: Created: latency-svc-77t24
Jan 18 22:47:21.144: INFO: Got endpoints: latency-svc-zrzmt [749.337861ms]
Jan 18 22:47:21.155: INFO: Created: latency-svc-x7m4g
Jan 18 22:47:21.193: INFO: Got endpoints: latency-svc-dvjdk [747.602902ms]
Jan 18 22:47:21.201: INFO: Created: latency-svc-ptbhv
Jan 18 22:47:21.245: INFO: Got endpoints: latency-svc-7hrcq [752.008507ms]
Jan 18 22:47:21.253: INFO: Created: latency-svc-qh7c7
Jan 18 22:47:21.295: INFO: Got endpoints: latency-svc-72rxl [752.292678ms]
Jan 18 22:47:21.303: INFO: Created: latency-svc-g2tj9
Jan 18 22:47:21.342: INFO: Got endpoints: latency-svc-xgcxv [749.214613ms]
Jan 18 22:47:21.351: INFO: Created: latency-svc-wzfjn
Jan 18 22:47:21.393: INFO: Got endpoints: latency-svc-bd4f6 [748.203695ms]
Jan 18 22:47:21.402: INFO: Created: latency-svc-7frrb
Jan 18 22:47:21.442: INFO: Got endpoints: latency-svc-2c254 [750.052926ms]
Jan 18 22:47:21.450: INFO: Created: latency-svc-99nkp
Jan 18 22:47:21.495: INFO: Got endpoints: latency-svc-8qrts [750.002769ms]
Jan 18 22:47:21.502: INFO: Created: latency-svc-jqbsd
Jan 18 22:47:21.545: INFO: Got endpoints: latency-svc-7cmmh [752.758136ms]
Jan 18 22:47:21.554: INFO: Created: latency-svc-ph25k
Jan 18 22:47:21.594: INFO: Got endpoints: latency-svc-d7cq9 [750.645052ms]
Jan 18 22:47:21.603: INFO: Created: latency-svc-c4nms
Jan 18 22:47:21.643: INFO: Got endpoints: latency-svc-vnn9j [750.509028ms]
Jan 18 22:47:21.653: INFO: Created: latency-svc-xwfjx
Jan 18 22:47:21.693: INFO: Got endpoints: latency-svc-g6h69 [746.533819ms]
Jan 18 22:47:21.704: INFO: Created: latency-svc-hgs77
Jan 18 22:47:21.742: INFO: Got endpoints: latency-svc-5chgn [747.029829ms]
Jan 18 22:47:21.750: INFO: Created: latency-svc-djq75
Jan 18 22:47:21.795: INFO: Got endpoints: latency-svc-5zfz6 [752.177076ms]
Jan 18 22:47:21.806: INFO: Created: latency-svc-6tmq2
Jan 18 22:47:21.842: INFO: Got endpoints: latency-svc-77t24 [749.960606ms]
Jan 18 22:47:21.850: INFO: Created: latency-svc-cbr5j
Jan 18 22:47:21.894: INFO: Got endpoints: latency-svc-x7m4g [750.333819ms]
Jan 18 22:47:21.902: INFO: Created: latency-svc-n6526
Jan 18 22:47:21.945: INFO: Got endpoints: latency-svc-ptbhv [751.859371ms]
Jan 18 22:47:21.953: INFO: Created: latency-svc-57mt7
Jan 18 22:47:21.993: INFO: Got endpoints: latency-svc-qh7c7 [747.749429ms]
Jan 18 22:47:22.003: INFO: Created: latency-svc-5jg4h
Jan 18 22:47:22.044: INFO: Got endpoints: latency-svc-g2tj9 [748.933452ms]
Jan 18 22:47:22.054: INFO: Created: latency-svc-vldzs
Jan 18 22:47:22.095: INFO: Got endpoints: latency-svc-wzfjn [752.388865ms]
Jan 18 22:47:22.102: INFO: Created: latency-svc-728d7
Jan 18 22:47:22.143: INFO: Got endpoints: latency-svc-7frrb [750.055255ms]
Jan 18 22:47:22.152: INFO: Created: latency-svc-8qfzf
Jan 18 22:47:22.195: INFO: Got endpoints: latency-svc-99nkp [752.802925ms]
Jan 18 22:47:22.203: INFO: Created: latency-svc-nl78n
Jan 18 22:47:22.243: INFO: Got endpoints: latency-svc-jqbsd [747.690215ms]
Jan 18 22:47:22.251: INFO: Created: latency-svc-w7bpq
Jan 18 22:47:22.300: INFO: Got endpoints: latency-svc-ph25k [754.336803ms]
Jan 18 22:47:22.308: INFO: Created: latency-svc-d2qhq
Jan 18 22:47:22.342: INFO: Got endpoints: latency-svc-c4nms [748.418547ms]
Jan 18 22:47:22.352: INFO: Created: latency-svc-l6gb6
Jan 18 22:47:22.398: INFO: Got endpoints: latency-svc-xwfjx [754.715097ms]
Jan 18 22:47:22.409: INFO: Created: latency-svc-rtm47
Jan 18 22:47:22.442: INFO: Got endpoints: latency-svc-hgs77 [749.361815ms]
Jan 18 22:47:22.449: INFO: Created: latency-svc-cqcqd
Jan 18 22:47:22.494: INFO: Got endpoints: latency-svc-djq75 [751.565086ms]
Jan 18 22:47:22.508: INFO: Created: latency-svc-2h8lh
Jan 18 22:47:22.541: INFO: Got endpoints: latency-svc-6tmq2 [746.676353ms]
Jan 18 22:47:22.551: INFO: Created: latency-svc-mzdf4
Jan 18 22:47:22.593: INFO: Got endpoints: latency-svc-cbr5j [750.053812ms]
Jan 18 22:47:22.602: INFO: Created: latency-svc-fmztk
Jan 18 22:47:22.642: INFO: Got endpoints: latency-svc-n6526 [747.804248ms]
Jan 18 22:47:22.654: INFO: Created: latency-svc-b8fd8
Jan 18 22:47:22.694: INFO: Got endpoints: latency-svc-57mt7 [749.439808ms]
Jan 18 22:47:22.702: INFO: Created: latency-svc-s6nwh
Jan 18 22:47:22.744: INFO: Got endpoints: latency-svc-5jg4h [751.222893ms]
Jan 18 22:47:22.754: INFO: Created: latency-svc-vkmqw
Jan 18 22:47:22.792: INFO: Got endpoints: latency-svc-vldzs [747.784904ms]
Jan 18 22:47:22.801: INFO: Created: latency-svc-jtk6w
Jan 18 22:47:22.842: INFO: Got endpoints: latency-svc-728d7 [747.575997ms]
Jan 18 22:47:22.852: INFO: Created: latency-svc-956x9
Jan 18 22:47:22.893: INFO: Got endpoints: latency-svc-8qfzf [749.360579ms]
Jan 18 22:47:22.901: INFO: Created: latency-svc-sh288
Jan 18 22:47:22.942: INFO: Got endpoints: latency-svc-nl78n [746.7371ms]
Jan 18 22:47:22.951: INFO: Created: latency-svc-tdbbq
Jan 18 22:47:22.993: INFO: Got endpoints: latency-svc-w7bpq [749.841363ms]
Jan 18 22:47:23.006: INFO: Created: latency-svc-n6786
Jan 18 22:47:23.043: INFO: Got endpoints: latency-svc-d2qhq [743.071687ms]
Jan 18 22:47:23.051: INFO: Created: latency-svc-xm2cc
Jan 18 22:47:23.092: INFO: Got endpoints: latency-svc-l6gb6 [749.918592ms]
Jan 18 22:47:23.099: INFO: Created: latency-svc-x8l7m
Jan 18 22:47:23.144: INFO: Got endpoints: latency-svc-rtm47 [745.668609ms]
Jan 18 22:47:23.152: INFO: Created: latency-svc-fvt45
Jan 18 22:47:23.194: INFO: Got endpoints: latency-svc-cqcqd [751.541774ms]
Jan 18 22:47:23.203: INFO: Created: latency-svc-ff626
Jan 18 22:47:23.244: INFO: Got endpoints: latency-svc-2h8lh [749.544885ms]
Jan 18 22:47:23.251: INFO: Created: latency-svc-sdpzh
Jan 18 22:47:23.294: INFO: Got endpoints: latency-svc-mzdf4 [753.081757ms]
Jan 18 22:47:23.302: INFO: Created: latency-svc-dqnc7
Jan 18 22:47:23.342: INFO: Got endpoints: latency-svc-fmztk [749.907151ms]
Jan 18 22:47:23.352: INFO: Created: latency-svc-zcr95
Jan 18 22:47:23.399: INFO: Got endpoints: latency-svc-b8fd8 [756.932407ms]
Jan 18 22:47:23.407: INFO: Created: latency-svc-jk2q4
Jan 18 22:47:23.444: INFO: Got endpoints: latency-svc-s6nwh [749.460955ms]
Jan 18 22:47:23.453: INFO: Created: latency-svc-kf98w
Jan 18 22:47:23.493: INFO: Got endpoints: latency-svc-vkmqw [748.502908ms]
Jan 18 22:47:23.502: INFO: Created: latency-svc-5bbq9
Jan 18 22:47:23.543: INFO: Got endpoints: latency-svc-jtk6w [750.969105ms]
Jan 18 22:47:23.552: INFO: Created: latency-svc-nwsfx
Jan 18 22:47:23.594: INFO: Got endpoints: latency-svc-956x9 [751.96542ms]
Jan 18 22:47:23.601: INFO: Created: latency-svc-947vq
Jan 18 22:47:23.645: INFO: Got endpoints: latency-svc-sh288 [752.06755ms]
Jan 18 22:47:23.656: INFO: Created: latency-svc-rrzbk
Jan 18 22:47:23.694: INFO: Got endpoints: latency-svc-tdbbq [752.174541ms]
Jan 18 22:47:23.703: INFO: Created: latency-svc-lc5b6
Jan 18 22:47:23.743: INFO: Got endpoints: latency-svc-n6786 [750.196538ms]
Jan 18 22:47:23.755: INFO: Created: latency-svc-8b9rv
Jan 18 22:47:23.792: INFO: Got endpoints: latency-svc-xm2cc [749.247136ms]
Jan 18 22:47:23.805: INFO: Created: latency-svc-d5jmn
Jan 18 22:47:23.843: INFO: Got endpoints: latency-svc-x8l7m [750.498985ms]
Jan 18 22:47:23.852: INFO: Created: latency-svc-t2ckx
Jan 18 22:47:23.892: INFO: Got endpoints: latency-svc-fvt45 [748.637711ms]
Jan 18 22:47:23.900: INFO: Created: latency-svc-bf7zp
Jan 18 22:47:23.945: INFO: Got endpoints: latency-svc-ff626 [751.499431ms]
Jan 18 22:47:23.953: INFO: Created: latency-svc-f8dpg
Jan 18 22:47:23.995: INFO: Got endpoints: latency-svc-sdpzh [751.029761ms]
Jan 18 22:47:24.003: INFO: Created: latency-svc-889wp
Jan 18 22:47:24.043: INFO: Got endpoints: latency-svc-dqnc7 [748.283633ms]
Jan 18 22:47:24.052: INFO: Created: latency-svc-9shh8
Jan 18 22:47:24.094: INFO: Got endpoints: latency-svc-zcr95 [751.123156ms]
Jan 18 22:47:24.102: INFO: Created: latency-svc-mqpbb
Jan 18 22:47:24.142: INFO: Got endpoints: latency-svc-jk2q4 [743.13423ms]
Jan 18 22:47:24.154: INFO: Created: latency-svc-9jh2w
Jan 18 22:47:24.197: INFO: Got endpoints: latency-svc-kf98w [753.299697ms]
Jan 18 22:47:24.205: INFO: Created: latency-svc-x2q5f
Jan 18 22:47:24.245: INFO: Got endpoints: latency-svc-5bbq9 [752.278938ms]
Jan 18 22:47:24.253: INFO: Created: latency-svc-rrczz
Jan 18 22:47:24.295: INFO: Got endpoints: latency-svc-nwsfx [751.72132ms]
Jan 18 22:47:24.303: INFO: Created: latency-svc-ltglb
Jan 18 22:47:24.344: INFO: Got endpoints: latency-svc-947vq [749.384644ms]
Jan 18 22:47:24.351: INFO: Created: latency-svc-s8df6
Jan 18 22:47:24.394: INFO: Got endpoints: latency-svc-rrzbk [749.191886ms]
Jan 18 22:47:24.403: INFO: Created: latency-svc-8wlrb
Jan 18 22:47:24.442: INFO: Got endpoints: latency-svc-lc5b6 [747.784639ms]
Jan 18 22:47:24.452: INFO: Created: latency-svc-xm4zk
Jan 18 22:47:24.492: INFO: Got endpoints: latency-svc-8b9rv [749.449222ms]
Jan 18 22:47:24.500: INFO: Created: latency-svc-tp7d7
Jan 18 22:47:24.544: INFO: Got endpoints: latency-svc-d5jmn [751.983252ms]
Jan 18 22:47:24.552: INFO: Created: latency-svc-cmfk8
Jan 18 22:47:24.596: INFO: Got endpoints: latency-svc-t2ckx [753.122806ms]
Jan 18 22:47:24.604: INFO: Created: latency-svc-bwvzh
Jan 18 22:47:24.646: INFO: Got endpoints: latency-svc-bf7zp [753.208518ms]
Jan 18 22:47:24.653: INFO: Created: latency-svc-n7bxt
Jan 18 22:47:24.695: INFO: Got endpoints: latency-svc-f8dpg [749.878343ms]
Jan 18 22:47:24.703: INFO: Created: latency-svc-kx4wr
Jan 18 22:47:24.742: INFO: Got endpoints: latency-svc-889wp [747.710224ms]
Jan 18 22:47:24.753: INFO: Created: latency-svc-9jwgp
Jan 18 22:47:24.793: INFO: Got endpoints: latency-svc-9shh8 [750.464701ms]
Jan 18 22:47:24.806: INFO: Created: latency-svc-wjx8t
Jan 18 22:47:24.842: INFO: Got endpoints: latency-svc-mqpbb [748.377039ms]
Jan 18 22:47:24.850: INFO: Created: latency-svc-sfm7s
Jan 18 22:47:24.894: INFO: Got endpoints: latency-svc-9jh2w [751.399514ms]
Jan 18 22:47:24.904: INFO: Created: latency-svc-4gxzt
Jan 18 22:47:24.942: INFO: Got endpoints: latency-svc-x2q5f [745.216301ms]
Jan 18 22:47:24.950: INFO: Created: latency-svc-2zczj
Jan 18 22:47:24.995: INFO: Got endpoints: latency-svc-rrczz [749.880968ms]
Jan 18 22:47:25.003: INFO: Created: latency-svc-9p57r
Jan 18 22:47:25.043: INFO: Got endpoints: latency-svc-ltglb [747.916257ms]
Jan 18 22:47:25.052: INFO: Created: latency-svc-zdsj7
Jan 18 22:47:25.093: INFO: Got endpoints: latency-svc-s8df6 [748.846143ms]
Jan 18 22:47:25.102: INFO: Created: latency-svc-gb7gq
Jan 18 22:47:25.144: INFO: Got endpoints: latency-svc-8wlrb [750.232387ms]
Jan 18 22:47:25.152: INFO: Created: latency-svc-bsz6t
Jan 18 22:47:25.195: INFO: Got endpoints: latency-svc-xm4zk [753.46396ms]
Jan 18 22:47:25.205: INFO: Created: latency-svc-2kcq7
Jan 18 22:47:25.243: INFO: Got endpoints: latency-svc-tp7d7 [750.719279ms]
Jan 18 22:47:25.251: INFO: Created: latency-svc-zwdd9
Jan 18 22:47:25.293: INFO: Got endpoints: latency-svc-cmfk8 [748.706586ms]
Jan 18 22:47:25.302: INFO: Created: latency-svc-ltsvn
Jan 18 22:47:25.343: INFO: Got endpoints: latency-svc-bwvzh [747.360039ms]
Jan 18 22:47:25.353: INFO: Created: latency-svc-cbzl9
Jan 18 22:47:25.393: INFO: Got endpoints: latency-svc-n7bxt [746.886677ms]
Jan 18 22:47:25.401: INFO: Created: latency-svc-cvczg
Jan 18 22:47:25.445: INFO: Got endpoints: latency-svc-kx4wr [750.30587ms]
Jan 18 22:47:25.454: INFO: Created: latency-svc-rtqkt
Jan 18 22:47:25.493: INFO: Got endpoints: latency-svc-9jwgp [750.200363ms]
Jan 18 22:47:25.501: INFO: Created: latency-svc-4zq6c
Jan 18 22:47:25.542: INFO: Got endpoints: latency-svc-wjx8t [748.504269ms]
Jan 18 22:47:25.550: INFO: Created: latency-svc-9cp6k
Jan 18 22:47:25.592: INFO: Got endpoints: latency-svc-sfm7s [750.265815ms]
Jan 18 22:47:25.601: INFO: Created: latency-svc-z7p4x
Jan 18 22:47:25.642: INFO: Got endpoints: latency-svc-4gxzt [748.412244ms]
Jan 18 22:47:25.651: INFO: Created: latency-svc-mv2m7
Jan 18 22:47:25.693: INFO: Got endpoints: latency-svc-2zczj [750.551226ms]
Jan 18 22:47:25.701: INFO: Created: latency-svc-rbbmh
Jan 18 22:47:25.745: INFO: Got endpoints: latency-svc-9p57r [749.969391ms]
Jan 18 22:47:25.753: INFO: Created: latency-svc-ng5r5
Jan 18 22:47:25.792: INFO: Got endpoints: latency-svc-zdsj7 [749.776324ms]
Jan 18 22:47:25.800: INFO: Created: latency-svc-7227w
Jan 18 22:47:25.846: INFO: Got endpoints: latency-svc-gb7gq [752.994897ms]
Jan 18 22:47:25.853: INFO: Created: latency-svc-z2lft
Jan 18 22:47:25.892: INFO: Got endpoints: latency-svc-bsz6t [747.756863ms]
Jan 18 22:47:25.900: INFO: Created: latency-svc-skwjr
Jan 18 22:47:25.942: INFO: Got endpoints: latency-svc-2kcq7 [746.515207ms]
Jan 18 22:47:25.955: INFO: Created: latency-svc-dh8hj
Jan 18 22:47:25.993: INFO: Got endpoints: latency-svc-zwdd9 [749.465835ms]
Jan 18 22:47:26.002: INFO: Created: latency-svc-z29qt
Jan 18 22:47:26.043: INFO: Got endpoints: latency-svc-ltsvn [749.781537ms]
Jan 18 22:47:26.053: INFO: Created: latency-svc-9lztx
Jan 18 22:47:26.094: INFO: Got endpoints: latency-svc-cbzl9 [750.770716ms]
Jan 18 22:47:26.102: INFO: Created: latency-svc-7cdhp
Jan 18 22:47:26.144: INFO: Got endpoints: latency-svc-cvczg [751.2098ms]
Jan 18 22:47:26.152: INFO: Created: latency-svc-qtkh4
Jan 18 22:47:26.193: INFO: Got endpoints: latency-svc-rtqkt [747.801892ms]
Jan 18 22:47:26.201: INFO: Created: latency-svc-sj8xd
Jan 18 22:47:26.242: INFO: Got endpoints: latency-svc-4zq6c [748.972165ms]
Jan 18 22:47:26.249: INFO: Created: latency-svc-bz6c7
Jan 18 22:47:26.292: INFO: Got endpoints: latency-svc-9cp6k [750.194506ms]
Jan 18 22:47:26.301: INFO: Created: latency-svc-xxbcx
Jan 18 22:47:26.346: INFO: Got endpoints: latency-svc-z7p4x [753.426489ms]
Jan 18 22:47:26.356: INFO: Created: latency-svc-2h8cv
Jan 18 22:47:26.395: INFO: Got endpoints: latency-svc-mv2m7 [752.69901ms]
Jan 18 22:47:26.402: INFO: Created: latency-svc-bvgvz
Jan 18 22:47:26.443: INFO: Got endpoints: latency-svc-rbbmh [750.219355ms]
Jan 18 22:47:26.452: INFO: Created: latency-svc-676bm
Jan 18 22:47:26.494: INFO: Got endpoints: latency-svc-ng5r5 [748.937262ms]
Jan 18 22:47:26.502: INFO: Created: latency-svc-8c5gs
Jan 18 22:47:26.543: INFO: Got endpoints: latency-svc-7227w [750.273907ms]
Jan 18 22:47:26.551: INFO: Created: latency-svc-pzd9m
Jan 18 22:47:26.592: INFO: Got endpoints: latency-svc-z2lft [746.653486ms]
Jan 18 22:47:26.601: INFO: Created: latency-svc-dgxh9
Jan 18 22:47:26.643: INFO: Got endpoints: latency-svc-skwjr [750.951999ms]
Jan 18 22:47:26.653: INFO: Created: latency-svc-86jvn
Jan 18 22:47:26.694: INFO: Got endpoints: latency-svc-dh8hj [751.747531ms]
Jan 18 22:47:26.702: INFO: Created: latency-svc-gx8zm
Jan 18 22:47:26.744: INFO: Got endpoints: latency-svc-z29qt [751.261551ms]
Jan 18 22:47:26.752: INFO: Created: latency-svc-2gtnn
Jan 18 22:47:26.794: INFO: Got endpoints: latency-svc-9lztx [751.649145ms]
Jan 18 22:47:26.802: INFO: Created: latency-svc-gd7bv
Jan 18 22:47:26.843: INFO: Got endpoints: latency-svc-7cdhp [748.769916ms]
Jan 18 22:47:26.851: INFO: Created: latency-svc-blmfw
Jan 18 22:47:26.892: INFO: Got endpoints: latency-svc-qtkh4 [748.162449ms]
Jan 18 22:47:26.901: INFO: Created: latency-svc-vkr4b
Jan 18 22:47:26.942: INFO: Got endpoints: latency-svc-sj8xd [749.023855ms]
Jan 18 22:47:26.952: INFO: Created: latency-svc-k5fj7
Jan 18 22:47:26.992: INFO: Got endpoints: latency-svc-bz6c7 [750.262746ms]
Jan 18 22:47:26.999: INFO: Created: latency-svc-fzj9r
Jan 18 22:47:27.045: INFO: Got endpoints: latency-svc-xxbcx [752.986942ms]
Jan 18 22:47:27.052: INFO: Created: latency-svc-8957w
Jan 18 22:47:27.096: INFO: Got endpoints: latency-svc-2h8cv [750.076393ms]
Jan 18 22:47:27.105: INFO: Created: latency-svc-x5b6d
Jan 18 22:47:27.143: INFO: Got endpoints: latency-svc-bvgvz [747.556728ms]
Jan 18 22:47:27.150: INFO: Created: latency-svc-wtslh
Jan 18 22:47:27.192: INFO: Got endpoints: latency-svc-676bm [748.891609ms]
Jan 18 22:47:27.204: INFO: Created: latency-svc-4z5gv
Jan 18 22:47:27.243: INFO: Got endpoints: latency-svc-8c5gs [749.167935ms]
Jan 18 22:47:27.251: INFO: Created: latency-svc-mcq5p
Jan 18 22:47:27.294: INFO: Got endpoints: latency-svc-pzd9m [751.367819ms]
Jan 18 22:47:27.303: INFO: Created: latency-svc-xt5zq
Jan 18 22:47:27.344: INFO: Got endpoints: latency-svc-dgxh9 [751.398811ms]
Jan 18 22:47:27.352: INFO: Created: latency-svc-htkn7
Jan 18 22:47:27.393: INFO: Got endpoints: latency-svc-86jvn [749.877584ms]
Jan 18 22:47:27.400: INFO: Created: latency-svc-pcx44
Jan 18 22:47:27.445: INFO: Got endpoints: latency-svc-gx8zm [751.101326ms]
Jan 18 22:47:27.492: INFO: Got endpoints: latency-svc-2gtnn [748.300636ms]
Jan 18 22:47:27.545: INFO: Got endpoints: latency-svc-gd7bv [750.10463ms]
Jan 18 22:47:27.592: INFO: Got endpoints: latency-svc-blmfw [749.271036ms]
Jan 18 22:47:27.645: INFO: Got endpoints: latency-svc-vkr4b [752.625246ms]
Jan 18 22:47:27.693: INFO: Got endpoints: latency-svc-k5fj7 [750.47325ms]
Jan 18 22:47:27.745: INFO: Got endpoints: latency-svc-fzj9r [752.711409ms]
Jan 18 22:47:27.793: INFO: Got endpoints: latency-svc-8957w [747.957393ms]
Jan 18 22:47:27.843: INFO: Got endpoints: latency-svc-x5b6d [747.453368ms]
Jan 18 22:47:27.892: INFO: Got endpoints: latency-svc-wtslh [749.093142ms]
Jan 18 22:47:27.943: INFO: Got endpoints: latency-svc-4z5gv [750.339774ms]
Jan 18 22:47:27.992: INFO: Got endpoints: latency-svc-mcq5p [749.049923ms]
Jan 18 22:47:28.043: INFO: Got endpoints: latency-svc-xt5zq [749.210191ms]
Jan 18 22:47:28.093: INFO: Got endpoints: latency-svc-htkn7 [748.745508ms]
Jan 18 22:47:28.142: INFO: Got endpoints: latency-svc-pcx44 [749.510976ms]
Jan 18 22:47:28.143: INFO: Latencies: [13.509542ms 17.360683ms 23.249864ms 30.303073ms 37.770443ms 40.346304ms 51.399967ms 54.684385ms 59.705048ms 65.531086ms 69.360698ms 72.979287ms 81.065089ms 82.122242ms 82.941053ms 84.334215ms 87.571674ms 88.421418ms 89.041054ms 89.720193ms 90.086516ms 90.275577ms 90.725964ms 92.348614ms 92.397646ms 95.683074ms 98.396808ms 99.572621ms 99.762946ms 100.63932ms 129.709078ms 131.93585ms 138.53337ms 142.412113ms 148.477056ms 203.625019ms 306.663652ms 322.730634ms 355.39054ms 417.940766ms 441.980272ms 446.257747ms 486.560209ms 543.993863ms 587.06499ms 637.634447ms 685.633358ms 729.864936ms 740.961411ms 743.071687ms 743.13423ms 745.216301ms 745.668609ms 746.515207ms 746.533819ms 746.653486ms 746.676353ms 746.7371ms 746.886677ms 747.029829ms 747.360039ms 747.453368ms 747.556728ms 747.575997ms 747.602902ms 747.690215ms 747.710224ms 747.749429ms 747.756863ms 747.784639ms 747.784904ms 747.801892ms 747.804248ms 747.916257ms 747.957393ms 748.162449ms 748.203695ms 748.283633ms 748.300636ms 748.377039ms 748.412244ms 748.418547ms 748.502908ms 748.504269ms 748.509678ms 748.637711ms 748.706586ms 748.745508ms 748.769916ms 748.821098ms 748.846143ms 748.891609ms 748.933452ms 748.937262ms 748.972165ms 749.023855ms 749.049923ms 749.093142ms 749.167935ms 749.191886ms 749.210191ms 749.214613ms 749.247136ms 749.271036ms 749.337861ms 749.360579ms 749.361815ms 749.384644ms 749.439808ms 749.449222ms 749.460955ms 749.465835ms 749.510976ms 749.544885ms 749.71253ms 749.776324ms 749.781537ms 749.788678ms 749.841363ms 749.849537ms 749.877584ms 749.878343ms 749.880968ms 749.907151ms 749.918592ms 749.960606ms 749.969391ms 750.002769ms 750.052926ms 750.053812ms 750.055255ms 750.076393ms 750.10463ms 750.194506ms 750.196538ms 750.200363ms 750.219355ms 750.232387ms 750.262746ms 750.265815ms 750.273907ms 750.30587ms 750.333819ms 750.339774ms 750.464701ms 750.47325ms 750.498985ms 750.509028ms 750.544466ms 750.551226ms 750.645052ms 750.679119ms 750.719279ms 750.770716ms 750.951999ms 750.955297ms 750.969105ms 751.029761ms 751.101326ms 751.123156ms 751.2098ms 751.222893ms 751.261551ms 751.367819ms 751.398811ms 751.399514ms 751.499431ms 751.541774ms 751.565086ms 751.649145ms 751.72132ms 751.747531ms 751.859371ms 751.96542ms 751.983252ms 752.008507ms 752.06755ms 752.174541ms 752.177076ms 752.201853ms 752.278938ms 752.292678ms 752.388865ms 752.625246ms 752.69901ms 752.711409ms 752.758136ms 752.802925ms 752.986942ms 752.994897ms 753.081757ms 753.122806ms 753.208518ms 753.299697ms 753.426489ms 753.46396ms 753.554737ms 754.336803ms 754.715097ms 756.932407ms]
Jan 18 22:47:28.143: INFO: 50 %ile: 749.210191ms
Jan 18 22:47:28.143: INFO: 90 %ile: 752.278938ms
Jan 18 22:47:28.143: INFO: 99 %ile: 754.715097ms
Jan 18 22:47:28.143: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:28.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-9200" for this suite. 01/18/23 22:47:28.147
------------------------------
â€¢ [SLOW TEST] [10.754 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:17.4
    Jan 18 22:47:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svc-latency 01/18/23 22:47:17.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:17.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:17.417
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 18 22:47:17.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-9200 01/18/23 22:47:17.42
    I0118 22:47:17.426091      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9200, replica count: 1
    I0118 22:47:18.476951      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 22:47:19.478164      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:47:19.587: INFO: Created: latency-svc-zjj7t
    Jan 18 22:47:19.592: INFO: Got endpoints: latency-svc-zjj7t [13.178615ms]
    Jan 18 22:47:19.601: INFO: Created: latency-svc-mnxcv
    Jan 18 22:47:19.605: INFO: Created: latency-svc-g9bxp
    Jan 18 22:47:19.605: INFO: Got endpoints: latency-svc-mnxcv [13.509542ms]
    Jan 18 22:47:19.609: INFO: Got endpoints: latency-svc-g9bxp [17.360683ms]
    Jan 18 22:47:19.612: INFO: Created: latency-svc-bv6z7
    Jan 18 22:47:19.615: INFO: Got endpoints: latency-svc-bv6z7 [23.249864ms]
    Jan 18 22:47:19.618: INFO: Created: latency-svc-pbtc2
    Jan 18 22:47:19.622: INFO: Got endpoints: latency-svc-pbtc2 [30.303073ms]
    Jan 18 22:47:19.623: INFO: Created: latency-svc-wmzzn
    Jan 18 22:47:19.626: INFO: Created: latency-svc-2w22k
    Jan 18 22:47:19.630: INFO: Got endpoints: latency-svc-wmzzn [37.770443ms]
    Jan 18 22:47:19.632: INFO: Got endpoints: latency-svc-2w22k [40.346304ms]
    Jan 18 22:47:19.636: INFO: Created: latency-svc-7pxh7
    Jan 18 22:47:19.639: INFO: Created: latency-svc-gtj5g
    Jan 18 22:47:19.643: INFO: Got endpoints: latency-svc-7pxh7 [51.399967ms]
    Jan 18 22:47:19.645: INFO: Created: latency-svc-5ct79
    Jan 18 22:47:19.647: INFO: Got endpoints: latency-svc-gtj5g [54.684385ms]
    Jan 18 22:47:19.650: INFO: Created: latency-svc-bv75f
    Jan 18 22:47:19.652: INFO: Got endpoints: latency-svc-5ct79 [59.705048ms]
    Jan 18 22:47:19.657: INFO: Created: latency-svc-88sbm
    Jan 18 22:47:19.658: INFO: Got endpoints: latency-svc-bv75f [65.531086ms]
    Jan 18 22:47:19.661: INFO: Created: latency-svc-4xsd2
    Jan 18 22:47:19.661: INFO: Got endpoints: latency-svc-88sbm [69.360698ms]
    Jan 18 22:47:19.665: INFO: Got endpoints: latency-svc-4xsd2 [72.979287ms]
    Jan 18 22:47:19.669: INFO: Created: latency-svc-b922r
    Jan 18 22:47:19.673: INFO: Created: latency-svc-gcblq
    Jan 18 22:47:19.674: INFO: Got endpoints: latency-svc-b922r [82.122242ms]
    Jan 18 22:47:19.676: INFO: Got endpoints: latency-svc-gcblq [84.334215ms]
    Jan 18 22:47:19.680: INFO: Created: latency-svc-lqpvd
    Jan 18 22:47:19.688: INFO: Got endpoints: latency-svc-lqpvd [95.683074ms]
    Jan 18 22:47:19.688: INFO: Created: latency-svc-frzwh
    Jan 18 22:47:19.696: INFO: Got endpoints: latency-svc-frzwh [90.725964ms]
    Jan 18 22:47:19.698: INFO: Created: latency-svc-mbppj
    Jan 18 22:47:19.698: INFO: Created: latency-svc-86drf
    Jan 18 22:47:19.708: INFO: Got endpoints: latency-svc-86drf [98.396808ms]
    Jan 18 22:47:19.708: INFO: Got endpoints: latency-svc-mbppj [92.397646ms]
    Jan 18 22:47:19.708: INFO: Created: latency-svc-4px5j
    Jan 18 22:47:19.712: INFO: Got endpoints: latency-svc-4px5j [89.720193ms]
    Jan 18 22:47:19.714: INFO: Created: latency-svc-f6rqq
    Jan 18 22:47:19.719: INFO: Created: latency-svc-7zm5f
    Jan 18 22:47:19.720: INFO: Got endpoints: latency-svc-f6rqq [90.086516ms]
    Jan 18 22:47:19.724: INFO: Created: latency-svc-6hbrq
    Jan 18 22:47:19.725: INFO: Got endpoints: latency-svc-7zm5f [92.348614ms]
    Jan 18 22:47:19.726: INFO: Got endpoints: latency-svc-6hbrq [82.941053ms]
    Jan 18 22:47:19.732: INFO: Created: latency-svc-2h27b
    Jan 18 22:47:19.735: INFO: Created: latency-svc-c69zt
    Jan 18 22:47:19.737: INFO: Got endpoints: latency-svc-2h27b [90.275577ms]
    Jan 18 22:47:19.740: INFO: Created: latency-svc-qdbhf
    Jan 18 22:47:19.740: INFO: Got endpoints: latency-svc-c69zt [88.421418ms]
    Jan 18 22:47:19.743: INFO: Created: latency-svc-22ln8
    Jan 18 22:47:19.745: INFO: Got endpoints: latency-svc-qdbhf [87.571674ms]
    Jan 18 22:47:19.750: INFO: Got endpoints: latency-svc-22ln8 [89.041054ms]
    Jan 18 22:47:19.792: INFO: Created: latency-svc-s8lc4
    Jan 18 22:47:19.795: INFO: Created: latency-svc-8pqzj
    Jan 18 22:47:19.797: INFO: Got endpoints: latency-svc-s8lc4 [100.63932ms]
    Jan 18 22:47:19.798: INFO: Created: latency-svc-vdjtf
    Jan 18 22:47:19.799: INFO: Created: latency-svc-qhtbt
    Jan 18 22:47:19.799: INFO: Created: latency-svc-z9jr8
    Jan 18 22:47:19.799: INFO: Created: latency-svc-c9mc8
    Jan 18 22:47:19.799: INFO: Created: latency-svc-f6svw
    Jan 18 22:47:19.799: INFO: Created: latency-svc-q7db5
    Jan 18 22:47:19.799: INFO: Created: latency-svc-szxsj
    Jan 18 22:47:19.799: INFO: Created: latency-svc-gjxzv
    Jan 18 22:47:19.799: INFO: Created: latency-svc-bxz9k
    Jan 18 22:47:19.799: INFO: Created: latency-svc-w4hjg
    Jan 18 22:47:19.799: INFO: Created: latency-svc-zdcqf
    Jan 18 22:47:19.799: INFO: Created: latency-svc-68qx2
    Jan 18 22:47:19.801: INFO: Created: latency-svc-g8fgx
    Jan 18 22:47:19.806: INFO: Got endpoints: latency-svc-68qx2 [129.709078ms]
    Jan 18 22:47:19.807: INFO: Got endpoints: latency-svc-zdcqf [99.572621ms]
    Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-g8fgx [99.762946ms]
    Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-8pqzj [142.412113ms]
    Jan 18 22:47:19.808: INFO: Got endpoints: latency-svc-w4hjg [81.065089ms]
    Jan 18 22:47:19.813: INFO: Got endpoints: latency-svc-bxz9k [138.53337ms]
    Jan 18 22:47:19.816: INFO: Created: latency-svc-hxzzb
    Jan 18 22:47:19.818: INFO: Created: latency-svc-8s7bb
    Jan 18 22:47:19.823: INFO: Created: latency-svc-5zzmg
    Jan 18 22:47:19.826: INFO: Created: latency-svc-7b7h9
    Jan 18 22:47:19.828: INFO: Created: latency-svc-5vztr
    Jan 18 22:47:19.832: INFO: Created: latency-svc-pzpj9
    Jan 18 22:47:19.837: INFO: Created: latency-svc-tzvz7
    Jan 18 22:47:19.844: INFO: Got endpoints: latency-svc-vdjtf [131.93585ms]
    Jan 18 22:47:19.854: INFO: Created: latency-svc-rvbll
    Jan 18 22:47:19.894: INFO: Got endpoints: latency-svc-gjxzv [148.477056ms]
    Jan 18 22:47:19.905: INFO: Created: latency-svc-v2svn
    Jan 18 22:47:19.944: INFO: Got endpoints: latency-svc-q7db5 [203.625019ms]
    Jan 18 22:47:19.953: INFO: Created: latency-svc-k9rjk
    Jan 18 22:47:19.994: INFO: Got endpoints: latency-svc-f6svw [306.663652ms]
    Jan 18 22:47:20.007: INFO: Created: latency-svc-xpddb
    Jan 18 22:47:20.043: INFO: Got endpoints: latency-svc-c9mc8 [322.730634ms]
    Jan 18 22:47:20.051: INFO: Created: latency-svc-f5zg5
    Jan 18 22:47:20.093: INFO: Got endpoints: latency-svc-z9jr8 [355.39054ms]
    Jan 18 22:47:20.102: INFO: Created: latency-svc-vxdgl
    Jan 18 22:47:20.143: INFO: Got endpoints: latency-svc-szxsj [417.940766ms]
    Jan 18 22:47:20.152: INFO: Created: latency-svc-tfng6
    Jan 18 22:47:20.192: INFO: Got endpoints: latency-svc-qhtbt [441.980272ms]
    Jan 18 22:47:20.203: INFO: Created: latency-svc-xw8fs
    Jan 18 22:47:20.243: INFO: Got endpoints: latency-svc-hxzzb [446.257747ms]
    Jan 18 22:47:20.253: INFO: Created: latency-svc-sq2r4
    Jan 18 22:47:20.293: INFO: Got endpoints: latency-svc-8s7bb [486.560209ms]
    Jan 18 22:47:20.303: INFO: Created: latency-svc-h8jdg
    Jan 18 22:47:20.351: INFO: Got endpoints: latency-svc-5zzmg [543.993863ms]
    Jan 18 22:47:20.360: INFO: Created: latency-svc-fcv6f
    Jan 18 22:47:20.395: INFO: Got endpoints: latency-svc-7b7h9 [587.06499ms]
    Jan 18 22:47:20.402: INFO: Created: latency-svc-zrzmt
    Jan 18 22:47:20.445: INFO: Got endpoints: latency-svc-5vztr [637.634447ms]
    Jan 18 22:47:20.454: INFO: Created: latency-svc-dvjdk
    Jan 18 22:47:20.493: INFO: Got endpoints: latency-svc-pzpj9 [685.633358ms]
    Jan 18 22:47:20.504: INFO: Created: latency-svc-7hrcq
    Jan 18 22:47:20.543: INFO: Got endpoints: latency-svc-tzvz7 [729.864936ms]
    Jan 18 22:47:20.553: INFO: Created: latency-svc-72rxl
    Jan 18 22:47:20.593: INFO: Got endpoints: latency-svc-rvbll [748.821098ms]
    Jan 18 22:47:20.601: INFO: Created: latency-svc-xgcxv
    Jan 18 22:47:20.645: INFO: Got endpoints: latency-svc-v2svn [750.955297ms]
    Jan 18 22:47:20.653: INFO: Created: latency-svc-bd4f6
    Jan 18 22:47:20.692: INFO: Got endpoints: latency-svc-k9rjk [748.509678ms]
    Jan 18 22:47:20.700: INFO: Created: latency-svc-2c254
    Jan 18 22:47:20.745: INFO: Got endpoints: latency-svc-xpddb [750.544466ms]
    Jan 18 22:47:20.753: INFO: Created: latency-svc-8qrts
    Jan 18 22:47:20.793: INFO: Got endpoints: latency-svc-f5zg5 [749.849537ms]
    Jan 18 22:47:20.803: INFO: Created: latency-svc-7cmmh
    Jan 18 22:47:20.843: INFO: Got endpoints: latency-svc-vxdgl [750.679119ms]
    Jan 18 22:47:20.853: INFO: Created: latency-svc-d7cq9
    Jan 18 22:47:20.893: INFO: Got endpoints: latency-svc-tfng6 [749.788678ms]
    Jan 18 22:47:20.901: INFO: Created: latency-svc-vnn9j
    Jan 18 22:47:20.946: INFO: Got endpoints: latency-svc-xw8fs [753.554737ms]
    Jan 18 22:47:20.954: INFO: Created: latency-svc-g6h69
    Jan 18 22:47:20.995: INFO: Got endpoints: latency-svc-sq2r4 [752.201853ms]
    Jan 18 22:47:21.004: INFO: Created: latency-svc-5chgn
    Jan 18 22:47:21.042: INFO: Got endpoints: latency-svc-h8jdg [749.71253ms]
    Jan 18 22:47:21.052: INFO: Created: latency-svc-5zfz6
    Jan 18 22:47:21.092: INFO: Got endpoints: latency-svc-fcv6f [740.961411ms]
    Jan 18 22:47:21.102: INFO: Created: latency-svc-77t24
    Jan 18 22:47:21.144: INFO: Got endpoints: latency-svc-zrzmt [749.337861ms]
    Jan 18 22:47:21.155: INFO: Created: latency-svc-x7m4g
    Jan 18 22:47:21.193: INFO: Got endpoints: latency-svc-dvjdk [747.602902ms]
    Jan 18 22:47:21.201: INFO: Created: latency-svc-ptbhv
    Jan 18 22:47:21.245: INFO: Got endpoints: latency-svc-7hrcq [752.008507ms]
    Jan 18 22:47:21.253: INFO: Created: latency-svc-qh7c7
    Jan 18 22:47:21.295: INFO: Got endpoints: latency-svc-72rxl [752.292678ms]
    Jan 18 22:47:21.303: INFO: Created: latency-svc-g2tj9
    Jan 18 22:47:21.342: INFO: Got endpoints: latency-svc-xgcxv [749.214613ms]
    Jan 18 22:47:21.351: INFO: Created: latency-svc-wzfjn
    Jan 18 22:47:21.393: INFO: Got endpoints: latency-svc-bd4f6 [748.203695ms]
    Jan 18 22:47:21.402: INFO: Created: latency-svc-7frrb
    Jan 18 22:47:21.442: INFO: Got endpoints: latency-svc-2c254 [750.052926ms]
    Jan 18 22:47:21.450: INFO: Created: latency-svc-99nkp
    Jan 18 22:47:21.495: INFO: Got endpoints: latency-svc-8qrts [750.002769ms]
    Jan 18 22:47:21.502: INFO: Created: latency-svc-jqbsd
    Jan 18 22:47:21.545: INFO: Got endpoints: latency-svc-7cmmh [752.758136ms]
    Jan 18 22:47:21.554: INFO: Created: latency-svc-ph25k
    Jan 18 22:47:21.594: INFO: Got endpoints: latency-svc-d7cq9 [750.645052ms]
    Jan 18 22:47:21.603: INFO: Created: latency-svc-c4nms
    Jan 18 22:47:21.643: INFO: Got endpoints: latency-svc-vnn9j [750.509028ms]
    Jan 18 22:47:21.653: INFO: Created: latency-svc-xwfjx
    Jan 18 22:47:21.693: INFO: Got endpoints: latency-svc-g6h69 [746.533819ms]
    Jan 18 22:47:21.704: INFO: Created: latency-svc-hgs77
    Jan 18 22:47:21.742: INFO: Got endpoints: latency-svc-5chgn [747.029829ms]
    Jan 18 22:47:21.750: INFO: Created: latency-svc-djq75
    Jan 18 22:47:21.795: INFO: Got endpoints: latency-svc-5zfz6 [752.177076ms]
    Jan 18 22:47:21.806: INFO: Created: latency-svc-6tmq2
    Jan 18 22:47:21.842: INFO: Got endpoints: latency-svc-77t24 [749.960606ms]
    Jan 18 22:47:21.850: INFO: Created: latency-svc-cbr5j
    Jan 18 22:47:21.894: INFO: Got endpoints: latency-svc-x7m4g [750.333819ms]
    Jan 18 22:47:21.902: INFO: Created: latency-svc-n6526
    Jan 18 22:47:21.945: INFO: Got endpoints: latency-svc-ptbhv [751.859371ms]
    Jan 18 22:47:21.953: INFO: Created: latency-svc-57mt7
    Jan 18 22:47:21.993: INFO: Got endpoints: latency-svc-qh7c7 [747.749429ms]
    Jan 18 22:47:22.003: INFO: Created: latency-svc-5jg4h
    Jan 18 22:47:22.044: INFO: Got endpoints: latency-svc-g2tj9 [748.933452ms]
    Jan 18 22:47:22.054: INFO: Created: latency-svc-vldzs
    Jan 18 22:47:22.095: INFO: Got endpoints: latency-svc-wzfjn [752.388865ms]
    Jan 18 22:47:22.102: INFO: Created: latency-svc-728d7
    Jan 18 22:47:22.143: INFO: Got endpoints: latency-svc-7frrb [750.055255ms]
    Jan 18 22:47:22.152: INFO: Created: latency-svc-8qfzf
    Jan 18 22:47:22.195: INFO: Got endpoints: latency-svc-99nkp [752.802925ms]
    Jan 18 22:47:22.203: INFO: Created: latency-svc-nl78n
    Jan 18 22:47:22.243: INFO: Got endpoints: latency-svc-jqbsd [747.690215ms]
    Jan 18 22:47:22.251: INFO: Created: latency-svc-w7bpq
    Jan 18 22:47:22.300: INFO: Got endpoints: latency-svc-ph25k [754.336803ms]
    Jan 18 22:47:22.308: INFO: Created: latency-svc-d2qhq
    Jan 18 22:47:22.342: INFO: Got endpoints: latency-svc-c4nms [748.418547ms]
    Jan 18 22:47:22.352: INFO: Created: latency-svc-l6gb6
    Jan 18 22:47:22.398: INFO: Got endpoints: latency-svc-xwfjx [754.715097ms]
    Jan 18 22:47:22.409: INFO: Created: latency-svc-rtm47
    Jan 18 22:47:22.442: INFO: Got endpoints: latency-svc-hgs77 [749.361815ms]
    Jan 18 22:47:22.449: INFO: Created: latency-svc-cqcqd
    Jan 18 22:47:22.494: INFO: Got endpoints: latency-svc-djq75 [751.565086ms]
    Jan 18 22:47:22.508: INFO: Created: latency-svc-2h8lh
    Jan 18 22:47:22.541: INFO: Got endpoints: latency-svc-6tmq2 [746.676353ms]
    Jan 18 22:47:22.551: INFO: Created: latency-svc-mzdf4
    Jan 18 22:47:22.593: INFO: Got endpoints: latency-svc-cbr5j [750.053812ms]
    Jan 18 22:47:22.602: INFO: Created: latency-svc-fmztk
    Jan 18 22:47:22.642: INFO: Got endpoints: latency-svc-n6526 [747.804248ms]
    Jan 18 22:47:22.654: INFO: Created: latency-svc-b8fd8
    Jan 18 22:47:22.694: INFO: Got endpoints: latency-svc-57mt7 [749.439808ms]
    Jan 18 22:47:22.702: INFO: Created: latency-svc-s6nwh
    Jan 18 22:47:22.744: INFO: Got endpoints: latency-svc-5jg4h [751.222893ms]
    Jan 18 22:47:22.754: INFO: Created: latency-svc-vkmqw
    Jan 18 22:47:22.792: INFO: Got endpoints: latency-svc-vldzs [747.784904ms]
    Jan 18 22:47:22.801: INFO: Created: latency-svc-jtk6w
    Jan 18 22:47:22.842: INFO: Got endpoints: latency-svc-728d7 [747.575997ms]
    Jan 18 22:47:22.852: INFO: Created: latency-svc-956x9
    Jan 18 22:47:22.893: INFO: Got endpoints: latency-svc-8qfzf [749.360579ms]
    Jan 18 22:47:22.901: INFO: Created: latency-svc-sh288
    Jan 18 22:47:22.942: INFO: Got endpoints: latency-svc-nl78n [746.7371ms]
    Jan 18 22:47:22.951: INFO: Created: latency-svc-tdbbq
    Jan 18 22:47:22.993: INFO: Got endpoints: latency-svc-w7bpq [749.841363ms]
    Jan 18 22:47:23.006: INFO: Created: latency-svc-n6786
    Jan 18 22:47:23.043: INFO: Got endpoints: latency-svc-d2qhq [743.071687ms]
    Jan 18 22:47:23.051: INFO: Created: latency-svc-xm2cc
    Jan 18 22:47:23.092: INFO: Got endpoints: latency-svc-l6gb6 [749.918592ms]
    Jan 18 22:47:23.099: INFO: Created: latency-svc-x8l7m
    Jan 18 22:47:23.144: INFO: Got endpoints: latency-svc-rtm47 [745.668609ms]
    Jan 18 22:47:23.152: INFO: Created: latency-svc-fvt45
    Jan 18 22:47:23.194: INFO: Got endpoints: latency-svc-cqcqd [751.541774ms]
    Jan 18 22:47:23.203: INFO: Created: latency-svc-ff626
    Jan 18 22:47:23.244: INFO: Got endpoints: latency-svc-2h8lh [749.544885ms]
    Jan 18 22:47:23.251: INFO: Created: latency-svc-sdpzh
    Jan 18 22:47:23.294: INFO: Got endpoints: latency-svc-mzdf4 [753.081757ms]
    Jan 18 22:47:23.302: INFO: Created: latency-svc-dqnc7
    Jan 18 22:47:23.342: INFO: Got endpoints: latency-svc-fmztk [749.907151ms]
    Jan 18 22:47:23.352: INFO: Created: latency-svc-zcr95
    Jan 18 22:47:23.399: INFO: Got endpoints: latency-svc-b8fd8 [756.932407ms]
    Jan 18 22:47:23.407: INFO: Created: latency-svc-jk2q4
    Jan 18 22:47:23.444: INFO: Got endpoints: latency-svc-s6nwh [749.460955ms]
    Jan 18 22:47:23.453: INFO: Created: latency-svc-kf98w
    Jan 18 22:47:23.493: INFO: Got endpoints: latency-svc-vkmqw [748.502908ms]
    Jan 18 22:47:23.502: INFO: Created: latency-svc-5bbq9
    Jan 18 22:47:23.543: INFO: Got endpoints: latency-svc-jtk6w [750.969105ms]
    Jan 18 22:47:23.552: INFO: Created: latency-svc-nwsfx
    Jan 18 22:47:23.594: INFO: Got endpoints: latency-svc-956x9 [751.96542ms]
    Jan 18 22:47:23.601: INFO: Created: latency-svc-947vq
    Jan 18 22:47:23.645: INFO: Got endpoints: latency-svc-sh288 [752.06755ms]
    Jan 18 22:47:23.656: INFO: Created: latency-svc-rrzbk
    Jan 18 22:47:23.694: INFO: Got endpoints: latency-svc-tdbbq [752.174541ms]
    Jan 18 22:47:23.703: INFO: Created: latency-svc-lc5b6
    Jan 18 22:47:23.743: INFO: Got endpoints: latency-svc-n6786 [750.196538ms]
    Jan 18 22:47:23.755: INFO: Created: latency-svc-8b9rv
    Jan 18 22:47:23.792: INFO: Got endpoints: latency-svc-xm2cc [749.247136ms]
    Jan 18 22:47:23.805: INFO: Created: latency-svc-d5jmn
    Jan 18 22:47:23.843: INFO: Got endpoints: latency-svc-x8l7m [750.498985ms]
    Jan 18 22:47:23.852: INFO: Created: latency-svc-t2ckx
    Jan 18 22:47:23.892: INFO: Got endpoints: latency-svc-fvt45 [748.637711ms]
    Jan 18 22:47:23.900: INFO: Created: latency-svc-bf7zp
    Jan 18 22:47:23.945: INFO: Got endpoints: latency-svc-ff626 [751.499431ms]
    Jan 18 22:47:23.953: INFO: Created: latency-svc-f8dpg
    Jan 18 22:47:23.995: INFO: Got endpoints: latency-svc-sdpzh [751.029761ms]
    Jan 18 22:47:24.003: INFO: Created: latency-svc-889wp
    Jan 18 22:47:24.043: INFO: Got endpoints: latency-svc-dqnc7 [748.283633ms]
    Jan 18 22:47:24.052: INFO: Created: latency-svc-9shh8
    Jan 18 22:47:24.094: INFO: Got endpoints: latency-svc-zcr95 [751.123156ms]
    Jan 18 22:47:24.102: INFO: Created: latency-svc-mqpbb
    Jan 18 22:47:24.142: INFO: Got endpoints: latency-svc-jk2q4 [743.13423ms]
    Jan 18 22:47:24.154: INFO: Created: latency-svc-9jh2w
    Jan 18 22:47:24.197: INFO: Got endpoints: latency-svc-kf98w [753.299697ms]
    Jan 18 22:47:24.205: INFO: Created: latency-svc-x2q5f
    Jan 18 22:47:24.245: INFO: Got endpoints: latency-svc-5bbq9 [752.278938ms]
    Jan 18 22:47:24.253: INFO: Created: latency-svc-rrczz
    Jan 18 22:47:24.295: INFO: Got endpoints: latency-svc-nwsfx [751.72132ms]
    Jan 18 22:47:24.303: INFO: Created: latency-svc-ltglb
    Jan 18 22:47:24.344: INFO: Got endpoints: latency-svc-947vq [749.384644ms]
    Jan 18 22:47:24.351: INFO: Created: latency-svc-s8df6
    Jan 18 22:47:24.394: INFO: Got endpoints: latency-svc-rrzbk [749.191886ms]
    Jan 18 22:47:24.403: INFO: Created: latency-svc-8wlrb
    Jan 18 22:47:24.442: INFO: Got endpoints: latency-svc-lc5b6 [747.784639ms]
    Jan 18 22:47:24.452: INFO: Created: latency-svc-xm4zk
    Jan 18 22:47:24.492: INFO: Got endpoints: latency-svc-8b9rv [749.449222ms]
    Jan 18 22:47:24.500: INFO: Created: latency-svc-tp7d7
    Jan 18 22:47:24.544: INFO: Got endpoints: latency-svc-d5jmn [751.983252ms]
    Jan 18 22:47:24.552: INFO: Created: latency-svc-cmfk8
    Jan 18 22:47:24.596: INFO: Got endpoints: latency-svc-t2ckx [753.122806ms]
    Jan 18 22:47:24.604: INFO: Created: latency-svc-bwvzh
    Jan 18 22:47:24.646: INFO: Got endpoints: latency-svc-bf7zp [753.208518ms]
    Jan 18 22:47:24.653: INFO: Created: latency-svc-n7bxt
    Jan 18 22:47:24.695: INFO: Got endpoints: latency-svc-f8dpg [749.878343ms]
    Jan 18 22:47:24.703: INFO: Created: latency-svc-kx4wr
    Jan 18 22:47:24.742: INFO: Got endpoints: latency-svc-889wp [747.710224ms]
    Jan 18 22:47:24.753: INFO: Created: latency-svc-9jwgp
    Jan 18 22:47:24.793: INFO: Got endpoints: latency-svc-9shh8 [750.464701ms]
    Jan 18 22:47:24.806: INFO: Created: latency-svc-wjx8t
    Jan 18 22:47:24.842: INFO: Got endpoints: latency-svc-mqpbb [748.377039ms]
    Jan 18 22:47:24.850: INFO: Created: latency-svc-sfm7s
    Jan 18 22:47:24.894: INFO: Got endpoints: latency-svc-9jh2w [751.399514ms]
    Jan 18 22:47:24.904: INFO: Created: latency-svc-4gxzt
    Jan 18 22:47:24.942: INFO: Got endpoints: latency-svc-x2q5f [745.216301ms]
    Jan 18 22:47:24.950: INFO: Created: latency-svc-2zczj
    Jan 18 22:47:24.995: INFO: Got endpoints: latency-svc-rrczz [749.880968ms]
    Jan 18 22:47:25.003: INFO: Created: latency-svc-9p57r
    Jan 18 22:47:25.043: INFO: Got endpoints: latency-svc-ltglb [747.916257ms]
    Jan 18 22:47:25.052: INFO: Created: latency-svc-zdsj7
    Jan 18 22:47:25.093: INFO: Got endpoints: latency-svc-s8df6 [748.846143ms]
    Jan 18 22:47:25.102: INFO: Created: latency-svc-gb7gq
    Jan 18 22:47:25.144: INFO: Got endpoints: latency-svc-8wlrb [750.232387ms]
    Jan 18 22:47:25.152: INFO: Created: latency-svc-bsz6t
    Jan 18 22:47:25.195: INFO: Got endpoints: latency-svc-xm4zk [753.46396ms]
    Jan 18 22:47:25.205: INFO: Created: latency-svc-2kcq7
    Jan 18 22:47:25.243: INFO: Got endpoints: latency-svc-tp7d7 [750.719279ms]
    Jan 18 22:47:25.251: INFO: Created: latency-svc-zwdd9
    Jan 18 22:47:25.293: INFO: Got endpoints: latency-svc-cmfk8 [748.706586ms]
    Jan 18 22:47:25.302: INFO: Created: latency-svc-ltsvn
    Jan 18 22:47:25.343: INFO: Got endpoints: latency-svc-bwvzh [747.360039ms]
    Jan 18 22:47:25.353: INFO: Created: latency-svc-cbzl9
    Jan 18 22:47:25.393: INFO: Got endpoints: latency-svc-n7bxt [746.886677ms]
    Jan 18 22:47:25.401: INFO: Created: latency-svc-cvczg
    Jan 18 22:47:25.445: INFO: Got endpoints: latency-svc-kx4wr [750.30587ms]
    Jan 18 22:47:25.454: INFO: Created: latency-svc-rtqkt
    Jan 18 22:47:25.493: INFO: Got endpoints: latency-svc-9jwgp [750.200363ms]
    Jan 18 22:47:25.501: INFO: Created: latency-svc-4zq6c
    Jan 18 22:47:25.542: INFO: Got endpoints: latency-svc-wjx8t [748.504269ms]
    Jan 18 22:47:25.550: INFO: Created: latency-svc-9cp6k
    Jan 18 22:47:25.592: INFO: Got endpoints: latency-svc-sfm7s [750.265815ms]
    Jan 18 22:47:25.601: INFO: Created: latency-svc-z7p4x
    Jan 18 22:47:25.642: INFO: Got endpoints: latency-svc-4gxzt [748.412244ms]
    Jan 18 22:47:25.651: INFO: Created: latency-svc-mv2m7
    Jan 18 22:47:25.693: INFO: Got endpoints: latency-svc-2zczj [750.551226ms]
    Jan 18 22:47:25.701: INFO: Created: latency-svc-rbbmh
    Jan 18 22:47:25.745: INFO: Got endpoints: latency-svc-9p57r [749.969391ms]
    Jan 18 22:47:25.753: INFO: Created: latency-svc-ng5r5
    Jan 18 22:47:25.792: INFO: Got endpoints: latency-svc-zdsj7 [749.776324ms]
    Jan 18 22:47:25.800: INFO: Created: latency-svc-7227w
    Jan 18 22:47:25.846: INFO: Got endpoints: latency-svc-gb7gq [752.994897ms]
    Jan 18 22:47:25.853: INFO: Created: latency-svc-z2lft
    Jan 18 22:47:25.892: INFO: Got endpoints: latency-svc-bsz6t [747.756863ms]
    Jan 18 22:47:25.900: INFO: Created: latency-svc-skwjr
    Jan 18 22:47:25.942: INFO: Got endpoints: latency-svc-2kcq7 [746.515207ms]
    Jan 18 22:47:25.955: INFO: Created: latency-svc-dh8hj
    Jan 18 22:47:25.993: INFO: Got endpoints: latency-svc-zwdd9 [749.465835ms]
    Jan 18 22:47:26.002: INFO: Created: latency-svc-z29qt
    Jan 18 22:47:26.043: INFO: Got endpoints: latency-svc-ltsvn [749.781537ms]
    Jan 18 22:47:26.053: INFO: Created: latency-svc-9lztx
    Jan 18 22:47:26.094: INFO: Got endpoints: latency-svc-cbzl9 [750.770716ms]
    Jan 18 22:47:26.102: INFO: Created: latency-svc-7cdhp
    Jan 18 22:47:26.144: INFO: Got endpoints: latency-svc-cvczg [751.2098ms]
    Jan 18 22:47:26.152: INFO: Created: latency-svc-qtkh4
    Jan 18 22:47:26.193: INFO: Got endpoints: latency-svc-rtqkt [747.801892ms]
    Jan 18 22:47:26.201: INFO: Created: latency-svc-sj8xd
    Jan 18 22:47:26.242: INFO: Got endpoints: latency-svc-4zq6c [748.972165ms]
    Jan 18 22:47:26.249: INFO: Created: latency-svc-bz6c7
    Jan 18 22:47:26.292: INFO: Got endpoints: latency-svc-9cp6k [750.194506ms]
    Jan 18 22:47:26.301: INFO: Created: latency-svc-xxbcx
    Jan 18 22:47:26.346: INFO: Got endpoints: latency-svc-z7p4x [753.426489ms]
    Jan 18 22:47:26.356: INFO: Created: latency-svc-2h8cv
    Jan 18 22:47:26.395: INFO: Got endpoints: latency-svc-mv2m7 [752.69901ms]
    Jan 18 22:47:26.402: INFO: Created: latency-svc-bvgvz
    Jan 18 22:47:26.443: INFO: Got endpoints: latency-svc-rbbmh [750.219355ms]
    Jan 18 22:47:26.452: INFO: Created: latency-svc-676bm
    Jan 18 22:47:26.494: INFO: Got endpoints: latency-svc-ng5r5 [748.937262ms]
    Jan 18 22:47:26.502: INFO: Created: latency-svc-8c5gs
    Jan 18 22:47:26.543: INFO: Got endpoints: latency-svc-7227w [750.273907ms]
    Jan 18 22:47:26.551: INFO: Created: latency-svc-pzd9m
    Jan 18 22:47:26.592: INFO: Got endpoints: latency-svc-z2lft [746.653486ms]
    Jan 18 22:47:26.601: INFO: Created: latency-svc-dgxh9
    Jan 18 22:47:26.643: INFO: Got endpoints: latency-svc-skwjr [750.951999ms]
    Jan 18 22:47:26.653: INFO: Created: latency-svc-86jvn
    Jan 18 22:47:26.694: INFO: Got endpoints: latency-svc-dh8hj [751.747531ms]
    Jan 18 22:47:26.702: INFO: Created: latency-svc-gx8zm
    Jan 18 22:47:26.744: INFO: Got endpoints: latency-svc-z29qt [751.261551ms]
    Jan 18 22:47:26.752: INFO: Created: latency-svc-2gtnn
    Jan 18 22:47:26.794: INFO: Got endpoints: latency-svc-9lztx [751.649145ms]
    Jan 18 22:47:26.802: INFO: Created: latency-svc-gd7bv
    Jan 18 22:47:26.843: INFO: Got endpoints: latency-svc-7cdhp [748.769916ms]
    Jan 18 22:47:26.851: INFO: Created: latency-svc-blmfw
    Jan 18 22:47:26.892: INFO: Got endpoints: latency-svc-qtkh4 [748.162449ms]
    Jan 18 22:47:26.901: INFO: Created: latency-svc-vkr4b
    Jan 18 22:47:26.942: INFO: Got endpoints: latency-svc-sj8xd [749.023855ms]
    Jan 18 22:47:26.952: INFO: Created: latency-svc-k5fj7
    Jan 18 22:47:26.992: INFO: Got endpoints: latency-svc-bz6c7 [750.262746ms]
    Jan 18 22:47:26.999: INFO: Created: latency-svc-fzj9r
    Jan 18 22:47:27.045: INFO: Got endpoints: latency-svc-xxbcx [752.986942ms]
    Jan 18 22:47:27.052: INFO: Created: latency-svc-8957w
    Jan 18 22:47:27.096: INFO: Got endpoints: latency-svc-2h8cv [750.076393ms]
    Jan 18 22:47:27.105: INFO: Created: latency-svc-x5b6d
    Jan 18 22:47:27.143: INFO: Got endpoints: latency-svc-bvgvz [747.556728ms]
    Jan 18 22:47:27.150: INFO: Created: latency-svc-wtslh
    Jan 18 22:47:27.192: INFO: Got endpoints: latency-svc-676bm [748.891609ms]
    Jan 18 22:47:27.204: INFO: Created: latency-svc-4z5gv
    Jan 18 22:47:27.243: INFO: Got endpoints: latency-svc-8c5gs [749.167935ms]
    Jan 18 22:47:27.251: INFO: Created: latency-svc-mcq5p
    Jan 18 22:47:27.294: INFO: Got endpoints: latency-svc-pzd9m [751.367819ms]
    Jan 18 22:47:27.303: INFO: Created: latency-svc-xt5zq
    Jan 18 22:47:27.344: INFO: Got endpoints: latency-svc-dgxh9 [751.398811ms]
    Jan 18 22:47:27.352: INFO: Created: latency-svc-htkn7
    Jan 18 22:47:27.393: INFO: Got endpoints: latency-svc-86jvn [749.877584ms]
    Jan 18 22:47:27.400: INFO: Created: latency-svc-pcx44
    Jan 18 22:47:27.445: INFO: Got endpoints: latency-svc-gx8zm [751.101326ms]
    Jan 18 22:47:27.492: INFO: Got endpoints: latency-svc-2gtnn [748.300636ms]
    Jan 18 22:47:27.545: INFO: Got endpoints: latency-svc-gd7bv [750.10463ms]
    Jan 18 22:47:27.592: INFO: Got endpoints: latency-svc-blmfw [749.271036ms]
    Jan 18 22:47:27.645: INFO: Got endpoints: latency-svc-vkr4b [752.625246ms]
    Jan 18 22:47:27.693: INFO: Got endpoints: latency-svc-k5fj7 [750.47325ms]
    Jan 18 22:47:27.745: INFO: Got endpoints: latency-svc-fzj9r [752.711409ms]
    Jan 18 22:47:27.793: INFO: Got endpoints: latency-svc-8957w [747.957393ms]
    Jan 18 22:47:27.843: INFO: Got endpoints: latency-svc-x5b6d [747.453368ms]
    Jan 18 22:47:27.892: INFO: Got endpoints: latency-svc-wtslh [749.093142ms]
    Jan 18 22:47:27.943: INFO: Got endpoints: latency-svc-4z5gv [750.339774ms]
    Jan 18 22:47:27.992: INFO: Got endpoints: latency-svc-mcq5p [749.049923ms]
    Jan 18 22:47:28.043: INFO: Got endpoints: latency-svc-xt5zq [749.210191ms]
    Jan 18 22:47:28.093: INFO: Got endpoints: latency-svc-htkn7 [748.745508ms]
    Jan 18 22:47:28.142: INFO: Got endpoints: latency-svc-pcx44 [749.510976ms]
    Jan 18 22:47:28.143: INFO: Latencies: [13.509542ms 17.360683ms 23.249864ms 30.303073ms 37.770443ms 40.346304ms 51.399967ms 54.684385ms 59.705048ms 65.531086ms 69.360698ms 72.979287ms 81.065089ms 82.122242ms 82.941053ms 84.334215ms 87.571674ms 88.421418ms 89.041054ms 89.720193ms 90.086516ms 90.275577ms 90.725964ms 92.348614ms 92.397646ms 95.683074ms 98.396808ms 99.572621ms 99.762946ms 100.63932ms 129.709078ms 131.93585ms 138.53337ms 142.412113ms 148.477056ms 203.625019ms 306.663652ms 322.730634ms 355.39054ms 417.940766ms 441.980272ms 446.257747ms 486.560209ms 543.993863ms 587.06499ms 637.634447ms 685.633358ms 729.864936ms 740.961411ms 743.071687ms 743.13423ms 745.216301ms 745.668609ms 746.515207ms 746.533819ms 746.653486ms 746.676353ms 746.7371ms 746.886677ms 747.029829ms 747.360039ms 747.453368ms 747.556728ms 747.575997ms 747.602902ms 747.690215ms 747.710224ms 747.749429ms 747.756863ms 747.784639ms 747.784904ms 747.801892ms 747.804248ms 747.916257ms 747.957393ms 748.162449ms 748.203695ms 748.283633ms 748.300636ms 748.377039ms 748.412244ms 748.418547ms 748.502908ms 748.504269ms 748.509678ms 748.637711ms 748.706586ms 748.745508ms 748.769916ms 748.821098ms 748.846143ms 748.891609ms 748.933452ms 748.937262ms 748.972165ms 749.023855ms 749.049923ms 749.093142ms 749.167935ms 749.191886ms 749.210191ms 749.214613ms 749.247136ms 749.271036ms 749.337861ms 749.360579ms 749.361815ms 749.384644ms 749.439808ms 749.449222ms 749.460955ms 749.465835ms 749.510976ms 749.544885ms 749.71253ms 749.776324ms 749.781537ms 749.788678ms 749.841363ms 749.849537ms 749.877584ms 749.878343ms 749.880968ms 749.907151ms 749.918592ms 749.960606ms 749.969391ms 750.002769ms 750.052926ms 750.053812ms 750.055255ms 750.076393ms 750.10463ms 750.194506ms 750.196538ms 750.200363ms 750.219355ms 750.232387ms 750.262746ms 750.265815ms 750.273907ms 750.30587ms 750.333819ms 750.339774ms 750.464701ms 750.47325ms 750.498985ms 750.509028ms 750.544466ms 750.551226ms 750.645052ms 750.679119ms 750.719279ms 750.770716ms 750.951999ms 750.955297ms 750.969105ms 751.029761ms 751.101326ms 751.123156ms 751.2098ms 751.222893ms 751.261551ms 751.367819ms 751.398811ms 751.399514ms 751.499431ms 751.541774ms 751.565086ms 751.649145ms 751.72132ms 751.747531ms 751.859371ms 751.96542ms 751.983252ms 752.008507ms 752.06755ms 752.174541ms 752.177076ms 752.201853ms 752.278938ms 752.292678ms 752.388865ms 752.625246ms 752.69901ms 752.711409ms 752.758136ms 752.802925ms 752.986942ms 752.994897ms 753.081757ms 753.122806ms 753.208518ms 753.299697ms 753.426489ms 753.46396ms 753.554737ms 754.336803ms 754.715097ms 756.932407ms]
    Jan 18 22:47:28.143: INFO: 50 %ile: 749.210191ms
    Jan 18 22:47:28.143: INFO: 90 %ile: 752.278938ms
    Jan 18 22:47:28.143: INFO: 99 %ile: 754.715097ms
    Jan 18 22:47:28.143: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:28.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-9200" for this suite. 01/18/23 22:47:28.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:28.155
Jan 18 22:47:28.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:47:28.156
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:28.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:28.17
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:47:28.173
Jan 18 22:47:28.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502" in namespace "projected-5344" to be "Succeeded or Failed"
Jan 18 22:47:28.187: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936187ms
Jan 18 22:47:30.191: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994672s
Jan 18 22:47:32.192: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008315774s
STEP: Saw pod success 01/18/23 22:47:32.192
Jan 18 22:47:32.192: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502" satisfied condition "Succeeded or Failed"
Jan 18 22:47:32.195: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 container client-container: <nil>
STEP: delete the pod 01/18/23 22:47:32.2
Jan 18 22:47:32.213: INFO: Waiting for pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 to disappear
Jan 18 22:47:32.215: INFO: Pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:32.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5344" for this suite. 01/18/23 22:47:32.218
------------------------------
â€¢ [4.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:28.155
    Jan 18 22:47:28.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:47:28.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:28.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:28.17
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:47:28.173
    Jan 18 22:47:28.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502" in namespace "projected-5344" to be "Succeeded or Failed"
    Jan 18 22:47:28.187: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936187ms
    Jan 18 22:47:30.191: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994672s
    Jan 18 22:47:32.192: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008315774s
    STEP: Saw pod success 01/18/23 22:47:32.192
    Jan 18 22:47:32.192: INFO: Pod "downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502" satisfied condition "Succeeded or Failed"
    Jan 18 22:47:32.195: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:47:32.2
    Jan 18 22:47:32.213: INFO: Waiting for pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 to disappear
    Jan 18 22:47:32.215: INFO: Pod downwardapi-volume-72db330c-82a0-4767-a019-334de39c1502 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:32.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5344" for this suite. 01/18/23 22:47:32.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:32.225
Jan 18 22:47:32.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 22:47:32.226
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:32.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:32.242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/18/23 22:47:32.245
STEP: patching the Namespace 01/18/23 22:47:32.257
STEP: get the Namespace and ensuring it has the label 01/18/23 22:47:32.263
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:32.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1043" for this suite. 01/18/23 22:47:32.269
STEP: Destroying namespace "nspatchtest-bdd65325-c061-4243-a5bd-fc383add0315-2002" for this suite. 01/18/23 22:47:32.274
------------------------------
â€¢ [0.055 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:32.225
    Jan 18 22:47:32.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:47:32.226
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:32.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:32.242
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/18/23 22:47:32.245
    STEP: patching the Namespace 01/18/23 22:47:32.257
    STEP: get the Namespace and ensuring it has the label 01/18/23 22:47:32.263
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:32.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1043" for this suite. 01/18/23 22:47:32.269
    STEP: Destroying namespace "nspatchtest-bdd65325-c061-4243-a5bd-fc383add0315-2002" for this suite. 01/18/23 22:47:32.274
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:32.28
Jan 18 22:47:32.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:47:32.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:32.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:32.298
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:47:32.302
Jan 18 22:47:32.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85" in namespace "downward-api-7173" to be "Succeeded or Failed"
Jan 18 22:47:32.313: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928021ms
Jan 18 22:47:34.317: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250269s
Jan 18 22:47:36.318: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007803535s
STEP: Saw pod success 01/18/23 22:47:36.318
Jan 18 22:47:36.318: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85" satisfied condition "Succeeded or Failed"
Jan 18 22:47:36.321: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 container client-container: <nil>
STEP: delete the pod 01/18/23 22:47:36.326
Jan 18 22:47:36.341: INFO: Waiting for pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 to disappear
Jan 18 22:47:36.344: INFO: Pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:36.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7173" for this suite. 01/18/23 22:47:36.346
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:32.28
    Jan 18 22:47:32.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:47:32.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:32.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:32.298
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:47:32.302
    Jan 18 22:47:32.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85" in namespace "downward-api-7173" to be "Succeeded or Failed"
    Jan 18 22:47:32.313: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928021ms
    Jan 18 22:47:34.317: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250269s
    Jan 18 22:47:36.318: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007803535s
    STEP: Saw pod success 01/18/23 22:47:36.318
    Jan 18 22:47:36.318: INFO: Pod "downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85" satisfied condition "Succeeded or Failed"
    Jan 18 22:47:36.321: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:47:36.326
    Jan 18 22:47:36.341: INFO: Waiting for pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 to disappear
    Jan 18 22:47:36.344: INFO: Pod downwardapi-volume-cd090528-e8d7-4ca8-81af-3a4b7f938b85 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:36.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7173" for this suite. 01/18/23 22:47:36.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:36.355
Jan 18 22:47:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-webhook 01/18/23 22:47:36.356
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:36.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:36.368
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 22:47:36.371
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 22:47:36.829
STEP: Deploying the custom resource conversion webhook pod 01/18/23 22:47:36.836
STEP: Wait for the deployment to be ready 01/18/23 22:47:36.85
Jan 18 22:47:36.857: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:47:38.866
STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:38.873
Jan 18 22:47:39.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:40.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:41.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:42.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:43.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:44.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Jan 18 22:47:45.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 18 22:47:45.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Creating a v1 custom resource 01/18/23 22:47:48.463
STEP: Create a v2 custom resource 01/18/23 22:47:48.479
STEP: List CRs in v1 01/18/23 22:47:48.534
STEP: List CRs in v2 01/18/23 22:47:48.539
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:49.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8448" for this suite. 01/18/23 22:47:49.092
------------------------------
â€¢ [SLOW TEST] [12.742 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:36.355
    Jan 18 22:47:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 22:47:36.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:36.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:36.368
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 22:47:36.371
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 22:47:36.829
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 22:47:36.836
    STEP: Wait for the deployment to be ready 01/18/23 22:47:36.85
    Jan 18 22:47:36.857: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:47:38.866
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:38.873
    Jan 18 22:47:39.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:40.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:41.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:42.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:43.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:44.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Jan 18 22:47:45.874: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 18 22:47:45.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Creating a v1 custom resource 01/18/23 22:47:48.463
    STEP: Create a v2 custom resource 01/18/23 22:47:48.479
    STEP: List CRs in v1 01/18/23 22:47:48.534
    STEP: List CRs in v2 01/18/23 22:47:48.539
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:49.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8448" for this suite. 01/18/23 22:47:49.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:49.097
Jan 18 22:47:49.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:47:49.098
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:49.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:49.117
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-d486383b-7ede-4a3c-9b92-6217c1d1f254 01/18/23 22:47:49.12
STEP: Creating secret with name secret-projected-all-test-volume-4173b3d1-bca4-43f0-bc8d-a46449867e92 01/18/23 22:47:49.124
STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 22:47:49.13
Jan 18 22:47:49.138: INFO: Waiting up to 5m0s for pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a" in namespace "projected-1640" to be "Succeeded or Failed"
Jan 18 22:47:49.141: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638887ms
Jan 18 22:47:51.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006285299s
Jan 18 22:47:53.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006933689s
STEP: Saw pod success 01/18/23 22:47:53.145
Jan 18 22:47:53.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a" satisfied condition "Succeeded or Failed"
Jan 18 22:47:53.148: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a container projected-all-volume-test: <nil>
STEP: delete the pod 01/18/23 22:47:53.153
Jan 18 22:47:53.164: INFO: Waiting for pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a to disappear
Jan 18 22:47:53.167: INFO: Pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:53.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1640" for this suite. 01/18/23 22:47:53.17
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:49.097
    Jan 18 22:47:49.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:47:49.098
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:49.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:49.117
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-d486383b-7ede-4a3c-9b92-6217c1d1f254 01/18/23 22:47:49.12
    STEP: Creating secret with name secret-projected-all-test-volume-4173b3d1-bca4-43f0-bc8d-a46449867e92 01/18/23 22:47:49.124
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 22:47:49.13
    Jan 18 22:47:49.138: INFO: Waiting up to 5m0s for pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a" in namespace "projected-1640" to be "Succeeded or Failed"
    Jan 18 22:47:49.141: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638887ms
    Jan 18 22:47:51.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006285299s
    Jan 18 22:47:53.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006933689s
    STEP: Saw pod success 01/18/23 22:47:53.145
    Jan 18 22:47:53.145: INFO: Pod "projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a" satisfied condition "Succeeded or Failed"
    Jan 18 22:47:53.148: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a container projected-all-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:47:53.153
    Jan 18 22:47:53.164: INFO: Waiting for pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a to disappear
    Jan 18 22:47:53.167: INFO: Pod projected-volume-5a03cab3-50b8-4da5-beed-2a61f073782a no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:53.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1640" for this suite. 01/18/23 22:47:53.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:53.179
Jan 18 22:47:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:47:53.18
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:53.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:53.193
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:47:53.207
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:47:53.591
STEP: Deploying the webhook pod 01/18/23 22:47:53.596
STEP: Wait for the deployment to be ready 01/18/23 22:47:53.607
Jan 18 22:47:53.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:47:55.624
STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:55.631
Jan 18 22:47:56.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 22:47:56.635
STEP: create a pod that should be updated by the webhook 01/18/23 22:47:56.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:47:56.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8777" for this suite. 01/18/23 22:47:56.705
STEP: Destroying namespace "webhook-8777-markers" for this suite. 01/18/23 22:47:56.712
------------------------------
â€¢ [3.538 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:53.179
    Jan 18 22:47:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:47:53.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:53.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:53.193
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:47:53.207
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:47:53.591
    STEP: Deploying the webhook pod 01/18/23 22:47:53.596
    STEP: Wait for the deployment to be ready 01/18/23 22:47:53.607
    Jan 18 22:47:53.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:47:55.624
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:55.631
    Jan 18 22:47:56.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 22:47:56.635
    STEP: create a pod that should be updated by the webhook 01/18/23 22:47:56.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:47:56.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8777" for this suite. 01/18/23 22:47:56.705
    STEP: Destroying namespace "webhook-8777-markers" for this suite. 01/18/23 22:47:56.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:47:56.718
Jan 18 22:47:56.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:47:56.719
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:56.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:56.738
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/18/23 22:47:56.741
Jan 18 22:47:56.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 22:47:56.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:57.551: INFO: stderr: ""
Jan 18 22:47:57.551: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 22:47:57.551: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 22:47:57.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:57.764: INFO: stderr: ""
Jan 18 22:47:57.764: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 22:47:57.764: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 22:47:57.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:57.973: INFO: stderr: ""
Jan 18 22:47:57.973: INFO: stdout: "service/frontend created\n"
Jan 18 22:47:57.973: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 22:47:57.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:58.180: INFO: stderr: ""
Jan 18 22:47:58.180: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 22:47:58.180: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 22:47:58.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:58.377: INFO: stderr: ""
Jan 18 22:47:58.377: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 22:47:58.377: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 22:47:58.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
Jan 18 22:47:58.586: INFO: stderr: ""
Jan 18 22:47:58.586: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/18/23 22:47:58.586
Jan 18 22:47:58.586: INFO: Waiting for all frontend pods to be Running.
Jan 18 22:48:03.638: INFO: Waiting for frontend to serve content.
Jan 18 22:48:03.647: INFO: Trying to add a new entry to the guestbook.
Jan 18 22:48:03.657: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/18/23 22:48:03.665
Jan 18 22:48:03.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:03.743: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:03.743: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 22:48:03.743
Jan 18 22:48:03.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:03.823: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:03.823: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 22:48:03.823
Jan 18 22:48:03.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:03.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:03.910: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 22:48:03.91
Jan 18 22:48:03.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:03.987: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:03.987: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 22:48:03.987
Jan 18 22:48:03.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:04.072: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:04.072: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 22:48:04.072
Jan 18 22:48:04.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
Jan 18 22:48:04.176: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:48:04.176: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:48:04.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8774" for this suite. 01/18/23 22:48:04.18
------------------------------
â€¢ [SLOW TEST] [7.467 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:47:56.718
    Jan 18 22:47:56.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:47:56.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:56.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:56.738
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/18/23 22:47:56.741
    Jan 18 22:47:56.741: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 18 22:47:56.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:57.551: INFO: stderr: ""
    Jan 18 22:47:57.551: INFO: stdout: "service/agnhost-replica created\n"
    Jan 18 22:47:57.551: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 18 22:47:57.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:57.764: INFO: stderr: ""
    Jan 18 22:47:57.764: INFO: stdout: "service/agnhost-primary created\n"
    Jan 18 22:47:57.764: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 18 22:47:57.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:57.973: INFO: stderr: ""
    Jan 18 22:47:57.973: INFO: stdout: "service/frontend created\n"
    Jan 18 22:47:57.973: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 18 22:47:57.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:58.180: INFO: stderr: ""
    Jan 18 22:47:58.180: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 18 22:47:58.180: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 22:47:58.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:58.377: INFO: stderr: ""
    Jan 18 22:47:58.377: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 18 22:47:58.377: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 22:47:58.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 create -f -'
    Jan 18 22:47:58.586: INFO: stderr: ""
    Jan 18 22:47:58.586: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/18/23 22:47:58.586
    Jan 18 22:47:58.586: INFO: Waiting for all frontend pods to be Running.
    Jan 18 22:48:03.638: INFO: Waiting for frontend to serve content.
    Jan 18 22:48:03.647: INFO: Trying to add a new entry to the guestbook.
    Jan 18 22:48:03.657: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/18/23 22:48:03.665
    Jan 18 22:48:03.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:03.743: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:03.743: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 22:48:03.743
    Jan 18 22:48:03.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:03.823: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:03.823: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 22:48:03.823
    Jan 18 22:48:03.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:03.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:03.910: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 22:48:03.91
    Jan 18 22:48:03.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:03.987: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:03.987: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 22:48:03.987
    Jan 18 22:48:03.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:04.072: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:04.072: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 22:48:04.072
    Jan 18 22:48:04.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8774 delete --grace-period=0 --force -f -'
    Jan 18 22:48:04.176: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:48:04.176: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:48:04.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8774" for this suite. 01/18/23 22:48:04.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:48:04.187
Jan 18 22:48:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:48:04.19
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:48:04.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:48:04.211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a in namespace container-probe-3614 01/18/23 22:48:04.215
Jan 18 22:48:04.224: INFO: Waiting up to 5m0s for pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a" in namespace "container-probe-3614" to be "not pending"
Jan 18 22:48:04.227: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838725ms
Jan 18 22:48:06.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338614s
Jan 18 22:48:08.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Running", Reason="", readiness=true. Elapsed: 4.0072325s
Jan 18 22:48:08.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a" satisfied condition "not pending"
Jan 18 22:48:08.232: INFO: Started pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a in namespace container-probe-3614
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:48:08.232
Jan 18 22:48:08.235: INFO: Initial restart count of pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a is 0
STEP: deleting the pod 01/18/23 22:52:08.73
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:52:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3614" for this suite. 01/18/23 22:52:08.747
------------------------------
â€¢ [SLOW TEST] [244.567 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:48:04.187
    Jan 18 22:48:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:48:04.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:48:04.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:48:04.211
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a in namespace container-probe-3614 01/18/23 22:48:04.215
    Jan 18 22:48:04.224: INFO: Waiting up to 5m0s for pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a" in namespace "container-probe-3614" to be "not pending"
    Jan 18 22:48:04.227: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838725ms
    Jan 18 22:48:06.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338614s
    Jan 18 22:48:08.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a": Phase="Running", Reason="", readiness=true. Elapsed: 4.0072325s
    Jan 18 22:48:08.232: INFO: Pod "busybox-b58dce55-e673-407a-883b-2e1f7fc8285a" satisfied condition "not pending"
    Jan 18 22:48:08.232: INFO: Started pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a in namespace container-probe-3614
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:48:08.232
    Jan 18 22:48:08.235: INFO: Initial restart count of pod busybox-b58dce55-e673-407a-883b-2e1f7fc8285a is 0
    STEP: deleting the pod 01/18/23 22:52:08.73
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:52:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3614" for this suite. 01/18/23 22:52:08.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:52:08.754
Jan 18 22:52:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context 01/18/23 22:52:08.755
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:08.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:08.778
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:52:08.781
Jan 18 22:52:08.788: INFO: Waiting up to 5m0s for pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005" in namespace "security-context-5002" to be "Succeeded or Failed"
Jan 18 22:52:08.791: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489367ms
Jan 18 22:52:10.794: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005821801s
Jan 18 22:52:12.795: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006572239s
STEP: Saw pod success 01/18/23 22:52:12.795
Jan 18 22:52:12.795: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005" satisfied condition "Succeeded or Failed"
Jan 18 22:52:12.798: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 container test-container: <nil>
STEP: delete the pod 01/18/23 22:52:12.813
Jan 18 22:52:12.825: INFO: Waiting for pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 to disappear
Jan 18 22:52:12.828: INFO: Pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:52:12.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5002" for this suite. 01/18/23 22:52:12.831
------------------------------
â€¢ [4.082 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:52:08.754
    Jan 18 22:52:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context 01/18/23 22:52:08.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:08.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:08.778
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:52:08.781
    Jan 18 22:52:08.788: INFO: Waiting up to 5m0s for pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005" in namespace "security-context-5002" to be "Succeeded or Failed"
    Jan 18 22:52:08.791: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489367ms
    Jan 18 22:52:10.794: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005821801s
    Jan 18 22:52:12.795: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006572239s
    STEP: Saw pod success 01/18/23 22:52:12.795
    Jan 18 22:52:12.795: INFO: Pod "security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005" satisfied condition "Succeeded or Failed"
    Jan 18 22:52:12.798: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:52:12.813
    Jan 18 22:52:12.825: INFO: Waiting for pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 to disappear
    Jan 18 22:52:12.828: INFO: Pod security-context-8ac0b0d9-6d78-4125-9473-a30c9e9f2005 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:52:12.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5002" for this suite. 01/18/23 22:52:12.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:52:12.838
Jan 18 22:52:12.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:52:12.839
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:12.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:12.855
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-c51e64df-3c2b-45d7-8682-b5c0b867b838 01/18/23 22:52:12.858
STEP: Creating a pod to test consume configMaps 01/18/23 22:52:12.862
Jan 18 22:52:12.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4" in namespace "configmap-6821" to be "Succeeded or Failed"
Jan 18 22:52:12.873: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685407ms
Jan 18 22:52:14.877: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00713882s
Jan 18 22:52:16.878: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007422658s
STEP: Saw pod success 01/18/23 22:52:16.878
Jan 18 22:52:16.878: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4" satisfied condition "Succeeded or Failed"
Jan 18 22:52:16.881: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:52:16.886
Jan 18 22:52:16.896: INFO: Waiting for pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 to disappear
Jan 18 22:52:16.898: INFO: Pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:52:16.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6821" for this suite. 01/18/23 22:52:16.902
------------------------------
â€¢ [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:52:12.838
    Jan 18 22:52:12.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:52:12.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:12.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:12.855
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-c51e64df-3c2b-45d7-8682-b5c0b867b838 01/18/23 22:52:12.858
    STEP: Creating a pod to test consume configMaps 01/18/23 22:52:12.862
    Jan 18 22:52:12.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4" in namespace "configmap-6821" to be "Succeeded or Failed"
    Jan 18 22:52:12.873: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685407ms
    Jan 18 22:52:14.877: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00713882s
    Jan 18 22:52:16.878: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007422658s
    STEP: Saw pod success 01/18/23 22:52:16.878
    Jan 18 22:52:16.878: INFO: Pod "pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4" satisfied condition "Succeeded or Failed"
    Jan 18 22:52:16.881: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:52:16.886
    Jan 18 22:52:16.896: INFO: Waiting for pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 to disappear
    Jan 18 22:52:16.898: INFO: Pod pod-configmaps-7e48b4da-7857-4860-a6bc-ebab4667acc4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:52:16.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6821" for this suite. 01/18/23 22:52:16.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:52:16.91
Jan 18 22:52:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 22:52:16.911
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:16.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:16.925
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 18 22:52:16.928: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:53:16.944: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 18 22:53:16.946: INFO: Starting informer...
STEP: Starting pods... 01/18/23 22:53:16.946
Jan 18 22:53:17.164: INFO: Pod1 is running on cncf-conformance-1-26-2. Tainting Node
Jan 18 22:53:17.373: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1639" to be "running"
Jan 18 22:53:17.376: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921358ms
Jan 18 22:53:19.380: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007034101s
Jan 18 22:53:19.381: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 18 22:53:19.381: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1639" to be "running"
Jan 18 22:53:19.383: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.511698ms
Jan 18 22:53:19.383: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 18 22:53:19.383: INFO: Pod2 is running on cncf-conformance-1-26-2. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 22:53:19.383
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:53:19.394
STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 22:53:19.397
Jan 18 22:53:25.059: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 18 22:53:45.109: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:53:45.122
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:53:45.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-1639" for this suite. 01/18/23 22:53:45.127
------------------------------
â€¢ [SLOW TEST] [88.223 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:52:16.91
    Jan 18 22:52:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 22:52:16.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:16.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:16.925
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 18 22:52:16.928: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:53:16.944: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 18 22:53:16.946: INFO: Starting informer...
    STEP: Starting pods... 01/18/23 22:53:16.946
    Jan 18 22:53:17.164: INFO: Pod1 is running on cncf-conformance-1-26-2. Tainting Node
    Jan 18 22:53:17.373: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1639" to be "running"
    Jan 18 22:53:17.376: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921358ms
    Jan 18 22:53:19.380: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007034101s
    Jan 18 22:53:19.381: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 18 22:53:19.381: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1639" to be "running"
    Jan 18 22:53:19.383: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.511698ms
    Jan 18 22:53:19.383: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 18 22:53:19.383: INFO: Pod2 is running on cncf-conformance-1-26-2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 22:53:19.383
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:53:19.394
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 22:53:19.397
    Jan 18 22:53:25.059: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 18 22:53:45.109: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:53:45.122
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:53:45.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-1639" for this suite. 01/18/23 22:53:45.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:53:45.134
Jan 18 22:53:45.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:53:45.135
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.148
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/18/23 22:53:45.153
STEP: watching for the Service to be added 01/18/23 22:53:45.161
Jan 18 22:53:45.163: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 22:53:45.163: INFO: Service test-service-z4zlw created
STEP: Getting /status 01/18/23 22:53:45.163
Jan 18 22:53:45.166: INFO: Service test-service-z4zlw has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/18/23 22:53:45.166
STEP: watching for the Service to be patched 01/18/23 22:53:45.171
Jan 18 22:53:45.173: INFO: observed Service test-service-z4zlw in namespace services-9026 with annotations: map[] & LoadBalancer: {[]}
Jan 18 22:53:45.173: INFO: Found Service test-service-z4zlw in namespace services-9026 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 22:53:45.173: INFO: Service test-service-z4zlw has service status patched
STEP: updating the ServiceStatus 01/18/23 22:53:45.173
Jan 18 22:53:45.186: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/18/23 22:53:45.186
Jan 18 22:53:45.187: INFO: Observed Service test-service-z4zlw in namespace services-9026 with annotations: map[] & Conditions: {[]}
Jan 18 22:53:45.188: INFO: Observed event: &Service{ObjectMeta:{test-service-z4zlw  services-9026  7e8c0f95-8b38-4957-b64f-3c5c91d62af4 14331 0 2023-01-18 22:53:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 22:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 22:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.1.83,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.1.83],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 22:53:45.188: INFO: Found Service test-service-z4zlw in namespace services-9026 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:53:45.188: INFO: Service test-service-z4zlw has service status updated
STEP: patching the service 01/18/23 22:53:45.188
STEP: watching for the Service to be patched 01/18/23 22:53:45.2
Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
Jan 18 22:53:45.202: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service:patched test-service-static:true]
Jan 18 22:53:45.202: INFO: Service test-service-z4zlw patched
STEP: deleting the service 01/18/23 22:53:45.202
STEP: watching for the Service to be deleted 01/18/23 22:53:45.212
Jan 18 22:53:45.213: INFO: Observed event: ADDED
Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
Jan 18 22:53:45.213: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 22:53:45.213: INFO: Service test-service-z4zlw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:53:45.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9026" for this suite. 01/18/23 22:53:45.217
------------------------------
â€¢ [0.088 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:53:45.134
    Jan 18 22:53:45.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:53:45.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.148
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/18/23 22:53:45.153
    STEP: watching for the Service to be added 01/18/23 22:53:45.161
    Jan 18 22:53:45.163: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 18 22:53:45.163: INFO: Service test-service-z4zlw created
    STEP: Getting /status 01/18/23 22:53:45.163
    Jan 18 22:53:45.166: INFO: Service test-service-z4zlw has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/18/23 22:53:45.166
    STEP: watching for the Service to be patched 01/18/23 22:53:45.171
    Jan 18 22:53:45.173: INFO: observed Service test-service-z4zlw in namespace services-9026 with annotations: map[] & LoadBalancer: {[]}
    Jan 18 22:53:45.173: INFO: Found Service test-service-z4zlw in namespace services-9026 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 18 22:53:45.173: INFO: Service test-service-z4zlw has service status patched
    STEP: updating the ServiceStatus 01/18/23 22:53:45.173
    Jan 18 22:53:45.186: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/18/23 22:53:45.186
    Jan 18 22:53:45.187: INFO: Observed Service test-service-z4zlw in namespace services-9026 with annotations: map[] & Conditions: {[]}
    Jan 18 22:53:45.188: INFO: Observed event: &Service{ObjectMeta:{test-service-z4zlw  services-9026  7e8c0f95-8b38-4957-b64f-3c5c91d62af4 14331 0 2023-01-18 22:53:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 22:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 22:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.1.83,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.1.83],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 18 22:53:45.188: INFO: Found Service test-service-z4zlw in namespace services-9026 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 22:53:45.188: INFO: Service test-service-z4zlw has service status updated
    STEP: patching the service 01/18/23 22:53:45.188
    STEP: watching for the Service to be patched 01/18/23 22:53:45.2
    Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
    Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
    Jan 18 22:53:45.202: INFO: observed Service test-service-z4zlw in namespace services-9026 with labels: map[test-service-static:true]
    Jan 18 22:53:45.202: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service:patched test-service-static:true]
    Jan 18 22:53:45.202: INFO: Service test-service-z4zlw patched
    STEP: deleting the service 01/18/23 22:53:45.202
    STEP: watching for the Service to be deleted 01/18/23 22:53:45.212
    Jan 18 22:53:45.213: INFO: Observed event: ADDED
    Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
    Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
    Jan 18 22:53:45.213: INFO: Observed event: MODIFIED
    Jan 18 22:53:45.213: INFO: Found Service test-service-z4zlw in namespace services-9026 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 18 22:53:45.213: INFO: Service test-service-z4zlw deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:53:45.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9026" for this suite. 01/18/23 22:53:45.217
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:53:45.222
Jan 18 22:53:45.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename podtemplate 01/18/23 22:53:45.223
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.24
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 22:53:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1394" for this suite. 01/18/23 22:53:45.272
------------------------------
â€¢ [0.055 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:53:45.222
    Jan 18 22:53:45.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 22:53:45.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.24
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:53:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1394" for this suite. 01/18/23 22:53:45.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:53:45.279
Jan 18 22:53:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:53:45.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.294
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-6322 01/18/23 22:53:45.297
STEP: creating service affinity-clusterip-transition in namespace services-6322 01/18/23 22:53:45.297
STEP: creating replication controller affinity-clusterip-transition in namespace services-6322 01/18/23 22:53:45.305
I0118 22:53:45.311796      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6322, replica count: 3
I0118 22:53:48.363382      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:53:48.368: INFO: Creating new exec pod
Jan 18 22:53:48.373: INFO: Waiting up to 5m0s for pod "execpod-affinityqr8dp" in namespace "services-6322" to be "running"
Jan 18 22:53:48.375: INFO: Pod "execpod-affinityqr8dp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525356ms
Jan 18 22:53:50.379: INFO: Pod "execpod-affinityqr8dp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006326838s
Jan 18 22:53:50.379: INFO: Pod "execpod-affinityqr8dp" satisfied condition "running"
Jan 18 22:53:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 18 22:53:51.511: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 22:53:51.511: INFO: stdout: ""
Jan 18 22:53:51.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c nc -v -z -w 2 10.96.3.113 80'
Jan 18 22:53:51.659: INFO: stderr: "+ nc -v -z -w 2 10.96.3.113 80\nConnection to 10.96.3.113 80 port [tcp/http] succeeded!\n"
Jan 18 22:53:51.659: INFO: stdout: ""
Jan 18 22:53:51.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.3.113:80/ ; done'
Jan 18 22:53:51.902: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n"
Jan 18 22:53:51.903: INFO: stdout: "\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n"
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
Jan 18 22:53:51.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.3.113:80/ ; done'
Jan 18 22:53:52.124: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n"
Jan 18 22:53:52.124: INFO: stdout: "\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh"
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
Jan 18 22:53:52.124: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6322, will wait for the garbage collector to delete the pods 01/18/23 22:53:52.137
Jan 18 22:53:52.196: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.451295ms
Jan 18 22:53:52.297: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.935838ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:53:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6322" for this suite. 01/18/23 22:53:54.214
------------------------------
â€¢ [SLOW TEST] [8.941 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:53:45.279
    Jan 18 22:53:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:53:45.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:45.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:45.294
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-6322 01/18/23 22:53:45.297
    STEP: creating service affinity-clusterip-transition in namespace services-6322 01/18/23 22:53:45.297
    STEP: creating replication controller affinity-clusterip-transition in namespace services-6322 01/18/23 22:53:45.305
    I0118 22:53:45.311796      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6322, replica count: 3
    I0118 22:53:48.363382      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:53:48.368: INFO: Creating new exec pod
    Jan 18 22:53:48.373: INFO: Waiting up to 5m0s for pod "execpod-affinityqr8dp" in namespace "services-6322" to be "running"
    Jan 18 22:53:48.375: INFO: Pod "execpod-affinityqr8dp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525356ms
    Jan 18 22:53:50.379: INFO: Pod "execpod-affinityqr8dp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006326838s
    Jan 18 22:53:50.379: INFO: Pod "execpod-affinityqr8dp" satisfied condition "running"
    Jan 18 22:53:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 18 22:53:51.511: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 22:53:51.511: INFO: stdout: ""
    Jan 18 22:53:51.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c nc -v -z -w 2 10.96.3.113 80'
    Jan 18 22:53:51.659: INFO: stderr: "+ nc -v -z -w 2 10.96.3.113 80\nConnection to 10.96.3.113 80 port [tcp/http] succeeded!\n"
    Jan 18 22:53:51.659: INFO: stdout: ""
    Jan 18 22:53:51.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.3.113:80/ ; done'
    Jan 18 22:53:51.902: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n"
    Jan 18 22:53:51.903: INFO: stdout: "\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n\naffinity-clusterip-transition-bkld4\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-h444n"
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-bkld4
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:51.903: INFO: Received response from host: affinity-clusterip-transition-h444n
    Jan 18 22:53:51.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-6322 exec execpod-affinityqr8dp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.3.113:80/ ; done'
    Jan 18 22:53:52.124: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.3.113:80/\n"
    Jan 18 22:53:52.124: INFO: stdout: "\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh\naffinity-clusterip-transition-5xfwh"
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Received response from host: affinity-clusterip-transition-5xfwh
    Jan 18 22:53:52.124: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6322, will wait for the garbage collector to delete the pods 01/18/23 22:53:52.137
    Jan 18 22:53:52.196: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.451295ms
    Jan 18 22:53:52.297: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.935838ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:53:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6322" for this suite. 01/18/23 22:53:54.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:53:54.222
Jan 18 22:53:54.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:53:54.223
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:54.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:54.238
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/18/23 22:53:54.242
STEP: wait for the container to reach Succeeded 01/18/23 22:53:54.252
STEP: get the container status 01/18/23 22:53:57.265
STEP: the container should be terminated 01/18/23 22:53:57.267
STEP: the termination message should be set 01/18/23 22:53:57.267
Jan 18 22:53:57.267: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/18/23 22:53:57.267
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 22:53:57.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4489" for this suite. 01/18/23 22:53:57.284
------------------------------
â€¢ [3.069 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:53:54.222
    Jan 18 22:53:54.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:53:54.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:54.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:54.238
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/18/23 22:53:54.242
    STEP: wait for the container to reach Succeeded 01/18/23 22:53:54.252
    STEP: get the container status 01/18/23 22:53:57.265
    STEP: the container should be terminated 01/18/23 22:53:57.267
    STEP: the termination message should be set 01/18/23 22:53:57.267
    Jan 18 22:53:57.267: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/18/23 22:53:57.267
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:53:57.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4489" for this suite. 01/18/23 22:53:57.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:53:57.293
Jan 18 22:53:57.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:53:57.294
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:57.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:57.307
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1200 01/18/23 22:53:57.31
STEP: creating service affinity-nodeport in namespace services-1200 01/18/23 22:53:57.31
STEP: creating replication controller affinity-nodeport in namespace services-1200 01/18/23 22:53:57.321
I0118 22:53:57.326888      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1200, replica count: 3
I0118 22:54:00.379003      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:54:00.386: INFO: Creating new exec pod
Jan 18 22:54:00.392: INFO: Waiting up to 5m0s for pod "execpod-affinityqxc4z" in namespace "services-1200" to be "running"
Jan 18 22:54:00.395: INFO: Pod "execpod-affinityqxc4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460653ms
Jan 18 22:54:02.397: INFO: Pod "execpod-affinityqxc4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004943293s
Jan 18 22:54:02.397: INFO: Pod "execpod-affinityqxc4z" satisfied condition "running"
Jan 18 22:54:03.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 18 22:54:03.548: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 22:54:03.548: INFO: stdout: ""
Jan 18 22:54:03.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.96.3.92 80'
Jan 18 22:54:03.694: INFO: stderr: "+ nc -v -z -w 2 10.96.3.92 80\nConnection to 10.96.3.92 80 port [tcp/http] succeeded!\n"
Jan 18 22:54:03.694: INFO: stdout: ""
Jan 18 22:54:03.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 30354'
Jan 18 22:54:03.826: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 30354\nConnection to 10.128.15.198 30354 port [tcp/*] succeeded!\n"
Jan 18 22:54:03.826: INFO: stdout: ""
Jan 18 22:54:03.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 30354'
Jan 18 22:54:03.965: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 30354\nConnection to 10.128.15.199 30354 port [tcp/*] succeeded!\n"
Jan 18 22:54:03.965: INFO: stdout: ""
Jan 18 22:54:03.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:30354/ ; done'
Jan 18 22:54:04.191: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n"
Jan 18 22:54:04.191: INFO: stdout: "\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl"
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
Jan 18 22:54:04.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1200, will wait for the garbage collector to delete the pods 01/18/23 22:54:04.202
Jan 18 22:54:04.261: INFO: Deleting ReplicationController affinity-nodeport took: 5.667762ms
Jan 18 22:54:04.362: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.797687ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:06.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1200" for this suite. 01/18/23 22:54:06.283
------------------------------
â€¢ [SLOW TEST] [8.994 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:53:57.293
    Jan 18 22:53:57.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:53:57.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:57.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:57.307
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1200 01/18/23 22:53:57.31
    STEP: creating service affinity-nodeport in namespace services-1200 01/18/23 22:53:57.31
    STEP: creating replication controller affinity-nodeport in namespace services-1200 01/18/23 22:53:57.321
    I0118 22:53:57.326888      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1200, replica count: 3
    I0118 22:54:00.379003      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:54:00.386: INFO: Creating new exec pod
    Jan 18 22:54:00.392: INFO: Waiting up to 5m0s for pod "execpod-affinityqxc4z" in namespace "services-1200" to be "running"
    Jan 18 22:54:00.395: INFO: Pod "execpod-affinityqxc4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460653ms
    Jan 18 22:54:02.397: INFO: Pod "execpod-affinityqxc4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004943293s
    Jan 18 22:54:02.397: INFO: Pod "execpod-affinityqxc4z" satisfied condition "running"
    Jan 18 22:54:03.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 18 22:54:03.548: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 18 22:54:03.548: INFO: stdout: ""
    Jan 18 22:54:03.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.96.3.92 80'
    Jan 18 22:54:03.694: INFO: stderr: "+ nc -v -z -w 2 10.96.3.92 80\nConnection to 10.96.3.92 80 port [tcp/http] succeeded!\n"
    Jan 18 22:54:03.694: INFO: stdout: ""
    Jan 18 22:54:03.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.128.15.198 30354'
    Jan 18 22:54:03.826: INFO: stderr: "+ nc -v -z -w 2 10.128.15.198 30354\nConnection to 10.128.15.198 30354 port [tcp/*] succeeded!\n"
    Jan 18 22:54:03.826: INFO: stdout: ""
    Jan 18 22:54:03.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c nc -v -z -w 2 10.128.15.199 30354'
    Jan 18 22:54:03.965: INFO: stderr: "+ nc -v -z -w 2 10.128.15.199 30354\nConnection to 10.128.15.199 30354 port [tcp/*] succeeded!\n"
    Jan 18 22:54:03.965: INFO: stdout: ""
    Jan 18 22:54:03.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-1200 exec execpod-affinityqxc4z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.15.198:30354/ ; done'
    Jan 18 22:54:04.191: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.15.198:30354/\n"
    Jan 18 22:54:04.191: INFO: stdout: "\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl\naffinity-nodeport-hhkcl"
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.191: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Received response from host: affinity-nodeport-hhkcl
    Jan 18 22:54:04.192: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1200, will wait for the garbage collector to delete the pods 01/18/23 22:54:04.202
    Jan 18 22:54:04.261: INFO: Deleting ReplicationController affinity-nodeport took: 5.667762ms
    Jan 18 22:54:04.362: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.797687ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:06.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1200" for this suite. 01/18/23 22:54:06.283
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:06.288
Jan 18 22:54:06.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:54:06.289
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:06.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:06.304
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/18/23 22:54:06.307
Jan 18 22:54:06.307: INFO: Creating e2e-svc-a-kbcwg
Jan 18 22:54:06.316: INFO: Creating e2e-svc-b-2zww6
Jan 18 22:54:06.323: INFO: Creating e2e-svc-c-dlbfk
STEP: deleting service collection 01/18/23 22:54:06.333
Jan 18 22:54:06.354: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:06.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4146" for this suite. 01/18/23 22:54:06.358
------------------------------
â€¢ [0.075 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:06.288
    Jan 18 22:54:06.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:54:06.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:06.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:06.304
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/18/23 22:54:06.307
    Jan 18 22:54:06.307: INFO: Creating e2e-svc-a-kbcwg
    Jan 18 22:54:06.316: INFO: Creating e2e-svc-b-2zww6
    Jan 18 22:54:06.323: INFO: Creating e2e-svc-c-dlbfk
    STEP: deleting service collection 01/18/23 22:54:06.333
    Jan 18 22:54:06.354: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:06.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4146" for this suite. 01/18/23 22:54:06.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:06.364
Jan 18 22:54:06.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:54:06.365
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:06.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:06.378
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-033c903b-4ab8-4013-a355-2d36b4b7d8f2 01/18/23 22:54:06.397
STEP: Creating a pod to test consume secrets 01/18/23 22:54:06.403
Jan 18 22:54:06.409: INFO: Waiting up to 5m0s for pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87" in namespace "secrets-5344" to be "Succeeded or Failed"
Jan 18 22:54:06.411: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.312837ms
Jan 18 22:54:08.415: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006266075s
Jan 18 22:54:10.417: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007594025s
STEP: Saw pod success 01/18/23 22:54:10.417
Jan 18 22:54:10.417: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87" satisfied condition "Succeeded or Failed"
Jan 18 22:54:10.419: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:54:10.432
Jan 18 22:54:10.444: INFO: Waiting for pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 to disappear
Jan 18 22:54:10.446: INFO: Pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:10.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5344" for this suite. 01/18/23 22:54:10.449
STEP: Destroying namespace "secret-namespace-4623" for this suite. 01/18/23 22:54:10.454
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:06.364
    Jan 18 22:54:06.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:54:06.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:06.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:06.378
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-033c903b-4ab8-4013-a355-2d36b4b7d8f2 01/18/23 22:54:06.397
    STEP: Creating a pod to test consume secrets 01/18/23 22:54:06.403
    Jan 18 22:54:06.409: INFO: Waiting up to 5m0s for pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87" in namespace "secrets-5344" to be "Succeeded or Failed"
    Jan 18 22:54:06.411: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.312837ms
    Jan 18 22:54:08.415: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006266075s
    Jan 18 22:54:10.417: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007594025s
    STEP: Saw pod success 01/18/23 22:54:10.417
    Jan 18 22:54:10.417: INFO: Pod "pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87" satisfied condition "Succeeded or Failed"
    Jan 18 22:54:10.419: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:54:10.432
    Jan 18 22:54:10.444: INFO: Waiting for pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 to disappear
    Jan 18 22:54:10.446: INFO: Pod pod-secrets-825f2c55-b8cb-4ec1-8e3f-2a27a29a8c87 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:10.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5344" for this suite. 01/18/23 22:54:10.449
    STEP: Destroying namespace "secret-namespace-4623" for this suite. 01/18/23 22:54:10.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:10.459
Jan 18 22:54:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:54:10.46
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:10.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:10.474
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:54:10.477
Jan 18 22:54:10.485: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43" in namespace "projected-4647" to be "Succeeded or Failed"
Jan 18 22:54:10.487: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210909ms
Jan 18 22:54:12.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Running", Reason="", readiness=false. Elapsed: 2.006594088s
Jan 18 22:54:14.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006952114s
STEP: Saw pod success 01/18/23 22:54:14.492
Jan 18 22:54:14.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43" satisfied condition "Succeeded or Failed"
Jan 18 22:54:14.494: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 container client-container: <nil>
STEP: delete the pod 01/18/23 22:54:14.499
Jan 18 22:54:14.510: INFO: Waiting for pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 to disappear
Jan 18 22:54:14.512: INFO: Pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:14.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4647" for this suite. 01/18/23 22:54:14.515
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:10.459
    Jan 18 22:54:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:54:10.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:10.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:10.474
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:54:10.477
    Jan 18 22:54:10.485: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43" in namespace "projected-4647" to be "Succeeded or Failed"
    Jan 18 22:54:10.487: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210909ms
    Jan 18 22:54:12.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Running", Reason="", readiness=false. Elapsed: 2.006594088s
    Jan 18 22:54:14.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006952114s
    STEP: Saw pod success 01/18/23 22:54:14.492
    Jan 18 22:54:14.492: INFO: Pod "downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43" satisfied condition "Succeeded or Failed"
    Jan 18 22:54:14.494: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:54:14.499
    Jan 18 22:54:14.510: INFO: Waiting for pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 to disappear
    Jan 18 22:54:14.512: INFO: Pod downwardapi-volume-7af5be70-c3ed-4e2f-9e66-a6d03d79aa43 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:14.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4647" for this suite. 01/18/23 22:54:14.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:14.521
Jan 18 22:54:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:54:14.522
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:14.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:14.534
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:54:14.548
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:54:15.319
STEP: Deploying the webhook pod 01/18/23 22:54:15.326
STEP: Wait for the deployment to be ready 01/18/23 22:54:15.337
Jan 18 22:54:15.342: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 22:54:17.351
STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:17.359
Jan 18 22:54:18.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/18/23 22:54:18.423
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:54:18.452
STEP: Deleting the collection of validation webhooks 01/18/23 22:54:18.478
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:54:18.513
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:18.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5086" for this suite. 01/18/23 22:54:18.556
STEP: Destroying namespace "webhook-5086-markers" for this suite. 01/18/23 22:54:18.564
------------------------------
â€¢ [4.051 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:14.521
    Jan 18 22:54:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:54:14.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:14.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:14.534
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:54:14.548
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:54:15.319
    STEP: Deploying the webhook pod 01/18/23 22:54:15.326
    STEP: Wait for the deployment to be ready 01/18/23 22:54:15.337
    Jan 18 22:54:15.342: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 22:54:17.351
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:17.359
    Jan 18 22:54:18.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/18/23 22:54:18.423
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:54:18.452
    STEP: Deleting the collection of validation webhooks 01/18/23 22:54:18.478
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:54:18.513
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:18.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5086" for this suite. 01/18/23 22:54:18.556
    STEP: Destroying namespace "webhook-5086-markers" for this suite. 01/18/23 22:54:18.564
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:18.573
Jan 18 22:54:18.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename limitrange 01/18/23 22:54:18.574
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.59
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-cgrht" in namespace "limitrange-8947" 01/18/23 22:54:18.593
STEP: Creating another limitRange in another namespace 01/18/23 22:54:18.598
Jan 18 22:54:18.608: INFO: Namespace "e2e-limitrange-cgrht-5333" created
Jan 18 22:54:18.608: INFO: Creating LimitRange "e2e-limitrange-cgrht" in namespace "e2e-limitrange-cgrht-5333"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-cgrht" 01/18/23 22:54:18.612
Jan 18 22:54:18.614: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-cgrht" in "limitrange-8947" namespace 01/18/23 22:54:18.614
Jan 18 22:54:18.621: INFO: LimitRange "e2e-limitrange-cgrht" has been patched
STEP: Delete LimitRange "e2e-limitrange-cgrht" by Collection with labelSelector: "e2e-limitrange-cgrht=patched" 01/18/23 22:54:18.621
STEP: Confirm that the limitRange "e2e-limitrange-cgrht" has been deleted 01/18/23 22:54:18.628
Jan 18 22:54:18.628: INFO: Requesting list of LimitRange to confirm quantity
Jan 18 22:54:18.631: INFO: Found 0 LimitRange with label "e2e-limitrange-cgrht=patched"
Jan 18 22:54:18.631: INFO: LimitRange "e2e-limitrange-cgrht" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-cgrht" 01/18/23 22:54:18.631
Jan 18 22:54:18.633: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:18.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8947" for this suite. 01/18/23 22:54:18.636
STEP: Destroying namespace "e2e-limitrange-cgrht-5333" for this suite. 01/18/23 22:54:18.643
------------------------------
â€¢ [0.076 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:18.573
    Jan 18 22:54:18.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename limitrange 01/18/23 22:54:18.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.59
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-cgrht" in namespace "limitrange-8947" 01/18/23 22:54:18.593
    STEP: Creating another limitRange in another namespace 01/18/23 22:54:18.598
    Jan 18 22:54:18.608: INFO: Namespace "e2e-limitrange-cgrht-5333" created
    Jan 18 22:54:18.608: INFO: Creating LimitRange "e2e-limitrange-cgrht" in namespace "e2e-limitrange-cgrht-5333"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-cgrht" 01/18/23 22:54:18.612
    Jan 18 22:54:18.614: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-cgrht" in "limitrange-8947" namespace 01/18/23 22:54:18.614
    Jan 18 22:54:18.621: INFO: LimitRange "e2e-limitrange-cgrht" has been patched
    STEP: Delete LimitRange "e2e-limitrange-cgrht" by Collection with labelSelector: "e2e-limitrange-cgrht=patched" 01/18/23 22:54:18.621
    STEP: Confirm that the limitRange "e2e-limitrange-cgrht" has been deleted 01/18/23 22:54:18.628
    Jan 18 22:54:18.628: INFO: Requesting list of LimitRange to confirm quantity
    Jan 18 22:54:18.631: INFO: Found 0 LimitRange with label "e2e-limitrange-cgrht=patched"
    Jan 18 22:54:18.631: INFO: LimitRange "e2e-limitrange-cgrht" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-cgrht" 01/18/23 22:54:18.631
    Jan 18 22:54:18.633: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:18.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8947" for this suite. 01/18/23 22:54:18.636
    STEP: Destroying namespace "e2e-limitrange-cgrht-5333" for this suite. 01/18/23 22:54:18.643
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:18.648
Jan 18 22:54:18.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:54:18.649
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.664
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/18/23 22:54:18.67
STEP: waiting for available Endpoint 01/18/23 22:54:18.674
STEP: listing all Endpoints 01/18/23 22:54:18.675
STEP: updating the Endpoint 01/18/23 22:54:18.678
STEP: fetching the Endpoint 01/18/23 22:54:18.684
STEP: patching the Endpoint 01/18/23 22:54:18.686
STEP: fetching the Endpoint 01/18/23 22:54:18.695
STEP: deleting the Endpoint by Collection 01/18/23 22:54:18.697
STEP: waiting for Endpoint deletion 01/18/23 22:54:18.703
STEP: fetching the Endpoint 01/18/23 22:54:18.704
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:18.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6153" for this suite. 01/18/23 22:54:18.71
------------------------------
â€¢ [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:18.648
    Jan 18 22:54:18.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:54:18.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.664
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/18/23 22:54:18.67
    STEP: waiting for available Endpoint 01/18/23 22:54:18.674
    STEP: listing all Endpoints 01/18/23 22:54:18.675
    STEP: updating the Endpoint 01/18/23 22:54:18.678
    STEP: fetching the Endpoint 01/18/23 22:54:18.684
    STEP: patching the Endpoint 01/18/23 22:54:18.686
    STEP: fetching the Endpoint 01/18/23 22:54:18.695
    STEP: deleting the Endpoint by Collection 01/18/23 22:54:18.697
    STEP: waiting for Endpoint deletion 01/18/23 22:54:18.703
    STEP: fetching the Endpoint 01/18/23 22:54:18.704
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:18.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6153" for this suite. 01/18/23 22:54:18.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:18.729
Jan 18 22:54:18.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 22:54:18.73
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.746
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-3656/secret-test-0b9bf2ae-c5bc-4693-9dd0-8850b6a61b75 01/18/23 22:54:18.749
STEP: Creating a pod to test consume secrets 01/18/23 22:54:18.753
Jan 18 22:54:18.762: INFO: Waiting up to 5m0s for pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43" in namespace "secrets-3656" to be "Succeeded or Failed"
Jan 18 22:54:18.765: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726521ms
Jan 18 22:54:20.769: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006549746s
Jan 18 22:54:22.769: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007295498s
STEP: Saw pod success 01/18/23 22:54:22.77
Jan 18 22:54:22.770: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43" satisfied condition "Succeeded or Failed"
Jan 18 22:54:22.773: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 container env-test: <nil>
STEP: delete the pod 01/18/23 22:54:22.778
Jan 18 22:54:22.791: INFO: Waiting for pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 to disappear
Jan 18 22:54:22.794: INFO: Pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:22.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3656" for this suite. 01/18/23 22:54:22.797
------------------------------
â€¢ [4.076 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:18.729
    Jan 18 22:54:18.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 22:54:18.73
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.746
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-3656/secret-test-0b9bf2ae-c5bc-4693-9dd0-8850b6a61b75 01/18/23 22:54:18.749
    STEP: Creating a pod to test consume secrets 01/18/23 22:54:18.753
    Jan 18 22:54:18.762: INFO: Waiting up to 5m0s for pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43" in namespace "secrets-3656" to be "Succeeded or Failed"
    Jan 18 22:54:18.765: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726521ms
    Jan 18 22:54:20.769: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006549746s
    Jan 18 22:54:22.769: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007295498s
    STEP: Saw pod success 01/18/23 22:54:22.77
    Jan 18 22:54:22.770: INFO: Pod "pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43" satisfied condition "Succeeded or Failed"
    Jan 18 22:54:22.773: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:54:22.778
    Jan 18 22:54:22.791: INFO: Waiting for pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 to disappear
    Jan 18 22:54:22.794: INFO: Pod pod-configmaps-31bdc534-d105-41ee-9fda-636e1b14fd43 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:22.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3656" for this suite. 01/18/23 22:54:22.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:22.808
Jan 18 22:54:22.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:54:22.809
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:22.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:22.822
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/18/23 22:54:22.826
Jan 18 22:54:22.835: INFO: Waiting up to 5m0s for pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a" in namespace "downward-api-6751" to be "Succeeded or Failed"
Jan 18 22:54:22.838: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932151ms
Jan 18 22:54:24.842: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006808587s
Jan 18 22:54:26.842: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007136437s
STEP: Saw pod success 01/18/23 22:54:26.843
Jan 18 22:54:26.843: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a" satisfied condition "Succeeded or Failed"
Jan 18 22:54:26.845: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:54:26.85
Jan 18 22:54:26.859: INFO: Waiting for pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a to disappear
Jan 18 22:54:26.861: INFO: Pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:26.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6751" for this suite. 01/18/23 22:54:26.863
------------------------------
â€¢ [4.060 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:22.808
    Jan 18 22:54:22.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:54:22.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:22.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:22.822
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/18/23 22:54:22.826
    Jan 18 22:54:22.835: INFO: Waiting up to 5m0s for pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a" in namespace "downward-api-6751" to be "Succeeded or Failed"
    Jan 18 22:54:22.838: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932151ms
    Jan 18 22:54:24.842: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006808587s
    Jan 18 22:54:26.842: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007136437s
    STEP: Saw pod success 01/18/23 22:54:26.843
    Jan 18 22:54:26.843: INFO: Pod "downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a" satisfied condition "Succeeded or Failed"
    Jan 18 22:54:26.845: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:54:26.85
    Jan 18 22:54:26.859: INFO: Waiting for pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a to disappear
    Jan 18 22:54:26.861: INFO: Pod downward-api-a02d4a69-fc33-4d1e-b5f3-495313ae0b2a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:26.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6751" for this suite. 01/18/23 22:54:26.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:26.868
Jan 18 22:54:26.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-webhook 01/18/23 22:54:26.869
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:26.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:26.882
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 22:54:26.885
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 22:54:27.557
STEP: Deploying the custom resource conversion webhook pod 01/18/23 22:54:27.564
STEP: Wait for the deployment to be ready 01/18/23 22:54:27.575
Jan 18 22:54:27.582: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:54:29.594
STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:29.601
Jan 18 22:54:30.601: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 18 22:54:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Creating a v1 custom resource 01/18/23 22:54:33.19
STEP: v2 custom resource should be converted 01/18/23 22:54:33.195
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4440" for this suite. 01/18/23 22:54:33.739
------------------------------
â€¢ [SLOW TEST] [6.879 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:26.868
    Jan 18 22:54:26.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 22:54:26.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:26.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:26.882
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 22:54:26.885
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 22:54:27.557
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 22:54:27.564
    STEP: Wait for the deployment to be ready 01/18/23 22:54:27.575
    Jan 18 22:54:27.582: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:54:29.594
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:29.601
    Jan 18 22:54:30.601: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 18 22:54:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Creating a v1 custom resource 01/18/23 22:54:33.19
    STEP: v2 custom resource should be converted 01/18/23 22:54:33.195
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4440" for this suite. 01/18/23 22:54:33.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:33.748
Jan 18 22:54:33.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 22:54:33.749
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:33.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:33.769
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 22:54:33.773
Jan 18 22:54:33.781: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:54:38.785: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:54:38.785
STEP: getting scale subresource 01/18/23 22:54:38.785
STEP: updating a scale subresource 01/18/23 22:54:38.787
STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 22:54:38.794
STEP: Patch a scale subresource 01/18/23 22:54:38.795
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:38.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4481" for this suite. 01/18/23 22:54:38.81
------------------------------
â€¢ [SLOW TEST] [5.067 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:33.748
    Jan 18 22:54:33.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:54:33.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:33.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:33.769
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 22:54:33.773
    Jan 18 22:54:33.781: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 22:54:38.785: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:54:38.785
    STEP: getting scale subresource 01/18/23 22:54:38.785
    STEP: updating a scale subresource 01/18/23 22:54:38.787
    STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 22:54:38.794
    STEP: Patch a scale subresource 01/18/23 22:54:38.795
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:38.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4481" for this suite. 01/18/23 22:54:38.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:38.817
Jan 18 22:54:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:54:38.817
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:38.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:38.831
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/18/23 22:54:38.833
STEP: Creating a ResourceQuota 01/18/23 22:54:43.836
STEP: Ensuring resource quota status is calculated 01/18/23 22:54:43.84
STEP: Creating a ReplicationController 01/18/23 22:54:45.843
STEP: Ensuring resource quota status captures replication controller creation 01/18/23 22:54:45.855
STEP: Deleting a ReplicationController 01/18/23 22:54:47.859
STEP: Ensuring resource quota status released usage 01/18/23 22:54:47.863
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:54:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9820" for this suite. 01/18/23 22:54:49.871
------------------------------
â€¢ [SLOW TEST] [11.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:38.817
    Jan 18 22:54:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:54:38.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:38.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:38.831
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/18/23 22:54:38.833
    STEP: Creating a ResourceQuota 01/18/23 22:54:43.836
    STEP: Ensuring resource quota status is calculated 01/18/23 22:54:43.84
    STEP: Creating a ReplicationController 01/18/23 22:54:45.843
    STEP: Ensuring resource quota status captures replication controller creation 01/18/23 22:54:45.855
    STEP: Deleting a ReplicationController 01/18/23 22:54:47.859
    STEP: Ensuring resource quota status released usage 01/18/23 22:54:47.863
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:54:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9820" for this suite. 01/18/23 22:54:49.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:54:49.877
Jan 18 22:54:49.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:54:49.877
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:49.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:49.892
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8420 01/18/23 22:54:49.895
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/18/23 22:54:49.899
STEP: Creating pod with conflicting port in namespace statefulset-8420 01/18/23 22:54:49.903
STEP: Waiting until pod test-pod will start running in namespace statefulset-8420 01/18/23 22:54:49.91
Jan 18 22:54:49.911: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8420" to be "running"
Jan 18 22:54:49.913: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8289ms
Jan 18 22:54:51.917: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006092716s
Jan 18 22:54:51.917: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8420 01/18/23 22:54:51.917
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8420 01/18/23 22:54:51.922
Jan 18 22:54:51.937: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 22:54:51.952: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 22:54:51.961: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 22:54:51.964: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8420
STEP: Removing pod with conflicting port in namespace statefulset-8420 01/18/23 22:54:51.964
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8420 and will be in running state 01/18/23 22:54:51.976
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:54:53.983: INFO: Deleting all statefulset in ns statefulset-8420
Jan 18 22:54:53.985: INFO: Scaling statefulset ss to 0
Jan 18 22:55:03.998: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:55:04.001: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:55:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8420" for this suite. 01/18/23 22:55:04.014
------------------------------
â€¢ [SLOW TEST] [14.142 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:54:49.877
    Jan 18 22:54:49.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:54:49.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:49.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:49.892
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8420 01/18/23 22:54:49.895
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/18/23 22:54:49.899
    STEP: Creating pod with conflicting port in namespace statefulset-8420 01/18/23 22:54:49.903
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8420 01/18/23 22:54:49.91
    Jan 18 22:54:49.911: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8420" to be "running"
    Jan 18 22:54:49.913: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8289ms
    Jan 18 22:54:51.917: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006092716s
    Jan 18 22:54:51.917: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8420 01/18/23 22:54:51.917
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8420 01/18/23 22:54:51.922
    Jan 18 22:54:51.937: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 18 22:54:51.952: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 22:54:51.961: INFO: Observed stateful pod in namespace: statefulset-8420, name: ss-0, uid: 0ac369a0-413e-440c-8ac7-02a547aff374, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 22:54:51.964: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8420
    STEP: Removing pod with conflicting port in namespace statefulset-8420 01/18/23 22:54:51.964
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8420 and will be in running state 01/18/23 22:54:51.976
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:54:53.983: INFO: Deleting all statefulset in ns statefulset-8420
    Jan 18 22:54:53.985: INFO: Scaling statefulset ss to 0
    Jan 18 22:55:03.998: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:55:04.001: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:55:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8420" for this suite. 01/18/23 22:55:04.014
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:55:04.019
Jan 18 22:55:04.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:55:04.02
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:04.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:04.035
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-f95c722e-574c-4cb0-868c-f5e03ad4af3e 01/18/23 22:55:04.038
STEP: Creating a pod to test consume secrets 01/18/23 22:55:04.042
Jan 18 22:55:04.051: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f" in namespace "projected-7100" to be "Succeeded or Failed"
Jan 18 22:55:04.053: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707065ms
Jan 18 22:55:06.056: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005802089s
Jan 18 22:55:08.057: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006557904s
STEP: Saw pod success 01/18/23 22:55:08.057
Jan 18 22:55:08.057: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f" satisfied condition "Succeeded or Failed"
Jan 18 22:55:08.060: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:55:08.064
Jan 18 22:55:08.073: INFO: Waiting for pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f to disappear
Jan 18 22:55:08.077: INFO: Pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:55:08.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7100" for this suite. 01/18/23 22:55:08.08
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:55:04.019
    Jan 18 22:55:04.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:55:04.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:04.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:04.035
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-f95c722e-574c-4cb0-868c-f5e03ad4af3e 01/18/23 22:55:04.038
    STEP: Creating a pod to test consume secrets 01/18/23 22:55:04.042
    Jan 18 22:55:04.051: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f" in namespace "projected-7100" to be "Succeeded or Failed"
    Jan 18 22:55:04.053: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707065ms
    Jan 18 22:55:06.056: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005802089s
    Jan 18 22:55:08.057: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006557904s
    STEP: Saw pod success 01/18/23 22:55:08.057
    Jan 18 22:55:08.057: INFO: Pod "pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f" satisfied condition "Succeeded or Failed"
    Jan 18 22:55:08.060: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:55:08.064
    Jan 18 22:55:08.073: INFO: Waiting for pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f to disappear
    Jan 18 22:55:08.077: INFO: Pod pod-projected-secrets-d6447b9e-87c3-4a64-90d4-75414e25bd8f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:55:08.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7100" for this suite. 01/18/23 22:55:08.08
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:55:08.085
Jan 18 22:55:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:55:08.086
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:08.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:08.099
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/18/23 22:55:08.102
STEP: Creating a ResourceQuota 01/18/23 22:55:13.105
STEP: Ensuring resource quota status is calculated 01/18/23 22:55:13.109
STEP: Creating a Pod that fits quota 01/18/23 22:55:15.113
STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 22:55:15.126
STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 22:55:17.13
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 22:55:17.133
STEP: Ensuring a pod cannot update its resource requirements 01/18/23 22:55:17.135
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 22:55:17.14
STEP: Deleting the pod 01/18/23 22:55:19.144
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:55:19.154
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:55:21.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9854" for this suite. 01/18/23 22:55:21.161
------------------------------
â€¢ [SLOW TEST] [13.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:55:08.085
    Jan 18 22:55:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:55:08.086
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:08.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:08.099
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/18/23 22:55:08.102
    STEP: Creating a ResourceQuota 01/18/23 22:55:13.105
    STEP: Ensuring resource quota status is calculated 01/18/23 22:55:13.109
    STEP: Creating a Pod that fits quota 01/18/23 22:55:15.113
    STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 22:55:15.126
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 22:55:17.13
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 22:55:17.133
    STEP: Ensuring a pod cannot update its resource requirements 01/18/23 22:55:17.135
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 22:55:17.14
    STEP: Deleting the pod 01/18/23 22:55:19.144
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:55:19.154
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:55:21.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9854" for this suite. 01/18/23 22:55:21.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:55:21.167
Jan 18 22:55:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 22:55:21.168
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:21.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:21.181
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-321295c2-72df-4683-b511-daad06f7f327 in namespace container-probe-3049 01/18/23 22:55:21.184
Jan 18 22:55:21.193: INFO: Waiting up to 5m0s for pod "busybox-321295c2-72df-4683-b511-daad06f7f327" in namespace "container-probe-3049" to be "not pending"
Jan 18 22:55:21.196: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437303ms
Jan 18 22:55:23.200: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327": Phase="Running", Reason="", readiness=true. Elapsed: 2.006362977s
Jan 18 22:55:23.200: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327" satisfied condition "not pending"
Jan 18 22:55:23.200: INFO: Started pod busybox-321295c2-72df-4683-b511-daad06f7f327 in namespace container-probe-3049
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:55:23.2
Jan 18 22:55:23.202: INFO: Initial restart count of pod busybox-321295c2-72df-4683-b511-daad06f7f327 is 0
Jan 18 22:56:13.304: INFO: Restart count of pod container-probe-3049/busybox-321295c2-72df-4683-b511-daad06f7f327 is now 1 (50.101866774s elapsed)
STEP: deleting the pod 01/18/23 22:56:13.304
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:13.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3049" for this suite. 01/18/23 22:56:13.321
------------------------------
â€¢ [SLOW TEST] [52.161 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:55:21.167
    Jan 18 22:55:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:55:21.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:55:21.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:55:21.181
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-321295c2-72df-4683-b511-daad06f7f327 in namespace container-probe-3049 01/18/23 22:55:21.184
    Jan 18 22:55:21.193: INFO: Waiting up to 5m0s for pod "busybox-321295c2-72df-4683-b511-daad06f7f327" in namespace "container-probe-3049" to be "not pending"
    Jan 18 22:55:21.196: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437303ms
    Jan 18 22:55:23.200: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327": Phase="Running", Reason="", readiness=true. Elapsed: 2.006362977s
    Jan 18 22:55:23.200: INFO: Pod "busybox-321295c2-72df-4683-b511-daad06f7f327" satisfied condition "not pending"
    Jan 18 22:55:23.200: INFO: Started pod busybox-321295c2-72df-4683-b511-daad06f7f327 in namespace container-probe-3049
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:55:23.2
    Jan 18 22:55:23.202: INFO: Initial restart count of pod busybox-321295c2-72df-4683-b511-daad06f7f327 is 0
    Jan 18 22:56:13.304: INFO: Restart count of pod container-probe-3049/busybox-321295c2-72df-4683-b511-daad06f7f327 is now 1 (50.101866774s elapsed)
    STEP: deleting the pod 01/18/23 22:56:13.304
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:13.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3049" for this suite. 01/18/23 22:56:13.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:13.329
Jan 18 22:56:13.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:56:13.33
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:13.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:13.344
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 18 22:56:13.355: INFO: Endpoints addresses: [10.128.15.198] , ports: [6443]
Jan 18 22:56:13.355: INFO: EndpointSlices addresses: [10.128.15.198] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:13.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3037" for this suite. 01/18/23 22:56:13.358
------------------------------
â€¢ [0.034 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:13.329
    Jan 18 22:56:13.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:56:13.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:13.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:13.344
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 18 22:56:13.355: INFO: Endpoints addresses: [10.128.15.198] , ports: [6443]
    Jan 18 22:56:13.355: INFO: EndpointSlices addresses: [10.128.15.198] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:13.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3037" for this suite. 01/18/23 22:56:13.358
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:13.363
Jan 18 22:56:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename watch 01/18/23 22:56:13.364
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:13.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:13.379
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/18/23 22:56:13.382
STEP: starting a background goroutine to produce watch events 01/18/23 22:56:13.385
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 22:56:13.385
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:16.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9440" for this suite. 01/18/23 22:56:16.219
------------------------------
â€¢ [2.908 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:13.363
    Jan 18 22:56:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename watch 01/18/23 22:56:13.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:13.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:13.379
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/18/23 22:56:13.382
    STEP: starting a background goroutine to produce watch events 01/18/23 22:56:13.385
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 22:56:13.385
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:16.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9440" for this suite. 01/18/23 22:56:16.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:16.272
Jan 18 22:56:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:56:16.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:16.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:16.287
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-9487 01/18/23 22:56:16.29
STEP: creating service affinity-clusterip in namespace services-9487 01/18/23 22:56:16.29
STEP: creating replication controller affinity-clusterip in namespace services-9487 01/18/23 22:56:16.298
I0118 22:56:16.305858      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9487, replica count: 3
I0118 22:56:19.358353      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:56:19.363: INFO: Creating new exec pod
Jan 18 22:56:19.368: INFO: Waiting up to 5m0s for pod "execpod-affinityzht4v" in namespace "services-9487" to be "running"
Jan 18 22:56:19.371: INFO: Pod "execpod-affinityzht4v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673051ms
Jan 18 22:56:21.373: INFO: Pod "execpod-affinityzht4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.005211575s
Jan 18 22:56:21.373: INFO: Pod "execpod-affinityzht4v" satisfied condition "running"
Jan 18 22:56:22.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 18 22:56:22.515: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 18 22:56:22.515: INFO: stdout: ""
Jan 18 22:56:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c nc -v -z -w 2 10.96.2.155 80'
Jan 18 22:56:22.656: INFO: stderr: "+ nc -v -z -w 2 10.96.2.155 80\nConnection to 10.96.2.155 80 port [tcp/http] succeeded!\n"
Jan 18 22:56:22.656: INFO: stdout: ""
Jan 18 22:56:22.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.2.155:80/ ; done'
Jan 18 22:56:22.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n"
Jan 18 22:56:22.869: INFO: stdout: "\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc"
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
Jan 18 22:56:22.869: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9487, will wait for the garbage collector to delete the pods 01/18/23 22:56:22.881
Jan 18 22:56:22.943: INFO: Deleting ReplicationController affinity-clusterip took: 8.93342ms
Jan 18 22:56:23.044: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.029668ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:24.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9487" for this suite. 01/18/23 22:56:24.564
------------------------------
â€¢ [SLOW TEST] [8.299 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:16.272
    Jan 18 22:56:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:56:16.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:16.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:16.287
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-9487 01/18/23 22:56:16.29
    STEP: creating service affinity-clusterip in namespace services-9487 01/18/23 22:56:16.29
    STEP: creating replication controller affinity-clusterip in namespace services-9487 01/18/23 22:56:16.298
    I0118 22:56:16.305858      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9487, replica count: 3
    I0118 22:56:19.358353      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:56:19.363: INFO: Creating new exec pod
    Jan 18 22:56:19.368: INFO: Waiting up to 5m0s for pod "execpod-affinityzht4v" in namespace "services-9487" to be "running"
    Jan 18 22:56:19.371: INFO: Pod "execpod-affinityzht4v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673051ms
    Jan 18 22:56:21.373: INFO: Pod "execpod-affinityzht4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.005211575s
    Jan 18 22:56:21.373: INFO: Pod "execpod-affinityzht4v" satisfied condition "running"
    Jan 18 22:56:22.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 18 22:56:22.515: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 18 22:56:22.515: INFO: stdout: ""
    Jan 18 22:56:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c nc -v -z -w 2 10.96.2.155 80'
    Jan 18 22:56:22.656: INFO: stderr: "+ nc -v -z -w 2 10.96.2.155 80\nConnection to 10.96.2.155 80 port [tcp/http] succeeded!\n"
    Jan 18 22:56:22.656: INFO: stdout: ""
    Jan 18 22:56:22.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-9487 exec execpod-affinityzht4v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.2.155:80/ ; done'
    Jan 18 22:56:22.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.155:80/\n"
    Jan 18 22:56:22.869: INFO: stdout: "\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc\naffinity-clusterip-9wvkc"
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Received response from host: affinity-clusterip-9wvkc
    Jan 18 22:56:22.869: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-9487, will wait for the garbage collector to delete the pods 01/18/23 22:56:22.881
    Jan 18 22:56:22.943: INFO: Deleting ReplicationController affinity-clusterip took: 8.93342ms
    Jan 18 22:56:23.044: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.029668ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:24.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9487" for this suite. 01/18/23 22:56:24.564
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:24.571
Jan 18 22:56:24.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 22:56:24.572
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:24.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:24.586
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/18/23 22:56:24.589
STEP: Ensure pods equal to parallelism count is attached to the job 01/18/23 22:56:24.595
STEP: patching /status 01/18/23 22:56:26.599
STEP: updating /status 01/18/23 22:56:26.606
STEP: get /status 01/18/23 22:56:26.632
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:26.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5308" for this suite. 01/18/23 22:56:26.637
------------------------------
â€¢ [2.070 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:24.571
    Jan 18 22:56:24.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 22:56:24.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:24.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:24.586
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/18/23 22:56:24.589
    STEP: Ensure pods equal to parallelism count is attached to the job 01/18/23 22:56:24.595
    STEP: patching /status 01/18/23 22:56:26.599
    STEP: updating /status 01/18/23 22:56:26.606
    STEP: get /status 01/18/23 22:56:26.632
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:26.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5308" for this suite. 01/18/23 22:56:26.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:26.643
Jan 18 22:56:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:56:26.644
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:26.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:26.656
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 18 22:56:26.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:56:28.084
Jan 18 22:56:28.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 create -f -'
Jan 18 22:56:28.718: INFO: stderr: ""
Jan 18 22:56:28.718: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 22:56:28.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 delete e2e-test-crd-publish-openapi-824-crds test-cr'
Jan 18 22:56:28.794: INFO: stderr: ""
Jan 18 22:56:28.794: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 22:56:28.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 apply -f -'
Jan 18 22:56:28.995: INFO: stderr: ""
Jan 18 22:56:28.995: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 22:56:28.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 delete e2e-test-crd-publish-openapi-824-crds test-cr'
Jan 18 22:56:29.069: INFO: stderr: ""
Jan 18 22:56:29.069: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 22:56:29.069
Jan 18 22:56:29.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 explain e2e-test-crd-publish-openapi-824-crds'
Jan 18 22:56:29.245: INFO: stderr: ""
Jan 18 22:56:29.245: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-824-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9167" for this suite. 01/18/23 22:56:30.667
------------------------------
â€¢ [4.029 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:26.643
    Jan 18 22:56:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:56:26.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:26.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:26.656
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 18 22:56:26.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:56:28.084
    Jan 18 22:56:28.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 create -f -'
    Jan 18 22:56:28.718: INFO: stderr: ""
    Jan 18 22:56:28.718: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 22:56:28.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 delete e2e-test-crd-publish-openapi-824-crds test-cr'
    Jan 18 22:56:28.794: INFO: stderr: ""
    Jan 18 22:56:28.794: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 18 22:56:28.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 apply -f -'
    Jan 18 22:56:28.995: INFO: stderr: ""
    Jan 18 22:56:28.995: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 22:56:28.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 --namespace=crd-publish-openapi-9167 delete e2e-test-crd-publish-openapi-824-crds test-cr'
    Jan 18 22:56:29.069: INFO: stderr: ""
    Jan 18 22:56:29.069: INFO: stdout: "e2e-test-crd-publish-openapi-824-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 22:56:29.069
    Jan 18 22:56:29.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-9167 explain e2e-test-crd-publish-openapi-824-crds'
    Jan 18 22:56:29.245: INFO: stderr: ""
    Jan 18 22:56:29.245: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-824-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9167" for this suite. 01/18/23 22:56:30.667
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:30.672
Jan 18 22:56:30.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:56:30.673
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:30.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:30.686
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:56:30.689
Jan 18 22:56:30.696: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2061" to be "running and ready"
Jan 18 22:56:30.699: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597945ms
Jan 18 22:56:30.699: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:56:32.703: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825244s
Jan 18 22:56:32.703: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 18 22:56:32.703: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/18/23 22:56:32.706
STEP: Then the orphan pod is adopted 01/18/23 22:56:32.712
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2061" for this suite. 01/18/23 22:56:33.721
------------------------------
â€¢ [3.056 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:30.672
    Jan 18 22:56:30.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:56:30.673
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:30.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:30.686
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:56:30.689
    Jan 18 22:56:30.696: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2061" to be "running and ready"
    Jan 18 22:56:30.699: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597945ms
    Jan 18 22:56:30.699: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:56:32.703: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825244s
    Jan 18 22:56:32.703: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 18 22:56:32.703: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/18/23 22:56:32.706
    STEP: Then the orphan pod is adopted 01/18/23 22:56:32.712
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2061" for this suite. 01/18/23 22:56:33.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:33.728
Jan 18 22:56:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename proxy 01/18/23 22:56:33.729
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:33.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:33.741
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/18/23 22:56:33.754
STEP: creating replication controller proxy-service-5rv5q in namespace proxy-2016 01/18/23 22:56:33.754
I0118 22:56:33.761816      23 runners.go:193] Created replication controller with name: proxy-service-5rv5q, namespace: proxy-2016, replica count: 1
I0118 22:56:34.812221      23 runners.go:193] proxy-service-5rv5q Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0118 22:56:35.812583      23 runners.go:193] proxy-service-5rv5q Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:56:35.817: INFO: setup took 2.072467513s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 22:56:35.817
Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 9.302091ms)
Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 9.320791ms)
Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 9.442569ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.411671ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.481891ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 9.64043ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 9.557601ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.495196ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 9.548214ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 9.528808ms)
Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.783905ms)
Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 16.947498ms)
Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 17.024116ms)
Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 17.065178ms)
Jan 18 22:56:35.836: INFO: (0) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 18.773389ms)
Jan 18 22:56:35.836: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 18.757844ms)
Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.41728ms)
Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.432239ms)
Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.432778ms)
Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.582694ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.65187ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.945442ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.891415ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.015524ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.050862ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.167787ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.1075ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.184272ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.51672ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.502627ms)
Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.621687ms)
Jan 18 22:56:35.843: INFO: (1) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 7.374084ms)
Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 3.739019ms)
Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.980027ms)
Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.940269ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.006083ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.07416ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.025735ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.203249ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.298883ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.264056ms)
Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.469885ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.294435ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.426612ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.472494ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.422034ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.421401ms)
Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.530808ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.037495ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.190609ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.181035ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.469092ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.325494ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.287741ms)
Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.376652ms)
Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.999654ms)
Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.861927ms)
Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.934277ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.483379ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.448119ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.476423ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.428425ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.604586ms)
Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.839665ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.864758ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 3.911776ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.163796ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.340906ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.38768ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.332406ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.338204ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.353042ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.552236ms)
Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.442275ms)
Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.272901ms)
Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.209459ms)
Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.23841ms)
Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.349049ms)
Jan 18 22:56:35.861: INFO: (4) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.488739ms)
Jan 18 22:56:35.861: INFO: (4) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.603957ms)
Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.411856ms)
Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.733124ms)
Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.668599ms)
Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.725992ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.009535ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.017449ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.128325ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.208102ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.481466ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.428199ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.485076ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.462961ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.459706ms)
Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.747084ms)
Jan 18 22:56:35.867: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.938729ms)
Jan 18 22:56:35.867: INFO: (5) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.075133ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.884532ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.009951ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.070711ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.157781ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.125603ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.13865ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.29753ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.267431ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.258797ms)
Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.441601ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.010386ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 4.982831ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.078328ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.313727ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.418086ms)
Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.394606ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 3.420655ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.433517ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.519976ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.724676ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 3.696286ms)
Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.659618ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 4.851649ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.863417ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.822652ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.852435ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.016488ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 4.958351ms)
Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.903992ms)
Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.596848ms)
Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.668575ms)
Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.687803ms)
Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.474614ms)
Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 3.571287ms)
Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 3.792525ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.545867ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.524534ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.476348ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.756527ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.921651ms)
Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.951383ms)
Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.573987ms)
Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.69618ms)
Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.096613ms)
Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 6.33231ms)
Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 6.246472ms)
Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.301385ms)
Jan 18 22:56:35.886: INFO: (8) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 7.939313ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.177239ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.217055ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.188049ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.366312ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.257202ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.413917ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.492674ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.412874ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.256678ms)
Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.504511ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.443635ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.358248ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.47494ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.613579ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.520195ms)
Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.661815ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.434506ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.750214ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.759903ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.947838ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.775268ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.75581ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.657816ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.789073ms)
Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.859514ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.884221ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 6.146584ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.149265ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 6.220107ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 6.328871ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 6.700883ms)
Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.778608ms)
Jan 18 22:56:35.911: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 11.173553ms)
Jan 18 22:56:35.913: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 13.233957ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 20.086964ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 20.106666ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 20.075388ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 19.976477ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 20.193009ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 20.060453ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 20.156678ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 20.097317ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 20.106095ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 20.133877ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 20.278958ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 20.26802ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 20.09365ms)
Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 20.114717ms)
Jan 18 22:56:35.933: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 13.452902ms)
Jan 18 22:56:35.933: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 13.5541ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 13.864738ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 14.007338ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 13.975158ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 14.096725ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 14.197879ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 14.14507ms)
Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 14.575338ms)
Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 14.80071ms)
Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 14.687105ms)
Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 15.22829ms)
Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 15.268595ms)
Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 15.371799ms)
Jan 18 22:56:35.937: INFO: (12) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 16.800206ms)
Jan 18 22:56:35.937: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 16.847457ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.942592ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 6.006478ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 6.170234ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 6.337971ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 6.315813ms)
Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.468813ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.628367ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 6.724621ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 6.678835ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.837052ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 6.660972ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 7.123955ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 7.186276ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 7.241375ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 7.161177ms)
Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 7.28776ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.379188ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.365944ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.052571ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.951442ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.972008ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.942309ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.059609ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.95461ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.051386ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.971816ms)
Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.015982ms)
Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.298745ms)
Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.514775ms)
Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.554231ms)
Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.85449ms)
Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 6.069855ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 8.79496ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 8.913085ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 8.900913ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.270462ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 9.20176ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.060206ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 9.529501ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 9.419399ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 9.661569ms)
Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 9.468271ms)
Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 10.341199ms)
Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 10.29403ms)
Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 10.367014ms)
Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 10.419113ms)
Jan 18 22:56:35.962: INFO: (15) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 11.49627ms)
Jan 18 22:56:35.962: INFO: (15) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 11.549737ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.468662ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.237823ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.816775ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.98884ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.196326ms)
Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.843946ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.099521ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.284028ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.437015ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.234652ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.744529ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.798486ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.525322ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.589942ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.56953ms)
Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.857004ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.44749ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.487935ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.577409ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.524803ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.679254ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.515892ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.747373ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.807329ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.841018ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.803207ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.876819ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.022464ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.882557ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 6.020779ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 6.052913ms)
Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.930884ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.549473ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.614739ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.644968ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.822317ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.688156ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.637837ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.727917ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.817623ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.726784ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.860457ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.703557ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.788722ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.755905ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.688855ms)
Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.843097ms)
Jan 18 22:56:35.983: INFO: (18) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 8.599808ms)
Jan 18 22:56:35.987: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.632099ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.620126ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.868004ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.786748ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.747446ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.84625ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.897067ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.858525ms)
Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.971933ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.265195ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.47476ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.609088ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.629731ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.678055ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.709241ms)
Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.807566ms)
STEP: deleting ReplicationController proxy-service-5rv5q in namespace proxy-2016, will wait for the garbage collector to delete the pods 01/18/23 22:56:35.989
Jan 18 22:56:36.048: INFO: Deleting ReplicationController proxy-service-5rv5q took: 5.614288ms
Jan 18 22:56:36.148: INFO: Terminating ReplicationController proxy-service-5rv5q pods took: 100.482558ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:38.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2016" for this suite. 01/18/23 22:56:38.553
------------------------------
â€¢ [4.829 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:33.728
    Jan 18 22:56:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename proxy 01/18/23 22:56:33.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:33.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:33.741
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/18/23 22:56:33.754
    STEP: creating replication controller proxy-service-5rv5q in namespace proxy-2016 01/18/23 22:56:33.754
    I0118 22:56:33.761816      23 runners.go:193] Created replication controller with name: proxy-service-5rv5q, namespace: proxy-2016, replica count: 1
    I0118 22:56:34.812221      23 runners.go:193] proxy-service-5rv5q Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0118 22:56:35.812583      23 runners.go:193] proxy-service-5rv5q Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:56:35.817: INFO: setup took 2.072467513s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 22:56:35.817
    Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 9.302091ms)
    Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 9.320791ms)
    Jan 18 22:56:35.826: INFO: (0) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 9.442569ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.411671ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.481891ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 9.64043ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 9.557601ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.495196ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 9.548214ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 9.528808ms)
    Jan 18 22:56:35.827: INFO: (0) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.783905ms)
    Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 16.947498ms)
    Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 17.024116ms)
    Jan 18 22:56:35.834: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 17.065178ms)
    Jan 18 22:56:35.836: INFO: (0) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 18.773389ms)
    Jan 18 22:56:35.836: INFO: (0) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 18.757844ms)
    Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.41728ms)
    Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.432239ms)
    Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.432778ms)
    Jan 18 22:56:35.840: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.582694ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.65187ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.945442ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.891415ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.015524ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.050862ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.167787ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.1075ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.184272ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.51672ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.502627ms)
    Jan 18 22:56:35.841: INFO: (1) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.621687ms)
    Jan 18 22:56:35.843: INFO: (1) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 7.374084ms)
    Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 3.739019ms)
    Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.980027ms)
    Jan 18 22:56:35.847: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.940269ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.006083ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.07416ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.025735ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.203249ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.298883ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.264056ms)
    Jan 18 22:56:35.848: INFO: (2) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.469885ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.294435ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.426612ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.472494ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.422034ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.421401ms)
    Jan 18 22:56:35.849: INFO: (2) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.530808ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.037495ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.190609ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.181035ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.469092ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.325494ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.287741ms)
    Jan 18 22:56:35.853: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.376652ms)
    Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.999654ms)
    Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.861927ms)
    Jan 18 22:56:35.854: INFO: (3) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.934277ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.483379ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.448119ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.476423ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.428425ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.604586ms)
    Jan 18 22:56:35.855: INFO: (3) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.839665ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.864758ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 3.911776ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.163796ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.340906ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.38768ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.332406ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.338204ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.353042ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.552236ms)
    Jan 18 22:56:35.859: INFO: (4) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.442275ms)
    Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.272901ms)
    Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.209459ms)
    Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.23841ms)
    Jan 18 22:56:35.860: INFO: (4) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.349049ms)
    Jan 18 22:56:35.861: INFO: (4) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.488739ms)
    Jan 18 22:56:35.861: INFO: (4) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.603957ms)
    Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.411856ms)
    Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.733124ms)
    Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.668599ms)
    Jan 18 22:56:35.865: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.725992ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.009535ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.017449ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.128325ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.208102ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.481466ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.428199ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.485076ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.462961ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.459706ms)
    Jan 18 22:56:35.866: INFO: (5) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.747084ms)
    Jan 18 22:56:35.867: INFO: (5) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.938729ms)
    Jan 18 22:56:35.867: INFO: (5) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.075133ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.884532ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.009951ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.070711ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.157781ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.125603ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.13865ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.29753ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.267431ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.258797ms)
    Jan 18 22:56:35.871: INFO: (6) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.441601ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.010386ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 4.982831ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.078328ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.313727ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.418086ms)
    Jan 18 22:56:35.872: INFO: (6) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.394606ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 3.420655ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.433517ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.519976ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 3.724676ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 3.696286ms)
    Jan 18 22:56:35.876: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.659618ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 4.851649ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.863417ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.822652ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.852435ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.016488ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 4.958351ms)
    Jan 18 22:56:35.877: INFO: (7) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.903992ms)
    Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.596848ms)
    Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.668575ms)
    Jan 18 22:56:35.878: INFO: (7) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.687803ms)
    Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.474614ms)
    Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 3.571287ms)
    Jan 18 22:56:35.882: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 3.792525ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.545867ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.524534ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.476348ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.756527ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.921651ms)
    Jan 18 22:56:35.883: INFO: (8) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.951383ms)
    Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.573987ms)
    Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.69618ms)
    Jan 18 22:56:35.884: INFO: (8) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.096613ms)
    Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 6.33231ms)
    Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 6.246472ms)
    Jan 18 22:56:35.885: INFO: (8) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.301385ms)
    Jan 18 22:56:35.886: INFO: (8) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 7.939313ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.177239ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.217055ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.188049ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.366312ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.257202ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.413917ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.492674ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.412874ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.256678ms)
    Jan 18 22:56:35.891: INFO: (9) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.504511ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.443635ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.358248ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.47494ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.613579ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.520195ms)
    Jan 18 22:56:35.892: INFO: (9) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.661815ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.434506ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.750214ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.759903ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.947838ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.775268ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.75581ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.657816ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.789073ms)
    Jan 18 22:56:35.898: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.859514ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.884221ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 6.146584ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 6.149265ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 6.220107ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 6.328871ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 6.700883ms)
    Jan 18 22:56:35.899: INFO: (10) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.778608ms)
    Jan 18 22:56:35.911: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 11.173553ms)
    Jan 18 22:56:35.913: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 13.233957ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 20.086964ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 20.106666ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 20.075388ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 19.976477ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 20.193009ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 20.060453ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 20.156678ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 20.097317ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 20.106095ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 20.133877ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 20.278958ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 20.26802ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 20.09365ms)
    Jan 18 22:56:35.920: INFO: (11) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 20.114717ms)
    Jan 18 22:56:35.933: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 13.452902ms)
    Jan 18 22:56:35.933: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 13.5541ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 13.864738ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 14.007338ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 13.975158ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 14.096725ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 14.197879ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 14.14507ms)
    Jan 18 22:56:35.934: INFO: (12) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 14.575338ms)
    Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 14.80071ms)
    Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 14.687105ms)
    Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 15.22829ms)
    Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 15.268595ms)
    Jan 18 22:56:35.935: INFO: (12) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 15.371799ms)
    Jan 18 22:56:35.937: INFO: (12) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 16.800206ms)
    Jan 18 22:56:35.937: INFO: (12) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 16.847457ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.942592ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 6.006478ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 6.170234ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 6.337971ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 6.315813ms)
    Jan 18 22:56:35.943: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.468813ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.628367ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 6.724621ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 6.678835ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 6.837052ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 6.660972ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 7.123955ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 7.186276ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 7.241375ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 7.161177ms)
    Jan 18 22:56:35.944: INFO: (13) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 7.28776ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.379188ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.365944ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.052571ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.951442ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.972008ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.942309ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.059609ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.95461ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.051386ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.971816ms)
    Jan 18 22:56:35.949: INFO: (14) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.015982ms)
    Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.298745ms)
    Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.514775ms)
    Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.554231ms)
    Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.85449ms)
    Jan 18 22:56:35.950: INFO: (14) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 6.069855ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 8.79496ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 8.913085ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 8.900913ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 9.270462ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 9.20176ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 9.060206ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 9.529501ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 9.419399ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 9.661569ms)
    Jan 18 22:56:35.960: INFO: (15) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 9.468271ms)
    Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 10.341199ms)
    Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 10.29403ms)
    Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 10.367014ms)
    Jan 18 22:56:35.961: INFO: (15) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 10.419113ms)
    Jan 18 22:56:35.962: INFO: (15) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 11.49627ms)
    Jan 18 22:56:35.962: INFO: (15) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 11.549737ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.468662ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.237823ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.816775ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.98884ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.196326ms)
    Jan 18 22:56:35.967: INFO: (16) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 4.843946ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.099521ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.284028ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.437015ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.234652ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.744529ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.798486ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.525322ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.589942ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.56953ms)
    Jan 18 22:56:35.968: INFO: (16) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.857004ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.44749ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.487935ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.577409ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.524803ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.679254ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.515892ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.747373ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.807329ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.841018ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.803207ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.876819ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 6.022464ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.882557ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 6.020779ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 6.052913ms)
    Jan 18 22:56:35.974: INFO: (17) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.930884ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 5.549473ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.614739ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 5.644968ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 5.822317ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.688156ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.637837ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 5.727917ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 5.817623ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.726784ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.860457ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.703557ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.788722ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 5.755905ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.688855ms)
    Jan 18 22:56:35.980: INFO: (18) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 5.843097ms)
    Jan 18 22:56:35.983: INFO: (18) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 8.599808ms)
    Jan 18 22:56:35.987: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 3.632099ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.620126ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:160/proxy/: foo (200; 4.868004ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">... (200; 4.786748ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/http:proxy-service-5rv5q-f52mk:162/proxy/: bar (200; 4.747446ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:443/proxy/tlsrewritem... (200; 4.84625ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:460/proxy/: tls baz (200; 4.897067ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/https:proxy-service-5rv5q-f52mk:462/proxy/: tls qux (200; 4.858525ms)
    Jan 18 22:56:35.988: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk:1080/proxy/rewriteme">test<... (200; 4.971933ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/: <a href="/api/v1/namespaces/proxy-2016/pods/proxy-service-5rv5q-f52mk/proxy/rewriteme">test</a> (200; 5.265195ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname2/proxy/: tls qux (200; 5.47476ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname1/proxy/: foo (200; 5.609088ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/https:proxy-service-5rv5q:tlsportname1/proxy/: tls baz (200; 5.629731ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname2/proxy/: bar (200; 5.678055ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/proxy-service-5rv5q:portname2/proxy/: bar (200; 5.709241ms)
    Jan 18 22:56:35.989: INFO: (19) /api/v1/namespaces/proxy-2016/services/http:proxy-service-5rv5q:portname1/proxy/: foo (200; 5.807566ms)
    STEP: deleting ReplicationController proxy-service-5rv5q in namespace proxy-2016, will wait for the garbage collector to delete the pods 01/18/23 22:56:35.989
    Jan 18 22:56:36.048: INFO: Deleting ReplicationController proxy-service-5rv5q took: 5.614288ms
    Jan 18 22:56:36.148: INFO: Terminating ReplicationController proxy-service-5rv5q pods took: 100.482558ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:38.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2016" for this suite. 01/18/23 22:56:38.553
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:38.558
Jan 18 22:56:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:56:38.559
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:38.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:38.575
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/18/23 22:56:38.579
Jan 18 22:56:38.579: INFO: namespace kubectl-7944
Jan 18 22:56:38.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 create -f -'
Jan 18 22:56:39.164: INFO: stderr: ""
Jan 18 22:56:39.164: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:56:39.164
Jan 18 22:56:40.168: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:56:40.168: INFO: Found 0 / 1
Jan 18 22:56:41.168: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:56:41.168: INFO: Found 1 / 1
Jan 18 22:56:41.168: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 22:56:41.171: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:56:41.171: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:56:41.171: INFO: wait on agnhost-primary startup in kubectl-7944 
Jan 18 22:56:41.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 logs agnhost-primary-86wzj agnhost-primary'
Jan 18 22:56:41.274: INFO: stderr: ""
Jan 18 22:56:41.274: INFO: stdout: "Paused\n"
STEP: exposing RC 01/18/23 22:56:41.274
Jan 18 22:56:41.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 22:56:41.363: INFO: stderr: ""
Jan 18 22:56:41.363: INFO: stdout: "service/rm2 exposed\n"
Jan 18 22:56:41.366: INFO: Service rm2 in namespace kubectl-7944 found.
STEP: exposing service 01/18/23 22:56:43.372
Jan 18 22:56:43.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 22:56:43.448: INFO: stderr: ""
Jan 18 22:56:43.448: INFO: stdout: "service/rm3 exposed\n"
Jan 18 22:56:43.452: INFO: Service rm3 in namespace kubectl-7944 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7944" for this suite. 01/18/23 22:56:45.461
------------------------------
â€¢ [SLOW TEST] [6.908 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:38.558
    Jan 18 22:56:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:56:38.559
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:38.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:38.575
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/18/23 22:56:38.579
    Jan 18 22:56:38.579: INFO: namespace kubectl-7944
    Jan 18 22:56:38.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 create -f -'
    Jan 18 22:56:39.164: INFO: stderr: ""
    Jan 18 22:56:39.164: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:56:39.164
    Jan 18 22:56:40.168: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:56:40.168: INFO: Found 0 / 1
    Jan 18 22:56:41.168: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:56:41.168: INFO: Found 1 / 1
    Jan 18 22:56:41.168: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 22:56:41.171: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:56:41.171: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:56:41.171: INFO: wait on agnhost-primary startup in kubectl-7944 
    Jan 18 22:56:41.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 logs agnhost-primary-86wzj agnhost-primary'
    Jan 18 22:56:41.274: INFO: stderr: ""
    Jan 18 22:56:41.274: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/18/23 22:56:41.274
    Jan 18 22:56:41.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 18 22:56:41.363: INFO: stderr: ""
    Jan 18 22:56:41.363: INFO: stdout: "service/rm2 exposed\n"
    Jan 18 22:56:41.366: INFO: Service rm2 in namespace kubectl-7944 found.
    STEP: exposing service 01/18/23 22:56:43.372
    Jan 18 22:56:43.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7944 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 18 22:56:43.448: INFO: stderr: ""
    Jan 18 22:56:43.448: INFO: stdout: "service/rm3 exposed\n"
    Jan 18 22:56:43.452: INFO: Service rm3 in namespace kubectl-7944 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7944" for this suite. 01/18/23 22:56:45.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:45.468
Jan 18 22:56:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:56:45.469
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:45.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:45.484
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:56:45.504
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:56:45.511
Jan 18 22:56:45.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:56:45.517: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:56:46.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:56:46.523: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 22:56:47.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:56:47.524: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/18/23 22:56:47.526
Jan 18 22:56:47.529: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/18/23 22:56:47.529
Jan 18 22:56:47.537: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/18/23 22:56:47.537
Jan 18 22:56:47.539: INFO: Observed &DaemonSet event: ADDED
Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.540: INFO: Found daemon set daemon-set in namespace daemonsets-4322 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:56:47.540: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/18/23 22:56:47.54
STEP: watching for the daemon set status to be patched 01/18/23 22:56:47.548
Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: ADDED
Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.550: INFO: Observed daemon set daemon-set in namespace daemonsets-4322 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:56:47.551: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:56:47.551: INFO: Found daemon set daemon-set in namespace daemonsets-4322 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 22:56:47.551: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:56:47.555
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4322, will wait for the garbage collector to delete the pods 01/18/23 22:56:47.555
Jan 18 22:56:47.613: INFO: Deleting DaemonSet.extensions daemon-set took: 5.435531ms
Jan 18 22:56:47.713: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.147942ms
Jan 18 22:56:49.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:56:49.618: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:56:49.621: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15884"},"items":null}

Jan 18 22:56:49.623: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15884"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4322" for this suite. 01/18/23 22:56:49.633
------------------------------
â€¢ [4.170 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:45.468
    Jan 18 22:56:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:56:45.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:45.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:45.484
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:56:45.504
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:56:45.511
    Jan 18 22:56:45.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:56:45.517: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:56:46.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:56:46.523: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 22:56:47.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:56:47.524: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/18/23 22:56:47.526
    Jan 18 22:56:47.529: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/18/23 22:56:47.529
    Jan 18 22:56:47.537: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/18/23 22:56:47.537
    Jan 18 22:56:47.539: INFO: Observed &DaemonSet event: ADDED
    Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.540: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.540: INFO: Found daemon set daemon-set in namespace daemonsets-4322 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 22:56:47.540: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/18/23 22:56:47.54
    STEP: watching for the daemon set status to be patched 01/18/23 22:56:47.548
    Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: ADDED
    Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.550: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.550: INFO: Observed daemon set daemon-set in namespace daemonsets-4322 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 22:56:47.551: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 22:56:47.551: INFO: Found daemon set daemon-set in namespace daemonsets-4322 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 18 22:56:47.551: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:56:47.555
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4322, will wait for the garbage collector to delete the pods 01/18/23 22:56:47.555
    Jan 18 22:56:47.613: INFO: Deleting DaemonSet.extensions daemon-set took: 5.435531ms
    Jan 18 22:56:47.713: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.147942ms
    Jan 18 22:56:49.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:56:49.618: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:56:49.621: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15884"},"items":null}

    Jan 18 22:56:49.623: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15884"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4322" for this suite. 01/18/23 22:56:49.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:49.638
Jan 18 22:56:49.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:56:49.639
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:49.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:49.654
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:56:49.657
Jan 18 22:56:49.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e" in namespace "projected-5916" to be "Succeeded or Failed"
Jan 18 22:56:49.668: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42643ms
Jan 18 22:56:51.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005996434s
Jan 18 22:56:53.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005953438s
STEP: Saw pod success 01/18/23 22:56:53.671
Jan 18 22:56:53.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e" satisfied condition "Succeeded or Failed"
Jan 18 22:56:53.674: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e container client-container: <nil>
STEP: delete the pod 01/18/23 22:56:53.678
Jan 18 22:56:53.687: INFO: Waiting for pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e to disappear
Jan 18 22:56:53.690: INFO: Pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:53.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5916" for this suite. 01/18/23 22:56:53.693
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:49.638
    Jan 18 22:56:49.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:56:49.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:49.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:49.654
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:56:49.657
    Jan 18 22:56:49.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e" in namespace "projected-5916" to be "Succeeded or Failed"
    Jan 18 22:56:49.668: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42643ms
    Jan 18 22:56:51.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005996434s
    Jan 18 22:56:53.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005953438s
    STEP: Saw pod success 01/18/23 22:56:53.671
    Jan 18 22:56:53.671: INFO: Pod "downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e" satisfied condition "Succeeded or Failed"
    Jan 18 22:56:53.674: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e container client-container: <nil>
    STEP: delete the pod 01/18/23 22:56:53.678
    Jan 18 22:56:53.687: INFO: Waiting for pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e to disappear
    Jan 18 22:56:53.690: INFO: Pod downwardapi-volume-4c473ea5-bb4e-49f7-b50e-ce6ff2ecd53e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:53.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5916" for this suite. 01/18/23 22:56:53.693
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:53.698
Jan 18 22:56:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:56:53.699
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:53.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:53.714
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/18/23 22:56:53.717
STEP: Getting a ResourceQuota 01/18/23 22:56:53.721
STEP: Updating a ResourceQuota 01/18/23 22:56:53.723
STEP: Verifying a ResourceQuota was modified 01/18/23 22:56:53.734
STEP: Deleting a ResourceQuota 01/18/23 22:56:53.736
STEP: Verifying the deleted ResourceQuota 01/18/23 22:56:53.746
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2707" for this suite. 01/18/23 22:56:53.75
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:53.698
    Jan 18 22:56:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:56:53.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:53.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:53.714
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/18/23 22:56:53.717
    STEP: Getting a ResourceQuota 01/18/23 22:56:53.721
    STEP: Updating a ResourceQuota 01/18/23 22:56:53.723
    STEP: Verifying a ResourceQuota was modified 01/18/23 22:56:53.734
    STEP: Deleting a ResourceQuota 01/18/23 22:56:53.736
    STEP: Verifying the deleted ResourceQuota 01/18/23 22:56:53.746
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2707" for this suite. 01/18/23 22:56:53.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:53.757
Jan 18 22:56:53.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:56:53.757
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:53.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:53.772
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:56:53.775
Jan 18 22:56:53.783: INFO: Waiting up to 5m0s for pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93" in namespace "emptydir-1178" to be "Succeeded or Failed"
Jan 18 22:56:53.786: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330758ms
Jan 18 22:56:55.789: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005754997s
Jan 18 22:56:57.790: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006375358s
STEP: Saw pod success 01/18/23 22:56:57.79
Jan 18 22:56:57.790: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93" satisfied condition "Succeeded or Failed"
Jan 18 22:56:57.792: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 container test-container: <nil>
STEP: delete the pod 01/18/23 22:56:57.797
Jan 18 22:56:57.805: INFO: Waiting for pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 to disappear
Jan 18 22:56:57.807: INFO: Pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1178" for this suite. 01/18/23 22:56:57.81
------------------------------
â€¢ [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:53.757
    Jan 18 22:56:53.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:56:53.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:53.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:53.772
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:56:53.775
    Jan 18 22:56:53.783: INFO: Waiting up to 5m0s for pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93" in namespace "emptydir-1178" to be "Succeeded or Failed"
    Jan 18 22:56:53.786: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330758ms
    Jan 18 22:56:55.789: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005754997s
    Jan 18 22:56:57.790: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006375358s
    STEP: Saw pod success 01/18/23 22:56:57.79
    Jan 18 22:56:57.790: INFO: Pod "pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93" satisfied condition "Succeeded or Failed"
    Jan 18 22:56:57.792: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:56:57.797
    Jan 18 22:56:57.805: INFO: Waiting for pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 to disappear
    Jan 18 22:56:57.807: INFO: Pod pod-eef65ac2-b1bf-43a2-98be-68bf2e522b93 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1178" for this suite. 01/18/23 22:56:57.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:57.815
Jan 18 22:56:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 22:56:57.817
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:57.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:57.83
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/18/23 22:56:57.833
Jan 18 22:56:57.842: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2" in namespace "emptydir-4665" to be "running"
Jan 18 22:56:57.844: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191698ms
Jan 18 22:56:59.847: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 2.005675287s
Jan 18 22:56:59.847: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/18/23 22:56:59.847
Jan 18 22:56:59.847: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4665 PodName:pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:56:59.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:56:59.848: INFO: ExecWithOptions: Clientset creation
Jan 18 22:56:59.848: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4665/pods/pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 22:56:59.917: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:56:59.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4665" for this suite. 01/18/23 22:56:59.919
------------------------------
â€¢ [2.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:57.815
    Jan 18 22:56:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:56:57.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:57.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:57.83
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/18/23 22:56:57.833
    Jan 18 22:56:57.842: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2" in namespace "emptydir-4665" to be "running"
    Jan 18 22:56:57.844: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191698ms
    Jan 18 22:56:59.847: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 2.005675287s
    Jan 18 22:56:59.847: INFO: Pod "pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/18/23 22:56:59.847
    Jan 18 22:56:59.847: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4665 PodName:pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:56:59.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:56:59.848: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:56:59.848: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4665/pods/pod-sharedvolume-506ec673-329c-4baa-9465-2d291f6d9ca2/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 18 22:56:59.917: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:56:59.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4665" for this suite. 01/18/23 22:56:59.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:56:59.925
Jan 18 22:56:59.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:56:59.926
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:59.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:59.939
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/18/23 22:56:59.942
Jan 18 22:56:59.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: rename a version 01/18/23 22:57:03.494
STEP: check the new version name is served 01/18/23 22:57:03.508
STEP: check the old version name is removed 01/18/23 22:57:04.42
STEP: check the other version is not changed 01/18/23 22:57:05.137
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9303" for this suite. 01/18/23 22:57:07.961
------------------------------
â€¢ [SLOW TEST] [8.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:56:59.925
    Jan 18 22:56:59.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:56:59.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:56:59.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:56:59.939
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/18/23 22:56:59.942
    Jan 18 22:56:59.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: rename a version 01/18/23 22:57:03.494
    STEP: check the new version name is served 01/18/23 22:57:03.508
    STEP: check the old version name is removed 01/18/23 22:57:04.42
    STEP: check the other version is not changed 01/18/23 22:57:05.137
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9303" for this suite. 01/18/23 22:57:07.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:07.966
Jan 18 22:57:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 22:57:07.967
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:07.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:07.979
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/18/23 22:57:07.982
STEP: submitting the pod to kubernetes 01/18/23 22:57:07.982
Jan 18 22:57:07.990: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" in namespace "pods-4347" to be "running and ready"
Jan 18 22:57:07.992: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214443ms
Jan 18 22:57:07.992: INFO: The phase of Pod pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:57:09.996: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005841451s
Jan 18 22:57:09.996: INFO: The phase of Pod pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3 is Running (Ready = true)
Jan 18 22:57:09.996: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 22:57:09.998
STEP: updating the pod 01/18/23 22:57:10.001
Jan 18 22:57:10.512: INFO: Successfully updated pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3"
Jan 18 22:57:10.512: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" in namespace "pods-4347" to be "terminated with reason DeadlineExceeded"
Jan 18 22:57:10.514: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.285453ms
Jan 18 22:57:12.519: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006595835s
Jan 18 22:57:14.519: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=false. Elapsed: 4.007164639s
Jan 18 22:57:16.517: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005412378s
Jan 18 22:57:16.517: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:16.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4347" for this suite. 01/18/23 22:57:16.52
------------------------------
â€¢ [SLOW TEST] [8.560 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:07.966
    Jan 18 22:57:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 22:57:07.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:07.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:07.979
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/18/23 22:57:07.982
    STEP: submitting the pod to kubernetes 01/18/23 22:57:07.982
    Jan 18 22:57:07.990: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" in namespace "pods-4347" to be "running and ready"
    Jan 18 22:57:07.992: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214443ms
    Jan 18 22:57:07.992: INFO: The phase of Pod pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:57:09.996: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005841451s
    Jan 18 22:57:09.996: INFO: The phase of Pod pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3 is Running (Ready = true)
    Jan 18 22:57:09.996: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 22:57:09.998
    STEP: updating the pod 01/18/23 22:57:10.001
    Jan 18 22:57:10.512: INFO: Successfully updated pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3"
    Jan 18 22:57:10.512: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" in namespace "pods-4347" to be "terminated with reason DeadlineExceeded"
    Jan 18 22:57:10.514: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.285453ms
    Jan 18 22:57:12.519: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006595835s
    Jan 18 22:57:14.519: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Running", Reason="", readiness=false. Elapsed: 4.007164639s
    Jan 18 22:57:16.517: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005412378s
    Jan 18 22:57:16.517: INFO: Pod "pod-update-activedeadlineseconds-732b7b75-c014-426b-abda-c6adcdf921a3" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:16.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4347" for this suite. 01/18/23 22:57:16.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:16.527
Jan 18 22:57:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:57:16.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:16.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:16.542
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/18/23 22:57:16.545
STEP: wait for the container to reach Succeeded 01/18/23 22:57:16.552
STEP: get the container status 01/18/23 22:57:19.566
STEP: the container should be terminated 01/18/23 22:57:19.569
STEP: the termination message should be set 01/18/23 22:57:19.569
Jan 18 22:57:19.569: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/18/23 22:57:19.569
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:19.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9015" for this suite. 01/18/23 22:57:19.591
------------------------------
â€¢ [3.069 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:16.527
    Jan 18 22:57:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:57:16.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:16.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:16.542
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/18/23 22:57:16.545
    STEP: wait for the container to reach Succeeded 01/18/23 22:57:16.552
    STEP: get the container status 01/18/23 22:57:19.566
    STEP: the container should be terminated 01/18/23 22:57:19.569
    STEP: the termination message should be set 01/18/23 22:57:19.569
    Jan 18 22:57:19.569: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/18/23 22:57:19.569
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:19.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9015" for this suite. 01/18/23 22:57:19.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:19.596
Jan 18 22:57:19.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:57:19.597
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:19.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:19.61
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-6632 01/18/23 22:57:19.613
STEP: creating a selector 01/18/23 22:57:19.613
STEP: Creating the service pods in kubernetes 01/18/23 22:57:19.613
Jan 18 22:57:19.613: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:57:19.636: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6632" to be "running and ready"
Jan 18 22:57:19.639: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010985ms
Jan 18 22:57:19.639: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:57:21.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006767274s
Jan 18 22:57:21.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:23.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006280968s
Jan 18 22:57:23.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:25.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006456808s
Jan 18 22:57:25.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:27.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006043808s
Jan 18 22:57:27.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:29.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006942891s
Jan 18 22:57:29.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:31.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007614233s
Jan 18 22:57:31.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:33.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006802157s
Jan 18 22:57:33.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:35.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00779531s
Jan 18 22:57:35.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:37.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006078792s
Jan 18 22:57:37.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:39.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008286318s
Jan 18 22:57:39.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:57:41.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007148779s
Jan 18 22:57:41.643: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:57:41.643: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:57:41.645: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6632" to be "running and ready"
Jan 18 22:57:41.647: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.996143ms
Jan 18 22:57:41.647: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:57:41.647: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:57:41.649
Jan 18 22:57:41.660: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6632" to be "running"
Jan 18 22:57:41.664: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015695ms
Jan 18 22:57:43.668: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007335011s
Jan 18 22:57:43.668: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:57:43.670: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6632" to be "running"
Jan 18 22:57:43.673: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.314676ms
Jan 18 22:57:43.673: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 22:57:43.675: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:57:43.675: INFO: Going to poll 10.32.0.2 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:57:43.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6632 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:57:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:57:43.678: INFO: ExecWithOptions: Clientset creation
Jan 18 22:57:43.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6632/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.32.0.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:57:43.751: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 22:57:43.751: INFO: Going to poll 10.32.12.3 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:57:43.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.12.3:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6632 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:57:43.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:57:43.754: INFO: ExecWithOptions: Clientset creation
Jan 18 22:57:43.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6632/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.32.12.3%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:57:43.832: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6632" for this suite. 01/18/23 22:57:43.835
------------------------------
â€¢ [SLOW TEST] [24.245 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:19.596
    Jan 18 22:57:19.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:57:19.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:19.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:19.61
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-6632 01/18/23 22:57:19.613
    STEP: creating a selector 01/18/23 22:57:19.613
    STEP: Creating the service pods in kubernetes 01/18/23 22:57:19.613
    Jan 18 22:57:19.613: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:57:19.636: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6632" to be "running and ready"
    Jan 18 22:57:19.639: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010985ms
    Jan 18 22:57:19.639: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:57:21.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006767274s
    Jan 18 22:57:21.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:23.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006280968s
    Jan 18 22:57:23.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:25.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006456808s
    Jan 18 22:57:25.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:27.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006043808s
    Jan 18 22:57:27.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:29.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006942891s
    Jan 18 22:57:29.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:31.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007614233s
    Jan 18 22:57:31.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:33.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006802157s
    Jan 18 22:57:33.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:35.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00779531s
    Jan 18 22:57:35.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:37.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006078792s
    Jan 18 22:57:37.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:39.644: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008286318s
    Jan 18 22:57:39.644: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:57:41.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007148779s
    Jan 18 22:57:41.643: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:57:41.643: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:57:41.645: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6632" to be "running and ready"
    Jan 18 22:57:41.647: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.996143ms
    Jan 18 22:57:41.647: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:57:41.647: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:57:41.649
    Jan 18 22:57:41.660: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6632" to be "running"
    Jan 18 22:57:41.664: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015695ms
    Jan 18 22:57:43.668: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007335011s
    Jan 18 22:57:43.668: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:57:43.670: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6632" to be "running"
    Jan 18 22:57:43.673: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.314676ms
    Jan 18 22:57:43.673: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 22:57:43.675: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:57:43.675: INFO: Going to poll 10.32.0.2 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:57:43.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6632 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:57:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:57:43.678: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:57:43.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6632/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.32.0.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:57:43.751: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 22:57:43.751: INFO: Going to poll 10.32.12.3 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:57:43.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.12.3:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6632 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:57:43.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:57:43.754: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:57:43.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6632/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.32.12.3%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:57:43.832: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6632" for this suite. 01/18/23 22:57:43.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:43.843
Jan 18 22:57:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename containers 01/18/23 22:57:43.844
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:43.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:43.86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/18/23 22:57:43.862
Jan 18 22:57:43.871: INFO: Waiting up to 5m0s for pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771" in namespace "containers-3938" to be "Succeeded or Failed"
Jan 18 22:57:43.873: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380788ms
Jan 18 22:57:45.876: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005378145s
Jan 18 22:57:47.877: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00598412s
STEP: Saw pod success 01/18/23 22:57:47.877
Jan 18 22:57:47.877: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771" satisfied condition "Succeeded or Failed"
Jan 18 22:57:47.879: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:57:47.886
Jan 18 22:57:47.895: INFO: Waiting for pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 to disappear
Jan 18 22:57:47.898: INFO: Pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3938" for this suite. 01/18/23 22:57:47.902
------------------------------
â€¢ [4.064 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:43.843
    Jan 18 22:57:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename containers 01/18/23 22:57:43.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:43.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:43.86
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/18/23 22:57:43.862
    Jan 18 22:57:43.871: INFO: Waiting up to 5m0s for pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771" in namespace "containers-3938" to be "Succeeded or Failed"
    Jan 18 22:57:43.873: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380788ms
    Jan 18 22:57:45.876: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005378145s
    Jan 18 22:57:47.877: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00598412s
    STEP: Saw pod success 01/18/23 22:57:47.877
    Jan 18 22:57:47.877: INFO: Pod "client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771" satisfied condition "Succeeded or Failed"
    Jan 18 22:57:47.879: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:57:47.886
    Jan 18 22:57:47.895: INFO: Waiting for pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 to disappear
    Jan 18 22:57:47.898: INFO: Pod client-containers-509bd334-b3dd-4eae-af2c-910adcd7e771 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3938" for this suite. 01/18/23 22:57:47.902
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:47.907
Jan 18 22:57:47.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename controllerrevisions 01/18/23 22:57:47.908
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:47.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:47.926
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-7lw64-daemon-set" 01/18/23 22:57:47.94
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:57:47.947
Jan 18 22:57:47.953: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 0
Jan 18 22:57:47.953: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:57:48.958: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 1
Jan 18 22:57:48.958: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 22:57:49.959: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 2
Jan 18 22:57:49.959: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-7lw64-daemon-set
STEP: Confirm DaemonSet "e2e-7lw64-daemon-set" successfully created with "daemonset-name=e2e-7lw64-daemon-set" label 01/18/23 22:57:49.961
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7lw64-daemon-set" 01/18/23 22:57:49.966
Jan 18 22:57:49.969: INFO: Located ControllerRevision: "e2e-7lw64-daemon-set-68bc87f46c"
STEP: Patching ControllerRevision "e2e-7lw64-daemon-set-68bc87f46c" 01/18/23 22:57:49.971
Jan 18 22:57:49.979: INFO: e2e-7lw64-daemon-set-68bc87f46c has been patched
STEP: Create a new ControllerRevision 01/18/23 22:57:49.979
Jan 18 22:57:49.983: INFO: Created ControllerRevision: e2e-7lw64-daemon-set-849bd849cf
STEP: Confirm that there are two ControllerRevisions 01/18/23 22:57:49.983
Jan 18 22:57:49.983: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:57:49.986: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-7lw64-daemon-set-68bc87f46c" 01/18/23 22:57:49.986
STEP: Confirm that there is only one ControllerRevision 01/18/23 22:57:49.99
Jan 18 22:57:49.990: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:57:49.993: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-7lw64-daemon-set-849bd849cf" 01/18/23 22:57:49.995
Jan 18 22:57:50.002: INFO: e2e-7lw64-daemon-set-849bd849cf has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 22:57:50.002
W0118 22:57:50.009861      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/18/23 22:57:50.009
Jan 18 22:57:50.010: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:57:51.012: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:57:51.016: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7lw64-daemon-set-849bd849cf=updated" 01/18/23 22:57:51.016
STEP: Confirm that there is only one ControllerRevision 01/18/23 22:57:51.022
Jan 18 22:57:51.022: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:57:51.024: INFO: Found 1 ControllerRevisions
Jan 18 22:57:51.026: INFO: ControllerRevision "e2e-7lw64-daemon-set-6954b6654" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-7lw64-daemon-set" 01/18/23 22:57:51.028
STEP: deleting DaemonSet.extensions e2e-7lw64-daemon-set in namespace controllerrevisions-5558, will wait for the garbage collector to delete the pods 01/18/23 22:57:51.029
Jan 18 22:57:51.088: INFO: Deleting DaemonSet.extensions e2e-7lw64-daemon-set took: 6.705291ms
Jan 18 22:57:51.189: INFO: Terminating DaemonSet.extensions e2e-7lw64-daemon-set pods took: 100.556874ms
Jan 18 22:57:52.692: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 0
Jan 18 22:57:52.692: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7lw64-daemon-set
Jan 18 22:57:52.694: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16326"},"items":null}

Jan 18 22:57:52.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16326"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-5558" for this suite. 01/18/23 22:57:52.706
------------------------------
â€¢ [4.804 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:47.907
    Jan 18 22:57:47.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename controllerrevisions 01/18/23 22:57:47.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:47.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:47.926
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-7lw64-daemon-set" 01/18/23 22:57:47.94
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:57:47.947
    Jan 18 22:57:47.953: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 0
    Jan 18 22:57:47.953: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:57:48.958: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 1
    Jan 18 22:57:48.958: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 22:57:49.959: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 2
    Jan 18 22:57:49.959: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-7lw64-daemon-set
    STEP: Confirm DaemonSet "e2e-7lw64-daemon-set" successfully created with "daemonset-name=e2e-7lw64-daemon-set" label 01/18/23 22:57:49.961
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7lw64-daemon-set" 01/18/23 22:57:49.966
    Jan 18 22:57:49.969: INFO: Located ControllerRevision: "e2e-7lw64-daemon-set-68bc87f46c"
    STEP: Patching ControllerRevision "e2e-7lw64-daemon-set-68bc87f46c" 01/18/23 22:57:49.971
    Jan 18 22:57:49.979: INFO: e2e-7lw64-daemon-set-68bc87f46c has been patched
    STEP: Create a new ControllerRevision 01/18/23 22:57:49.979
    Jan 18 22:57:49.983: INFO: Created ControllerRevision: e2e-7lw64-daemon-set-849bd849cf
    STEP: Confirm that there are two ControllerRevisions 01/18/23 22:57:49.983
    Jan 18 22:57:49.983: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:57:49.986: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-7lw64-daemon-set-68bc87f46c" 01/18/23 22:57:49.986
    STEP: Confirm that there is only one ControllerRevision 01/18/23 22:57:49.99
    Jan 18 22:57:49.990: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:57:49.993: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-7lw64-daemon-set-849bd849cf" 01/18/23 22:57:49.995
    Jan 18 22:57:50.002: INFO: e2e-7lw64-daemon-set-849bd849cf has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 22:57:50.002
    W0118 22:57:50.009861      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/18/23 22:57:50.009
    Jan 18 22:57:50.010: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:57:51.012: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:57:51.016: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7lw64-daemon-set-849bd849cf=updated" 01/18/23 22:57:51.016
    STEP: Confirm that there is only one ControllerRevision 01/18/23 22:57:51.022
    Jan 18 22:57:51.022: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:57:51.024: INFO: Found 1 ControllerRevisions
    Jan 18 22:57:51.026: INFO: ControllerRevision "e2e-7lw64-daemon-set-6954b6654" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-7lw64-daemon-set" 01/18/23 22:57:51.028
    STEP: deleting DaemonSet.extensions e2e-7lw64-daemon-set in namespace controllerrevisions-5558, will wait for the garbage collector to delete the pods 01/18/23 22:57:51.029
    Jan 18 22:57:51.088: INFO: Deleting DaemonSet.extensions e2e-7lw64-daemon-set took: 6.705291ms
    Jan 18 22:57:51.189: INFO: Terminating DaemonSet.extensions e2e-7lw64-daemon-set pods took: 100.556874ms
    Jan 18 22:57:52.692: INFO: Number of nodes with available pods controlled by daemonset e2e-7lw64-daemon-set: 0
    Jan 18 22:57:52.692: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7lw64-daemon-set
    Jan 18 22:57:52.694: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16326"},"items":null}

    Jan 18 22:57:52.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16326"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-5558" for this suite. 01/18/23 22:57:52.706
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:52.711
Jan 18 22:57:52.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 22:57:52.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:52.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:52.728
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:57:52.731
Jan 18 22:57:52.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1" in namespace "projected-9502" to be "Succeeded or Failed"
Jan 18 22:57:52.742: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370535ms
Jan 18 22:57:54.745: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162497s
Jan 18 22:57:56.746: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000669s
STEP: Saw pod success 01/18/23 22:57:56.746
Jan 18 22:57:56.746: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1" satisfied condition "Succeeded or Failed"
Jan 18 22:57:56.749: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 container client-container: <nil>
STEP: delete the pod 01/18/23 22:57:56.753
Jan 18 22:57:56.763: INFO: Waiting for pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 to disappear
Jan 18 22:57:56.765: INFO: Pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:57:56.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9502" for this suite. 01/18/23 22:57:56.768
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:52.711
    Jan 18 22:57:52.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 22:57:52.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:52.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:52.728
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:57:52.731
    Jan 18 22:57:52.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1" in namespace "projected-9502" to be "Succeeded or Failed"
    Jan 18 22:57:52.742: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370535ms
    Jan 18 22:57:54.745: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162497s
    Jan 18 22:57:56.746: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000669s
    STEP: Saw pod success 01/18/23 22:57:56.746
    Jan 18 22:57:56.746: INFO: Pod "downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1" satisfied condition "Succeeded or Failed"
    Jan 18 22:57:56.749: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:57:56.753
    Jan 18 22:57:56.763: INFO: Waiting for pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 to disappear
    Jan 18 22:57:56.765: INFO: Pod downwardapi-volume-0e7986ee-e523-4fa8-b71f-b65e059445f1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:57:56.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9502" for this suite. 01/18/23 22:57:56.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:57:56.773
Jan 18 22:57:56.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 22:57:56.774
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:56.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:56.787
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/18/23 22:57:56.79
Jan 18 22:57:56.797: INFO: Waiting up to 5m0s for pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6" in namespace "downward-api-4375" to be "running and ready"
Jan 18 22:57:56.799: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456041ms
Jan 18 22:57:56.799: INFO: The phase of Pod labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:57:58.803: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433086s
Jan 18 22:57:58.803: INFO: The phase of Pod labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6 is Running (Ready = true)
Jan 18 22:57:58.803: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6" satisfied condition "running and ready"
Jan 18 22:57:59.321: INFO: Successfully updated pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:58:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4375" for this suite. 01/18/23 22:58:03.342
------------------------------
â€¢ [SLOW TEST] [6.574 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:57:56.773
    Jan 18 22:57:56.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:57:56.774
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:57:56.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:57:56.787
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/18/23 22:57:56.79
    Jan 18 22:57:56.797: INFO: Waiting up to 5m0s for pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6" in namespace "downward-api-4375" to be "running and ready"
    Jan 18 22:57:56.799: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456041ms
    Jan 18 22:57:56.799: INFO: The phase of Pod labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:57:58.803: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433086s
    Jan 18 22:57:58.803: INFO: The phase of Pod labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6 is Running (Ready = true)
    Jan 18 22:57:58.803: INFO: Pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6" satisfied condition "running and ready"
    Jan 18 22:57:59.321: INFO: Successfully updated pod "labelsupdate0471689b-73bd-424c-970c-ce8b3562d2f6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:58:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4375" for this suite. 01/18/23 22:58:03.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:58:03.349
Jan 18 22:58:03.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:58:03.35
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:03.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:03.362
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/18/23 22:58:03.365
STEP: Getting a ResourceQuota 01/18/23 22:58:03.369
STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 22:58:03.371
STEP: Patching the ResourceQuota 01/18/23 22:58:03.373
STEP: Deleting a Collection of ResourceQuotas 01/18/23 22:58:03.379
STEP: Verifying the deleted ResourceQuota 01/18/23 22:58:03.387
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:58:03.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5879" for this suite. 01/18/23 22:58:03.392
------------------------------
â€¢ [0.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:58:03.349
    Jan 18 22:58:03.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:58:03.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:03.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:03.362
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/18/23 22:58:03.365
    STEP: Getting a ResourceQuota 01/18/23 22:58:03.369
    STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 22:58:03.371
    STEP: Patching the ResourceQuota 01/18/23 22:58:03.373
    STEP: Deleting a Collection of ResourceQuotas 01/18/23 22:58:03.379
    STEP: Verifying the deleted ResourceQuota 01/18/23 22:58:03.387
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:58:03.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5879" for this suite. 01/18/23 22:58:03.392
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:58:03.396
Jan 18 22:58:03.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:58:03.397
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:03.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:03.41
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 22:58:03.413
Jan 18 22:58:03.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 22:58:04.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:58:10.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8042" for this suite. 01/18/23 22:58:10.429
------------------------------
â€¢ [SLOW TEST] [7.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:58:03.396
    Jan 18 22:58:03.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:58:03.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:03.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:03.41
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 22:58:03.413
    Jan 18 22:58:03.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 22:58:04.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:58:10.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8042" for this suite. 01/18/23 22:58:10.429
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:58:10.436
Jan 18 22:58:10.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 22:58:10.437
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:10.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:10.45
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5019/configmap-test-b528ca98-d3bb-427a-ac13-412ea6e01b1e 01/18/23 22:58:10.452
STEP: Creating a pod to test consume configMaps 01/18/23 22:58:10.458
Jan 18 22:58:10.464: INFO: Waiting up to 5m0s for pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164" in namespace "configmap-5019" to be "Succeeded or Failed"
Jan 18 22:58:10.467: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.152953ms
Jan 18 22:58:12.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005929733s
Jan 18 22:58:14.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005604331s
STEP: Saw pod success 01/18/23 22:58:14.47
Jan 18 22:58:14.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164" satisfied condition "Succeeded or Failed"
Jan 18 22:58:14.473: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 container env-test: <nil>
STEP: delete the pod 01/18/23 22:58:14.478
Jan 18 22:58:14.487: INFO: Waiting for pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 to disappear
Jan 18 22:58:14.489: INFO: Pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:58:14.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5019" for this suite. 01/18/23 22:58:14.492
------------------------------
â€¢ [4.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:58:10.436
    Jan 18 22:58:10.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 22:58:10.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:10.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:10.45
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5019/configmap-test-b528ca98-d3bb-427a-ac13-412ea6e01b1e 01/18/23 22:58:10.452
    STEP: Creating a pod to test consume configMaps 01/18/23 22:58:10.458
    Jan 18 22:58:10.464: INFO: Waiting up to 5m0s for pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164" in namespace "configmap-5019" to be "Succeeded or Failed"
    Jan 18 22:58:10.467: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.152953ms
    Jan 18 22:58:12.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005929733s
    Jan 18 22:58:14.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005604331s
    STEP: Saw pod success 01/18/23 22:58:14.47
    Jan 18 22:58:14.470: INFO: Pod "pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164" satisfied condition "Succeeded or Failed"
    Jan 18 22:58:14.473: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:58:14.478
    Jan 18 22:58:14.487: INFO: Waiting for pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 to disappear
    Jan 18 22:58:14.489: INFO: Pod pod-configmaps-355c30f6-1f66-4b5d-8c5e-cd86eb38e164 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:58:14.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5019" for this suite. 01/18/23 22:58:14.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:58:14.498
Jan 18 22:58:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:58:14.499
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:14.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:14.513
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 22:58:14.527: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:59:14.546: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/18/23 22:59:14.548
Jan 18 22:59:14.567: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:59:14.574: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:59:14.591: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:59:14.598: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 22:59:14.598
Jan 18 22:59:14.599: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1770" to be "running"
Jan 18 22:59:14.601: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530585ms
Jan 18 22:59:16.604: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005698455s
Jan 18 22:59:18.605: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006932689s
Jan 18 22:59:20.606: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007232285s
Jan 18 22:59:22.605: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006453998s
Jan 18 22:59:22.605: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 22:59:22.605: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
Jan 18 22:59:22.607: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.224425ms
Jan 18 22:59:22.607: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:59:22.607: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
Jan 18 22:59:22.610: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.349036ms
Jan 18 22:59:22.610: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:59:22.610: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
Jan 18 22:59:22.612: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.279177ms
Jan 18 22:59:22.612: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 22:59:22.612
Jan 18 22:59:22.620: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 18 22:59:22.623: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484ms
Jan 18 22:59:24.626: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006268337s
Jan 18 22:59:26.628: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007606808s
Jan 18 22:59:28.626: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006233728s
Jan 18 22:59:28.627: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:59:28.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1770" for this suite. 01/18/23 22:59:28.679
------------------------------
â€¢ [SLOW TEST] [74.186 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:58:14.498
    Jan 18 22:58:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:58:14.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:58:14.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:58:14.513
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 22:58:14.527: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:59:14.546: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/18/23 22:59:14.548
    Jan 18 22:59:14.567: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 22:59:14.574: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 22:59:14.591: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 22:59:14.598: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 22:59:14.598
    Jan 18 22:59:14.599: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1770" to be "running"
    Jan 18 22:59:14.601: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530585ms
    Jan 18 22:59:16.604: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005698455s
    Jan 18 22:59:18.605: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006932689s
    Jan 18 22:59:20.606: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007232285s
    Jan 18 22:59:22.605: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006453998s
    Jan 18 22:59:22.605: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 22:59:22.605: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
    Jan 18 22:59:22.607: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.224425ms
    Jan 18 22:59:22.607: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:59:22.607: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
    Jan 18 22:59:22.610: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.349036ms
    Jan 18 22:59:22.610: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:59:22.610: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1770" to be "running"
    Jan 18 22:59:22.612: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.279177ms
    Jan 18 22:59:22.612: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 22:59:22.612
    Jan 18 22:59:22.620: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 18 22:59:22.623: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484ms
    Jan 18 22:59:24.626: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006268337s
    Jan 18 22:59:26.628: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007606808s
    Jan 18 22:59:28.626: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006233728s
    Jan 18 22:59:28.627: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:59:28.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1770" for this suite. 01/18/23 22:59:28.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:59:28.686
Jan 18 22:59:28.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 22:59:28.687
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:28.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:28.699
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:59:28.711
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:59:29.12
STEP: Deploying the webhook pod 01/18/23 22:59:29.126
STEP: Wait for the deployment to be ready 01/18/23 22:59:29.14
Jan 18 22:59:29.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:59:31.156
STEP: Verifying the service has paired with the endpoint 01/18/23 22:59:31.164
Jan 18 22:59:32.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:59:32.167
STEP: create a pod 01/18/23 22:59:32.185
Jan 18 22:59:32.190: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1424" to be "running"
Jan 18 22:59:32.192: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198791ms
Jan 18 22:59:34.195: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004808654s
Jan 18 22:59:34.195: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:59:34.195
Jan 18 22:59:34.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=webhook-1424 attach --namespace=webhook-1424 to-be-attached-pod -i -c=container1'
Jan 18 22:59:34.274: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:59:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1424" for this suite. 01/18/23 22:59:34.308
STEP: Destroying namespace "webhook-1424-markers" for this suite. 01/18/23 22:59:34.315
------------------------------
â€¢ [SLOW TEST] [5.635 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:59:28.686
    Jan 18 22:59:28.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 22:59:28.687
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:28.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:28.699
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:59:28.711
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:59:29.12
    STEP: Deploying the webhook pod 01/18/23 22:59:29.126
    STEP: Wait for the deployment to be ready 01/18/23 22:59:29.14
    Jan 18 22:59:29.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:59:31.156
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:59:31.164
    Jan 18 22:59:32.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:59:32.167
    STEP: create a pod 01/18/23 22:59:32.185
    Jan 18 22:59:32.190: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1424" to be "running"
    Jan 18 22:59:32.192: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198791ms
    Jan 18 22:59:34.195: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004808654s
    Jan 18 22:59:34.195: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:59:34.195
    Jan 18 22:59:34.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=webhook-1424 attach --namespace=webhook-1424 to-be-attached-pod -i -c=container1'
    Jan 18 22:59:34.274: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:59:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1424" for this suite. 01/18/23 22:59:34.308
    STEP: Destroying namespace "webhook-1424-markers" for this suite. 01/18/23 22:59:34.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:59:34.321
Jan 18 22:59:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 22:59:34.322
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:34.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:34.335
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/18/23 22:59:34.338
Jan 18 22:59:34.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 create -f -'
Jan 18 22:59:34.929: INFO: stderr: ""
Jan 18 22:59:34.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:59:34.929
Jan 18 22:59:34.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:59:35.007: INFO: stderr: ""
Jan 18 22:59:35.007: INFO: stdout: "update-demo-nautilus-fpn6z update-demo-nautilus-m5lxk "
Jan 18 22:59:35.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:59:35.076: INFO: stderr: ""
Jan 18 22:59:35.076: INFO: stdout: ""
Jan 18 22:59:35.076: INFO: update-demo-nautilus-fpn6z is created but not running
Jan 18 22:59:40.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:59:40.148: INFO: stderr: ""
Jan 18 22:59:40.148: INFO: stdout: "update-demo-nautilus-fpn6z update-demo-nautilus-m5lxk "
Jan 18 22:59:40.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:59:40.214: INFO: stderr: ""
Jan 18 22:59:40.214: INFO: stdout: "true"
Jan 18 22:59:40.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:59:40.278: INFO: stderr: ""
Jan 18 22:59:40.278: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:59:40.278: INFO: validating pod update-demo-nautilus-fpn6z
Jan 18 22:59:40.282: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:59:40.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:59:40.282: INFO: update-demo-nautilus-fpn6z is verified up and running
Jan 18 22:59:40.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-m5lxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:59:40.353: INFO: stderr: ""
Jan 18 22:59:40.353: INFO: stdout: "true"
Jan 18 22:59:40.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-m5lxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:59:40.421: INFO: stderr: ""
Jan 18 22:59:40.421: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:59:40.421: INFO: validating pod update-demo-nautilus-m5lxk
Jan 18 22:59:40.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:59:40.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:59:40.427: INFO: update-demo-nautilus-m5lxk is verified up and running
STEP: using delete to clean up resources 01/18/23 22:59:40.427
Jan 18 22:59:40.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 delete --grace-period=0 --force -f -'
Jan 18 22:59:40.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:59:40.495: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 22:59:40.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get rc,svc -l name=update-demo --no-headers'
Jan 18 22:59:40.567: INFO: stderr: "No resources found in kubectl-6182 namespace.\n"
Jan 18 22:59:40.567: INFO: stdout: ""
Jan 18 22:59:40.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 22:59:40.634: INFO: stderr: ""
Jan 18 22:59:40.634: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:59:40.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6182" for this suite. 01/18/23 22:59:40.638
------------------------------
â€¢ [SLOW TEST] [6.321 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:59:34.321
    Jan 18 22:59:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:59:34.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:34.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:34.335
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/18/23 22:59:34.338
    Jan 18 22:59:34.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 create -f -'
    Jan 18 22:59:34.929: INFO: stderr: ""
    Jan 18 22:59:34.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:59:34.929
    Jan 18 22:59:34.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:59:35.007: INFO: stderr: ""
    Jan 18 22:59:35.007: INFO: stdout: "update-demo-nautilus-fpn6z update-demo-nautilus-m5lxk "
    Jan 18 22:59:35.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:59:35.076: INFO: stderr: ""
    Jan 18 22:59:35.076: INFO: stdout: ""
    Jan 18 22:59:35.076: INFO: update-demo-nautilus-fpn6z is created but not running
    Jan 18 22:59:40.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:59:40.148: INFO: stderr: ""
    Jan 18 22:59:40.148: INFO: stdout: "update-demo-nautilus-fpn6z update-demo-nautilus-m5lxk "
    Jan 18 22:59:40.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:59:40.214: INFO: stderr: ""
    Jan 18 22:59:40.214: INFO: stdout: "true"
    Jan 18 22:59:40.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-fpn6z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:59:40.278: INFO: stderr: ""
    Jan 18 22:59:40.278: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:59:40.278: INFO: validating pod update-demo-nautilus-fpn6z
    Jan 18 22:59:40.282: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:59:40.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:59:40.282: INFO: update-demo-nautilus-fpn6z is verified up and running
    Jan 18 22:59:40.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-m5lxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:59:40.353: INFO: stderr: ""
    Jan 18 22:59:40.353: INFO: stdout: "true"
    Jan 18 22:59:40.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods update-demo-nautilus-m5lxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:59:40.421: INFO: stderr: ""
    Jan 18 22:59:40.421: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:59:40.421: INFO: validating pod update-demo-nautilus-m5lxk
    Jan 18 22:59:40.427: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:59:40.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:59:40.427: INFO: update-demo-nautilus-m5lxk is verified up and running
    STEP: using delete to clean up resources 01/18/23 22:59:40.427
    Jan 18 22:59:40.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 delete --grace-period=0 --force -f -'
    Jan 18 22:59:40.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:59:40.495: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 22:59:40.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get rc,svc -l name=update-demo --no-headers'
    Jan 18 22:59:40.567: INFO: stderr: "No resources found in kubectl-6182 namespace.\n"
    Jan 18 22:59:40.567: INFO: stdout: ""
    Jan 18 22:59:40.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6182 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 22:59:40.634: INFO: stderr: ""
    Jan 18 22:59:40.634: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:59:40.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6182" for this suite. 01/18/23 22:59:40.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:59:40.643
Jan 18 22:59:40.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 22:59:40.644
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:40.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:40.659
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/18/23 22:59:40.662
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:59:40.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1851" for this suite. 01/18/23 22:59:40.667
------------------------------
â€¢ [0.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:59:40.643
    Jan 18 22:59:40.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 22:59:40.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:40.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:40.659
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/18/23 22:59:40.662
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:59:40.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1851" for this suite. 01/18/23 22:59:40.667
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:59:40.672
Jan 18 22:59:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 22:59:40.673
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:40.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:40.686
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/18/23 22:59:40.689
STEP: Ensuring job reaches completions 01/18/23 22:59:40.696
STEP: Ensuring pods with index for job exist 01/18/23 22:59:50.7
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:59:50.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6163" for this suite. 01/18/23 22:59:50.706
------------------------------
â€¢ [SLOW TEST] [10.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:59:40.672
    Jan 18 22:59:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 22:59:40.673
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:40.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:40.686
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/18/23 22:59:40.689
    STEP: Ensuring job reaches completions 01/18/23 22:59:40.696
    STEP: Ensuring pods with index for job exist 01/18/23 22:59:50.7
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:59:50.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6163" for this suite. 01/18/23 22:59:50.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:59:50.712
Jan 18 22:59:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 22:59:50.713
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:50.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:50.726
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8951 01/18/23 22:59:50.729
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/18/23 22:59:50.735
Jan 18 22:59:50.743: INFO: Found 0 stateful pods, waiting for 3
Jan 18 23:00:00.747: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:00:00.747: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:00:00.747: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 23:00:00.754
Jan 18 23:00:00.773: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 23:00:00.773
STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 23:00:10.788
STEP: Performing a canary update 01/18/23 23:00:10.788
Jan 18 23:00:10.809: INFO: Updating stateful set ss2
Jan 18 23:00:10.817: INFO: Waiting for Pod statefulset-8951/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 23:00:20.824
Jan 18 23:00:20.853: INFO: Found 2 stateful pods, waiting for 3
Jan 18 23:00:30.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:00:30.858: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:00:30.858: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/18/23 23:00:30.863
Jan 18 23:00:30.882: INFO: Updating stateful set ss2
Jan 18 23:00:30.890: INFO: Waiting for Pod statefulset-8951/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 18 23:00:40.918: INFO: Updating stateful set ss2
Jan 18 23:00:40.927: INFO: Waiting for StatefulSet statefulset-8951/ss2 to complete update
Jan 18 23:00:40.927: INFO: Waiting for Pod statefulset-8951/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 23:00:50.937: INFO: Deleting all statefulset in ns statefulset-8951
Jan 18 23:00:50.939: INFO: Scaling statefulset ss2 to 0
Jan 18 23:01:00.956: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:01:00.959: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8951" for this suite. 01/18/23 23:01:00.971
------------------------------
â€¢ [SLOW TEST] [70.264 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:59:50.712
    Jan 18 22:59:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:59:50.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:59:50.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:59:50.726
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8951 01/18/23 22:59:50.729
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/18/23 22:59:50.735
    Jan 18 22:59:50.743: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 23:00:00.747: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:00:00.747: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:00:00.747: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 23:00:00.754
    Jan 18 23:00:00.773: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 23:00:00.773
    STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 23:00:10.788
    STEP: Performing a canary update 01/18/23 23:00:10.788
    Jan 18 23:00:10.809: INFO: Updating stateful set ss2
    Jan 18 23:00:10.817: INFO: Waiting for Pod statefulset-8951/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 23:00:20.824
    Jan 18 23:00:20.853: INFO: Found 2 stateful pods, waiting for 3
    Jan 18 23:00:30.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:00:30.858: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:00:30.858: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/18/23 23:00:30.863
    Jan 18 23:00:30.882: INFO: Updating stateful set ss2
    Jan 18 23:00:30.890: INFO: Waiting for Pod statefulset-8951/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 18 23:00:40.918: INFO: Updating stateful set ss2
    Jan 18 23:00:40.927: INFO: Waiting for StatefulSet statefulset-8951/ss2 to complete update
    Jan 18 23:00:40.927: INFO: Waiting for Pod statefulset-8951/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 23:00:50.937: INFO: Deleting all statefulset in ns statefulset-8951
    Jan 18 23:00:50.939: INFO: Scaling statefulset ss2 to 0
    Jan 18 23:01:00.956: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:01:00.959: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8951" for this suite. 01/18/23 23:01:00.971
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:00.977
Jan 18 23:01:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:01:00.978
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:00.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:00.993
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/18/23 23:01:01.01
Jan 18 23:01:01.028: INFO: Waiting up to 5m0s for pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1" in namespace "svcaccounts-1639" to be "Succeeded or Failed"
Jan 18 23:01:01.031: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.62983ms
Jan 18 23:01:03.034: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006211508s
Jan 18 23:01:05.035: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006847869s
STEP: Saw pod success 01/18/23 23:01:05.035
Jan 18 23:01:05.035: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1" satisfied condition "Succeeded or Failed"
Jan 18 23:01:05.037: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:01:05.054
Jan 18 23:01:05.065: INFO: Waiting for pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 to disappear
Jan 18 23:01:05.068: INFO: Pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:05.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1639" for this suite. 01/18/23 23:01:05.071
------------------------------
â€¢ [4.099 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:00.977
    Jan 18 23:01:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:01:00.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:00.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:00.993
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/18/23 23:01:01.01
    Jan 18 23:01:01.028: INFO: Waiting up to 5m0s for pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1" in namespace "svcaccounts-1639" to be "Succeeded or Failed"
    Jan 18 23:01:01.031: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.62983ms
    Jan 18 23:01:03.034: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006211508s
    Jan 18 23:01:05.035: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006847869s
    STEP: Saw pod success 01/18/23 23:01:05.035
    Jan 18 23:01:05.035: INFO: Pod "test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:05.037: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:01:05.054
    Jan 18 23:01:05.065: INFO: Waiting for pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 to disappear
    Jan 18 23:01:05.068: INFO: Pod test-pod-348ffaa6-a694-4b28-ba94-102b1eb108e1 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:05.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1639" for this suite. 01/18/23 23:01:05.071
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:05.076
Jan 18 23:01:05.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename containers 01/18/23 23:01:05.077
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:05.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:05.092
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/18/23 23:01:05.094
Jan 18 23:01:05.101: INFO: Waiting up to 5m0s for pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5" in namespace "containers-6532" to be "Succeeded or Failed"
Jan 18 23:01:05.104: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335172ms
Jan 18 23:01:07.108: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006727045s
Jan 18 23:01:09.107: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006048911s
STEP: Saw pod success 01/18/23 23:01:09.107
Jan 18 23:01:09.108: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5" satisfied condition "Succeeded or Failed"
Jan 18 23:01:09.110: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:01:09.115
Jan 18 23:01:09.126: INFO: Waiting for pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 to disappear
Jan 18 23:01:09.128: INFO: Pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6532" for this suite. 01/18/23 23:01:09.131
------------------------------
â€¢ [4.060 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:05.076
    Jan 18 23:01:05.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename containers 01/18/23 23:01:05.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:05.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:05.092
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/18/23 23:01:05.094
    Jan 18 23:01:05.101: INFO: Waiting up to 5m0s for pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5" in namespace "containers-6532" to be "Succeeded or Failed"
    Jan 18 23:01:05.104: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335172ms
    Jan 18 23:01:07.108: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006727045s
    Jan 18 23:01:09.107: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006048911s
    STEP: Saw pod success 01/18/23 23:01:09.107
    Jan 18 23:01:09.108: INFO: Pod "client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:09.110: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:01:09.115
    Jan 18 23:01:09.126: INFO: Waiting for pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 to disappear
    Jan 18 23:01:09.128: INFO: Pod client-containers-2edc7f5b-9175-4030-ad03-c53f231f48f5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6532" for this suite. 01/18/23 23:01:09.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:09.137
Jan 18 23:01:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:01:09.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.153
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:09.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9601" for this suite. 01/18/23 23:01:09.188
------------------------------
â€¢ [0.057 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:09.137
    Jan 18 23:01:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:01:09.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.153
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:09.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9601" for this suite. 01/18/23 23:01:09.188
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:09.194
Jan 18 23:01:09.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:01:09.195
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.21
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/18/23 23:01:09.213
Jan 18 23:01:09.213: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-1927 proxy --unix-socket=/tmp/kubectl-proxy-unix1827579098/test'
STEP: retrieving proxy /api/ output 01/18/23 23:01:09.27
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1927" for this suite. 01/18/23 23:01:09.275
------------------------------
â€¢ [0.086 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:09.194
    Jan 18 23:01:09.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:01:09.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.21
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/18/23 23:01:09.213
    Jan 18 23:01:09.213: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-1927 proxy --unix-socket=/tmp/kubectl-proxy-unix1827579098/test'
    STEP: retrieving proxy /api/ output 01/18/23 23:01:09.27
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1927" for this suite. 01/18/23 23:01:09.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:09.281
Jan 18 23:01:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:01:09.282
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.298
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:01:09.314
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:10.333
STEP: Deploying the webhook pod 01/18/23 23:01:10.34
STEP: Wait for the deployment to be ready 01/18/23 23:01:10.35
Jan 18 23:01:10.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:01:12.367
STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:12.374
Jan 18 23:01:13.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/18/23 23:01:13.378
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.396
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 23:01:13.405
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.415
STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 23:01:13.424
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.431
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7461" for this suite. 01/18/23 23:01:13.467
STEP: Destroying namespace "webhook-7461-markers" for this suite. 01/18/23 23:01:13.475
------------------------------
â€¢ [4.199 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:09.281
    Jan 18 23:01:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:01:09.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:09.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:09.298
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:01:09.314
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:10.333
    STEP: Deploying the webhook pod 01/18/23 23:01:10.34
    STEP: Wait for the deployment to be ready 01/18/23 23:01:10.35
    Jan 18 23:01:10.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:01:12.367
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:12.374
    Jan 18 23:01:13.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/18/23 23:01:13.378
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.396
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 23:01:13.405
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.415
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 23:01:13.424
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 23:01:13.431
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7461" for this suite. 01/18/23 23:01:13.467
    STEP: Destroying namespace "webhook-7461-markers" for this suite. 01/18/23 23:01:13.475
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:13.48
Jan 18 23:01:13.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:01:13.481
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:13.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:13.496
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:01:13.511
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:13.953
STEP: Deploying the webhook pod 01/18/23 23:01:13.957
STEP: Wait for the deployment to be ready 01/18/23 23:01:13.969
Jan 18 23:01:13.974: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:01:15.982
STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:15.99
Jan 18 23:01:16.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 18 23:01:16.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3439-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:01:17.505
STEP: Creating a custom resource while v1 is storage version 01/18/23 23:01:17.525
STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 23:01:19.579
STEP: Patching the custom resource while v2 is storage version 01/18/23 23:01:19.62
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:20.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1293" for this suite. 01/18/23 23:01:20.219
STEP: Destroying namespace "webhook-1293-markers" for this suite. 01/18/23 23:01:20.226
------------------------------
â€¢ [SLOW TEST] [6.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:13.48
    Jan 18 23:01:13.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:01:13.481
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:13.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:13.496
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:01:13.511
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:13.953
    STEP: Deploying the webhook pod 01/18/23 23:01:13.957
    STEP: Wait for the deployment to be ready 01/18/23 23:01:13.969
    Jan 18 23:01:13.974: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:01:15.982
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:15.99
    Jan 18 23:01:16.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 18 23:01:16.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3439-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:01:17.505
    STEP: Creating a custom resource while v1 is storage version 01/18/23 23:01:17.525
    STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 23:01:19.579
    STEP: Patching the custom resource while v2 is storage version 01/18/23 23:01:19.62
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:20.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1293" for this suite. 01/18/23 23:01:20.219
    STEP: Destroying namespace "webhook-1293-markers" for this suite. 01/18/23 23:01:20.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:20.237
Jan 18 23:01:20.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename conformance-tests 01/18/23 23:01:20.238
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:20.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:20.255
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/18/23 23:01:20.258
Jan 18 23:01:20.258: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9379" for this suite. 01/18/23 23:01:20.267
------------------------------
â€¢ [0.035 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:20.237
    Jan 18 23:01:20.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename conformance-tests 01/18/23 23:01:20.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:20.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:20.255
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/18/23 23:01:20.258
    Jan 18 23:01:20.258: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9379" for this suite. 01/18/23 23:01:20.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:20.273
Jan 18 23:01:20.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:01:20.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:20.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:20.287
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-d733a911-d398-43b9-b701-ddcc413940ea 01/18/23 23:01:20.293
STEP: Creating configMap with name cm-test-opt-upd-d2e0d55b-b175-4350-8afb-85ea9d7e3ace 01/18/23 23:01:20.296
STEP: Creating the pod 01/18/23 23:01:20.3
Jan 18 23:01:20.310: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03" in namespace "projected-1934" to be "running and ready"
Jan 18 23:01:20.313: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942288ms
Jan 18 23:01:20.313: INFO: The phase of Pod pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:01:22.317: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03": Phase="Running", Reason="", readiness=true. Elapsed: 2.006597889s
Jan 18 23:01:22.317: INFO: The phase of Pod pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03 is Running (Ready = true)
Jan 18 23:01:22.317: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d733a911-d398-43b9-b701-ddcc413940ea 01/18/23 23:01:22.333
STEP: Updating configmap cm-test-opt-upd-d2e0d55b-b175-4350-8afb-85ea9d7e3ace 01/18/23 23:01:22.338
STEP: Creating configMap with name cm-test-opt-create-af95546b-1965-42c2-ac28-ce5d3969728a 01/18/23 23:01:22.342
STEP: waiting to observe update in volume 01/18/23 23:01:22.347
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:24.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1934" for this suite. 01/18/23 23:01:24.373
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:20.273
    Jan 18 23:01:20.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:01:20.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:20.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:20.287
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-d733a911-d398-43b9-b701-ddcc413940ea 01/18/23 23:01:20.293
    STEP: Creating configMap with name cm-test-opt-upd-d2e0d55b-b175-4350-8afb-85ea9d7e3ace 01/18/23 23:01:20.296
    STEP: Creating the pod 01/18/23 23:01:20.3
    Jan 18 23:01:20.310: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03" in namespace "projected-1934" to be "running and ready"
    Jan 18 23:01:20.313: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942288ms
    Jan 18 23:01:20.313: INFO: The phase of Pod pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:01:22.317: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03": Phase="Running", Reason="", readiness=true. Elapsed: 2.006597889s
    Jan 18 23:01:22.317: INFO: The phase of Pod pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03 is Running (Ready = true)
    Jan 18 23:01:22.317: INFO: Pod "pod-projected-configmaps-1befc139-5ee1-4670-bf9e-b51d5a083a03" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d733a911-d398-43b9-b701-ddcc413940ea 01/18/23 23:01:22.333
    STEP: Updating configmap cm-test-opt-upd-d2e0d55b-b175-4350-8afb-85ea9d7e3ace 01/18/23 23:01:22.338
    STEP: Creating configMap with name cm-test-opt-create-af95546b-1965-42c2-ac28-ce5d3969728a 01/18/23 23:01:22.342
    STEP: waiting to observe update in volume 01/18/23 23:01:22.347
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:24.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1934" for this suite. 01/18/23 23:01:24.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:24.38
Jan 18 23:01:24.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 23:01:24.381
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:24.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:24.398
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 18 23:01:24.411: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9919 to be scheduled
Jan 18 23:01:24.414: INFO: 1 pods are not scheduled: [runtimeclass-9919/test-runtimeclass-runtimeclass-9919-preconfigured-handler-pzwf7(7c1381ee-035e-444f-a519-28237969fb39)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9919" for this suite. 01/18/23 23:01:26.427
------------------------------
â€¢ [2.054 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:24.38
    Jan 18 23:01:24.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 23:01:24.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:24.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:24.398
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 18 23:01:24.411: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9919 to be scheduled
    Jan 18 23:01:24.414: INFO: 1 pods are not scheduled: [runtimeclass-9919/test-runtimeclass-runtimeclass-9919-preconfigured-handler-pzwf7(7c1381ee-035e-444f-a519-28237969fb39)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9919" for this suite. 01/18/23 23:01:26.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:26.436
Jan 18 23:01:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:01:26.437
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:26.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:26.449
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-f03e476f-ffad-495a-adac-0d360f602f1b 01/18/23 23:01:26.452
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:26.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7105" for this suite. 01/18/23 23:01:26.456
------------------------------
â€¢ [0.027 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:26.436
    Jan 18 23:01:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:01:26.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:26.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:26.449
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-f03e476f-ffad-495a-adac-0d360f602f1b 01/18/23 23:01:26.452
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:26.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7105" for this suite. 01/18/23 23:01:26.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:26.463
Jan 18 23:01:26.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename proxy 01/18/23 23:01:26.464
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:26.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:26.476
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 18 23:01:26.479: INFO: Creating pod...
Jan 18 23:01:26.487: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1815" to be "running"
Jan 18 23:01:26.490: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337059ms
Jan 18 23:01:28.493: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564533s
Jan 18 23:01:28.493: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 23:01:28.493: INFO: Creating service...
Jan 18 23:01:28.500: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 23:01:28.506: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:01:28.506: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/GET
Jan 18 23:01:28.509: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 23:01:28.509: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 23:01:28.511: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 23:01:28.511: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 23:01:28.514: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:01:28.514: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 23:01:28.518: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:01:28.518: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/POST
Jan 18 23:01:28.522: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:01:28.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/PUT
Jan 18 23:01:28.525: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:01:28.525: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/DELETE
Jan 18 23:01:28.528: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:01:28.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/GET
Jan 18 23:01:28.533: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 23:01:28.533: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/HEAD
Jan 18 23:01:28.537: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 23:01:28.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 23:01:28.540: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:01:28.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/PATCH
Jan 18 23:01:28.544: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:01:28.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/POST
Jan 18 23:01:28.547: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:01:28.547: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/PUT
Jan 18 23:01:28.551: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:28.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1815" for this suite. 01/18/23 23:01:28.554
------------------------------
â€¢ [2.098 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:26.463
    Jan 18 23:01:26.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename proxy 01/18/23 23:01:26.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:26.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:26.476
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 18 23:01:26.479: INFO: Creating pod...
    Jan 18 23:01:26.487: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1815" to be "running"
    Jan 18 23:01:26.490: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337059ms
    Jan 18 23:01:28.493: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564533s
    Jan 18 23:01:28.493: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 23:01:28.493: INFO: Creating service...
    Jan 18 23:01:28.500: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/DELETE
    Jan 18 23:01:28.506: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 23:01:28.506: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/GET
    Jan 18 23:01:28.509: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 23:01:28.509: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/HEAD
    Jan 18 23:01:28.511: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 23:01:28.511: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 18 23:01:28.514: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 23:01:28.514: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/PATCH
    Jan 18 23:01:28.518: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 23:01:28.518: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/POST
    Jan 18 23:01:28.522: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 23:01:28.523: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/pods/agnhost/proxy/some/path/with/PUT
    Jan 18 23:01:28.525: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 23:01:28.525: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/DELETE
    Jan 18 23:01:28.528: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 23:01:28.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/GET
    Jan 18 23:01:28.533: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 23:01:28.533: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/HEAD
    Jan 18 23:01:28.537: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 23:01:28.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/OPTIONS
    Jan 18 23:01:28.540: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 23:01:28.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/PATCH
    Jan 18 23:01:28.544: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 23:01:28.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/POST
    Jan 18 23:01:28.547: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 23:01:28.547: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1815/services/test-service/proxy/some/path/with/PUT
    Jan 18 23:01:28.551: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:28.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1815" for this suite. 01/18/23 23:01:28.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:28.562
Jan 18 23:01:28.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:01:28.563
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:28.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:28.576
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 23:01:28.579
Jan 18 23:01:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 23:01:34.266
Jan 18 23:01:34.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:01:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:41.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4109" for this suite. 01/18/23 23:01:41.329
------------------------------
â€¢ [SLOW TEST] [12.772 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:28.562
    Jan 18 23:01:28.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:01:28.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:28.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:28.576
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 23:01:28.579
    Jan 18 23:01:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 23:01:34.266
    Jan 18 23:01:34.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:01:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:41.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4109" for this suite. 01/18/23 23:01:41.329
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:41.335
Jan 18 23:01:41.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename containers 01/18/23 23:01:41.336
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:41.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:41.351
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/18/23 23:01:41.353
Jan 18 23:01:41.362: INFO: Waiting up to 5m0s for pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06" in namespace "containers-3064" to be "Succeeded or Failed"
Jan 18 23:01:41.365: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.628072ms
Jan 18 23:01:43.368: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040436s
Jan 18 23:01:45.369: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007265127s
STEP: Saw pod success 01/18/23 23:01:45.369
Jan 18 23:01:45.369: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06" satisfied condition "Succeeded or Failed"
Jan 18 23:01:45.372: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:01:45.377
Jan 18 23:01:45.387: INFO: Waiting for pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 to disappear
Jan 18 23:01:45.389: INFO: Pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:45.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3064" for this suite. 01/18/23 23:01:45.392
------------------------------
â€¢ [4.062 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:41.335
    Jan 18 23:01:41.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename containers 01/18/23 23:01:41.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:41.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:41.351
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/18/23 23:01:41.353
    Jan 18 23:01:41.362: INFO: Waiting up to 5m0s for pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06" in namespace "containers-3064" to be "Succeeded or Failed"
    Jan 18 23:01:41.365: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.628072ms
    Jan 18 23:01:43.368: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040436s
    Jan 18 23:01:45.369: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007265127s
    STEP: Saw pod success 01/18/23 23:01:45.369
    Jan 18 23:01:45.369: INFO: Pod "client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:45.372: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:01:45.377
    Jan 18 23:01:45.387: INFO: Waiting for pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 to disappear
    Jan 18 23:01:45.389: INFO: Pod client-containers-77e3bc83-f0fc-4202-8273-c675e1883b06 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:45.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3064" for this suite. 01/18/23 23:01:45.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:45.397
Jan 18 23:01:45.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 23:01:45.398
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:45.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:45.412
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/18/23 23:01:45.417
STEP: waiting for RC to be added 01/18/23 23:01:45.421
STEP: waiting for available Replicas 01/18/23 23:01:45.421
STEP: patching ReplicationController 01/18/23 23:01:46.322
STEP: waiting for RC to be modified 01/18/23 23:01:46.329
STEP: patching ReplicationController status 01/18/23 23:01:46.329
STEP: waiting for RC to be modified 01/18/23 23:01:46.335
STEP: waiting for available Replicas 01/18/23 23:01:46.336
STEP: fetching ReplicationController status 01/18/23 23:01:46.34
STEP: patching ReplicationController scale 01/18/23 23:01:46.343
STEP: waiting for RC to be modified 01/18/23 23:01:46.351
STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 23:01:46.351
STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 23:01:47.201
STEP: updating ReplicationController status 01/18/23 23:01:47.203
STEP: waiting for RC to be modified 01/18/23 23:01:47.209
STEP: listing all ReplicationControllers 01/18/23 23:01:47.209
STEP: checking that ReplicationController has expected values 01/18/23 23:01:47.211
STEP: deleting ReplicationControllers by collection 01/18/23 23:01:47.211
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 23:01:47.218
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:47.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5724" for this suite. 01/18/23 23:01:47.27
------------------------------
â€¢ [1.877 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:45.397
    Jan 18 23:01:45.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 23:01:45.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:45.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:45.412
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/18/23 23:01:45.417
    STEP: waiting for RC to be added 01/18/23 23:01:45.421
    STEP: waiting for available Replicas 01/18/23 23:01:45.421
    STEP: patching ReplicationController 01/18/23 23:01:46.322
    STEP: waiting for RC to be modified 01/18/23 23:01:46.329
    STEP: patching ReplicationController status 01/18/23 23:01:46.329
    STEP: waiting for RC to be modified 01/18/23 23:01:46.335
    STEP: waiting for available Replicas 01/18/23 23:01:46.336
    STEP: fetching ReplicationController status 01/18/23 23:01:46.34
    STEP: patching ReplicationController scale 01/18/23 23:01:46.343
    STEP: waiting for RC to be modified 01/18/23 23:01:46.351
    STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 23:01:46.351
    STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 23:01:47.201
    STEP: updating ReplicationController status 01/18/23 23:01:47.203
    STEP: waiting for RC to be modified 01/18/23 23:01:47.209
    STEP: listing all ReplicationControllers 01/18/23 23:01:47.209
    STEP: checking that ReplicationController has expected values 01/18/23 23:01:47.211
    STEP: deleting ReplicationControllers by collection 01/18/23 23:01:47.211
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 23:01:47.218
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:47.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5724" for this suite. 01/18/23 23:01:47.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:47.28
Jan 18 23:01:47.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:01:47.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:47.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:47.296
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:01:47.309
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:47.851
STEP: Deploying the webhook pod 01/18/23 23:01:47.858
STEP: Wait for the deployment to be ready 01/18/23 23:01:47.872
Jan 18 23:01:47.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:01:49.887
STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:49.894
Jan 18 23:01:50.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 18 23:01:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6274-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:01:51.411
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 23:01:51.429
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8366" for this suite. 01/18/23 23:01:54.029
STEP: Destroying namespace "webhook-8366-markers" for this suite. 01/18/23 23:01:54.033
------------------------------
â€¢ [SLOW TEST] [6.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:47.28
    Jan 18 23:01:47.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:01:47.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:47.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:47.296
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:01:47.309
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:01:47.851
    STEP: Deploying the webhook pod 01/18/23 23:01:47.858
    STEP: Wait for the deployment to be ready 01/18/23 23:01:47.872
    Jan 18 23:01:47.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:01:49.887
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:01:49.894
    Jan 18 23:01:50.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 18 23:01:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6274-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:01:51.411
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 23:01:51.429
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8366" for this suite. 01/18/23 23:01:54.029
    STEP: Destroying namespace "webhook-8366-markers" for this suite. 01/18/23 23:01:54.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:54.04
Jan 18 23:01:54.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:01:54.041
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:54.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:54.054
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-18a1d4e3-73fd-4f02-9433-33ebcd15cfff 01/18/23 23:01:54.057
STEP: Creating a pod to test consume configMaps 01/18/23 23:01:54.063
Jan 18 23:01:54.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb" in namespace "configmap-1203" to be "Succeeded or Failed"
Jan 18 23:01:54.073: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.31444ms
Jan 18 23:01:56.077: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00602539s
Jan 18 23:01:58.076: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005958428s
STEP: Saw pod success 01/18/23 23:01:58.077
Jan 18 23:01:58.077: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb" satisfied condition "Succeeded or Failed"
Jan 18 23:01:58.079: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:01:58.084
Jan 18 23:01:58.096: INFO: Waiting for pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb to disappear
Jan 18 23:01:58.098: INFO: Pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1203" for this suite. 01/18/23 23:01:58.101
------------------------------
â€¢ [4.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:54.04
    Jan 18 23:01:54.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:01:54.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:54.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:54.054
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-18a1d4e3-73fd-4f02-9433-33ebcd15cfff 01/18/23 23:01:54.057
    STEP: Creating a pod to test consume configMaps 01/18/23 23:01:54.063
    Jan 18 23:01:54.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb" in namespace "configmap-1203" to be "Succeeded or Failed"
    Jan 18 23:01:54.073: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.31444ms
    Jan 18 23:01:56.077: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00602539s
    Jan 18 23:01:58.076: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005958428s
    STEP: Saw pod success 01/18/23 23:01:58.077
    Jan 18 23:01:58.077: INFO: Pod "pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:58.079: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:01:58.084
    Jan 18 23:01:58.096: INFO: Waiting for pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb to disappear
    Jan 18 23:01:58.098: INFO: Pod pod-configmaps-974d302f-6ddb-40d2-992b-7408a4f948cb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1203" for this suite. 01/18/23 23:01:58.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:58.109
Jan 18 23:01:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename certificates 01/18/23 23:01:58.11
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:58.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:58.124
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/18/23 23:01:58.967
STEP: getting /apis/certificates.k8s.io 01/18/23 23:01:58.97
STEP: getting /apis/certificates.k8s.io/v1 01/18/23 23:01:58.971
STEP: creating 01/18/23 23:01:58.973
STEP: getting 01/18/23 23:01:58.99
STEP: listing 01/18/23 23:01:58.992
STEP: watching 01/18/23 23:01:58.994
Jan 18 23:01:58.995: INFO: starting watch
STEP: patching 01/18/23 23:01:58.996
STEP: updating 01/18/23 23:01:59.001
Jan 18 23:01:59.008: INFO: waiting for watch events with expected annotations
Jan 18 23:01:59.008: INFO: saw patched and updated annotations
STEP: getting /approval 01/18/23 23:01:59.009
STEP: patching /approval 01/18/23 23:01:59.011
STEP: updating /approval 01/18/23 23:01:59.016
STEP: getting /status 01/18/23 23:01:59.023
STEP: patching /status 01/18/23 23:01:59.025
STEP: updating /status 01/18/23 23:01:59.032
STEP: deleting 01/18/23 23:01:59.039
STEP: deleting a collection 01/18/23 23:01:59.05
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:59.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3711" for this suite. 01/18/23 23:01:59.065
------------------------------
â€¢ [0.960 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:58.109
    Jan 18 23:01:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename certificates 01/18/23 23:01:58.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:58.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:58.124
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/18/23 23:01:58.967
    STEP: getting /apis/certificates.k8s.io 01/18/23 23:01:58.97
    STEP: getting /apis/certificates.k8s.io/v1 01/18/23 23:01:58.971
    STEP: creating 01/18/23 23:01:58.973
    STEP: getting 01/18/23 23:01:58.99
    STEP: listing 01/18/23 23:01:58.992
    STEP: watching 01/18/23 23:01:58.994
    Jan 18 23:01:58.995: INFO: starting watch
    STEP: patching 01/18/23 23:01:58.996
    STEP: updating 01/18/23 23:01:59.001
    Jan 18 23:01:59.008: INFO: waiting for watch events with expected annotations
    Jan 18 23:01:59.008: INFO: saw patched and updated annotations
    STEP: getting /approval 01/18/23 23:01:59.009
    STEP: patching /approval 01/18/23 23:01:59.011
    STEP: updating /approval 01/18/23 23:01:59.016
    STEP: getting /status 01/18/23 23:01:59.023
    STEP: patching /status 01/18/23 23:01:59.025
    STEP: updating /status 01/18/23 23:01:59.032
    STEP: deleting 01/18/23 23:01:59.039
    STEP: deleting a collection 01/18/23 23:01:59.05
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:59.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3711" for this suite. 01/18/23 23:01:59.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:59.07
Jan 18 23:01:59.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:01:59.071
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:59.084
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:01:59.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2220" for this suite. 01/18/23 23:01:59.12
------------------------------
â€¢ [0.054 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:59.07
    Jan 18 23:01:59.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:01:59.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:59.084
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:01:59.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2220" for this suite. 01/18/23 23:01:59.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:01:59.125
Jan 18 23:01:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename namespaces 01/18/23 23:01:59.126
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:59.141
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/18/23 23:01:59.144
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.156
STEP: Creating a pod in the namespace 01/18/23 23:01:59.159
STEP: Waiting for the pod to have running status 01/18/23 23:01:59.165
Jan 18 23:01:59.165: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8236" to be "running"
Jan 18 23:01:59.167: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028382ms
Jan 18 23:02:01.170: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005260904s
Jan 18 23:02:01.170: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/18/23 23:02:01.171
STEP: Waiting for the namespace to be removed. 01/18/23 23:02:01.175
STEP: Recreating the namespace 01/18/23 23:02:12.179
STEP: Verifying there are no pods in the namespace 01/18/23 23:02:12.193
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:02:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4845" for this suite. 01/18/23 23:02:12.197
STEP: Destroying namespace "nsdeletetest-8236" for this suite. 01/18/23 23:02:12.202
Jan 18 23:02:12.205: INFO: Namespace nsdeletetest-8236 was already deleted
STEP: Destroying namespace "nsdeletetest-8127" for this suite. 01/18/23 23:02:12.205
------------------------------
â€¢ [SLOW TEST] [13.086 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:01:59.125
    Jan 18 23:01:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename namespaces 01/18/23 23:01:59.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:59.141
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/18/23 23:01:59.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:59.156
    STEP: Creating a pod in the namespace 01/18/23 23:01:59.159
    STEP: Waiting for the pod to have running status 01/18/23 23:01:59.165
    Jan 18 23:01:59.165: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8236" to be "running"
    Jan 18 23:01:59.167: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028382ms
    Jan 18 23:02:01.170: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005260904s
    Jan 18 23:02:01.170: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/18/23 23:02:01.171
    STEP: Waiting for the namespace to be removed. 01/18/23 23:02:01.175
    STEP: Recreating the namespace 01/18/23 23:02:12.179
    STEP: Verifying there are no pods in the namespace 01/18/23 23:02:12.193
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:02:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4845" for this suite. 01/18/23 23:02:12.197
    STEP: Destroying namespace "nsdeletetest-8236" for this suite. 01/18/23 23:02:12.202
    Jan 18 23:02:12.205: INFO: Namespace nsdeletetest-8236 was already deleted
    STEP: Destroying namespace "nsdeletetest-8127" for this suite. 01/18/23 23:02:12.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:02:12.211
Jan 18 23:02:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:02:12.212
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:12.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:12.225
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/18/23 23:02:12.228
STEP: Counting existing ResourceQuota 01/18/23 23:02:17.231
STEP: Creating a ResourceQuota 01/18/23 23:02:22.234
STEP: Ensuring resource quota status is calculated 01/18/23 23:02:22.242
STEP: Creating a Secret 01/18/23 23:02:24.246
STEP: Ensuring resource quota status captures secret creation 01/18/23 23:02:24.258
STEP: Deleting a secret 01/18/23 23:02:26.262
STEP: Ensuring resource quota status released usage 01/18/23 23:02:26.267
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 23:02:28.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7873" for this suite. 01/18/23 23:02:28.274
------------------------------
â€¢ [SLOW TEST] [16.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:02:12.211
    Jan 18 23:02:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:02:12.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:12.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:12.225
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/18/23 23:02:12.228
    STEP: Counting existing ResourceQuota 01/18/23 23:02:17.231
    STEP: Creating a ResourceQuota 01/18/23 23:02:22.234
    STEP: Ensuring resource quota status is calculated 01/18/23 23:02:22.242
    STEP: Creating a Secret 01/18/23 23:02:24.246
    STEP: Ensuring resource quota status captures secret creation 01/18/23 23:02:24.258
    STEP: Deleting a secret 01/18/23 23:02:26.262
    STEP: Ensuring resource quota status released usage 01/18/23 23:02:26.267
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:02:28.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7873" for this suite. 01/18/23 23:02:28.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:02:28.279
Jan 18 23:02:28.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-runtime 01/18/23 23:02:28.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:28.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:28.295
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 23:02:28.306
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 23:02:46.37
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 23:02:46.373
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 23:02:46.378
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 23:02:46.378
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 23:02:46.399
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 23:02:49.411
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 23:02:51.42
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 23:02:51.425
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 23:02:51.425
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 23:02:51.445
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 23:02:52.452
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 23:02:55.466
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 23:02:55.47
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 23:02:55.47
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 23:02:55.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3734" for this suite. 01/18/23 23:02:55.492
------------------------------
â€¢ [SLOW TEST] [27.218 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:02:28.279
    Jan 18 23:02:28.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 23:02:28.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:28.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:28.295
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 23:02:28.306
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 23:02:46.37
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 23:02:46.373
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 23:02:46.378
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 23:02:46.378
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 23:02:46.399
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 23:02:49.411
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 23:02:51.42
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 23:02:51.425
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 23:02:51.425
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 23:02:51.445
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 23:02:52.452
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 23:02:55.466
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 23:02:55.47
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 23:02:55.47
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:02:55.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3734" for this suite. 01/18/23 23:02:55.492
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:02:55.497
Jan 18 23:02:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename events 01/18/23 23:02:55.498
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:55.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:55.512
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/18/23 23:02:55.514
Jan 18 23:02:55.518: INFO: created test-event-1
Jan 18 23:02:55.522: INFO: created test-event-2
Jan 18 23:02:55.526: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/18/23 23:02:55.526
STEP: delete collection of events 01/18/23 23:02:55.529
Jan 18 23:02:55.529: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 23:02:55.541
Jan 18 23:02:55.541: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 18 23:02:55.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5204" for this suite. 01/18/23 23:02:55.547
------------------------------
â€¢ [0.056 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:02:55.497
    Jan 18 23:02:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename events 01/18/23 23:02:55.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:55.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:55.512
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/18/23 23:02:55.514
    Jan 18 23:02:55.518: INFO: created test-event-1
    Jan 18 23:02:55.522: INFO: created test-event-2
    Jan 18 23:02:55.526: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/18/23 23:02:55.526
    STEP: delete collection of events 01/18/23 23:02:55.529
    Jan 18 23:02:55.529: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 23:02:55.541
    Jan 18 23:02:55.541: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:02:55.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5204" for this suite. 01/18/23 23:02:55.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:02:55.554
Jan 18 23:02:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:02:55.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:55.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:55.568
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 23:02:55.571
Jan 18 23:02:55.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7047 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 18 23:02:55.649: INFO: stderr: ""
Jan 18 23:02:55.649: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 23:02:55.649
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 18 23:02:55.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7047 delete pods e2e-test-httpd-pod'
Jan 18 23:02:57.500: INFO: stderr: ""
Jan 18 23:02:57.500: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:02:57.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7047" for this suite. 01/18/23 23:02:57.503
------------------------------
â€¢ [1.955 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:02:55.554
    Jan 18 23:02:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:02:55.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:55.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:55.568
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 23:02:55.571
    Jan 18 23:02:55.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7047 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 18 23:02:55.649: INFO: stderr: ""
    Jan 18 23:02:55.649: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 23:02:55.649
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 18 23:02:55.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-7047 delete pods e2e-test-httpd-pod'
    Jan 18 23:02:57.500: INFO: stderr: ""
    Jan 18 23:02:57.500: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:02:57.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7047" for this suite. 01/18/23 23:02:57.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:02:57.509
Jan 18 23:02:57.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:02:57.51
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:57.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:57.525
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-0c5bd17b-960a-425a-bd54-79191fea2bf0 01/18/23 23:02:57.528
STEP: Creating a pod to test consume secrets 01/18/23 23:02:57.532
Jan 18 23:02:57.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2" in namespace "projected-820" to be "Succeeded or Failed"
Jan 18 23:02:57.543: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244738ms
Jan 18 23:02:59.546: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005785769s
Jan 18 23:03:01.547: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006111492s
STEP: Saw pod success 01/18/23 23:03:01.547
Jan 18 23:03:01.547: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2" satisfied condition "Succeeded or Failed"
Jan 18 23:03:01.549: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:03:01.556
Jan 18 23:03:01.565: INFO: Waiting for pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 to disappear
Jan 18 23:03:01.567: INFO: Pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 23:03:01.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-820" for this suite. 01/18/23 23:03:01.57
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:02:57.509
    Jan 18 23:02:57.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:02:57.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:57.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:57.525
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-0c5bd17b-960a-425a-bd54-79191fea2bf0 01/18/23 23:02:57.528
    STEP: Creating a pod to test consume secrets 01/18/23 23:02:57.532
    Jan 18 23:02:57.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2" in namespace "projected-820" to be "Succeeded or Failed"
    Jan 18 23:02:57.543: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244738ms
    Jan 18 23:02:59.546: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005785769s
    Jan 18 23:03:01.547: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006111492s
    STEP: Saw pod success 01/18/23 23:03:01.547
    Jan 18 23:03:01.547: INFO: Pod "pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2" satisfied condition "Succeeded or Failed"
    Jan 18 23:03:01.549: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:03:01.556
    Jan 18 23:03:01.565: INFO: Waiting for pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 to disappear
    Jan 18 23:03:01.567: INFO: Pod pod-projected-secrets-62a69aff-4d14-4db8-bf2a-7bbb627b98a2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:03:01.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-820" for this suite. 01/18/23 23:03:01.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:03:01.577
Jan 18 23:03:01.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:03:01.578
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:01.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:01.594
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-4lddx" 01/18/23 23:03:01.599
Jan 18 23:03:01.604: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard cpu limit of 500m
Jan 18 23:03:01.604: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.604
STEP: Confirm /status for "e2e-rq-status-4lddx" resourceQuota via watch 01/18/23 23:03:01.612
Jan 18 23:03:01.613: INFO: observed resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList(nil)
Jan 18 23:03:01.613: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 18 23:03:01.613: INFO: ResourceQuota "e2e-rq-status-4lddx" /status was updated
STEP: Patching hard spec values for cpu & memory 01/18/23 23:03:01.615
Jan 18 23:03:01.620: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard cpu limit of 1
Jan 18 23:03:01.620: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.62
STEP: Confirm /status for "e2e-rq-status-4lddx" resourceQuota via watch 01/18/23 23:03:01.625
Jan 18 23:03:01.627: INFO: observed resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 18 23:03:01.627: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 18 23:03:01.627: INFO: ResourceQuota "e2e-rq-status-4lddx" /status was patched
STEP: Get "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.627
Jan 18 23:03:01.630: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard cpu of 1
Jan 18 23:03:01.630: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-4lddx" /status before checking Spec is unchanged 01/18/23 23:03:01.632
Jan 18 23:03:01.638: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard cpu of 2
Jan 18 23:03:01.638: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard memory of 2Gi
Jan 18 23:03:01.639: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 18 23:06:51.647: INFO: ResourceQuota "e2e-rq-status-4lddx" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 23:06:51.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-489" for this suite. 01/18/23 23:06:51.65
------------------------------
â€¢ [SLOW TEST] [230.078 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:03:01.577
    Jan 18 23:03:01.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:03:01.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:01.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:01.594
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-4lddx" 01/18/23 23:03:01.599
    Jan 18 23:03:01.604: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard cpu limit of 500m
    Jan 18 23:03:01.604: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.604
    STEP: Confirm /status for "e2e-rq-status-4lddx" resourceQuota via watch 01/18/23 23:03:01.612
    Jan 18 23:03:01.613: INFO: observed resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList(nil)
    Jan 18 23:03:01.613: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 18 23:03:01.613: INFO: ResourceQuota "e2e-rq-status-4lddx" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/18/23 23:03:01.615
    Jan 18 23:03:01.620: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard cpu limit of 1
    Jan 18 23:03:01.620: INFO: Resource quota "e2e-rq-status-4lddx" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.62
    STEP: Confirm /status for "e2e-rq-status-4lddx" resourceQuota via watch 01/18/23 23:03:01.625
    Jan 18 23:03:01.627: INFO: observed resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 18 23:03:01.627: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 18 23:03:01.627: INFO: ResourceQuota "e2e-rq-status-4lddx" /status was patched
    STEP: Get "e2e-rq-status-4lddx" /status 01/18/23 23:03:01.627
    Jan 18 23:03:01.630: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard cpu of 1
    Jan 18 23:03:01.630: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-4lddx" /status before checking Spec is unchanged 01/18/23 23:03:01.632
    Jan 18 23:03:01.638: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard cpu of 2
    Jan 18 23:03:01.638: INFO: Resourcequota "e2e-rq-status-4lddx" reports status: hard memory of 2Gi
    Jan 18 23:03:01.639: INFO: Found resourceQuota "e2e-rq-status-4lddx" in namespace "resourcequota-489" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 18 23:06:51.647: INFO: ResourceQuota "e2e-rq-status-4lddx" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:06:51.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-489" for this suite. 01/18/23 23:06:51.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:06:51.657
Jan 18 23:06:51.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:06:51.658
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:51.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:51.671
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-f66111d2-8169-419b-a7a8-00205c8af53f 01/18/23 23:06:51.674
STEP: Creating a pod to test consume secrets 01/18/23 23:06:51.678
Jan 18 23:06:51.686: INFO: Waiting up to 5m0s for pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023" in namespace "secrets-5717" to be "Succeeded or Failed"
Jan 18 23:06:51.689: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911674ms
Jan 18 23:06:53.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006478267s
Jan 18 23:06:55.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006143546s
STEP: Saw pod success 01/18/23 23:06:55.692
Jan 18 23:06:55.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023" satisfied condition "Succeeded or Failed"
Jan 18 23:06:55.694: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:06:55.707
Jan 18 23:06:55.716: INFO: Waiting for pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 to disappear
Jan 18 23:06:55.719: INFO: Pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:06:55.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5717" for this suite. 01/18/23 23:06:55.721
------------------------------
â€¢ [4.069 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:06:51.657
    Jan 18 23:06:51.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:06:51.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:51.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:51.671
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-f66111d2-8169-419b-a7a8-00205c8af53f 01/18/23 23:06:51.674
    STEP: Creating a pod to test consume secrets 01/18/23 23:06:51.678
    Jan 18 23:06:51.686: INFO: Waiting up to 5m0s for pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023" in namespace "secrets-5717" to be "Succeeded or Failed"
    Jan 18 23:06:51.689: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911674ms
    Jan 18 23:06:53.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006478267s
    Jan 18 23:06:55.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006143546s
    STEP: Saw pod success 01/18/23 23:06:55.692
    Jan 18 23:06:55.692: INFO: Pod "pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023" satisfied condition "Succeeded or Failed"
    Jan 18 23:06:55.694: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:06:55.707
    Jan 18 23:06:55.716: INFO: Waiting for pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 to disappear
    Jan 18 23:06:55.719: INFO: Pod pod-secrets-717e8468-8c37-4cd8-a2bd-06f57454c023 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:06:55.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5717" for this suite. 01/18/23 23:06:55.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:06:55.728
Jan 18 23:06:55.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:06:55.729
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:55.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:55.742
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:06:55.756
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:06:56.303
STEP: Deploying the webhook pod 01/18/23 23:06:56.312
STEP: Wait for the deployment to be ready 01/18/23 23:06:56.323
Jan 18 23:06:56.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:06:58.34
STEP: Verifying the service has paired with the endpoint 01/18/23 23:06:58.348
Jan 18 23:06:59.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 18 23:06:59.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5983-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:06:59.862
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 23:06:59.879
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:07:02.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7002" for this suite. 01/18/23 23:07:02.463
STEP: Destroying namespace "webhook-7002-markers" for this suite. 01/18/23 23:07:02.468
------------------------------
â€¢ [SLOW TEST] [6.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:06:55.728
    Jan 18 23:06:55.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:06:55.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:55.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:55.742
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:06:55.756
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:06:56.303
    STEP: Deploying the webhook pod 01/18/23 23:06:56.312
    STEP: Wait for the deployment to be ready 01/18/23 23:06:56.323
    Jan 18 23:06:56.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:06:58.34
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:06:58.348
    Jan 18 23:06:59.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 18 23:06:59.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5983-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:06:59.862
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 23:06:59.879
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:07:02.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7002" for this suite. 01/18/23 23:07:02.463
    STEP: Destroying namespace "webhook-7002-markers" for this suite. 01/18/23 23:07:02.468
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:07:02.476
Jan 18 23:07:02.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-pred 01/18/23 23:07:02.477
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:02.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:02.495
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 23:07:02.499: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 23:07:02.504: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 23:07:02.507: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
Jan 18 23:07:02.512: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:07:02.512: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:07:02.512: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container etcd ready: true, restart count 0
Jan 18 23:07:02.512: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:07:02.512: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:07:02.512: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:07:02.512: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:07:02.512: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container weave ready: true, restart count 1
Jan 18 23:07:02.512: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:07:02.512: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:07:02.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:07:02.512: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:07:02.512: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
Jan 18 23:07:02.516: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.516: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:07:02.516: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
Jan 18 23:07:02.516: INFO: 	Container weave ready: true, restart count 0
Jan 18 23:07:02.516: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:07:02.516: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:07:02.516: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:07:02.516: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:07:02.516: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:07:02.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:07:02.516: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:07:02.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:07:02.516: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 23:07:02.516
Jan 18 23:07:02.525: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9052" to be "running"
Jan 18 23:07:02.527: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272931ms
Jan 18 23:07:04.531: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006140301s
Jan 18 23:07:04.531: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 23:07:04.534
STEP: Trying to apply a random label on the found node. 01/18/23 23:07:04.55
STEP: verifying the node has the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 95 01/18/23 23:07:04.56
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 23:07:04.563
Jan 18 23:07:04.567: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9052" to be "not pending"
Jan 18 23:07:04.570: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507916ms
Jan 18 23:07:06.574: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006011474s
Jan 18 23:07:06.574: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.128.15.199 on the node which pod4 resides and expect not scheduled 01/18/23 23:07:06.574
Jan 18 23:07:06.581: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9052" to be "not pending"
Jan 18 23:07:06.584: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494802ms
Jan 18 23:07:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00587792s
Jan 18 23:07:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007083078s
Jan 18 23:07:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006396281s
Jan 18 23:07:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007994834s
Jan 18 23:07:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006956579s
Jan 18 23:07:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006421321s
Jan 18 23:07:20.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007313565s
Jan 18 23:07:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006260056s
Jan 18 23:07:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007892957s
Jan 18 23:07:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007489526s
Jan 18 23:07:28.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005761133s
Jan 18 23:07:30.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007045616s
Jan 18 23:07:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007094431s
Jan 18 23:07:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007459604s
Jan 18 23:07:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007825093s
Jan 18 23:07:38.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006444171s
Jan 18 23:07:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007372608s
Jan 18 23:07:42.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00753931s
Jan 18 23:07:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006825481s
Jan 18 23:07:46.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006269779s
Jan 18 23:07:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006604189s
Jan 18 23:07:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006650084s
Jan 18 23:07:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00612474s
Jan 18 23:07:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006184121s
Jan 18 23:07:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006393784s
Jan 18 23:07:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005901977s
Jan 18 23:08:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00716285s
Jan 18 23:08:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007326994s
Jan 18 23:08:04.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006334997s
Jan 18 23:08:06.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005660281s
Jan 18 23:08:08.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006248303s
Jan 18 23:08:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007144127s
Jan 18 23:08:12.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007264455s
Jan 18 23:08:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007628406s
Jan 18 23:08:16.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007482193s
Jan 18 23:08:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006250972s
Jan 18 23:08:20.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007471832s
Jan 18 23:08:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00619377s
Jan 18 23:08:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007439222s
Jan 18 23:08:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007641742s
Jan 18 23:08:28.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006167528s
Jan 18 23:08:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007338125s
Jan 18 23:08:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007036638s
Jan 18 23:08:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007458996s
Jan 18 23:08:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007620851s
Jan 18 23:08:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006010055s
Jan 18 23:08:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007240995s
Jan 18 23:08:42.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007660748s
Jan 18 23:08:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007005317s
Jan 18 23:08:46.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006406195s
Jan 18 23:08:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006286212s
Jan 18 23:08:50.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007392427s
Jan 18 23:08:52.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006564768s
Jan 18 23:08:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00716408s
Jan 18 23:08:56.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007448926s
Jan 18 23:08:58.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006190022s
Jan 18 23:09:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006768822s
Jan 18 23:09:02.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00664941s
Jan 18 23:09:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007371112s
Jan 18 23:09:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007277731s
Jan 18 23:09:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005801506s
Jan 18 23:09:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006872667s
Jan 18 23:09:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006891515s
Jan 18 23:09:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0076941s
Jan 18 23:09:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006799002s
Jan 18 23:09:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006216307s
Jan 18 23:09:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007061649s
Jan 18 23:09:22.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005960482s
Jan 18 23:09:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007505025s
Jan 18 23:09:26.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008201716s
Jan 18 23:09:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006591031s
Jan 18 23:09:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007769562s
Jan 18 23:09:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007018045s
Jan 18 23:09:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007542151s
Jan 18 23:09:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007461816s
Jan 18 23:09:38.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006486979s
Jan 18 23:09:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007738969s
Jan 18 23:09:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006863871s
Jan 18 23:09:44.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007228053s
Jan 18 23:09:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007300555s
Jan 18 23:09:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006217357s
Jan 18 23:09:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006476278s
Jan 18 23:09:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005643484s
Jan 18 23:09:54.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005726476s
Jan 18 23:09:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006623452s
Jan 18 23:09:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005739671s
Jan 18 23:10:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007054985s
Jan 18 23:10:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007296534s
Jan 18 23:10:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008052681s
Jan 18 23:10:06.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006326466s
Jan 18 23:10:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005677404s
Jan 18 23:10:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00656187s
Jan 18 23:10:12.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007712943s
Jan 18 23:10:14.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007045663s
Jan 18 23:10:16.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007245173s
Jan 18 23:10:18.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005979203s
Jan 18 23:10:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007138662s
Jan 18 23:10:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006431123s
Jan 18 23:10:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007095672s
Jan 18 23:10:26.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00697977s
Jan 18 23:10:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006384562s
Jan 18 23:10:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007484961s
Jan 18 23:10:32.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007297118s
Jan 18 23:10:34.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008607557s
Jan 18 23:10:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007952528s
Jan 18 23:10:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005633656s
Jan 18 23:10:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007944805s
Jan 18 23:10:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006931915s
Jan 18 23:10:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007110781s
Jan 18 23:10:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00767652s
Jan 18 23:10:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006257338s
Jan 18 23:10:50.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.005749381s
Jan 18 23:10:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005853858s
Jan 18 23:10:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006893697s
Jan 18 23:10:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006631805s
Jan 18 23:10:58.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00650905s
Jan 18 23:11:00.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007620503s
Jan 18 23:11:02.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006906835s
Jan 18 23:11:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007212536s
Jan 18 23:11:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007557537s
Jan 18 23:11:08.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006836114s
Jan 18 23:11:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006699872s
Jan 18 23:11:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006768428s
Jan 18 23:11:14.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00710247s
Jan 18 23:11:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007163482s
Jan 18 23:11:18.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005825359s
Jan 18 23:11:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007146684s
Jan 18 23:11:22.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006013785s
Jan 18 23:11:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.006537055s
Jan 18 23:11:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007585042s
Jan 18 23:11:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006290617s
Jan 18 23:11:30.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008227678s
Jan 18 23:11:32.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007509506s
Jan 18 23:11:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007243431s
Jan 18 23:11:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007296813s
Jan 18 23:11:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00567407s
Jan 18 23:11:40.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006653038s
Jan 18 23:11:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006982399s
Jan 18 23:11:44.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007253168s
Jan 18 23:11:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007751254s
Jan 18 23:11:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006762721s
Jan 18 23:11:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006597113s
Jan 18 23:11:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005872979s
Jan 18 23:11:54.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007860406s
Jan 18 23:11:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006794606s
Jan 18 23:11:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006161314s
Jan 18 23:12:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007084548s
Jan 18 23:12:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00736324s
Jan 18 23:12:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007475745s
Jan 18 23:12:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008033008s
Jan 18 23:12:06.592: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010816942s
STEP: removing the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 off the node cncf-conformance-1-26-2 01/18/23 23:12:06.592
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 01/18/23 23:12:06.605
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9052" for this suite. 01/18/23 23:12:06.613
------------------------------
â€¢ [SLOW TEST] [304.142 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:07:02.476
    Jan 18 23:07:02.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 23:07:02.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:02.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:02.495
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 23:07:02.499: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 23:07:02.504: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 23:07:02.507: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
    Jan 18 23:07:02.512: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container etcd ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container weave ready: true, restart count 1
    Jan 18 23:07:02.512: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:07:02.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 23:07:02.512: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
    Jan 18 23:07:02.516: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.516: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
    Jan 18 23:07:02.516: INFO: 	Container weave ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 18 23:07:02.516: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:07:02.516: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:07:02.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:07:02.516: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 23:07:02.516
    Jan 18 23:07:02.525: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9052" to be "running"
    Jan 18 23:07:02.527: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272931ms
    Jan 18 23:07:04.531: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006140301s
    Jan 18 23:07:04.531: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 23:07:04.534
    STEP: Trying to apply a random label on the found node. 01/18/23 23:07:04.55
    STEP: verifying the node has the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 95 01/18/23 23:07:04.56
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 23:07:04.563
    Jan 18 23:07:04.567: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9052" to be "not pending"
    Jan 18 23:07:04.570: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507916ms
    Jan 18 23:07:06.574: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006011474s
    Jan 18 23:07:06.574: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.128.15.199 on the node which pod4 resides and expect not scheduled 01/18/23 23:07:06.574
    Jan 18 23:07:06.581: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9052" to be "not pending"
    Jan 18 23:07:06.584: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494802ms
    Jan 18 23:07:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00587792s
    Jan 18 23:07:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007083078s
    Jan 18 23:07:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006396281s
    Jan 18 23:07:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007994834s
    Jan 18 23:07:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006956579s
    Jan 18 23:07:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006421321s
    Jan 18 23:07:20.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007313565s
    Jan 18 23:07:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006260056s
    Jan 18 23:07:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007892957s
    Jan 18 23:07:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007489526s
    Jan 18 23:07:28.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005761133s
    Jan 18 23:07:30.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007045616s
    Jan 18 23:07:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007094431s
    Jan 18 23:07:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007459604s
    Jan 18 23:07:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007825093s
    Jan 18 23:07:38.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006444171s
    Jan 18 23:07:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007372608s
    Jan 18 23:07:42.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00753931s
    Jan 18 23:07:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006825481s
    Jan 18 23:07:46.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006269779s
    Jan 18 23:07:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006604189s
    Jan 18 23:07:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006650084s
    Jan 18 23:07:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00612474s
    Jan 18 23:07:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006184121s
    Jan 18 23:07:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006393784s
    Jan 18 23:07:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005901977s
    Jan 18 23:08:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00716285s
    Jan 18 23:08:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007326994s
    Jan 18 23:08:04.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006334997s
    Jan 18 23:08:06.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005660281s
    Jan 18 23:08:08.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006248303s
    Jan 18 23:08:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007144127s
    Jan 18 23:08:12.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007264455s
    Jan 18 23:08:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007628406s
    Jan 18 23:08:16.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007482193s
    Jan 18 23:08:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006250972s
    Jan 18 23:08:20.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007471832s
    Jan 18 23:08:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00619377s
    Jan 18 23:08:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007439222s
    Jan 18 23:08:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007641742s
    Jan 18 23:08:28.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006167528s
    Jan 18 23:08:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007338125s
    Jan 18 23:08:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007036638s
    Jan 18 23:08:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007458996s
    Jan 18 23:08:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007620851s
    Jan 18 23:08:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006010055s
    Jan 18 23:08:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007240995s
    Jan 18 23:08:42.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007660748s
    Jan 18 23:08:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007005317s
    Jan 18 23:08:46.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006406195s
    Jan 18 23:08:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006286212s
    Jan 18 23:08:50.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007392427s
    Jan 18 23:08:52.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006564768s
    Jan 18 23:08:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00716408s
    Jan 18 23:08:56.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007448926s
    Jan 18 23:08:58.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006190022s
    Jan 18 23:09:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006768822s
    Jan 18 23:09:02.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00664941s
    Jan 18 23:09:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007371112s
    Jan 18 23:09:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007277731s
    Jan 18 23:09:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005801506s
    Jan 18 23:09:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006872667s
    Jan 18 23:09:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006891515s
    Jan 18 23:09:14.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0076941s
    Jan 18 23:09:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006799002s
    Jan 18 23:09:18.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006216307s
    Jan 18 23:09:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007061649s
    Jan 18 23:09:22.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005960482s
    Jan 18 23:09:24.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007505025s
    Jan 18 23:09:26.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008201716s
    Jan 18 23:09:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006591031s
    Jan 18 23:09:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007769562s
    Jan 18 23:09:32.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007018045s
    Jan 18 23:09:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007542151s
    Jan 18 23:09:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007461816s
    Jan 18 23:09:38.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006486979s
    Jan 18 23:09:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007738969s
    Jan 18 23:09:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006863871s
    Jan 18 23:09:44.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007228053s
    Jan 18 23:09:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007300555s
    Jan 18 23:09:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006217357s
    Jan 18 23:09:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006476278s
    Jan 18 23:09:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005643484s
    Jan 18 23:09:54.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005726476s
    Jan 18 23:09:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006623452s
    Jan 18 23:09:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005739671s
    Jan 18 23:10:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007054985s
    Jan 18 23:10:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007296534s
    Jan 18 23:10:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008052681s
    Jan 18 23:10:06.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006326466s
    Jan 18 23:10:08.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005677404s
    Jan 18 23:10:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00656187s
    Jan 18 23:10:12.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007712943s
    Jan 18 23:10:14.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007045663s
    Jan 18 23:10:16.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007245173s
    Jan 18 23:10:18.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005979203s
    Jan 18 23:10:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007138662s
    Jan 18 23:10:22.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006431123s
    Jan 18 23:10:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007095672s
    Jan 18 23:10:26.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00697977s
    Jan 18 23:10:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006384562s
    Jan 18 23:10:30.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007484961s
    Jan 18 23:10:32.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007297118s
    Jan 18 23:10:34.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008607557s
    Jan 18 23:10:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007952528s
    Jan 18 23:10:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005633656s
    Jan 18 23:10:40.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007944805s
    Jan 18 23:10:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006931915s
    Jan 18 23:10:44.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007110781s
    Jan 18 23:10:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00767652s
    Jan 18 23:10:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006257338s
    Jan 18 23:10:50.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.005749381s
    Jan 18 23:10:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005853858s
    Jan 18 23:10:54.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006893697s
    Jan 18 23:10:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006631805s
    Jan 18 23:10:58.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00650905s
    Jan 18 23:11:00.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007620503s
    Jan 18 23:11:02.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006906835s
    Jan 18 23:11:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007212536s
    Jan 18 23:11:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007557537s
    Jan 18 23:11:08.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006836114s
    Jan 18 23:11:10.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006699872s
    Jan 18 23:11:12.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006768428s
    Jan 18 23:11:14.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00710247s
    Jan 18 23:11:16.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007163482s
    Jan 18 23:11:18.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005825359s
    Jan 18 23:11:20.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007146684s
    Jan 18 23:11:22.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006013785s
    Jan 18 23:11:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.006537055s
    Jan 18 23:11:26.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007585042s
    Jan 18 23:11:28.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006290617s
    Jan 18 23:11:30.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008227678s
    Jan 18 23:11:32.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007509506s
    Jan 18 23:11:34.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007243431s
    Jan 18 23:11:36.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007296813s
    Jan 18 23:11:38.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00567407s
    Jan 18 23:11:40.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006653038s
    Jan 18 23:11:42.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006982399s
    Jan 18 23:11:44.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007253168s
    Jan 18 23:11:46.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007751254s
    Jan 18 23:11:48.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006762721s
    Jan 18 23:11:50.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006597113s
    Jan 18 23:11:52.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005872979s
    Jan 18 23:11:54.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007860406s
    Jan 18 23:11:56.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006794606s
    Jan 18 23:11:58.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006161314s
    Jan 18 23:12:00.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007084548s
    Jan 18 23:12:02.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00736324s
    Jan 18 23:12:04.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007475745s
    Jan 18 23:12:06.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008033008s
    Jan 18 23:12:06.592: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010816942s
    STEP: removing the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 off the node cncf-conformance-1-26-2 01/18/23 23:12:06.592
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b736c1ba-6d2e-46ed-8974-69d597c26b17 01/18/23 23:12:06.605
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9052" for this suite. 01/18/23 23:12:06.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:06.62
Jan 18 23:12:06.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:12:06.622
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:06.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:06.638
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:12:06.645
Jan 18 23:12:06.651: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8611" to be "running and ready"
Jan 18 23:12:06.657: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868544ms
Jan 18 23:12:06.657: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:08.661: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009944882s
Jan 18 23:12:08.661: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 23:12:08.661: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/18/23 23:12:08.664
Jan 18 23:12:08.669: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8611" to be "running and ready"
Jan 18 23:12:08.671: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398529ms
Jan 18 23:12:08.671: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:10.675: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00603913s
Jan 18 23:12:10.675: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 18 23:12:10.675: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 23:12:10.677
STEP: delete the pod with lifecycle hook 01/18/23 23:12:10.691
Jan 18 23:12:10.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 23:12:10.700: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 23:12:12.701: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 23:12:12.704: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:12.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8611" for this suite. 01/18/23 23:12:12.707
------------------------------
â€¢ [SLOW TEST] [6.094 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:06.62
    Jan 18 23:12:06.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:12:06.622
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:06.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:06.638
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:12:06.645
    Jan 18 23:12:06.651: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8611" to be "running and ready"
    Jan 18 23:12:06.657: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868544ms
    Jan 18 23:12:06.657: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:08.661: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009944882s
    Jan 18 23:12:08.661: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 23:12:08.661: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/18/23 23:12:08.664
    Jan 18 23:12:08.669: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8611" to be "running and ready"
    Jan 18 23:12:08.671: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398529ms
    Jan 18 23:12:08.671: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:10.675: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00603913s
    Jan 18 23:12:10.675: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 18 23:12:10.675: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 23:12:10.677
    STEP: delete the pod with lifecycle hook 01/18/23 23:12:10.691
    Jan 18 23:12:10.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 23:12:10.700: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 23:12:12.701: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 23:12:12.704: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:12.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8611" for this suite. 01/18/23 23:12:12.707
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:12.714
Jan 18 23:12:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename containers 01/18/23 23:12:12.716
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:12.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:12.729
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 18 23:12:12.743: INFO: Waiting up to 5m0s for pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c" in namespace "containers-2968" to be "running"
Jan 18 23:12:12.745: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503873ms
Jan 18 23:12:14.750: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007651383s
Jan 18 23:12:14.750: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2968" for this suite. 01/18/23 23:12:14.769
------------------------------
â€¢ [2.059 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:12.714
    Jan 18 23:12:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename containers 01/18/23 23:12:12.716
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:12.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:12.729
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 18 23:12:12.743: INFO: Waiting up to 5m0s for pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c" in namespace "containers-2968" to be "running"
    Jan 18 23:12:12.745: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503873ms
    Jan 18 23:12:14.750: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007651383s
    Jan 18 23:12:14.750: INFO: Pod "client-containers-b003b13d-6dd5-459e-9bfc-26d991a1a46c" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2968" for this suite. 01/18/23 23:12:14.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:14.775
Jan 18 23:12:14.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 23:12:14.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:14.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:14.792
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/18/23 23:12:14.795
STEP: Creating hostNetwork=false pod 01/18/23 23:12:14.795
Jan 18 23:12:14.804: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4147" to be "running and ready"
Jan 18 23:12:14.807: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915628ms
Jan 18 23:12:14.807: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:16.810: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006539756s
Jan 18 23:12:16.810: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 18 23:12:16.810: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/18/23 23:12:16.813
Jan 18 23:12:16.820: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4147" to be "running and ready"
Jan 18 23:12:16.822: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501391ms
Jan 18 23:12:16.822: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:18.826: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006328907s
Jan 18 23:12:18.826: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 18 23:12:18.826: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/18/23 23:12:18.829
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 23:12:18.829
Jan 18 23:12:18.829: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:18.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:18.829: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:18.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:12:18.901: INFO: Exec stderr: ""
Jan 18 23:12:18.901: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:18.902: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:18.902: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:12:18.949: INFO: Exec stderr: ""
Jan 18 23:12:18.949: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:18.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:18.950: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:18.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:12:19.015: INFO: Exec stderr: ""
Jan 18 23:12:19.015: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.016: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.016: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:12:19.091: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 23:12:19.091
Jan 18 23:12:19.091: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.092: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.092: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 23:12:19.136: INFO: Exec stderr: ""
Jan 18 23:12:19.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.137: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 23:12:19.200: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 23:12:19.2
Jan 18 23:12:19.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.201: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:12:19.244: INFO: Exec stderr: ""
Jan 18 23:12:19.244: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.245: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.245: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:12:19.319: INFO: Exec stderr: ""
Jan 18 23:12:19.319: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.320: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.320: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:12:19.361: INFO: Exec stderr: ""
Jan 18 23:12:19.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:12:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:12:19.362: INFO: ExecWithOptions: Clientset creation
Jan 18 23:12:19.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:12:19.434: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4147" for this suite. 01/18/23 23:12:19.437
------------------------------
â€¢ [4.667 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:14.775
    Jan 18 23:12:14.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 23:12:14.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:14.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:14.792
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/18/23 23:12:14.795
    STEP: Creating hostNetwork=false pod 01/18/23 23:12:14.795
    Jan 18 23:12:14.804: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4147" to be "running and ready"
    Jan 18 23:12:14.807: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915628ms
    Jan 18 23:12:14.807: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:16.810: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006539756s
    Jan 18 23:12:16.810: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 18 23:12:16.810: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/18/23 23:12:16.813
    Jan 18 23:12:16.820: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4147" to be "running and ready"
    Jan 18 23:12:16.822: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501391ms
    Jan 18 23:12:16.822: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:18.826: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006328907s
    Jan 18 23:12:18.826: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 18 23:12:18.826: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/18/23 23:12:18.829
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 23:12:18.829
    Jan 18 23:12:18.829: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:18.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:18.829: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:18.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 23:12:18.901: INFO: Exec stderr: ""
    Jan 18 23:12:18.901: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:18.902: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:18.902: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 23:12:18.949: INFO: Exec stderr: ""
    Jan 18 23:12:18.949: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:18.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:18.950: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:18.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 23:12:19.015: INFO: Exec stderr: ""
    Jan 18 23:12:19.015: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.016: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.016: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 23:12:19.091: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 23:12:19.091
    Jan 18 23:12:19.091: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.092: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.092: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 23:12:19.136: INFO: Exec stderr: ""
    Jan 18 23:12:19.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.137: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 23:12:19.200: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 23:12:19.2
    Jan 18 23:12:19.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.201: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 23:12:19.244: INFO: Exec stderr: ""
    Jan 18 23:12:19.244: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.245: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.245: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 23:12:19.319: INFO: Exec stderr: ""
    Jan 18 23:12:19.319: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.320: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.320: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 23:12:19.361: INFO: Exec stderr: ""
    Jan 18 23:12:19.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4147 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:12:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:12:19.362: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:12:19.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4147/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 23:12:19.434: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4147" for this suite. 01/18/23 23:12:19.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:19.443
Jan 18 23:12:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:12:19.444
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:19.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:19.46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-445600f5-2e24-4e35-8f3c-6309dc61e5f2 01/18/23 23:12:19.463
STEP: Creating a pod to test consume configMaps 01/18/23 23:12:19.467
Jan 18 23:12:19.475: INFO: Waiting up to 5m0s for pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b" in namespace "configmap-4530" to be "Succeeded or Failed"
Jan 18 23:12:19.477: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426433ms
Jan 18 23:12:21.481: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00665765s
Jan 18 23:12:23.480: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00560456s
STEP: Saw pod success 01/18/23 23:12:23.48
Jan 18 23:12:23.480: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b" satisfied condition "Succeeded or Failed"
Jan 18 23:12:23.483: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:12:23.487
Jan 18 23:12:23.495: INFO: Waiting for pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b to disappear
Jan 18 23:12:23.497: INFO: Pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:23.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4530" for this suite. 01/18/23 23:12:23.504
------------------------------
â€¢ [4.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:19.443
    Jan 18 23:12:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:12:19.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:19.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:19.46
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-445600f5-2e24-4e35-8f3c-6309dc61e5f2 01/18/23 23:12:19.463
    STEP: Creating a pod to test consume configMaps 01/18/23 23:12:19.467
    Jan 18 23:12:19.475: INFO: Waiting up to 5m0s for pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b" in namespace "configmap-4530" to be "Succeeded or Failed"
    Jan 18 23:12:19.477: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426433ms
    Jan 18 23:12:21.481: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00665765s
    Jan 18 23:12:23.480: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00560456s
    STEP: Saw pod success 01/18/23 23:12:23.48
    Jan 18 23:12:23.480: INFO: Pod "pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b" satisfied condition "Succeeded or Failed"
    Jan 18 23:12:23.483: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:12:23.487
    Jan 18 23:12:23.495: INFO: Waiting for pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b to disappear
    Jan 18 23:12:23.497: INFO: Pod pod-configmaps-121eb11e-2e2c-48df-ba8c-cc05723fbc5b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:23.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4530" for this suite. 01/18/23 23:12:23.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:23.512
Jan 18 23:12:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 23:12:23.513
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:23.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:23.527
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 18 23:12:23.530: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 18 23:12:23.539: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 23:12:28.542: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 23:12:28.542
Jan 18 23:12:28.542: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 23:12:28.548: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 23:12:28.552: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 23:12:30.558: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 23:12:30.560: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:12:30.567: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-31  6677aca9-77ca-42a0-be8a-a7f5c03f301c 19478 1 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c27408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:12:28 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-18 23:12:29 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:12:30.570: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-31  426a634d-5fbe-4aa4-8a7a-29248f04812e 19468 1 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6677aca9-77ca-42a0-be8a-a7f5c03f301c 0xc003c27907 0xc003c27908}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6677aca9-77ca-42a0-be8a-a7f5c03f301c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c279b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:12:30.570: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 23:12:30.570: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-31  55e2d172-f21f-42cb-9b72-298c8d779450 19477 2 2023-01-18 23:12:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6677aca9-77ca-42a0-be8a-a7f5c03f301c 0xc003c277d7 0xc003c277d8}] [] [{e2e.test Update apps/v1 2023-01-18 23:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6677aca9-77ca-42a0-be8a-a7f5c03f301c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c27898 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:12:30.572: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4dhm6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4dhm6 test-rolling-update-deployment-7549d9f46d- deployment-31  2769533e-7e3a-431b-a7b1-b5f341129c79 19467 0 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 426a634d-5fbe-4aa4-8a7a-29248f04812e 0xc003c27e17 0xc003c27e18}] [] [{kube-controller-manager Update v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"426a634d-5fbe-4aa4-8a7a-29248f04812e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq86k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq86k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.4,StartTime:2023-01-18 23:12:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:12:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3b3357360841fc84920a9acd679b72e47d6753259625d9d01e72ecf2e15f80a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:30.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-31" for this suite. 01/18/23 23:12:30.575
------------------------------
â€¢ [SLOW TEST] [7.069 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:23.512
    Jan 18 23:12:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 23:12:23.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:23.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:23.527
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 18 23:12:23.530: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 18 23:12:23.539: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 23:12:28.542: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 23:12:28.542
    Jan 18 23:12:28.542: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 18 23:12:28.548: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 18 23:12:28.552: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 18 23:12:30.558: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 18 23:12:30.560: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 23:12:30.567: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-31  6677aca9-77ca-42a0-be8a-a7f5c03f301c 19478 1 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c27408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:12:28 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-18 23:12:29 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 23:12:30.570: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-31  426a634d-5fbe-4aa4-8a7a-29248f04812e 19468 1 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6677aca9-77ca-42a0-be8a-a7f5c03f301c 0xc003c27907 0xc003c27908}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6677aca9-77ca-42a0-be8a-a7f5c03f301c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c279b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:12:30.570: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 18 23:12:30.570: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-31  55e2d172-f21f-42cb-9b72-298c8d779450 19477 2 2023-01-18 23:12:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6677aca9-77ca-42a0-be8a-a7f5c03f301c 0xc003c277d7 0xc003c277d8}] [] [{e2e.test Update apps/v1 2023-01-18 23:12:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6677aca9-77ca-42a0-be8a-a7f5c03f301c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c27898 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:12:30.572: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4dhm6" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4dhm6 test-rolling-update-deployment-7549d9f46d- deployment-31  2769533e-7e3a-431b-a7b1-b5f341129c79 19467 0 2023-01-18 23:12:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 426a634d-5fbe-4aa4-8a7a-29248f04812e 0xc003c27e17 0xc003c27e18}] [] [{kube-controller-manager Update v1 2023-01-18 23:12:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"426a634d-5fbe-4aa4-8a7a-29248f04812e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:12:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq86k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq86k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:12:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.4,StartTime:2023-01-18 23:12:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:12:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3b3357360841fc84920a9acd679b72e47d6753259625d9d01e72ecf2e15f80a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:30.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-31" for this suite. 01/18/23 23:12:30.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:30.582
Jan 18 23:12:30.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-pred 01/18/23 23:12:30.583
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:30.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:30.601
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 23:12:30.604: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 23:12:30.611: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 23:12:30.613: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
Jan 18 23:12:30.618: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:12:30.618: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:12:30.618: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container etcd ready: true, restart count 0
Jan 18 23:12:30.618: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:12:30.618: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:12:30.618: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:12:30.618: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:12:30.618: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container weave ready: true, restart count 1
Jan 18 23:12:30.618: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:12:30.618: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:12:30.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:12:30.618: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:12:30.618: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
Jan 18 23:12:30.622: INFO: test-rolling-update-deployment-7549d9f46d-4dhm6 from deployment-31 started at 2023-01-18 23:12:28 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container agnhost ready: true, restart count 0
Jan 18 23:12:30.622: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:12:30.622: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container weave ready: true, restart count 0
Jan 18 23:12:30.622: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:12:30.622: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:12:30.622: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:12:30.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:12:30.622: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:12:30.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:12:30.622: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 23:12:30.622
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b8a1ea015aa7f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/18/23 23:12:30.643
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:12:31.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5253" for this suite. 01/18/23 23:12:31.644
------------------------------
â€¢ [1.069 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:30.582
    Jan 18 23:12:30.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 23:12:30.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:30.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:30.601
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 23:12:30.604: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 23:12:30.611: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 23:12:30.613: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
    Jan 18 23:12:30.618: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container etcd ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container weave ready: true, restart count 1
    Jan 18 23:12:30.618: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:12:30.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 23:12:30.618: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
    Jan 18 23:12:30.622: INFO: test-rolling-update-deployment-7549d9f46d-4dhm6 from deployment-31 started at 2023-01-18 23:12:28 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container agnhost ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container weave ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:12:30.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:12:30.622: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 23:12:30.622
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b8a1ea015aa7f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/18/23 23:12:30.643
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:12:31.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5253" for this suite. 01/18/23 23:12:31.644
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:12:31.651
Jan 18 23:12:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 23:12:31.652
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:31.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:31.664
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/18/23 23:12:31.666
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2413;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2413;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +notcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_tcp@PTR;sleep 1; done
 01/18/23 23:12:31.679
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2413;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2413;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +notcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_tcp@PTR;sleep 1; done
 01/18/23 23:12:31.679
STEP: creating a pod to probe DNS 01/18/23 23:12:31.679
STEP: submitting the pod to kubernetes 01/18/23 23:12:31.679
Jan 18 23:12:31.689: INFO: Waiting up to 15m0s for pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a" in namespace "dns-2413" to be "running"
Jan 18 23:12:31.692: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.74665ms
Jan 18 23:12:33.696: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695222s
Jan 18 23:12:33.696: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a" satisfied condition "running"
STEP: retrieving the pod 01/18/23 23:12:33.696
STEP: looking for the results for each expected name from probers 01/18/23 23:12:33.699
Jan 18 23:12:33.704: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.707: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.710: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.717: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.722: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.743: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.746: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.748: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.754: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.757: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:33.774: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:12:38.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.830: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:38.851: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:12:43.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.830: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:43.854: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:12:48.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.784: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.799: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.821: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.824: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.827: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.832: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:48.853: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:12:53.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.816: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.819: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:53.850: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:12:58.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.822: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
Jan 18 23:12:58.849: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

Jan 18 23:13:03.856: INFO: DNS probes using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a succeeded

STEP: deleting the pod 01/18/23 23:13:03.856
STEP: deleting the test service 01/18/23 23:13:03.87
STEP: deleting the test headless service 01/18/23 23:13:03.889
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2413" for this suite. 01/18/23 23:13:03.9
------------------------------
â€¢ [SLOW TEST] [32.255 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:12:31.651
    Jan 18 23:12:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 23:12:31.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:31.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:31.664
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/18/23 23:12:31.666
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2413;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2413;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +notcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_tcp@PTR;sleep 1; done
     01/18/23 23:12:31.679
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2413;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2413;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2413.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2413.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2413.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2413.svc;check="$$(dig +notcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.225_tcp@PTR;sleep 1; done
     01/18/23 23:12:31.679
    STEP: creating a pod to probe DNS 01/18/23 23:12:31.679
    STEP: submitting the pod to kubernetes 01/18/23 23:12:31.679
    Jan 18 23:12:31.689: INFO: Waiting up to 15m0s for pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a" in namespace "dns-2413" to be "running"
    Jan 18 23:12:31.692: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.74665ms
    Jan 18 23:12:33.696: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695222s
    Jan 18 23:12:33.696: INFO: Pod "dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 23:12:33.696
    STEP: looking for the results for each expected name from probers 01/18/23 23:12:33.699
    Jan 18 23:12:33.704: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.707: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.710: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.717: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.722: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.743: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.746: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.748: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.754: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.757: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:33.774: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:12:38.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.830: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:38.851: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:12:43.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.821: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.830: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:43.854: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:12:48.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.784: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.799: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.821: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.824: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.827: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.832: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:48.853: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:12:53.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.816: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.819: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.824: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:53.850: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:12:58.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.822: INFO: Unable to read jessie_udp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413 from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-2413.svc from pod dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a: the server could not find the requested resource (get pods dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a)
    Jan 18 23:12:58.849: INFO: Lookups using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2413 wheezy_tcp@dns-test-service.dns-2413 wheezy_udp@dns-test-service.dns-2413.svc wheezy_tcp@dns-test-service.dns-2413.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2413 jessie_tcp@dns-test-service.dns-2413 jessie_udp@dns-test-service.dns-2413.svc jessie_tcp@dns-test-service.dns-2413.svc]

    Jan 18 23:13:03.856: INFO: DNS probes using dns-2413/dns-test-21377306-8a9b-46f7-a982-da8a7ebd692a succeeded

    STEP: deleting the pod 01/18/23 23:13:03.856
    STEP: deleting the test service 01/18/23 23:13:03.87
    STEP: deleting the test headless service 01/18/23 23:13:03.889
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2413" for this suite. 01/18/23 23:13:03.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:03.907
Jan 18 23:13:03.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:13:03.908
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:03.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:03.921
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:13:03.936
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:13:04.201
STEP: Deploying the webhook pod 01/18/23 23:13:04.21
STEP: Wait for the deployment to be ready 01/18/23 23:13:04.221
Jan 18 23:13:04.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:13:06.237
STEP: Verifying the service has paired with the endpoint 01/18/23 23:13:06.246
Jan 18 23:13:07.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 23:13:07.25
STEP: create a pod that should be denied by the webhook 01/18/23 23:13:07.267
STEP: create a pod that causes the webhook to hang 01/18/23 23:13:07.28
STEP: create a configmap that should be denied by the webhook 01/18/23 23:13:17.287
STEP: create a configmap that should be admitted by the webhook 01/18/23 23:13:17.308
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 23:13:17.318
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 23:13:17.326
STEP: create a namespace that bypass the webhook 01/18/23 23:13:17.33
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 23:13:17.337
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:17.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7895" for this suite. 01/18/23 23:13:17.397
STEP: Destroying namespace "webhook-7895-markers" for this suite. 01/18/23 23:13:17.406
------------------------------
â€¢ [SLOW TEST] [13.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:03.907
    Jan 18 23:13:03.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:13:03.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:03.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:03.921
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:13:03.936
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:13:04.201
    STEP: Deploying the webhook pod 01/18/23 23:13:04.21
    STEP: Wait for the deployment to be ready 01/18/23 23:13:04.221
    Jan 18 23:13:04.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:13:06.237
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:13:06.246
    Jan 18 23:13:07.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 23:13:07.25
    STEP: create a pod that should be denied by the webhook 01/18/23 23:13:07.267
    STEP: create a pod that causes the webhook to hang 01/18/23 23:13:07.28
    STEP: create a configmap that should be denied by the webhook 01/18/23 23:13:17.287
    STEP: create a configmap that should be admitted by the webhook 01/18/23 23:13:17.308
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 23:13:17.318
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 23:13:17.326
    STEP: create a namespace that bypass the webhook 01/18/23 23:13:17.33
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 23:13:17.337
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:17.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7895" for this suite. 01/18/23 23:13:17.397
    STEP: Destroying namespace "webhook-7895-markers" for this suite. 01/18/23 23:13:17.406
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:17.412
Jan 18 23:13:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 23:13:17.413
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:17.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:17.428
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 23:13:17.431
Jan 18 23:13:17.438: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4317" to be "running and ready"
Jan 18 23:13:17.441: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402968ms
Jan 18 23:13:17.441: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:13:19.445: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00690081s
Jan 18 23:13:19.445: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 18 23:13:19.445: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/18/23 23:13:19.448
STEP: Then the orphan pod is adopted 01/18/23 23:13:19.453
STEP: When the matched label of one of its pods change 01/18/23 23:13:20.459
Jan 18 23:13:20.462: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 23:13:20.471
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:21.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4317" for this suite. 01/18/23 23:13:21.482
------------------------------
â€¢ [4.077 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:17.412
    Jan 18 23:13:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 23:13:17.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:17.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:17.428
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 23:13:17.431
    Jan 18 23:13:17.438: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4317" to be "running and ready"
    Jan 18 23:13:17.441: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402968ms
    Jan 18 23:13:17.441: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:13:19.445: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00690081s
    Jan 18 23:13:19.445: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 18 23:13:19.445: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/18/23 23:13:19.448
    STEP: Then the orphan pod is adopted 01/18/23 23:13:19.453
    STEP: When the matched label of one of its pods change 01/18/23 23:13:20.459
    Jan 18 23:13:20.462: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 23:13:20.471
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:21.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4317" for this suite. 01/18/23 23:13:21.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:21.489
Jan 18 23:13:21.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:13:21.49
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:21.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:21.504
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-bf8kt"  01/18/23 23:13:21.507
Jan 18 23:13:21.511: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-bf8kt"  01/18/23 23:13:21.511
Jan 18 23:13:21.518: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:21.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3758" for this suite. 01/18/23 23:13:21.521
------------------------------
â€¢ [0.036 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:21.489
    Jan 18 23:13:21.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:13:21.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:21.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:21.504
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-bf8kt"  01/18/23 23:13:21.507
    Jan 18 23:13:21.511: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-bf8kt"  01/18/23 23:13:21.511
    Jan 18 23:13:21.518: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:21.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3758" for this suite. 01/18/23 23:13:21.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:21.527
Jan 18 23:13:21.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 23:13:21.528
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:21.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:21.542
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e in namespace container-probe-3446 01/18/23 23:13:21.545
Jan 18 23:13:21.553: INFO: Waiting up to 5m0s for pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e" in namespace "container-probe-3446" to be "not pending"
Jan 18 23:13:21.555: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551468ms
Jan 18 23:13:23.559: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006677677s
Jan 18 23:13:23.559: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e" satisfied condition "not pending"
Jan 18 23:13:23.559: INFO: Started pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e in namespace container-probe-3446
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 23:13:23.559
Jan 18 23:13:23.562: INFO: Initial restart count of pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e is 0
Jan 18 23:13:43.604: INFO: Restart count of pod container-probe-3446/liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e is now 1 (20.041605558s elapsed)
STEP: deleting the pod 01/18/23 23:13:43.604
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3446" for this suite. 01/18/23 23:13:43.618
------------------------------
â€¢ [SLOW TEST] [22.097 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:21.527
    Jan 18 23:13:21.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 23:13:21.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:21.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:21.542
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e in namespace container-probe-3446 01/18/23 23:13:21.545
    Jan 18 23:13:21.553: INFO: Waiting up to 5m0s for pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e" in namespace "container-probe-3446" to be "not pending"
    Jan 18 23:13:21.555: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551468ms
    Jan 18 23:13:23.559: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006677677s
    Jan 18 23:13:23.559: INFO: Pod "liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e" satisfied condition "not pending"
    Jan 18 23:13:23.559: INFO: Started pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e in namespace container-probe-3446
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 23:13:23.559
    Jan 18 23:13:23.562: INFO: Initial restart count of pod liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e is 0
    Jan 18 23:13:43.604: INFO: Restart count of pod container-probe-3446/liveness-bcf4b7c9-e9ba-4a7d-beca-b4c3359e946e is now 1 (20.041605558s elapsed)
    STEP: deleting the pod 01/18/23 23:13:43.604
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3446" for this suite. 01/18/23 23:13:43.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:43.626
Jan 18 23:13:43.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:13:43.627
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:43.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:43.641
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:13:43.644
Jan 18 23:13:43.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70" in namespace "projected-4643" to be "Succeeded or Failed"
Jan 18 23:13:43.656: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489418ms
Jan 18 23:13:45.659: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887697s
Jan 18 23:13:47.660: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006490195s
STEP: Saw pod success 01/18/23 23:13:47.66
Jan 18 23:13:47.660: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70" satisfied condition "Succeeded or Failed"
Jan 18 23:13:47.662: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 container client-container: <nil>
STEP: delete the pod 01/18/23 23:13:47.667
Jan 18 23:13:47.678: INFO: Waiting for pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 to disappear
Jan 18 23:13:47.680: INFO: Pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4643" for this suite. 01/18/23 23:13:47.684
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:43.626
    Jan 18 23:13:43.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:13:43.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:43.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:43.641
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:13:43.644
    Jan 18 23:13:43.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70" in namespace "projected-4643" to be "Succeeded or Failed"
    Jan 18 23:13:43.656: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489418ms
    Jan 18 23:13:45.659: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887697s
    Jan 18 23:13:47.660: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006490195s
    STEP: Saw pod success 01/18/23 23:13:47.66
    Jan 18 23:13:47.660: INFO: Pod "downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70" satisfied condition "Succeeded or Failed"
    Jan 18 23:13:47.662: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:13:47.667
    Jan 18 23:13:47.678: INFO: Waiting for pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 to disappear
    Jan 18 23:13:47.680: INFO: Pod downwardapi-volume-11023d0d-5387-45c1-9811-b60b9ab76e70 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4643" for this suite. 01/18/23 23:13:47.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:47.689
Jan 18 23:13:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:13:47.69
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:47.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:47.707
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:13:47.71
Jan 18 23:13:47.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09" in namespace "downward-api-3447" to be "Succeeded or Failed"
Jan 18 23:13:47.722: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279581ms
Jan 18 23:13:49.725: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150261s
Jan 18 23:13:51.726: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008831698s
STEP: Saw pod success 01/18/23 23:13:51.726
Jan 18 23:13:51.726: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09" satisfied condition "Succeeded or Failed"
Jan 18 23:13:51.729: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 container client-container: <nil>
STEP: delete the pod 01/18/23 23:13:51.734
Jan 18 23:13:51.745: INFO: Waiting for pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 to disappear
Jan 18 23:13:51.747: INFO: Pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:51.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3447" for this suite. 01/18/23 23:13:51.75
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:47.689
    Jan 18 23:13:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:13:47.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:47.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:47.707
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:13:47.71
    Jan 18 23:13:47.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09" in namespace "downward-api-3447" to be "Succeeded or Failed"
    Jan 18 23:13:47.722: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279581ms
    Jan 18 23:13:49.725: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150261s
    Jan 18 23:13:51.726: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008831698s
    STEP: Saw pod success 01/18/23 23:13:51.726
    Jan 18 23:13:51.726: INFO: Pod "downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09" satisfied condition "Succeeded or Failed"
    Jan 18 23:13:51.729: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:13:51.734
    Jan 18 23:13:51.745: INFO: Waiting for pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 to disappear
    Jan 18 23:13:51.747: INFO: Pod downwardapi-volume-85f5ff8e-cc9a-461c-85f9-bceea250ae09 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:51.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3447" for this suite. 01/18/23 23:13:51.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:51.756
Jan 18 23:13:51.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:13:51.757
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:51.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:51.773
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/18/23 23:13:51.776
Jan 18 23:13:51.776: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3884 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/18/23 23:13:51.835
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3884" for this suite. 01/18/23 23:13:51.846
------------------------------
â€¢ [0.095 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:51.756
    Jan 18 23:13:51.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:13:51.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:51.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:51.773
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/18/23 23:13:51.776
    Jan 18 23:13:51.776: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3884 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/18/23 23:13:51.835
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3884" for this suite. 01/18/23 23:13:51.846
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:51.852
Jan 18 23:13:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 23:13:51.853
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:51.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:51.866
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 23:13:51.869
Jan 18 23:13:51.878: INFO: Waiting up to 5m0s for pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685" in namespace "emptydir-9391" to be "Succeeded or Failed"
Jan 18 23:13:51.886: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445094ms
Jan 18 23:13:53.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011722298s
Jan 18 23:13:55.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011646166s
STEP: Saw pod success 01/18/23 23:13:55.89
Jan 18 23:13:55.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685" satisfied condition "Succeeded or Failed"
Jan 18 23:13:55.893: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 container test-container: <nil>
STEP: delete the pod 01/18/23 23:13:55.897
Jan 18 23:13:55.906: INFO: Waiting for pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 to disappear
Jan 18 23:13:55.908: INFO: Pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:55.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9391" for this suite. 01/18/23 23:13:55.911
------------------------------
â€¢ [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:51.852
    Jan 18 23:13:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 23:13:51.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:51.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:51.866
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 23:13:51.869
    Jan 18 23:13:51.878: INFO: Waiting up to 5m0s for pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685" in namespace "emptydir-9391" to be "Succeeded or Failed"
    Jan 18 23:13:51.886: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445094ms
    Jan 18 23:13:53.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011722298s
    Jan 18 23:13:55.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011646166s
    STEP: Saw pod success 01/18/23 23:13:55.89
    Jan 18 23:13:55.890: INFO: Pod "pod-a342a4ef-8c89-4d4f-a587-38b957835685" satisfied condition "Succeeded or Failed"
    Jan 18 23:13:55.893: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 container test-container: <nil>
    STEP: delete the pod 01/18/23 23:13:55.897
    Jan 18 23:13:55.906: INFO: Waiting for pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 to disappear
    Jan 18 23:13:55.908: INFO: Pod pod-a342a4ef-8c89-4d4f-a587-38b957835685 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:55.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9391" for this suite. 01/18/23 23:13:55.911
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:55.916
Jan 18 23:13:55.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:13:55.917
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:55.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:55.931
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-7402f091-5cc4-4e08-aad9-980f38f95ab4 01/18/23 23:13:55.934
STEP: Creating a pod to test consume configMaps 01/18/23 23:13:55.938
Jan 18 23:13:55.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884" in namespace "projected-5274" to be "Succeeded or Failed"
Jan 18 23:13:55.949: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.845002ms
Jan 18 23:13:57.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006789438s
Jan 18 23:13:59.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006856461s
STEP: Saw pod success 01/18/23 23:13:59.953
Jan 18 23:13:59.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884" satisfied condition "Succeeded or Failed"
Jan 18 23:13:59.956: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:13:59.96
Jan 18 23:13:59.969: INFO: Waiting for pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 to disappear
Jan 18 23:13:59.972: INFO: Pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:13:59.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5274" for this suite. 01/18/23 23:13:59.974
------------------------------
â€¢ [4.062 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:55.916
    Jan 18 23:13:55.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:13:55.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:55.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:55.931
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-7402f091-5cc4-4e08-aad9-980f38f95ab4 01/18/23 23:13:55.934
    STEP: Creating a pod to test consume configMaps 01/18/23 23:13:55.938
    Jan 18 23:13:55.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884" in namespace "projected-5274" to be "Succeeded or Failed"
    Jan 18 23:13:55.949: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.845002ms
    Jan 18 23:13:57.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006789438s
    Jan 18 23:13:59.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006856461s
    STEP: Saw pod success 01/18/23 23:13:59.953
    Jan 18 23:13:59.953: INFO: Pod "pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884" satisfied condition "Succeeded or Failed"
    Jan 18 23:13:59.956: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:13:59.96
    Jan 18 23:13:59.969: INFO: Waiting for pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 to disappear
    Jan 18 23:13:59.972: INFO: Pod pod-projected-configmaps-03e7804a-6619-4789-95e7-fc0e58727884 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:13:59.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5274" for this suite. 01/18/23 23:13:59.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:13:59.98
Jan 18 23:13:59.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 23:13:59.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:59.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:59.993
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 23:14:00.008
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:14:00.012
Jan 18 23:14:00.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:14:00.019: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:14:01.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 23:14:01.024: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/18/23 23:14:01.027
STEP: DeleteCollection of the DaemonSets 01/18/23 23:14:01.03
STEP: Verify that ReplicaSets have been deleted 01/18/23 23:14:01.036
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 18 23:14:01.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20000"},"items":null}

Jan 18 23:14:01.046: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20000"},"items":[{"metadata":{"name":"daemon-set-nxjnk","generateName":"daemon-set-","namespace":"daemonsets-5687","uid":"c2d719e0-f68b-4724-88cc-b333db7456ab","resourceVersion":"19992","creationTimestamp":"2023-01-18T23:14:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca5c47c2-cc98-4e89-9de1-6fe57f775a99","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5c47c2-cc98-4e89-9de1-6fe57f775a99\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8f5mj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8f5mj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-conformance-1-26-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-conformance-1-26-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"}],"hostIP":"10.128.15.199","podIP":"10.32.12.3","podIPs":[{"ip":"10.32.12.3"}],"startTime":"2023-01-18T23:14:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:14:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b24e9d6cd0cd5da1c6c888107b2ad90276aba50bbc6edd46c20882d86f0f52be","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w6n4d","generateName":"daemon-set-","namespace":"daemonsets-5687","uid":"78b92503-3945-494e-9529-e59cd8381ee6","resourceVersion":"19990","creationTimestamp":"2023-01-18T23:14:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca5c47c2-cc98-4e89-9de1-6fe57f775a99","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5c47c2-cc98-4e89-9de1-6fe57f775a99\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6987","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6987","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-conformance-1-26-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-conformance-1-26-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"}],"hostIP":"10.128.15.198","podIP":"10.32.0.2","podIPs":[{"ip":"10.32.0.2"}],"startTime":"2023-01-18T23:14:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:14:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b5802bd49f085f8724149483f012d608b12aca816f2dd3517df1ea9daad46058","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:01.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5687" for this suite. 01/18/23 23:14:01.058
------------------------------
â€¢ [1.085 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:13:59.98
    Jan 18 23:13:59.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 23:13:59.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:13:59.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:13:59.993
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 23:14:00.008
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:14:00.012
    Jan 18 23:14:00.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:14:00.019: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:14:01.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 23:14:01.024: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/18/23 23:14:01.027
    STEP: DeleteCollection of the DaemonSets 01/18/23 23:14:01.03
    STEP: Verify that ReplicaSets have been deleted 01/18/23 23:14:01.036
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 18 23:14:01.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20000"},"items":null}

    Jan 18 23:14:01.046: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20000"},"items":[{"metadata":{"name":"daemon-set-nxjnk","generateName":"daemon-set-","namespace":"daemonsets-5687","uid":"c2d719e0-f68b-4724-88cc-b333db7456ab","resourceVersion":"19992","creationTimestamp":"2023-01-18T23:14:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca5c47c2-cc98-4e89-9de1-6fe57f775a99","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5c47c2-cc98-4e89-9de1-6fe57f775a99\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8f5mj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8f5mj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-conformance-1-26-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-conformance-1-26-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"}],"hostIP":"10.128.15.199","podIP":"10.32.12.3","podIPs":[{"ip":"10.32.12.3"}],"startTime":"2023-01-18T23:14:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:14:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b24e9d6cd0cd5da1c6c888107b2ad90276aba50bbc6edd46c20882d86f0f52be","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w6n4d","generateName":"daemon-set-","namespace":"daemonsets-5687","uid":"78b92503-3945-494e-9529-e59cd8381ee6","resourceVersion":"19990","creationTimestamp":"2023-01-18T23:14:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca5c47c2-cc98-4e89-9de1-6fe57f775a99","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca5c47c2-cc98-4e89-9de1-6fe57f775a99\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:14:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6987","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6987","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-conformance-1-26-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-conformance-1-26-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:14:00Z"}],"hostIP":"10.128.15.198","podIP":"10.32.0.2","podIPs":[{"ip":"10.32.0.2"}],"startTime":"2023-01-18T23:14:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:14:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b5802bd49f085f8724149483f012d608b12aca816f2dd3517df1ea9daad46058","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:01.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5687" for this suite. 01/18/23 23:14:01.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:01.067
Jan 18 23:14:01.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:14:01.068
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:01.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:01.082
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:14:01.085
Jan 18 23:14:01.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95" in namespace "downward-api-6173" to be "Succeeded or Failed"
Jan 18 23:14:01.096: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085422ms
Jan 18 23:14:03.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005696888s
Jan 18 23:14:05.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005373073s
STEP: Saw pod success 01/18/23 23:14:05.099
Jan 18 23:14:05.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95" satisfied condition "Succeeded or Failed"
Jan 18 23:14:05.102: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 container client-container: <nil>
STEP: delete the pod 01/18/23 23:14:05.106
Jan 18 23:14:05.114: INFO: Waiting for pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 to disappear
Jan 18 23:14:05.117: INFO: Pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:05.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6173" for this suite. 01/18/23 23:14:05.12
------------------------------
â€¢ [4.058 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:01.067
    Jan 18 23:14:01.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:14:01.068
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:01.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:01.082
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:14:01.085
    Jan 18 23:14:01.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95" in namespace "downward-api-6173" to be "Succeeded or Failed"
    Jan 18 23:14:01.096: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085422ms
    Jan 18 23:14:03.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005696888s
    Jan 18 23:14:05.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005373073s
    STEP: Saw pod success 01/18/23 23:14:05.099
    Jan 18 23:14:05.099: INFO: Pod "downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95" satisfied condition "Succeeded or Failed"
    Jan 18 23:14:05.102: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:14:05.106
    Jan 18 23:14:05.114: INFO: Waiting for pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 to disappear
    Jan 18 23:14:05.117: INFO: Pod downwardapi-volume-4b84157d-ccd6-4310-a52a-4c96ce7d5a95 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:05.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6173" for this suite. 01/18/23 23:14:05.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:05.125
Jan 18 23:14:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:14:05.126
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:05.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:05.14
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-0ec88279-dd49-471c-ad34-6fe36a45ca1d 01/18/23 23:14:05.143
STEP: Creating a pod to test consume secrets 01/18/23 23:14:05.147
Jan 18 23:14:05.156: INFO: Waiting up to 5m0s for pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0" in namespace "secrets-125" to be "Succeeded or Failed"
Jan 18 23:14:05.159: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38918ms
Jan 18 23:14:07.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007074412s
Jan 18 23:14:09.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007500545s
STEP: Saw pod success 01/18/23 23:14:09.163
Jan 18 23:14:09.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0" satisfied condition "Succeeded or Failed"
Jan 18 23:14:09.166: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:14:09.17
Jan 18 23:14:09.180: INFO: Waiting for pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 to disappear
Jan 18 23:14:09.183: INFO: Pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:09.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-125" for this suite. 01/18/23 23:14:09.186
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:05.125
    Jan 18 23:14:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:14:05.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:05.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:05.14
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-0ec88279-dd49-471c-ad34-6fe36a45ca1d 01/18/23 23:14:05.143
    STEP: Creating a pod to test consume secrets 01/18/23 23:14:05.147
    Jan 18 23:14:05.156: INFO: Waiting up to 5m0s for pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0" in namespace "secrets-125" to be "Succeeded or Failed"
    Jan 18 23:14:05.159: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38918ms
    Jan 18 23:14:07.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007074412s
    Jan 18 23:14:09.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007500545s
    STEP: Saw pod success 01/18/23 23:14:09.163
    Jan 18 23:14:09.163: INFO: Pod "pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0" satisfied condition "Succeeded or Failed"
    Jan 18 23:14:09.166: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:14:09.17
    Jan 18 23:14:09.180: INFO: Waiting for pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 to disappear
    Jan 18 23:14:09.183: INFO: Pod pod-secrets-b1f07d36-086e-4ffb-a566-d6ddaf8de9f0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:09.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-125" for this suite. 01/18/23 23:14:09.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:09.191
Jan 18 23:14:09.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 23:14:09.192
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:09.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:09.204
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/18/23 23:14:09.209
Jan 18 23:14:09.209: INFO: Creating simple deployment test-deployment-msvz5
Jan 18 23:14:09.223: INFO: deployment "test-deployment-msvz5" doesn't have the required revision set
STEP: Getting /status 01/18/23 23:14:11.233
Jan 18 23:14:11.237: INFO: Deployment test-deployment-msvz5 has Conditions: [{Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/18/23 23:14:11.237
Jan 18 23:14:11.247: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-msvz5-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/18/23 23:14:11.247
Jan 18 23:14:11.249: INFO: Observed &Deployment event: ADDED
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-msvz5-54bc444df" is progressing.}
Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
Jan 18 23:14:11.250: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.250: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:14:11.250: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
Jan 18 23:14:11.250: INFO: Found Deployment test-deployment-msvz5 in namespace deployment-5156 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:14:11.250: INFO: Deployment test-deployment-msvz5 has an updated status
STEP: patching the Statefulset Status 01/18/23 23:14:11.25
Jan 18 23:14:11.250: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 23:14:11.258: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/18/23 23:14:11.258
Jan 18 23:14:11.260: INFO: Observed &Deployment event: ADDED
Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
Jan 18 23:14:11.260: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:14:11.260: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-msvz5-54bc444df" is progressing.}
Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:14:11.261: INFO: Found deployment test-deployment-msvz5 in namespace deployment-5156 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 23:14:11.261: INFO: Deployment test-deployment-msvz5 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:14:11.264: INFO: Deployment "test-deployment-msvz5":
&Deployment{ObjectMeta:{test-deployment-msvz5  deployment-5156  c72b83f5-f688-44e8-89f5-19d88128337f 20115 1 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 23:14:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b9f5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-msvz5-54bc444df",LastUpdateTime:2023-01-18 23:14:11 +0000 UTC,LastTransitionTime:2023-01-18 23:14:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:14:11.267: INFO: New ReplicaSet "test-deployment-msvz5-54bc444df" of Deployment "test-deployment-msvz5":
&ReplicaSet{ObjectMeta:{test-deployment-msvz5-54bc444df  deployment-5156  f109f300-4c2a-429f-b634-c7d156fcdf51 20101 1 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-msvz5 c72b83f5-f688-44e8-89f5-19d88128337f 0xc003de3590 0xc003de3591}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72b83f5-f688-44e8-89f5-19d88128337f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003de3638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:14:11.270: INFO: Pod "test-deployment-msvz5-54bc444df-8mzvl" is available:
&Pod{ObjectMeta:{test-deployment-msvz5-54bc444df-8mzvl test-deployment-msvz5-54bc444df- deployment-5156  a1f0ddb6-d5ed-40b7-9c3c-0ad37e31a86a 20100 0 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-msvz5-54bc444df f109f300-4c2a-429f-b634-c7d156fcdf51 0xc003de39e0 0xc003de39e1}] [] [{kube-controller-manager Update v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f109f300-4c2a-429f-b634-c7d156fcdf51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6h7lx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6h7lx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.3,StartTime:2023-01-18 23:14:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:14:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4d49a2861f64a06b43eeb8cd532bad8d4b0cbdf89b1ceb2c6c041d76c628211b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:11.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5156" for this suite. 01/18/23 23:14:11.275
------------------------------
â€¢ [2.089 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:09.191
    Jan 18 23:14:09.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 23:14:09.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:09.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:09.204
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/18/23 23:14:09.209
    Jan 18 23:14:09.209: INFO: Creating simple deployment test-deployment-msvz5
    Jan 18 23:14:09.223: INFO: deployment "test-deployment-msvz5" doesn't have the required revision set
    STEP: Getting /status 01/18/23 23:14:11.233
    Jan 18 23:14:11.237: INFO: Deployment test-deployment-msvz5 has Conditions: [{Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/18/23 23:14:11.237
    Jan 18 23:14:11.247: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 14, 9, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-msvz5-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/18/23 23:14:11.247
    Jan 18 23:14:11.249: INFO: Observed &Deployment event: ADDED
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
    Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-msvz5-54bc444df" is progressing.}
    Jan 18 23:14:11.249: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 23:14:11.249: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
    Jan 18 23:14:11.250: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.250: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 23:14:11.250: INFO: Observed Deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
    Jan 18 23:14:11.250: INFO: Found Deployment test-deployment-msvz5 in namespace deployment-5156 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 23:14:11.250: INFO: Deployment test-deployment-msvz5 has an updated status
    STEP: patching the Statefulset Status 01/18/23 23:14:11.25
    Jan 18 23:14:11.250: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 23:14:11.258: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/18/23 23:14:11.258
    Jan 18 23:14:11.260: INFO: Observed &Deployment event: ADDED
    Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
    Jan 18 23:14:11.260: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-msvz5-54bc444df"}
    Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 23:14:11.260: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 23:14:11.260: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-msvz5-54bc444df" is progressing.}
    Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
    Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:14:09 +0000 UTC 2023-01-18 23:14:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-msvz5-54bc444df" has successfully progressed.}
    Jan 18 23:14:11.261: INFO: Observed deployment test-deployment-msvz5 in namespace deployment-5156 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 23:14:11.261: INFO: Observed &Deployment event: MODIFIED
    Jan 18 23:14:11.261: INFO: Found deployment test-deployment-msvz5 in namespace deployment-5156 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 23:14:11.261: INFO: Deployment test-deployment-msvz5 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 23:14:11.264: INFO: Deployment "test-deployment-msvz5":
    &Deployment{ObjectMeta:{test-deployment-msvz5  deployment-5156  c72b83f5-f688-44e8-89f5-19d88128337f 20115 1 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 23:14:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b9f5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-msvz5-54bc444df",LastUpdateTime:2023-01-18 23:14:11 +0000 UTC,LastTransitionTime:2023-01-18 23:14:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 23:14:11.267: INFO: New ReplicaSet "test-deployment-msvz5-54bc444df" of Deployment "test-deployment-msvz5":
    &ReplicaSet{ObjectMeta:{test-deployment-msvz5-54bc444df  deployment-5156  f109f300-4c2a-429f-b634-c7d156fcdf51 20101 1 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-msvz5 c72b83f5-f688-44e8-89f5-19d88128337f 0xc003de3590 0xc003de3591}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72b83f5-f688-44e8-89f5-19d88128337f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003de3638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:14:11.270: INFO: Pod "test-deployment-msvz5-54bc444df-8mzvl" is available:
    &Pod{ObjectMeta:{test-deployment-msvz5-54bc444df-8mzvl test-deployment-msvz5-54bc444df- deployment-5156  a1f0ddb6-d5ed-40b7-9c3c-0ad37e31a86a 20100 0 2023-01-18 23:14:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-msvz5-54bc444df f109f300-4c2a-429f-b634-c7d156fcdf51 0xc003de39e0 0xc003de39e1}] [] [{kube-controller-manager Update v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f109f300-4c2a-429f-b634-c7d156fcdf51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:14:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6h7lx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6h7lx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:14:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.3,StartTime:2023-01-18 23:14:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:14:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4d49a2861f64a06b43eeb8cd532bad8d4b0cbdf89b1ceb2c6c041d76c628211b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:11.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5156" for this suite. 01/18/23 23:14:11.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:11.282
Jan 18 23:14:11.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:14:11.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:11.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:11.296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 18 23:14:11.307: INFO: Waiting up to 2m0s for pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" in namespace "var-expansion-5009" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 23:14:11.309: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597902ms
Jan 18 23:14:13.313: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006358535s
Jan 18 23:14:13.313: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 23:14:13.313: INFO: Deleting pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" in namespace "var-expansion-5009"
Jan 18 23:14:13.320: INFO: Wait up to 5m0s for pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:15.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5009" for this suite. 01/18/23 23:14:15.329
------------------------------
â€¢ [4.056 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:11.282
    Jan 18 23:14:11.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:14:11.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:11.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:11.296
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 18 23:14:11.307: INFO: Waiting up to 2m0s for pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" in namespace "var-expansion-5009" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 23:14:11.309: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597902ms
    Jan 18 23:14:13.313: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006358535s
    Jan 18 23:14:13.313: INFO: Pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 23:14:13.313: INFO: Deleting pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" in namespace "var-expansion-5009"
    Jan 18 23:14:13.320: INFO: Wait up to 5m0s for pod "var-expansion-580f6f4e-1f5f-4c74-a0ae-f28aa28afb79" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:15.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5009" for this suite. 01/18/23 23:14:15.329
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:15.338
Jan 18 23:14:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:14:15.339
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:15.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:15.354
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/18/23 23:14:15.357
Jan 18 23:14:15.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: mark a version not serverd 01/18/23 23:14:18.946
STEP: check the unserved version gets removed 01/18/23 23:14:18.966
STEP: check the other version is not changed 01/18/23 23:14:19.87
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:22.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-10" for this suite. 01/18/23 23:14:22.681
------------------------------
â€¢ [SLOW TEST] [7.349 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:15.338
    Jan 18 23:14:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:14:15.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:15.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:15.354
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/18/23 23:14:15.357
    Jan 18 23:14:15.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: mark a version not serverd 01/18/23 23:14:18.946
    STEP: check the unserved version gets removed 01/18/23 23:14:18.966
    STEP: check the other version is not changed 01/18/23 23:14:19.87
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:22.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-10" for this suite. 01/18/23 23:14:22.681
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:22.687
Jan 18 23:14:22.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 23:14:22.688
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:22.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:22.702
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6 01/18/23 23:14:22.705
Jan 18 23:14:22.715: INFO: Pod name my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Found 0 pods out of 1
Jan 18 23:14:27.719: INFO: Pod name my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Found 1 pods out of 1
Jan 18 23:14:27.719: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6" are running
Jan 18 23:14:27.719: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" in namespace "replication-controller-2584" to be "running"
Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj": Phase="Running", Reason="", readiness=true. Elapsed: 2.092007ms
Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" satisfied condition "running"
Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:22 +0000 UTC Reason: Message:}])
Jan 18 23:14:27.721: INFO: Trying to dial the pod
Jan 18 23:14:32.733: INFO: Controller my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Got expected result from replica 1 [my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj]: "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:14:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2584" for this suite. 01/18/23 23:14:32.737
------------------------------
â€¢ [SLOW TEST] [10.055 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:22.687
    Jan 18 23:14:22.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 23:14:22.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:22.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:22.702
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6 01/18/23 23:14:22.705
    Jan 18 23:14:22.715: INFO: Pod name my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Found 0 pods out of 1
    Jan 18 23:14:27.719: INFO: Pod name my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Found 1 pods out of 1
    Jan 18 23:14:27.719: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6" are running
    Jan 18 23:14:27.719: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" in namespace "replication-controller-2584" to be "running"
    Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj": Phase="Running", Reason="", readiness=true. Elapsed: 2.092007ms
    Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" satisfied condition "running"
    Jan 18 23:14:27.721: INFO: Pod "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:14:22 +0000 UTC Reason: Message:}])
    Jan 18 23:14:27.721: INFO: Trying to dial the pod
    Jan 18 23:14:32.733: INFO: Controller my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6: Got expected result from replica 1 [my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj]: "my-hostname-basic-3a9ce14e-b8f7-41c2-bd0c-5d3e7ccacdf6-t75wj", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:14:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2584" for this suite. 01/18/23 23:14:32.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:14:32.743
Jan 18 23:14:32.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename endpointslice 01/18/23 23:14:32.744
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:32.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:32.76
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/18/23 23:14:37.816
STEP: referencing matching pods with named port 01/18/23 23:14:42.822
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 23:14:47.829
STEP: recreating EndpointSlices after they've been deleted 01/18/23 23:14:52.836
Jan 18 23:14:52.851: INFO: EndpointSlice for Service endpointslice-3578/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3578" for this suite. 01/18/23 23:15:02.861
------------------------------
â€¢ [SLOW TEST] [30.123 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:14:32.743
    Jan 18 23:14:32.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 23:14:32.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:14:32.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:14:32.76
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/18/23 23:14:37.816
    STEP: referencing matching pods with named port 01/18/23 23:14:42.822
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 23:14:47.829
    STEP: recreating EndpointSlices after they've been deleted 01/18/23 23:14:52.836
    Jan 18 23:14:52.851: INFO: EndpointSlice for Service endpointslice-3578/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3578" for this suite. 01/18/23 23:15:02.861
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:02.866
Jan 18 23:15:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename limitrange 01/18/23 23:15:02.867
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:02.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:02.883
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/18/23 23:15:02.886
STEP: Setting up watch 01/18/23 23:15:02.886
STEP: Submitting a LimitRange 01/18/23 23:15:02.989
STEP: Verifying LimitRange creation was observed 01/18/23 23:15:02.994
STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 23:15:02.994
Jan 18 23:15:02.996: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 23:15:02.996: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/18/23 23:15:02.996
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 23:15:03.003
Jan 18 23:15:03.006: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 23:15:03.006: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/18/23 23:15:03.006
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 23:15:03.013
Jan 18 23:15:03.016: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 23:15:03.016: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/18/23 23:15:03.016
STEP: Failing to create a Pod with more than max resources 01/18/23 23:15:03.018
STEP: Updating a LimitRange 01/18/23 23:15:03.02
STEP: Verifying LimitRange updating is effective 01/18/23 23:15:03.027
STEP: Creating a Pod with less than former min resources 01/18/23 23:15:05.031
STEP: Failing to create a Pod with more than max resources 01/18/23 23:15:05.041
STEP: Deleting a LimitRange 01/18/23 23:15:05.044
STEP: Verifying the LimitRange was deleted 01/18/23 23:15:05.049
Jan 18 23:15:10.053: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/18/23 23:15:10.053
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:10.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5467" for this suite. 01/18/23 23:15:10.063
------------------------------
â€¢ [SLOW TEST] [7.203 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:02.866
    Jan 18 23:15:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename limitrange 01/18/23 23:15:02.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:02.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:02.883
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/18/23 23:15:02.886
    STEP: Setting up watch 01/18/23 23:15:02.886
    STEP: Submitting a LimitRange 01/18/23 23:15:02.989
    STEP: Verifying LimitRange creation was observed 01/18/23 23:15:02.994
    STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 23:15:02.994
    Jan 18 23:15:02.996: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 23:15:02.996: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/18/23 23:15:02.996
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 23:15:03.003
    Jan 18 23:15:03.006: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 23:15:03.006: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/18/23 23:15:03.006
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 23:15:03.013
    Jan 18 23:15:03.016: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 18 23:15:03.016: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/18/23 23:15:03.016
    STEP: Failing to create a Pod with more than max resources 01/18/23 23:15:03.018
    STEP: Updating a LimitRange 01/18/23 23:15:03.02
    STEP: Verifying LimitRange updating is effective 01/18/23 23:15:03.027
    STEP: Creating a Pod with less than former min resources 01/18/23 23:15:05.031
    STEP: Failing to create a Pod with more than max resources 01/18/23 23:15:05.041
    STEP: Deleting a LimitRange 01/18/23 23:15:05.044
    STEP: Verifying the LimitRange was deleted 01/18/23 23:15:05.049
    Jan 18 23:15:10.053: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/18/23 23:15:10.053
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:10.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5467" for this suite. 01/18/23 23:15:10.063
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:10.07
Jan 18 23:15:10.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption 01/18/23 23:15:10.071
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:10.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:10.086
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/18/23 23:15:10.089
STEP: Waiting for the pdb to be processed 01/18/23 23:15:10.093
STEP: updating the pdb 01/18/23 23:15:12.099
STEP: Waiting for the pdb to be processed 01/18/23 23:15:12.105
STEP: patching the pdb 01/18/23 23:15:14.112
STEP: Waiting for the pdb to be processed 01/18/23 23:15:14.12
STEP: Waiting for the pdb to be deleted 01/18/23 23:15:16.13
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:16.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5145" for this suite. 01/18/23 23:15:16.135
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:10.07
    Jan 18 23:15:10.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption 01/18/23 23:15:10.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:10.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:10.086
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/18/23 23:15:10.089
    STEP: Waiting for the pdb to be processed 01/18/23 23:15:10.093
    STEP: updating the pdb 01/18/23 23:15:12.099
    STEP: Waiting for the pdb to be processed 01/18/23 23:15:12.105
    STEP: patching the pdb 01/18/23 23:15:14.112
    STEP: Waiting for the pdb to be processed 01/18/23 23:15:14.12
    STEP: Waiting for the pdb to be deleted 01/18/23 23:15:16.13
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:16.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5145" for this suite. 01/18/23 23:15:16.135
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:16.141
Jan 18 23:15:16.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:15:16.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:16.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:16.156
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/18/23 23:15:16.159
Jan 18 23:15:16.170: INFO: Waiting up to 5m0s for pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c" in namespace "var-expansion-2551" to be "Succeeded or Failed"
Jan 18 23:15:16.172: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668382ms
Jan 18 23:15:18.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00608742s
Jan 18 23:15:20.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006038703s
STEP: Saw pod success 01/18/23 23:15:20.176
Jan 18 23:15:20.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c" satisfied condition "Succeeded or Failed"
Jan 18 23:15:20.178: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:15:20.183
Jan 18 23:15:20.193: INFO: Waiting for pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c to disappear
Jan 18 23:15:20.195: INFO: Pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:20.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2551" for this suite. 01/18/23 23:15:20.198
------------------------------
â€¢ [4.063 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:16.141
    Jan 18 23:15:16.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:15:16.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:16.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:16.156
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/18/23 23:15:16.159
    Jan 18 23:15:16.170: INFO: Waiting up to 5m0s for pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c" in namespace "var-expansion-2551" to be "Succeeded or Failed"
    Jan 18 23:15:16.172: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668382ms
    Jan 18 23:15:18.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00608742s
    Jan 18 23:15:20.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006038703s
    STEP: Saw pod success 01/18/23 23:15:20.176
    Jan 18 23:15:20.176: INFO: Pod "var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c" satisfied condition "Succeeded or Failed"
    Jan 18 23:15:20.178: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:15:20.183
    Jan 18 23:15:20.193: INFO: Waiting for pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c to disappear
    Jan 18 23:15:20.195: INFO: Pod var-expansion-7d0c2f4f-85a8-4996-a963-b5a34af1156c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:20.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2551" for this suite. 01/18/23 23:15:20.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:20.208
Jan 18 23:15:20.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:15:20.209
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:20.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:20.222
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/18/23 23:15:20.225
Jan 18 23:15:20.234: INFO: Waiting up to 5m0s for pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087" in namespace "var-expansion-8219" to be "Succeeded or Failed"
Jan 18 23:15:20.237: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274935ms
Jan 18 23:15:22.240: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005948021s
Jan 18 23:15:24.240: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006167161s
STEP: Saw pod success 01/18/23 23:15:24.241
Jan 18 23:15:24.241: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087" satisfied condition "Succeeded or Failed"
Jan 18 23:15:24.243: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:15:24.248
Jan 18 23:15:24.258: INFO: Waiting for pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 to disappear
Jan 18 23:15:24.260: INFO: Pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8219" for this suite. 01/18/23 23:15:24.263
------------------------------
â€¢ [4.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:20.208
    Jan 18 23:15:20.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:15:20.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:20.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:20.222
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/18/23 23:15:20.225
    Jan 18 23:15:20.234: INFO: Waiting up to 5m0s for pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087" in namespace "var-expansion-8219" to be "Succeeded or Failed"
    Jan 18 23:15:20.237: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274935ms
    Jan 18 23:15:22.240: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005948021s
    Jan 18 23:15:24.240: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006167161s
    STEP: Saw pod success 01/18/23 23:15:24.241
    Jan 18 23:15:24.241: INFO: Pod "var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087" satisfied condition "Succeeded or Failed"
    Jan 18 23:15:24.243: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:15:24.248
    Jan 18 23:15:24.258: INFO: Waiting for pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 to disappear
    Jan 18 23:15:24.260: INFO: Pod var-expansion-9df0072f-9269-4eff-a7af-3e6384d0b087 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8219" for this suite. 01/18/23 23:15:24.263
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:24.27
Jan 18 23:15:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 23:15:24.271
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:24.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:24.285
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/18/23 23:15:24.288
STEP: Ensuring active pods == parallelism 01/18/23 23:15:24.295
STEP: Orphaning one of the Job's Pods 01/18/23 23:15:26.299
Jan 18 23:15:26.814: INFO: Successfully updated pod "adopt-release-84m4h"
STEP: Checking that the Job readopts the Pod 01/18/23 23:15:26.814
Jan 18 23:15:26.814: INFO: Waiting up to 15m0s for pod "adopt-release-84m4h" in namespace "job-4423" to be "adopted"
Jan 18 23:15:26.817: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.406428ms
Jan 18 23:15:28.820: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012597s
Jan 18 23:15:28.820: INFO: Pod "adopt-release-84m4h" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/18/23 23:15:28.82
Jan 18 23:15:29.331: INFO: Successfully updated pod "adopt-release-84m4h"
STEP: Checking that the Job releases the Pod 01/18/23 23:15:29.331
Jan 18 23:15:29.331: INFO: Waiting up to 15m0s for pod "adopt-release-84m4h" in namespace "job-4423" to be "released"
Jan 18 23:15:29.333: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.068476ms
Jan 18 23:15:31.337: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.00584195s
Jan 18 23:15:31.337: INFO: Pod "adopt-release-84m4h" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:31.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4423" for this suite. 01/18/23 23:15:31.34
------------------------------
â€¢ [SLOW TEST] [7.075 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:24.27
    Jan 18 23:15:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 23:15:24.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:24.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:24.285
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/18/23 23:15:24.288
    STEP: Ensuring active pods == parallelism 01/18/23 23:15:24.295
    STEP: Orphaning one of the Job's Pods 01/18/23 23:15:26.299
    Jan 18 23:15:26.814: INFO: Successfully updated pod "adopt-release-84m4h"
    STEP: Checking that the Job readopts the Pod 01/18/23 23:15:26.814
    Jan 18 23:15:26.814: INFO: Waiting up to 15m0s for pod "adopt-release-84m4h" in namespace "job-4423" to be "adopted"
    Jan 18 23:15:26.817: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.406428ms
    Jan 18 23:15:28.820: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012597s
    Jan 18 23:15:28.820: INFO: Pod "adopt-release-84m4h" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/18/23 23:15:28.82
    Jan 18 23:15:29.331: INFO: Successfully updated pod "adopt-release-84m4h"
    STEP: Checking that the Job releases the Pod 01/18/23 23:15:29.331
    Jan 18 23:15:29.331: INFO: Waiting up to 15m0s for pod "adopt-release-84m4h" in namespace "job-4423" to be "released"
    Jan 18 23:15:29.333: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.068476ms
    Jan 18 23:15:31.337: INFO: Pod "adopt-release-84m4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.00584195s
    Jan 18 23:15:31.337: INFO: Pod "adopt-release-84m4h" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:31.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4423" for this suite. 01/18/23 23:15:31.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:31.346
Jan 18 23:15:31.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename deployment 01/18/23 23:15:31.347
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:31.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:31.359
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 18 23:15:31.362: INFO: Creating deployment "webserver-deployment"
Jan 18 23:15:31.368: INFO: Waiting for observed generation 1
Jan 18 23:15:33.373: INFO: Waiting for all required pods to come up
Jan 18 23:15:33.377: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/18/23 23:15:33.377
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s5pr4" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2t6fh" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pdrnm" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-b6c9n" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qfhxw" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-brvz4" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8rx76" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qv5fx" in namespace "deployment-704" to be "running"
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.067987ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139423ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.10741ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245456ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358165ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273236ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487956ms
Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416543ms
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76": Phase="Running", Reason="", readiness=true. Elapsed: 2.007456065s
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76" satisfied condition "running"
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.007791427s
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n" satisfied condition "running"
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007848895s
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4" satisfied condition "running"
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007981419s
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm" satisfied condition "running"
Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00799626s
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008263696s
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4" satisfied condition "running"
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008086991s
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw" satisfied condition "running"
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx" satisfied condition "running"
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008279644s
Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh" satisfied condition "running"
Jan 18 23:15:35.386: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 23:15:35.391: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 23:15:35.398: INFO: Updating deployment webserver-deployment
Jan 18 23:15:35.398: INFO: Waiting for observed generation 2
Jan 18 23:15:37.404: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 23:15:37.406: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 23:15:37.408: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:15:37.418: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 23:15:37.418: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 23:15:37.420: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:15:37.424: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 23:15:37.424: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 23:15:37.434: INFO: Updating deployment webserver-deployment
Jan 18 23:15:37.434: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:15:37.441: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 23:15:37.444: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:15:37.451: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-704  bc935fcc-900d-4695-8be1-ecdf1925a837 20810 3 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dc6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:15:34 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-18 23:15:35 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 23:15:37.457: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-704  982f841d-ecfa-47d5-85f6-61e5ecd42607 20814 3 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bc935fcc-900d-4695-8be1-ecdf1925a837 0xc0044dcbe7 0xc0044dcbe8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc935fcc-900d-4695-8be1-ecdf1925a837\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dcc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:15:37.457: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 23:15:37.457: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-704  fb1663bc-277c-4a97-8d53-4b290b75a101 20811 3 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bc935fcc-900d-4695-8be1-ecdf1925a837 0xc0044dcaf7 0xc0044dcaf8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc935fcc-900d-4695-8be1-ecdf1925a837\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dcb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:15:37.463: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2t6fh webserver-deployment-7f5969cbc7- deployment-704  6fe09445-15cb-4b43-8fd1-947add5459c7 20681 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc0044dd617 0xc0044dd618}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px46g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px46g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.2,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://449c6f2da3c57f0ed67697a5e6460ed180c14ddde15f3fcd8fe9f7cbf263b871,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.463: INFO: Pod "webserver-deployment-7f5969cbc7-7dkt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7dkt8 webserver-deployment-7f5969cbc7- deployment-704  e3a04ca8-6b82-4cd0-852a-5be83b0dbaa4 20819 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc0044ddf10 0xc0044ddf11}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr9xw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr9xw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8rx76 webserver-deployment-7f5969cbc7- deployment-704  a46d44ad-c4a5-4407-8673-ff16acb44801 20687 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4047 0xc002ff4048}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccn4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccn4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.6,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50c0a986fe21b72a0b346a18597045255c8ab01a5f7d3a76b74f2bd74c787a1e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-brvz4 webserver-deployment-7f5969cbc7- deployment-704  2b26d5a7-18aa-49c3-8a8f-b960f7bd5787 20699 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4220 0xc002ff4221}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7m9qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7m9qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.7,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65db92a1311df7efa01361e7d215b7d712ca58b301902239970f5af686dc6de1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-gv9mt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gv9mt webserver-deployment-7f5969cbc7- deployment-704  134d5701-74b4-43ff-a777-4c296c3a142c 20668 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff43f0 0xc002ff43f1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4fc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4fc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.3,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4563d5ff24b460467de93a5010c934f20e24ea3c27e6881cf07e5e3cdcbef1af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-jqrrq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqrrq webserver-deployment-7f5969cbc7- deployment-704  ca2056c0-ade2-47d8-b71c-586c8e3a77d0 20818 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff45c0 0xc002ff45c1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nbp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nbp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-nzhz9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nzhz9 webserver-deployment-7f5969cbc7- deployment-704  7615c0fc-805f-49bb-af4d-513dd66500a6 20672 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff46f7 0xc002ff46f8}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lplqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lplqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.8,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://534f671bfa8b66d3d42d69dbb3e00a17e7f46314b944457c2d876f60b2e27cf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pdrnm webserver-deployment-7f5969cbc7- deployment-704  45e1859f-3f6d-490d-8c77-97db56c82a97 20696 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff48d0 0xc002ff48d1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdf4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdf4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.9,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad17c0bc8d6b2ed0ff52550a791eee6092d940529c20f0738616358f878d5984,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qfhxw webserver-deployment-7f5969cbc7- deployment-704  fa4f7c1c-dd3d-463b-a423-cd03f2648b99 20684 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4aa0 0xc002ff4aa1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nc55m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nc55m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.7,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://46106a441b32c25378114847863385375bb04eb886eae5bb06ec93aa4013da50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-s29wg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s29wg webserver-deployment-7f5969cbc7- deployment-704  ec8cff3a-5d9c-419d-8c33-a178b03e868d 20815 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4c70 0xc002ff4c71}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-945pd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-945pd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s5pr4 webserver-deployment-7f5969cbc7- deployment-704  442c0e4b-9f28-45ba-aee0-85061b3dba70 20693 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4dc0 0xc002ff4dc1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9bqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9bqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.8,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f4580a2caac75f0da0620e66f77207b6adf66557542d5783e6a7191039d71df7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-d9f79cb5-2s8fv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2s8fv webserver-deployment-d9f79cb5- deployment-704  a8089bf4-b0bb-443f-808a-c4e83f8eb433 20800 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff4f7f 0xc002ff4f90}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz5v4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz5v4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.9,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-5ftzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5ftzj webserver-deployment-d9f79cb5- deployment-704  83726423-bb3c-4a6f-9dc8-a286f09c86ce 20804 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff517f 0xc002ff5190}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8q7fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8q7fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.10,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-5n7vp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5n7vp webserver-deployment-d9f79cb5- deployment-704  4257b0b1-995d-477c-8668-c91eec2c6db4 20724 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff537f 0xc002ff5390}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gc4c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc4c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-dr6wf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dr6wf webserver-deployment-d9f79cb5- deployment-704  55cacced-d048-46e0-880f-620da1cd94a1 20793 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5567 0xc002ff5568}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-758hn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-758hn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-gdz5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gdz5v webserver-deployment-d9f79cb5- deployment-704  2ca692ab-1c0d-4266-899a-2524954d4ae6 20817 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5747 0xc002ff5748}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmtbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmtbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:15:37.467: INFO: Pod "webserver-deployment-d9f79cb5-j5jcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5jcl webserver-deployment-d9f79cb5- deployment-704  13539dd4-32db-4a72-8daa-bd6e51f1af37 20756 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5897 0xc002ff5898}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfrf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfrf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-704" for this suite. 01/18/23 23:15:37.472
------------------------------
â€¢ [SLOW TEST] [6.139 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:31.346
    Jan 18 23:15:31.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename deployment 01/18/23 23:15:31.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:31.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:31.359
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 18 23:15:31.362: INFO: Creating deployment "webserver-deployment"
    Jan 18 23:15:31.368: INFO: Waiting for observed generation 1
    Jan 18 23:15:33.373: INFO: Waiting for all required pods to come up
    Jan 18 23:15:33.377: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/18/23 23:15:33.377
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s5pr4" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2t6fh" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pdrnm" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-b6c9n" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qfhxw" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-brvz4" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8rx76" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.377: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qv5fx" in namespace "deployment-704" to be "running"
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.067987ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139423ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.10741ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245456ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358165ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273236ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487956ms
    Jan 18 23:15:33.381: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416543ms
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76": Phase="Running", Reason="", readiness=true. Elapsed: 2.007456065s
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76" satisfied condition "running"
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.007791427s
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-b6c9n" satisfied condition "running"
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007848895s
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4" satisfied condition "running"
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007981419s
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm" satisfied condition "running"
    Jan 18 23:15:35.385: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00799626s
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008263696s
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4" satisfied condition "running"
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008086991s
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw" satisfied condition "running"
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-qv5fx" satisfied condition "running"
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008279644s
    Jan 18 23:15:35.386: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh" satisfied condition "running"
    Jan 18 23:15:35.386: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 18 23:15:35.391: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 18 23:15:35.398: INFO: Updating deployment webserver-deployment
    Jan 18 23:15:35.398: INFO: Waiting for observed generation 2
    Jan 18 23:15:37.404: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 18 23:15:37.406: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 18 23:15:37.408: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 23:15:37.418: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 18 23:15:37.418: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 18 23:15:37.420: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 23:15:37.424: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 18 23:15:37.424: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 18 23:15:37.434: INFO: Updating deployment webserver-deployment
    Jan 18 23:15:37.434: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 23:15:37.441: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 18 23:15:37.444: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 23:15:37.451: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-704  bc935fcc-900d-4695-8be1-ecdf1925a837 20810 3 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dc6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:15:34 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-18 23:15:35 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 18 23:15:37.457: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-704  982f841d-ecfa-47d5-85f6-61e5ecd42607 20814 3 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bc935fcc-900d-4695-8be1-ecdf1925a837 0xc0044dcbe7 0xc0044dcbe8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc935fcc-900d-4695-8be1-ecdf1925a837\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dcc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:15:37.457: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 18 23:15:37.457: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-704  fb1663bc-277c-4a97-8d53-4b290b75a101 20811 3 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bc935fcc-900d-4695-8be1-ecdf1925a837 0xc0044dcaf7 0xc0044dcaf8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc935fcc-900d-4695-8be1-ecdf1925a837\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044dcb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:15:37.463: INFO: Pod "webserver-deployment-7f5969cbc7-2t6fh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2t6fh webserver-deployment-7f5969cbc7- deployment-704  6fe09445-15cb-4b43-8fd1-947add5459c7 20681 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc0044dd617 0xc0044dd618}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px46g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px46g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.2,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://449c6f2da3c57f0ed67697a5e6460ed180c14ddde15f3fcd8fe9f7cbf263b871,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.463: INFO: Pod "webserver-deployment-7f5969cbc7-7dkt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7dkt8 webserver-deployment-7f5969cbc7- deployment-704  e3a04ca8-6b82-4cd0-852a-5be83b0dbaa4 20819 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc0044ddf10 0xc0044ddf11}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr9xw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr9xw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-8rx76" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8rx76 webserver-deployment-7f5969cbc7- deployment-704  a46d44ad-c4a5-4407-8673-ff16acb44801 20687 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4047 0xc002ff4048}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccn4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccn4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.6,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50c0a986fe21b72a0b346a18597045255c8ab01a5f7d3a76b74f2bd74c787a1e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-brvz4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-brvz4 webserver-deployment-7f5969cbc7- deployment-704  2b26d5a7-18aa-49c3-8a8f-b960f7bd5787 20699 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4220 0xc002ff4221}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7m9qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7m9qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.7,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65db92a1311df7efa01361e7d215b7d712ca58b301902239970f5af686dc6de1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-gv9mt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gv9mt webserver-deployment-7f5969cbc7- deployment-704  134d5701-74b4-43ff-a777-4c296c3a142c 20668 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff43f0 0xc002ff43f1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4fc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4fc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.3,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4563d5ff24b460467de93a5010c934f20e24ea3c27e6881cf07e5e3cdcbef1af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-jqrrq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqrrq webserver-deployment-7f5969cbc7- deployment-704  ca2056c0-ade2-47d8-b71c-586c8e3a77d0 20818 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff45c0 0xc002ff45c1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nbp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nbp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.464: INFO: Pod "webserver-deployment-7f5969cbc7-nzhz9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nzhz9 webserver-deployment-7f5969cbc7- deployment-704  7615c0fc-805f-49bb-af4d-513dd66500a6 20672 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff46f7 0xc002ff46f8}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lplqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lplqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.8,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://534f671bfa8b66d3d42d69dbb3e00a17e7f46314b944457c2d876f60b2e27cf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-pdrnm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pdrnm webserver-deployment-7f5969cbc7- deployment-704  45e1859f-3f6d-490d-8c77-97db56c82a97 20696 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff48d0 0xc002ff48d1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdf4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdf4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.9,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad17c0bc8d6b2ed0ff52550a791eee6092d940529c20f0738616358f878d5984,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-qfhxw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qfhxw webserver-deployment-7f5969cbc7- deployment-704  fa4f7c1c-dd3d-463b-a423-cd03f2648b99 20684 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4aa0 0xc002ff4aa1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.12.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nc55m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nc55m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:10.32.12.7,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://46106a441b32c25378114847863385375bb04eb886eae5bb06ec93aa4013da50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.12.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-s29wg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s29wg webserver-deployment-7f5969cbc7- deployment-704  ec8cff3a-5d9c-419d-8c33-a178b03e868d 20815 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4c70 0xc002ff4c71}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-945pd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-945pd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-7f5969cbc7-s5pr4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s5pr4 webserver-deployment-7f5969cbc7- deployment-704  442c0e4b-9f28-45ba-aee0-85061b3dba70 20693 0 2023-01-18 23:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fb1663bc-277c-4a97-8d53-4b290b75a101 0xc002ff4dc0 0xc002ff4dc1}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb1663bc-277c-4a97-8d53-4b290b75a101\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9bqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9bqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.8,StartTime:2023-01-18 23:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f4580a2caac75f0da0620e66f77207b6adf66557542d5783e6a7191039d71df7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.465: INFO: Pod "webserver-deployment-d9f79cb5-2s8fv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2s8fv webserver-deployment-d9f79cb5- deployment-704  a8089bf4-b0bb-443f-808a-c4e83f8eb433 20800 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff4f7f 0xc002ff4f90}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz5v4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz5v4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.9,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-5ftzj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5ftzj webserver-deployment-d9f79cb5- deployment-704  83726423-bb3c-4a6f-9dc8-a286f09c86ce 20804 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff517f 0xc002ff5190}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.32.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8q7fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8q7fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.198,PodIP:10.32.0.10,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.32.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-5n7vp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5n7vp webserver-deployment-d9f79cb5- deployment-704  4257b0b1-995d-477c-8668-c91eec2c6db4 20724 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff537f 0xc002ff5390}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gc4c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc4c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-dr6wf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dr6wf webserver-deployment-d9f79cb5- deployment-704  55cacced-d048-46e0-880f-620da1cd94a1 20793 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5567 0xc002ff5568}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-758hn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-758hn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.466: INFO: Pod "webserver-deployment-d9f79cb5-gdz5v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gdz5v webserver-deployment-d9f79cb5- deployment-704  2ca692ab-1c0d-4266-899a-2524954d4ae6 20817 0 2023-01-18 23:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5747 0xc002ff5748}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmtbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmtbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 23:15:37.467: INFO: Pod "webserver-deployment-d9f79cb5-j5jcl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5jcl webserver-deployment-d9f79cb5- deployment-704  13539dd4-32db-4a72-8daa-bd6e51f1af37 20756 0 2023-01-18 23:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 982f841d-ecfa-47d5-85f6-61e5ecd42607 0xc002ff5897 0xc002ff5898}] [] [{kube-controller-manager Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"982f841d-ecfa-47d5-85f6-61e5ecd42607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfrf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfrf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-conformance-1-26-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.199,PodIP:,StartTime:2023-01-18 23:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-704" for this suite. 01/18/23 23:15:37.472
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:37.486
Jan 18 23:15:37.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 23:15:37.487
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:37.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:37.531
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/18/23 23:15:37.54
STEP: create the rc2 01/18/23 23:15:37.55
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 23:15:42.561
STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 23:15:43.077
STEP: wait for the rc to be deleted 01/18/23 23:15:43.085
Jan 18 23:15:48.095: INFO: 79 pods remaining
Jan 18 23:15:48.095: INFO: 79 pods has nil DeletionTimestamp
Jan 18 23:15:48.095: INFO: 
STEP: Gathering metrics 01/18/23 23:15:53.094
Jan 18 23:15:53.111: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 23:15:53.114: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.02221ms
Jan 18 23:15:53.114: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 23:15:53.114: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 23:15:54.457: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 23:15:54.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hr5f" in namespace "gc-1164"
Jan 18 23:15:54.465: INFO: Deleting pod "simpletest-rc-to-be-deleted-466lq" in namespace "gc-1164"
Jan 18 23:15:54.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hw8l" in namespace "gc-1164"
Jan 18 23:15:54.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dj4n" in namespace "gc-1164"
Jan 18 23:15:54.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxbv" in namespace "gc-1164"
Jan 18 23:15:54.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m8dc" in namespace "gc-1164"
Jan 18 23:15:54.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qdj5" in namespace "gc-1164"
Jan 18 23:15:54.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mlz7" in namespace "gc-1164"
Jan 18 23:15:54.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nljb" in namespace "gc-1164"
Jan 18 23:15:54.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-6npc5" in namespace "gc-1164"
Jan 18 23:15:54.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rtc5" in namespace "gc-1164"
Jan 18 23:15:54.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-6scsp" in namespace "gc-1164"
Jan 18 23:15:54.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-74phr" in namespace "gc-1164"
Jan 18 23:15:54.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-79crq" in namespace "gc-1164"
Jan 18 23:15:54.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f5wf" in namespace "gc-1164"
Jan 18 23:15:54.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grq8" in namespace "gc-1164"
Jan 18 23:15:54.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vtjb" in namespace "gc-1164"
Jan 18 23:15:54.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-8564v" in namespace "gc-1164"
Jan 18 23:15:54.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mgr2" in namespace "gc-1164"
Jan 18 23:15:54.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thfs" in namespace "gc-1164"
Jan 18 23:15:54.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nq9j" in namespace "gc-1164"
Jan 18 23:15:54.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v6ml" in namespace "gc-1164"
Jan 18 23:15:54.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x7zr" in namespace "gc-1164"
Jan 18 23:15:54.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xp5f" in namespace "gc-1164"
Jan 18 23:15:54.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-bphkc" in namespace "gc-1164"
Jan 18 23:15:54.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-c28bw" in namespace "gc-1164"
Jan 18 23:15:54.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5wcx" in namespace "gc-1164"
Jan 18 23:15:54.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc2p4" in namespace "gc-1164"
Jan 18 23:15:54.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-czwwh" in namespace "gc-1164"
Jan 18 23:15:54.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddndp" in namespace "gc-1164"
Jan 18 23:15:54.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv78k" in namespace "gc-1164"
Jan 18 23:15:54.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvf8w" in namespace "gc-1164"
Jan 18 23:15:54.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwffl" in namespace "gc-1164"
Jan 18 23:15:54.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2ngc" in namespace "gc-1164"
Jan 18 23:15:54.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2tqv" in namespace "gc-1164"
Jan 18 23:15:54.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-flvgb" in namespace "gc-1164"
Jan 18 23:15:54.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmlxp" in namespace "gc-1164"
Jan 18 23:15:54.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq7gr" in namespace "gc-1164"
Jan 18 23:15:54.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqtbl" in namespace "gc-1164"
Jan 18 23:15:54.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqxwk" in namespace "gc-1164"
Jan 18 23:15:54.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxccz" in namespace "gc-1164"
Jan 18 23:15:54.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2sh9" in namespace "gc-1164"
Jan 18 23:15:54.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-gghxs" in namespace "gc-1164"
Jan 18 23:15:54.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp27k" in namespace "gc-1164"
Jan 18 23:15:54.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2tgl" in namespace "gc-1164"
Jan 18 23:15:54.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlf72" in namespace "gc-1164"
Jan 18 23:15:55.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-j27rp" in namespace "gc-1164"
Jan 18 23:15:55.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmkj2" in namespace "gc-1164"
Jan 18 23:15:55.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt24p" in namespace "gc-1164"
Jan 18 23:15:55.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtq2v" in namespace "gc-1164"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:55.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1164" for this suite. 01/18/23 23:15:55.048
------------------------------
â€¢ [SLOW TEST] [17.573 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:37.486
    Jan 18 23:15:37.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 23:15:37.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:37.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:37.531
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/18/23 23:15:37.54
    STEP: create the rc2 01/18/23 23:15:37.55
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 23:15:42.561
    STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 23:15:43.077
    STEP: wait for the rc to be deleted 01/18/23 23:15:43.085
    Jan 18 23:15:48.095: INFO: 79 pods remaining
    Jan 18 23:15:48.095: INFO: 79 pods has nil DeletionTimestamp
    Jan 18 23:15:48.095: INFO: 
    STEP: Gathering metrics 01/18/23 23:15:53.094
    Jan 18 23:15:53.111: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 23:15:53.114: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.02221ms
    Jan 18 23:15:53.114: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 23:15:53.114: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 23:15:54.457: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 23:15:54.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hr5f" in namespace "gc-1164"
    Jan 18 23:15:54.465: INFO: Deleting pod "simpletest-rc-to-be-deleted-466lq" in namespace "gc-1164"
    Jan 18 23:15:54.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hw8l" in namespace "gc-1164"
    Jan 18 23:15:54.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dj4n" in namespace "gc-1164"
    Jan 18 23:15:54.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxbv" in namespace "gc-1164"
    Jan 18 23:15:54.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m8dc" in namespace "gc-1164"
    Jan 18 23:15:54.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qdj5" in namespace "gc-1164"
    Jan 18 23:15:54.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mlz7" in namespace "gc-1164"
    Jan 18 23:15:54.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nljb" in namespace "gc-1164"
    Jan 18 23:15:54.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-6npc5" in namespace "gc-1164"
    Jan 18 23:15:54.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rtc5" in namespace "gc-1164"
    Jan 18 23:15:54.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-6scsp" in namespace "gc-1164"
    Jan 18 23:15:54.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-74phr" in namespace "gc-1164"
    Jan 18 23:15:54.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-79crq" in namespace "gc-1164"
    Jan 18 23:15:54.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f5wf" in namespace "gc-1164"
    Jan 18 23:15:54.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grq8" in namespace "gc-1164"
    Jan 18 23:15:54.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vtjb" in namespace "gc-1164"
    Jan 18 23:15:54.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-8564v" in namespace "gc-1164"
    Jan 18 23:15:54.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mgr2" in namespace "gc-1164"
    Jan 18 23:15:54.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thfs" in namespace "gc-1164"
    Jan 18 23:15:54.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nq9j" in namespace "gc-1164"
    Jan 18 23:15:54.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v6ml" in namespace "gc-1164"
    Jan 18 23:15:54.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x7zr" in namespace "gc-1164"
    Jan 18 23:15:54.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xp5f" in namespace "gc-1164"
    Jan 18 23:15:54.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-bphkc" in namespace "gc-1164"
    Jan 18 23:15:54.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-c28bw" in namespace "gc-1164"
    Jan 18 23:15:54.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5wcx" in namespace "gc-1164"
    Jan 18 23:15:54.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc2p4" in namespace "gc-1164"
    Jan 18 23:15:54.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-czwwh" in namespace "gc-1164"
    Jan 18 23:15:54.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddndp" in namespace "gc-1164"
    Jan 18 23:15:54.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv78k" in namespace "gc-1164"
    Jan 18 23:15:54.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvf8w" in namespace "gc-1164"
    Jan 18 23:15:54.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwffl" in namespace "gc-1164"
    Jan 18 23:15:54.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2ngc" in namespace "gc-1164"
    Jan 18 23:15:54.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2tqv" in namespace "gc-1164"
    Jan 18 23:15:54.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-flvgb" in namespace "gc-1164"
    Jan 18 23:15:54.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmlxp" in namespace "gc-1164"
    Jan 18 23:15:54.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq7gr" in namespace "gc-1164"
    Jan 18 23:15:54.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqtbl" in namespace "gc-1164"
    Jan 18 23:15:54.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqxwk" in namespace "gc-1164"
    Jan 18 23:15:54.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxccz" in namespace "gc-1164"
    Jan 18 23:15:54.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2sh9" in namespace "gc-1164"
    Jan 18 23:15:54.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-gghxs" in namespace "gc-1164"
    Jan 18 23:15:54.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp27k" in namespace "gc-1164"
    Jan 18 23:15:54.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2tgl" in namespace "gc-1164"
    Jan 18 23:15:54.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlf72" in namespace "gc-1164"
    Jan 18 23:15:55.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-j27rp" in namespace "gc-1164"
    Jan 18 23:15:55.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmkj2" in namespace "gc-1164"
    Jan 18 23:15:55.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt24p" in namespace "gc-1164"
    Jan 18 23:15:55.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtq2v" in namespace "gc-1164"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:55.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1164" for this suite. 01/18/23 23:15:55.048
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:55.059
Jan 18 23:15:55.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename watch 01/18/23 23:15:55.061
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:55.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:55.09
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/18/23 23:15:55.094
STEP: creating a new configmap 01/18/23 23:15:55.095
STEP: modifying the configmap once 01/18/23 23:15:55.099
STEP: closing the watch once it receives two notifications 01/18/23 23:15:55.111
Jan 18 23:15:55.111: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21969 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:15:55.111: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21970 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/18/23 23:15:55.112
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 23:15:55.12
STEP: deleting the configmap 01/18/23 23:15:55.123
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 23:15:55.128
Jan 18 23:15:55.128: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21971 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:15:55.128: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21972 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:15:55.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7321" for this suite. 01/18/23 23:15:55.131
------------------------------
â€¢ [0.081 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:55.059
    Jan 18 23:15:55.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename watch 01/18/23 23:15:55.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:55.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:55.09
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/18/23 23:15:55.094
    STEP: creating a new configmap 01/18/23 23:15:55.095
    STEP: modifying the configmap once 01/18/23 23:15:55.099
    STEP: closing the watch once it receives two notifications 01/18/23 23:15:55.111
    Jan 18 23:15:55.111: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21969 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:15:55.111: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21970 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/18/23 23:15:55.112
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 23:15:55.12
    STEP: deleting the configmap 01/18/23 23:15:55.123
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 23:15:55.128
    Jan 18 23:15:55.128: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21971 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:15:55.128: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7321  740389c2-d769-4dde-8ac6-247b546a1df8 21972 0 2023-01-18 23:15:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 23:15:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:15:55.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7321" for this suite. 01/18/23 23:15:55.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:15:55.14
Jan 18 23:15:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 23:15:55.142
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:55.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:55.164
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 23:15:55.188
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:15:55.196
Jan 18 23:15:55.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:15:55.206: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:15:56.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:15:56.213: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:15:57.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:15:57.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:15:58.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:15:58.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:15:59.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:15:59.211: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:00.220: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:00.220: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:01.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:01.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:02.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:02.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:03.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:03.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:04.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:04.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:05.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:05.213: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:16:06.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:06.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:07.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:07.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:08.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:08.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:09.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:09.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:10.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 23:16:10.212: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 23:16:10.215
Jan 18 23:16:10.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:10.227: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:11.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:11.234: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:12.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:12.234: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:13.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:16:13.233: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:16:14.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 23:16:14.234: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 23:16:14.236
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-590, will wait for the garbage collector to delete the pods 01/18/23 23:16:14.236
Jan 18 23:16:14.296: INFO: Deleting DaemonSet.extensions daemon-set took: 6.070322ms
Jan 18 23:16:14.396: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.664263ms
Jan 18 23:16:16.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:16:16.899: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 23:16:16.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22329"},"items":null}

Jan 18 23:16:16.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22329"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:16:16.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-590" for this suite. 01/18/23 23:16:16.913
------------------------------
â€¢ [SLOW TEST] [21.777 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:15:55.14
    Jan 18 23:15:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 23:15:55.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:15:55.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:15:55.164
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 23:15:55.188
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:15:55.196
    Jan 18 23:15:55.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:15:55.206: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:15:56.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:15:56.213: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:15:57.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:15:57.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:15:58.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:15:58.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:15:59.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:15:59.211: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:00.220: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:00.220: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:01.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:01.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:02.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:02.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:03.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:03.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:04.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:04.212: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:05.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:05.213: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:16:06.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:06.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:07.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:07.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:08.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:08.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:09.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:09.212: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:10.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 23:16:10.212: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 23:16:10.215
    Jan 18 23:16:10.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:10.227: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:11.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:11.234: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:12.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:12.234: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:13.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:16:13.233: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:16:14.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 23:16:14.234: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 23:16:14.236
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-590, will wait for the garbage collector to delete the pods 01/18/23 23:16:14.236
    Jan 18 23:16:14.296: INFO: Deleting DaemonSet.extensions daemon-set took: 6.070322ms
    Jan 18 23:16:14.396: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.664263ms
    Jan 18 23:16:16.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:16:16.899: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 23:16:16.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22329"},"items":null}

    Jan 18 23:16:16.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22329"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:16:16.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-590" for this suite. 01/18/23 23:16:16.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:16:16.919
Jan 18 23:16:16.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 23:16:16.92
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:16.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:16.935
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/18/23 23:16:16.938
STEP: delete the rc 01/18/23 23:16:21.947
STEP: wait for all pods to be garbage collected 01/18/23 23:16:21.952
STEP: Gathering metrics 01/18/23 23:16:26.957
Jan 18 23:16:26.966: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 23:16:26.969: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.837496ms
Jan 18 23:16:26.969: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 23:16:26.969: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 23:16:27.022: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 23:16:27.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-374" for this suite. 01/18/23 23:16:27.025
------------------------------
â€¢ [SLOW TEST] [10.114 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:16:16.919
    Jan 18 23:16:16.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 23:16:16.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:16.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:16.935
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/18/23 23:16:16.938
    STEP: delete the rc 01/18/23 23:16:21.947
    STEP: wait for all pods to be garbage collected 01/18/23 23:16:21.952
    STEP: Gathering metrics 01/18/23 23:16:26.957
    Jan 18 23:16:26.966: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 23:16:26.969: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.837496ms
    Jan 18 23:16:26.969: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 23:16:26.969: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 23:16:27.022: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:16:27.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-374" for this suite. 01/18/23 23:16:27.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:16:27.033
Jan 18 23:16:27.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:16:27.034
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:27.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:27.047
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 18 23:16:27.058: INFO: Waiting up to 5m0s for pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab" in namespace "kubelet-test-257" to be "running and ready"
Jan 18 23:16:27.060: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510014ms
Jan 18 23:16:27.060: INFO: The phase of Pod busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:16:29.064: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.005595929s
Jan 18 23:16:29.064: INFO: The phase of Pod busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab is Running (Ready = true)
Jan 18 23:16:29.064: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:16:29.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-257" for this suite. 01/18/23 23:16:29.074
------------------------------
â€¢ [2.046 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:16:27.033
    Jan 18 23:16:27.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:16:27.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:27.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:27.047
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 18 23:16:27.058: INFO: Waiting up to 5m0s for pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab" in namespace "kubelet-test-257" to be "running and ready"
    Jan 18 23:16:27.060: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510014ms
    Jan 18 23:16:27.060: INFO: The phase of Pod busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:16:29.064: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.005595929s
    Jan 18 23:16:29.064: INFO: The phase of Pod busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab is Running (Ready = true)
    Jan 18 23:16:29.064: INFO: Pod "busybox-readonly-fseefb0caa-8f9e-4ad9-b023-547005afc6ab" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:16:29.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-257" for this suite. 01/18/23 23:16:29.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:16:29.08
Jan 18 23:16:29.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:16:29.081
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:29.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:29.096
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/18/23 23:16:29.099
STEP: Creating a ResourceQuota 01/18/23 23:16:34.102
STEP: Ensuring resource quota status is calculated 01/18/23 23:16:34.109
STEP: Creating a Service 01/18/23 23:16:36.113
STEP: Creating a NodePort Service 01/18/23 23:16:36.127
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 23:16:36.146
STEP: Ensuring resource quota status captures service creation 01/18/23 23:16:36.161
STEP: Deleting Services 01/18/23 23:16:38.165
STEP: Ensuring resource quota status released usage 01/18/23 23:16:38.189
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 23:16:40.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5157" for this suite. 01/18/23 23:16:40.196
------------------------------
â€¢ [SLOW TEST] [11.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:16:29.08
    Jan 18 23:16:29.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:16:29.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:29.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:29.096
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/18/23 23:16:29.099
    STEP: Creating a ResourceQuota 01/18/23 23:16:34.102
    STEP: Ensuring resource quota status is calculated 01/18/23 23:16:34.109
    STEP: Creating a Service 01/18/23 23:16:36.113
    STEP: Creating a NodePort Service 01/18/23 23:16:36.127
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 23:16:36.146
    STEP: Ensuring resource quota status captures service creation 01/18/23 23:16:36.161
    STEP: Deleting Services 01/18/23 23:16:38.165
    STEP: Ensuring resource quota status released usage 01/18/23 23:16:38.189
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:16:40.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5157" for this suite. 01/18/23 23:16:40.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:16:40.202
Jan 18 23:16:40.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 23:16:40.203
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:40.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:40.217
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8160 01/18/23 23:16:40.22
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8160 01/18/23 23:16:40.235
Jan 18 23:16:40.250: INFO: Found 0 stateful pods, waiting for 1
Jan 18 23:16:50.254: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/18/23 23:16:50.259
STEP: Getting /status 01/18/23 23:16:50.265
Jan 18 23:16:50.268: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/18/23 23:16:50.268
Jan 18 23:16:50.277: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/18/23 23:16:50.277
Jan 18 23:16:50.279: INFO: Observed &StatefulSet event: ADDED
Jan 18 23:16:50.279: INFO: Found Statefulset ss in namespace statefulset-8160 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:16:50.279: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/18/23 23:16:50.279
Jan 18 23:16:50.279: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 23:16:50.285: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/18/23 23:16:50.285
Jan 18 23:16:50.287: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 23:16:50.287: INFO: Deleting all statefulset in ns statefulset-8160
Jan 18 23:16:50.290: INFO: Scaling statefulset ss to 0
Jan 18 23:17:00.307: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:17:00.310: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:17:00.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8160" for this suite. 01/18/23 23:17:00.322
------------------------------
â€¢ [SLOW TEST] [20.124 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:16:40.202
    Jan 18 23:16:40.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 23:16:40.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:16:40.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:16:40.217
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8160 01/18/23 23:16:40.22
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8160 01/18/23 23:16:40.235
    Jan 18 23:16:40.250: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 23:16:50.254: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/18/23 23:16:50.259
    STEP: Getting /status 01/18/23 23:16:50.265
    Jan 18 23:16:50.268: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/18/23 23:16:50.268
    Jan 18 23:16:50.277: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/18/23 23:16:50.277
    Jan 18 23:16:50.279: INFO: Observed &StatefulSet event: ADDED
    Jan 18 23:16:50.279: INFO: Found Statefulset ss in namespace statefulset-8160 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 23:16:50.279: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/18/23 23:16:50.279
    Jan 18 23:16:50.279: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 23:16:50.285: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/18/23 23:16:50.285
    Jan 18 23:16:50.287: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 23:16:50.287: INFO: Deleting all statefulset in ns statefulset-8160
    Jan 18 23:16:50.290: INFO: Scaling statefulset ss to 0
    Jan 18 23:17:00.307: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:17:00.310: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:17:00.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8160" for this suite. 01/18/23 23:17:00.322
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:17:00.327
Jan 18 23:17:00.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:17:00.328
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:00.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:00.347
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/18/23 23:17:00.35
Jan 18 23:17:00.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 23:17:00.426: INFO: stderr: ""
Jan 18 23:17:00.426: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/18/23 23:17:00.426
Jan 18 23:17:00.426: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 23:17:00.426: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6580" to be "running and ready, or succeeded"
Jan 18 23:17:00.429: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991821ms
Jan 18 23:17:00.429: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-conformance-1-26-2' to be 'Running' but was 'Pending'
Jan 18 23:17:02.433: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006476503s
Jan 18 23:17:02.433: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 23:17:02.433: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/18/23 23:17:02.433
Jan 18 23:17:02.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator'
Jan 18 23:17:02.507: INFO: stderr: ""
Jan 18 23:17:02.507: INFO: stdout: "I0118 23:17:01.107139       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5rks 417\nI0118 23:17:01.307565       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/cvz5 540\nI0118 23:17:01.508138       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7d45 362\nI0118 23:17:01.707587       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/dtz 239\nI0118 23:17:01.907936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/jcl 587\nI0118 23:17:02.107169       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hhp 342\nI0118 23:17:02.307572       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/tb2 432\n"
STEP: limiting log lines 01/18/23 23:17:02.507
Jan 18 23:17:02.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --tail=1'
Jan 18 23:17:02.583: INFO: stderr: ""
Jan 18 23:17:02.583: INFO: stdout: "I0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\n"
Jan 18 23:17:02.583: INFO: got output "I0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\n"
STEP: limiting log bytes 01/18/23 23:17:02.583
Jan 18 23:17:02.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 23:17:02.662: INFO: stderr: ""
Jan 18 23:17:02.662: INFO: stdout: "I"
Jan 18 23:17:02.662: INFO: got output "I"
STEP: exposing timestamps 01/18/23 23:17:02.662
Jan 18 23:17:02.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 23:17:02.738: INFO: stderr: ""
Jan 18 23:17:02.738: INFO: stdout: "2023-01-18T23:17:02.707406871Z I0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\n"
Jan 18 23:17:02.738: INFO: got output "2023-01-18T23:17:02.707406871Z I0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\n"
STEP: restricting to a time range 01/18/23 23:17:02.738
Jan 18 23:17:05.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --since=1s'
Jan 18 23:17:05.311: INFO: stderr: ""
Jan 18 23:17:05.311: INFO: stdout: "I0118 23:17:04.507785       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/f2xn 503\nI0118 23:17:04.708194       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/4jb 538\nI0118 23:17:04.907671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fmr 568\nI0118 23:17:05.108095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hlgv 480\nI0118 23:17:05.307547       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/b7j 287\n"
Jan 18 23:17:05.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --since=24h'
Jan 18 23:17:05.388: INFO: stderr: ""
Jan 18 23:17:05.388: INFO: stdout: "I0118 23:17:01.107139       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5rks 417\nI0118 23:17:01.307565       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/cvz5 540\nI0118 23:17:01.508138       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7d45 362\nI0118 23:17:01.707587       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/dtz 239\nI0118 23:17:01.907936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/jcl 587\nI0118 23:17:02.107169       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hhp 342\nI0118 23:17:02.307572       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/tb2 432\nI0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\nI0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\nI0118 23:17:02.907694       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/m96 570\nI0118 23:17:03.108170       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/jwr9 451\nI0118 23:17:03.307521       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/pshk 211\nI0118 23:17:03.507948       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/862 459\nI0118 23:17:03.707277       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vmh 403\nI0118 23:17:03.907661       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/2pb 587\nI0118 23:17:04.108072       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/zf7h 336\nI0118 23:17:04.307364       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/lkk 320\nI0118 23:17:04.507785       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/f2xn 503\nI0118 23:17:04.708194       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/4jb 538\nI0118 23:17:04.907671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fmr 568\nI0118 23:17:05.108095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hlgv 480\nI0118 23:17:05.307547       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/b7j 287\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 18 23:17:05.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 delete pod logs-generator'
Jan 18 23:17:05.945: INFO: stderr: ""
Jan 18 23:17:05.945: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:17:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6580" for this suite. 01/18/23 23:17:05.948
------------------------------
â€¢ [SLOW TEST] [5.626 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:17:00.327
    Jan 18 23:17:00.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:17:00.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:00.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:00.347
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/18/23 23:17:00.35
    Jan 18 23:17:00.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 18 23:17:00.426: INFO: stderr: ""
    Jan 18 23:17:00.426: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/18/23 23:17:00.426
    Jan 18 23:17:00.426: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 18 23:17:00.426: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6580" to be "running and ready, or succeeded"
    Jan 18 23:17:00.429: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991821ms
    Jan 18 23:17:00.429: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-conformance-1-26-2' to be 'Running' but was 'Pending'
    Jan 18 23:17:02.433: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006476503s
    Jan 18 23:17:02.433: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 18 23:17:02.433: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/18/23 23:17:02.433
    Jan 18 23:17:02.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator'
    Jan 18 23:17:02.507: INFO: stderr: ""
    Jan 18 23:17:02.507: INFO: stdout: "I0118 23:17:01.107139       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5rks 417\nI0118 23:17:01.307565       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/cvz5 540\nI0118 23:17:01.508138       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7d45 362\nI0118 23:17:01.707587       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/dtz 239\nI0118 23:17:01.907936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/jcl 587\nI0118 23:17:02.107169       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hhp 342\nI0118 23:17:02.307572       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/tb2 432\n"
    STEP: limiting log lines 01/18/23 23:17:02.507
    Jan 18 23:17:02.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --tail=1'
    Jan 18 23:17:02.583: INFO: stderr: ""
    Jan 18 23:17:02.583: INFO: stdout: "I0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\n"
    Jan 18 23:17:02.583: INFO: got output "I0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\n"
    STEP: limiting log bytes 01/18/23 23:17:02.583
    Jan 18 23:17:02.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --limit-bytes=1'
    Jan 18 23:17:02.662: INFO: stderr: ""
    Jan 18 23:17:02.662: INFO: stdout: "I"
    Jan 18 23:17:02.662: INFO: got output "I"
    STEP: exposing timestamps 01/18/23 23:17:02.662
    Jan 18 23:17:02.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 18 23:17:02.738: INFO: stderr: ""
    Jan 18 23:17:02.738: INFO: stdout: "2023-01-18T23:17:02.707406871Z I0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\n"
    Jan 18 23:17:02.738: INFO: got output "2023-01-18T23:17:02.707406871Z I0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\n"
    STEP: restricting to a time range 01/18/23 23:17:02.738
    Jan 18 23:17:05.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --since=1s'
    Jan 18 23:17:05.311: INFO: stderr: ""
    Jan 18 23:17:05.311: INFO: stdout: "I0118 23:17:04.507785       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/f2xn 503\nI0118 23:17:04.708194       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/4jb 538\nI0118 23:17:04.907671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fmr 568\nI0118 23:17:05.108095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hlgv 480\nI0118 23:17:05.307547       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/b7j 287\n"
    Jan 18 23:17:05.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 logs logs-generator logs-generator --since=24h'
    Jan 18 23:17:05.388: INFO: stderr: ""
    Jan 18 23:17:05.388: INFO: stdout: "I0118 23:17:01.107139       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5rks 417\nI0118 23:17:01.307565       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/cvz5 540\nI0118 23:17:01.508138       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/7d45 362\nI0118 23:17:01.707587       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/dtz 239\nI0118 23:17:01.907936       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/jcl 587\nI0118 23:17:02.107169       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/hhp 342\nI0118 23:17:02.307572       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/tb2 432\nI0118 23:17:02.507968       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/g6q 276\nI0118 23:17:02.707251       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hgp 375\nI0118 23:17:02.907694       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/m96 570\nI0118 23:17:03.108170       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/jwr9 451\nI0118 23:17:03.307521       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/pshk 211\nI0118 23:17:03.507948       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/862 459\nI0118 23:17:03.707277       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vmh 403\nI0118 23:17:03.907661       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/2pb 587\nI0118 23:17:04.108072       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/zf7h 336\nI0118 23:17:04.307364       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/lkk 320\nI0118 23:17:04.507785       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/f2xn 503\nI0118 23:17:04.708194       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/4jb 538\nI0118 23:17:04.907671       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fmr 568\nI0118 23:17:05.108095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hlgv 480\nI0118 23:17:05.307547       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/b7j 287\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 18 23:17:05.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6580 delete pod logs-generator'
    Jan 18 23:17:05.945: INFO: stderr: ""
    Jan 18 23:17:05.945: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:17:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6580" for this suite. 01/18/23 23:17:05.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:17:05.954
Jan 18 23:17:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename watch 01/18/23 23:17:05.955
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:05.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:05.971
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/18/23 23:17:05.973
STEP: creating a watch on configmaps with label B 01/18/23 23:17:05.975
STEP: creating a watch on configmaps with label A or B 01/18/23 23:17:05.976
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.977
Jan 18 23:17:05.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22573 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:05.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22573 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.982
Jan 18 23:17:05.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22574 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:05.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22574 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 23:17:05.988
Jan 18 23:17:05.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22575 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:05.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22575 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.994
Jan 18 23:17:05.999: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22576 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:05.999: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22576 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 23:17:05.999
Jan 18 23:17:06.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22577 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:06.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22577 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 23:17:16.004
Jan 18 23:17:16.009: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22604 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:17:16.009: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22604 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:17:26.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9141" for this suite. 01/18/23 23:17:26.013
------------------------------
â€¢ [SLOW TEST] [20.066 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:17:05.954
    Jan 18 23:17:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename watch 01/18/23 23:17:05.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:05.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:05.971
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/18/23 23:17:05.973
    STEP: creating a watch on configmaps with label B 01/18/23 23:17:05.975
    STEP: creating a watch on configmaps with label A or B 01/18/23 23:17:05.976
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.977
    Jan 18 23:17:05.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22573 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:05.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22573 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.982
    Jan 18 23:17:05.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22574 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:05.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22574 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 23:17:05.988
    Jan 18 23:17:05.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22575 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:05.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22575 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 23:17:05.994
    Jan 18 23:17:05.999: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22576 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:05.999: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9141  938658da-6e9b-4f35-b8c6-668289f0ce3e 22576 0 2023-01-18 23:17:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 23:17:05.999
    Jan 18 23:17:06.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22577 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:06.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22577 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 23:17:16.004
    Jan 18 23:17:16.009: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22604 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:17:16.009: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9141  f9cffca8-a8c9-451b-884a-915a92e3aa59 22604 0 2023-01-18 23:17:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 23:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:17:26.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9141" for this suite. 01/18/23 23:17:26.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:17:26.022
Jan 18 23:17:26.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:17:26.023
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:26.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:26.038
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/18/23 23:17:26.04
Jan 18 23:17:26.049: INFO: Waiting up to 5m0s for pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908" in namespace "projected-7932" to be "running and ready"
Jan 18 23:17:26.051: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361588ms
Jan 18 23:17:26.051: INFO: The phase of Pod annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:17:28.055: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082975s
Jan 18 23:17:28.055: INFO: The phase of Pod annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908 is Running (Ready = true)
Jan 18 23:17:28.055: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908" satisfied condition "running and ready"
Jan 18 23:17:28.576: INFO: Successfully updated pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 23:17:32.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7932" for this suite. 01/18/23 23:17:32.595
------------------------------
â€¢ [SLOW TEST] [6.580 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:17:26.022
    Jan 18 23:17:26.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:17:26.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:26.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:26.038
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/18/23 23:17:26.04
    Jan 18 23:17:26.049: INFO: Waiting up to 5m0s for pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908" in namespace "projected-7932" to be "running and ready"
    Jan 18 23:17:26.051: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361588ms
    Jan 18 23:17:26.051: INFO: The phase of Pod annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:17:28.055: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082975s
    Jan 18 23:17:28.055: INFO: The phase of Pod annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908 is Running (Ready = true)
    Jan 18 23:17:28.055: INFO: Pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908" satisfied condition "running and ready"
    Jan 18 23:17:28.576: INFO: Successfully updated pod "annotationupdatebbb9e6a6-4c70-44a0-8d06-987f48ebb908"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:17:32.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7932" for this suite. 01/18/23 23:17:32.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:17:32.602
Jan 18 23:17:32.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename cronjob 01/18/23 23:17:32.603
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:32.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:32.617
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/18/23 23:17:32.62
STEP: Ensuring a job is scheduled 01/18/23 23:17:32.626
STEP: Ensuring exactly one is scheduled 01/18/23 23:18:00.63
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 23:18:00.632
STEP: Ensuring no more jobs are scheduled 01/18/23 23:18:00.638
STEP: Removing cronjob 01/18/23 23:23:00.645
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:00.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-51" for this suite. 01/18/23 23:23:00.653
------------------------------
â€¢ [SLOW TEST] [328.056 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:17:32.602
    Jan 18 23:17:32.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename cronjob 01/18/23 23:17:32.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:17:32.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:17:32.617
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/18/23 23:17:32.62
    STEP: Ensuring a job is scheduled 01/18/23 23:17:32.626
    STEP: Ensuring exactly one is scheduled 01/18/23 23:18:00.63
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 23:18:00.632
    STEP: Ensuring no more jobs are scheduled 01/18/23 23:18:00.638
    STEP: Removing cronjob 01/18/23 23:23:00.645
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:00.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-51" for this suite. 01/18/23 23:23:00.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:00.66
Jan 18 23:23:00.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:23:00.661
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:00.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:00.677
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3935 01/18/23 23:23:00.68
STEP: creating a selector 01/18/23 23:23:00.68
STEP: Creating the service pods in kubernetes 01/18/23 23:23:00.68
Jan 18 23:23:00.680: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:23:00.700: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3935" to be "running and ready"
Jan 18 23:23:00.705: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989626ms
Jan 18 23:23:00.705: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:23:02.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008509184s
Jan 18 23:23:02.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:23:04.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008847059s
Jan 18 23:23:04.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:23:06.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010578318s
Jan 18 23:23:06.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:23:08.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008875714s
Jan 18 23:23:08.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:23:10.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009246959s
Jan 18 23:23:10.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:23:12.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008756346s
Jan 18 23:23:12.709: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 23:23:12.709: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 23:23:12.712: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3935" to be "running and ready"
Jan 18 23:23:12.714: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.139362ms
Jan 18 23:23:12.714: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 23:23:12.714: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 23:23:12.716
Jan 18 23:23:12.721: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3935" to be "running"
Jan 18 23:23:12.723: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439155ms
Jan 18 23:23:14.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005712535s
Jan 18 23:23:14.726: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 23:23:14.729: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 23:23:14.729: INFO: Breadth first check of 10.32.0.2 on host 10.128.15.198...
Jan 18 23:23:14.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.4:9080/dial?request=hostname&protocol=udp&host=10.32.0.2&port=8081&tries=1'] Namespace:pod-network-test-3935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:23:14.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:23:14.732: INFO: ExecWithOptions: Clientset creation
Jan 18 23:23:14.732: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.4%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.32.0.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:23:14.807: INFO: Waiting for responses: map[]
Jan 18 23:23:14.807: INFO: reached 10.32.0.2 after 0/1 tries
Jan 18 23:23:14.807: INFO: Breadth first check of 10.32.12.3 on host 10.128.15.199...
Jan 18 23:23:14.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.4:9080/dial?request=hostname&protocol=udp&host=10.32.12.3&port=8081&tries=1'] Namespace:pod-network-test-3935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:23:14.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:23:14.811: INFO: ExecWithOptions: Clientset creation
Jan 18 23:23:14.811: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.4%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.32.12.3%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:23:14.882: INFO: Waiting for responses: map[]
Jan 18 23:23:14.882: INFO: reached 10.32.12.3 after 0/1 tries
Jan 18 23:23:14.882: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:14.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3935" for this suite. 01/18/23 23:23:14.885
------------------------------
â€¢ [SLOW TEST] [14.230 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:00.66
    Jan 18 23:23:00.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:23:00.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:00.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:00.677
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3935 01/18/23 23:23:00.68
    STEP: creating a selector 01/18/23 23:23:00.68
    STEP: Creating the service pods in kubernetes 01/18/23 23:23:00.68
    Jan 18 23:23:00.680: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 23:23:00.700: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3935" to be "running and ready"
    Jan 18 23:23:00.705: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989626ms
    Jan 18 23:23:00.705: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:23:02.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008509184s
    Jan 18 23:23:02.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:23:04.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008847059s
    Jan 18 23:23:04.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:23:06.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010578318s
    Jan 18 23:23:06.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:23:08.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008875714s
    Jan 18 23:23:08.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:23:10.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009246959s
    Jan 18 23:23:10.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:23:12.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008756346s
    Jan 18 23:23:12.709: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 23:23:12.709: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 23:23:12.712: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3935" to be "running and ready"
    Jan 18 23:23:12.714: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.139362ms
    Jan 18 23:23:12.714: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 23:23:12.714: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 23:23:12.716
    Jan 18 23:23:12.721: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3935" to be "running"
    Jan 18 23:23:12.723: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439155ms
    Jan 18 23:23:14.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005712535s
    Jan 18 23:23:14.726: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 23:23:14.729: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 23:23:14.729: INFO: Breadth first check of 10.32.0.2 on host 10.128.15.198...
    Jan 18 23:23:14.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.4:9080/dial?request=hostname&protocol=udp&host=10.32.0.2&port=8081&tries=1'] Namespace:pod-network-test-3935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:23:14.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:23:14.732: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:23:14.732: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.4%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.32.0.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 23:23:14.807: INFO: Waiting for responses: map[]
    Jan 18 23:23:14.807: INFO: reached 10.32.0.2 after 0/1 tries
    Jan 18 23:23:14.807: INFO: Breadth first check of 10.32.12.3 on host 10.128.15.199...
    Jan 18 23:23:14.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.4:9080/dial?request=hostname&protocol=udp&host=10.32.12.3&port=8081&tries=1'] Namespace:pod-network-test-3935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:23:14.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:23:14.811: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:23:14.811: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.4%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.32.12.3%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 23:23:14.882: INFO: Waiting for responses: map[]
    Jan 18 23:23:14.882: INFO: reached 10.32.12.3 after 0/1 tries
    Jan 18 23:23:14.882: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:14.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3935" for this suite. 01/18/23 23:23:14.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:14.891
Jan 18 23:23:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 23:23:14.892
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:14.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:14.906
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-zp5r4" 01/18/23 23:23:14.909
Jan 18 23:23:14.914: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
Jan 18 23:23:15.916: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
Jan 18 23:23:15.919: INFO: Found 1 replicas for "e2e-rc-zp5r4" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-zp5r4" 01/18/23 23:23:15.919
STEP: Updating a scale subresource 01/18/23 23:23:15.921
STEP: Verifying replicas where modified for replication controller "e2e-rc-zp5r4" 01/18/23 23:23:15.926
Jan 18 23:23:15.927: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
Jan 18 23:23:16.929: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
Jan 18 23:23:16.932: INFO: Found 2 replicas for "e2e-rc-zp5r4" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5066" for this suite. 01/18/23 23:23:16.935
------------------------------
â€¢ [2.048 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:14.891
    Jan 18 23:23:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 23:23:14.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:14.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:14.906
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-zp5r4" 01/18/23 23:23:14.909
    Jan 18 23:23:14.914: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
    Jan 18 23:23:15.916: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
    Jan 18 23:23:15.919: INFO: Found 1 replicas for "e2e-rc-zp5r4" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-zp5r4" 01/18/23 23:23:15.919
    STEP: Updating a scale subresource 01/18/23 23:23:15.921
    STEP: Verifying replicas where modified for replication controller "e2e-rc-zp5r4" 01/18/23 23:23:15.926
    Jan 18 23:23:15.927: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
    Jan 18 23:23:16.929: INFO: Get Replication Controller "e2e-rc-zp5r4" to confirm replicas
    Jan 18 23:23:16.932: INFO: Found 2 replicas for "e2e-rc-zp5r4" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5066" for this suite. 01/18/23 23:23:16.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:16.941
Jan 18 23:23:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename server-version 01/18/23 23:23:16.942
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:16.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:16.955
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/18/23 23:23:16.957
STEP: Confirm major version 01/18/23 23:23:16.958
Jan 18 23:23:16.958: INFO: Major version: 1
STEP: Confirm minor version 01/18/23 23:23:16.958
Jan 18 23:23:16.958: INFO: cleanMinorVersion: 26
Jan 18 23:23:16.958: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-6711" for this suite. 01/18/23 23:23:16.961
------------------------------
â€¢ [0.025 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:16.941
    Jan 18 23:23:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename server-version 01/18/23 23:23:16.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:16.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:16.955
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/18/23 23:23:16.957
    STEP: Confirm major version 01/18/23 23:23:16.958
    Jan 18 23:23:16.958: INFO: Major version: 1
    STEP: Confirm minor version 01/18/23 23:23:16.958
    Jan 18 23:23:16.958: INFO: cleanMinorVersion: 26
    Jan 18 23:23:16.958: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-6711" for this suite. 01/18/23 23:23:16.961
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:16.966
Jan 18 23:23:16.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:23:16.967
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:16.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:16.985
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:23:16.998
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:23:17.676
STEP: Deploying the webhook pod 01/18/23 23:23:17.683
STEP: Wait for the deployment to be ready 01/18/23 23:23:17.696
Jan 18 23:23:17.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:23:19.712
STEP: Verifying the service has paired with the endpoint 01/18/23 23:23:19.719
Jan 18 23:23:20.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 23:23:20.723
STEP: create a namespace for the webhook 01/18/23 23:23:20.743
STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 23:23:20.749
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7012" for this suite. 01/18/23 23:23:20.804
STEP: Destroying namespace "webhook-7012-markers" for this suite. 01/18/23 23:23:20.81
------------------------------
â€¢ [3.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:16.966
    Jan 18 23:23:16.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:23:16.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:16.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:16.985
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:23:16.998
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:23:17.676
    STEP: Deploying the webhook pod 01/18/23 23:23:17.683
    STEP: Wait for the deployment to be ready 01/18/23 23:23:17.696
    Jan 18 23:23:17.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:23:19.712
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:23:19.719
    Jan 18 23:23:20.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 23:23:20.723
    STEP: create a namespace for the webhook 01/18/23 23:23:20.743
    STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 23:23:20.749
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7012" for this suite. 01/18/23 23:23:20.804
    STEP: Destroying namespace "webhook-7012-markers" for this suite. 01/18/23 23:23:20.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:20.817
Jan 18 23:23:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:23:20.818
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:20.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:20.836
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-47e2f5d6-f1f0-4334-887c-9745abdc0658 01/18/23 23:23:20.839
STEP: Creating a pod to test consume configMaps 01/18/23 23:23:20.844
Jan 18 23:23:20.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d" in namespace "configmap-6806" to be "Succeeded or Failed"
Jan 18 23:23:20.854: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.875104ms
Jan 18 23:23:22.858: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006811664s
Jan 18 23:23:24.859: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007947336s
STEP: Saw pod success 01/18/23 23:23:24.859
Jan 18 23:23:24.859: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d" satisfied condition "Succeeded or Failed"
Jan 18 23:23:24.862: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:23:24.879
Jan 18 23:23:24.891: INFO: Waiting for pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d to disappear
Jan 18 23:23:24.894: INFO: Pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:24.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6806" for this suite. 01/18/23 23:23:24.896
------------------------------
â€¢ [4.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:20.817
    Jan 18 23:23:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:23:20.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:20.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:20.836
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-47e2f5d6-f1f0-4334-887c-9745abdc0658 01/18/23 23:23:20.839
    STEP: Creating a pod to test consume configMaps 01/18/23 23:23:20.844
    Jan 18 23:23:20.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d" in namespace "configmap-6806" to be "Succeeded or Failed"
    Jan 18 23:23:20.854: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.875104ms
    Jan 18 23:23:22.858: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006811664s
    Jan 18 23:23:24.859: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007947336s
    STEP: Saw pod success 01/18/23 23:23:24.859
    Jan 18 23:23:24.859: INFO: Pod "pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d" satisfied condition "Succeeded or Failed"
    Jan 18 23:23:24.862: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:23:24.879
    Jan 18 23:23:24.891: INFO: Waiting for pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d to disappear
    Jan 18 23:23:24.894: INFO: Pod pod-configmaps-8192f71d-21a0-4a30-aec4-885314a9512d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:24.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6806" for this suite. 01/18/23 23:23:24.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:24.904
Jan 18 23:23:24.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename hostport 01/18/23 23:23:24.905
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:24.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:24.919
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 23:23:24.926
Jan 18 23:23:24.935: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9565" to be "running and ready"
Jan 18 23:23:24.938: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838956ms
Jan 18 23:23:24.938: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:23:26.942: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153591s
Jan 18 23:23:26.942: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 23:23:26.942: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.128.15.198 on the node which pod1 resides and expect scheduled 01/18/23 23:23:26.942
Jan 18 23:23:26.949: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9565" to be "running and ready"
Jan 18 23:23:26.952: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881023ms
Jan 18 23:23:26.952: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:23:28.956: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006671799s
Jan 18 23:23:28.956: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 23:23:28.956: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.128.15.198 but use UDP protocol on the node which pod2 resides 01/18/23 23:23:28.956
Jan 18 23:23:28.961: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9565" to be "running and ready"
Jan 18 23:23:28.964: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789318ms
Jan 18 23:23:28.964: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:23:30.968: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006808472s
Jan 18 23:23:30.968: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 18 23:23:30.968: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 18 23:23:30.973: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9565" to be "running and ready"
Jan 18 23:23:30.975: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44638ms
Jan 18 23:23:30.975: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:23:32.978: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005911186s
Jan 18 23:23:32.979: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 18 23:23:32.979: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 23:23:32.981
Jan 18 23:23:32.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.128.15.198 http://127.0.0.1:54323/hostname] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:23:32.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:23:32.982: INFO: ExecWithOptions: Clientset creation
Jan 18 23:23:32.982: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.128.15.198+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.15.198, port: 54323 01/18/23 23:23:33.059
Jan 18 23:23:33.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.128.15.198:54323/hostname] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:23:33.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:23:33.060: INFO: ExecWithOptions: Clientset creation
Jan 18 23:23:33.060: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.128.15.198%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.15.198, port: 54323 UDP 01/18/23 23:23:33.146
Jan 18 23:23:33.146: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.128.15.198 54323] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:23:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:23:33.147: INFO: ExecWithOptions: Clientset creation
Jan 18 23:23:33.147: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.128.15.198+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:38.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9565" for this suite. 01/18/23 23:23:38.219
------------------------------
â€¢ [SLOW TEST] [13.321 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:24.904
    Jan 18 23:23:24.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename hostport 01/18/23 23:23:24.905
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:24.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:24.919
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 23:23:24.926
    Jan 18 23:23:24.935: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9565" to be "running and ready"
    Jan 18 23:23:24.938: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838956ms
    Jan 18 23:23:24.938: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:23:26.942: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153591s
    Jan 18 23:23:26.942: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 23:23:26.942: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.128.15.198 on the node which pod1 resides and expect scheduled 01/18/23 23:23:26.942
    Jan 18 23:23:26.949: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9565" to be "running and ready"
    Jan 18 23:23:26.952: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881023ms
    Jan 18 23:23:26.952: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:23:28.956: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006671799s
    Jan 18 23:23:28.956: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 23:23:28.956: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.128.15.198 but use UDP protocol on the node which pod2 resides 01/18/23 23:23:28.956
    Jan 18 23:23:28.961: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9565" to be "running and ready"
    Jan 18 23:23:28.964: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789318ms
    Jan 18 23:23:28.964: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:23:30.968: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006808472s
    Jan 18 23:23:30.968: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 18 23:23:30.968: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 18 23:23:30.973: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9565" to be "running and ready"
    Jan 18 23:23:30.975: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44638ms
    Jan 18 23:23:30.975: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:23:32.978: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005911186s
    Jan 18 23:23:32.979: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 18 23:23:32.979: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 23:23:32.981
    Jan 18 23:23:32.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.128.15.198 http://127.0.0.1:54323/hostname] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:23:32.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:23:32.982: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:23:32.982: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.128.15.198+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.15.198, port: 54323 01/18/23 23:23:33.059
    Jan 18 23:23:33.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.128.15.198:54323/hostname] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:23:33.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:23:33.060: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:23:33.060: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.128.15.198%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.15.198, port: 54323 UDP 01/18/23 23:23:33.146
    Jan 18 23:23:33.146: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.128.15.198 54323] Namespace:hostport-9565 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:23:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:23:33.147: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:23:33.147: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9565/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.128.15.198+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:38.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9565" for this suite. 01/18/23 23:23:38.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:38.226
Jan 18 23:23:38.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 23:23:38.227
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:38.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:38.243
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/18/23 23:23:38.249
STEP: delete the rc 01/18/23 23:23:43.257
STEP: wait for the rc to be deleted 01/18/23 23:23:43.265
Jan 18 23:23:44.280: INFO: 81 pods remaining
Jan 18 23:23:44.280: INFO: 80 pods has nil DeletionTimestamp
Jan 18 23:23:44.280: INFO: 
Jan 18 23:23:45.280: INFO: 70 pods remaining
Jan 18 23:23:45.280: INFO: 70 pods has nil DeletionTimestamp
Jan 18 23:23:45.280: INFO: 
Jan 18 23:23:46.276: INFO: 60 pods remaining
Jan 18 23:23:46.276: INFO: 60 pods has nil DeletionTimestamp
Jan 18 23:23:46.276: INFO: 
Jan 18 23:23:47.292: INFO: 41 pods remaining
Jan 18 23:23:47.292: INFO: 40 pods has nil DeletionTimestamp
Jan 18 23:23:47.292: INFO: 
Jan 18 23:23:48.279: INFO: 30 pods remaining
Jan 18 23:23:48.279: INFO: 30 pods has nil DeletionTimestamp
Jan 18 23:23:48.279: INFO: 
Jan 18 23:23:49.273: INFO: 20 pods remaining
Jan 18 23:23:49.273: INFO: 20 pods has nil DeletionTimestamp
Jan 18 23:23:49.273: INFO: 
STEP: Gathering metrics 01/18/23 23:23:50.272
Jan 18 23:23:51.480: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 23:23:51.483: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.043967ms
Jan 18 23:23:51.483: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 23:23:51.483: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 23:23:51.559: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 23:23:51.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9691" for this suite. 01/18/23 23:23:51.563
------------------------------
â€¢ [SLOW TEST] [13.345 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:38.226
    Jan 18 23:23:38.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 23:23:38.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:38.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:38.243
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/18/23 23:23:38.249
    STEP: delete the rc 01/18/23 23:23:43.257
    STEP: wait for the rc to be deleted 01/18/23 23:23:43.265
    Jan 18 23:23:44.280: INFO: 81 pods remaining
    Jan 18 23:23:44.280: INFO: 80 pods has nil DeletionTimestamp
    Jan 18 23:23:44.280: INFO: 
    Jan 18 23:23:45.280: INFO: 70 pods remaining
    Jan 18 23:23:45.280: INFO: 70 pods has nil DeletionTimestamp
    Jan 18 23:23:45.280: INFO: 
    Jan 18 23:23:46.276: INFO: 60 pods remaining
    Jan 18 23:23:46.276: INFO: 60 pods has nil DeletionTimestamp
    Jan 18 23:23:46.276: INFO: 
    Jan 18 23:23:47.292: INFO: 41 pods remaining
    Jan 18 23:23:47.292: INFO: 40 pods has nil DeletionTimestamp
    Jan 18 23:23:47.292: INFO: 
    Jan 18 23:23:48.279: INFO: 30 pods remaining
    Jan 18 23:23:48.279: INFO: 30 pods has nil DeletionTimestamp
    Jan 18 23:23:48.279: INFO: 
    Jan 18 23:23:49.273: INFO: 20 pods remaining
    Jan 18 23:23:49.273: INFO: 20 pods has nil DeletionTimestamp
    Jan 18 23:23:49.273: INFO: 
    STEP: Gathering metrics 01/18/23 23:23:50.272
    Jan 18 23:23:51.480: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 23:23:51.483: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.043967ms
    Jan 18 23:23:51.483: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 23:23:51.483: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 23:23:51.559: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:23:51.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9691" for this suite. 01/18/23 23:23:51.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:23:51.572
Jan 18 23:23:51.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 23:23:51.573
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:51.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:51.589
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 23:23:51.592
Jan 18 23:23:51.601: INFO: Waiting up to 5m0s for pod "pod-024c567e-616f-48c9-b10f-ef54d6232940" in namespace "emptydir-3209" to be "Succeeded or Failed"
Jan 18 23:23:51.604: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293779ms
Jan 18 23:23:53.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005864975s
Jan 18 23:23:55.609: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007285618s
Jan 18 23:23:57.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005479579s
Jan 18 23:23:59.608: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006327943s
Jan 18 23:24:01.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005556664s
STEP: Saw pod success 01/18/23 23:24:01.607
Jan 18 23:24:01.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940" satisfied condition "Succeeded or Failed"
Jan 18 23:24:01.610: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-024c567e-616f-48c9-b10f-ef54d6232940 container test-container: <nil>
STEP: delete the pod 01/18/23 23:24:01.614
Jan 18 23:24:01.625: INFO: Waiting for pod pod-024c567e-616f-48c9-b10f-ef54d6232940 to disappear
Jan 18 23:24:01.629: INFO: Pod pod-024c567e-616f-48c9-b10f-ef54d6232940 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:01.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3209" for this suite. 01/18/23 23:24:01.633
------------------------------
â€¢ [SLOW TEST] [10.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:23:51.572
    Jan 18 23:23:51.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 23:23:51.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:23:51.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:23:51.589
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 23:23:51.592
    Jan 18 23:23:51.601: INFO: Waiting up to 5m0s for pod "pod-024c567e-616f-48c9-b10f-ef54d6232940" in namespace "emptydir-3209" to be "Succeeded or Failed"
    Jan 18 23:23:51.604: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293779ms
    Jan 18 23:23:53.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005864975s
    Jan 18 23:23:55.609: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007285618s
    Jan 18 23:23:57.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005479579s
    Jan 18 23:23:59.608: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006327943s
    Jan 18 23:24:01.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005556664s
    STEP: Saw pod success 01/18/23 23:24:01.607
    Jan 18 23:24:01.607: INFO: Pod "pod-024c567e-616f-48c9-b10f-ef54d6232940" satisfied condition "Succeeded or Failed"
    Jan 18 23:24:01.610: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-024c567e-616f-48c9-b10f-ef54d6232940 container test-container: <nil>
    STEP: delete the pod 01/18/23 23:24:01.614
    Jan 18 23:24:01.625: INFO: Waiting for pod pod-024c567e-616f-48c9-b10f-ef54d6232940 to disappear
    Jan 18 23:24:01.629: INFO: Pod pod-024c567e-616f-48c9-b10f-ef54d6232940 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:01.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3209" for this suite. 01/18/23 23:24:01.633
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:01.646
Jan 18 23:24:01.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:24:01.647
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:01.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:01.661
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/18/23 23:24:01.663
STEP: Creating a ResourceQuota 01/18/23 23:24:06.665
STEP: Ensuring resource quota status is calculated 01/18/23 23:24:06.67
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:08.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4060" for this suite. 01/18/23 23:24:08.677
------------------------------
â€¢ [SLOW TEST] [7.037 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:01.646
    Jan 18 23:24:01.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:24:01.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:01.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:01.661
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/18/23 23:24:01.663
    STEP: Creating a ResourceQuota 01/18/23 23:24:06.665
    STEP: Ensuring resource quota status is calculated 01/18/23 23:24:06.67
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:08.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4060" for this suite. 01/18/23 23:24:08.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:08.683
Jan 18 23:24:08.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:24:08.684
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:08.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:08.701
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/18/23 23:24:08.712
Jan 18 23:24:08.712: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f" in namespace "kubelet-test-6054" to be "completed"
Jan 18 23:24:08.714: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368582ms
Jan 18 23:24:10.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006751828s
Jan 18 23:24:12.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006644122s
Jan 18 23:24:12.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:12.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6054" for this suite. 01/18/23 23:24:12.727
------------------------------
â€¢ [4.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:08.683
    Jan 18 23:24:08.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:24:08.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:08.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:08.701
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/18/23 23:24:08.712
    Jan 18 23:24:08.712: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f" in namespace "kubelet-test-6054" to be "completed"
    Jan 18 23:24:08.714: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368582ms
    Jan 18 23:24:10.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006751828s
    Jan 18 23:24:12.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006644122s
    Jan 18 23:24:12.719: INFO: Pod "agnhost-host-aliases5d1581c4-80b3-49f6-b1e1-1b51c646ee0f" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:12.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6054" for this suite. 01/18/23 23:24:12.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:12.733
Jan 18 23:24:12.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:24:12.734
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:12.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:12.751
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:24:12.765
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:24:13.129
STEP: Deploying the webhook pod 01/18/23 23:24:13.136
STEP: Wait for the deployment to be ready 01/18/23 23:24:13.15
Jan 18 23:24:13.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:24:15.166
STEP: Verifying the service has paired with the endpoint 01/18/23 23:24:15.176
Jan 18 23:24:16.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 23:24:16.179
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:16.179
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 23:24:16.196
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 23:24:17.203
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:17.203
STEP: Having no error when timeout is longer than webhook latency 01/18/23 23:24:18.227
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:18.227
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 23:24:23.257
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:23.257
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7883" for this suite. 01/18/23 23:24:28.313
STEP: Destroying namespace "webhook-7883-markers" for this suite. 01/18/23 23:24:28.318
------------------------------
â€¢ [SLOW TEST] [15.590 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:12.733
    Jan 18 23:24:12.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:24:12.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:12.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:12.751
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:24:12.765
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:24:13.129
    STEP: Deploying the webhook pod 01/18/23 23:24:13.136
    STEP: Wait for the deployment to be ready 01/18/23 23:24:13.15
    Jan 18 23:24:13.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:24:15.166
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:24:15.176
    Jan 18 23:24:16.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 23:24:16.179
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:16.179
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 23:24:16.196
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 23:24:17.203
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:17.203
    STEP: Having no error when timeout is longer than webhook latency 01/18/23 23:24:18.227
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:18.227
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 23:24:23.257
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 23:24:23.257
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7883" for this suite. 01/18/23 23:24:28.313
    STEP: Destroying namespace "webhook-7883-markers" for this suite. 01/18/23 23:24:28.318
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:28.323
Jan 18 23:24:28.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 23:24:28.324
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:28.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:28.339
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2371 01/18/23 23:24:28.343
STEP: changing the ExternalName service to type=ClusterIP 01/18/23 23:24:28.348
STEP: creating replication controller externalname-service in namespace services-2371 01/18/23 23:24:28.361
I0118 23:24:28.368914      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2371, replica count: 2
I0118 23:24:31.420783      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:24:31.420: INFO: Creating new exec pod
Jan 18 23:24:31.427: INFO: Waiting up to 5m0s for pod "execpod2tqhm" in namespace "services-2371" to be "running"
Jan 18 23:24:31.430: INFO: Pod "execpod2tqhm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024602ms
Jan 18 23:24:33.433: INFO: Pod "execpod2tqhm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005889873s
Jan 18 23:24:33.433: INFO: Pod "execpod2tqhm" satisfied condition "running"
Jan 18 23:24:34.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-2371 exec execpod2tqhm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 18 23:24:34.576: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:24:34.576: INFO: stdout: ""
Jan 18 23:24:34.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-2371 exec execpod2tqhm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.231 80'
Jan 18 23:24:34.706: INFO: stderr: "+ nc -v -z -w 2 10.96.2.231 80\nConnection to 10.96.2.231 80 port [tcp/http] succeeded!\n"
Jan 18 23:24:34.706: INFO: stdout: ""
Jan 18 23:24:34.706: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2371" for this suite. 01/18/23 23:24:34.72
------------------------------
â€¢ [SLOW TEST] [6.402 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:28.323
    Jan 18 23:24:28.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 23:24:28.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:28.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:28.339
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2371 01/18/23 23:24:28.343
    STEP: changing the ExternalName service to type=ClusterIP 01/18/23 23:24:28.348
    STEP: creating replication controller externalname-service in namespace services-2371 01/18/23 23:24:28.361
    I0118 23:24:28.368914      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2371, replica count: 2
    I0118 23:24:31.420783      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 23:24:31.420: INFO: Creating new exec pod
    Jan 18 23:24:31.427: INFO: Waiting up to 5m0s for pod "execpod2tqhm" in namespace "services-2371" to be "running"
    Jan 18 23:24:31.430: INFO: Pod "execpod2tqhm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024602ms
    Jan 18 23:24:33.433: INFO: Pod "execpod2tqhm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005889873s
    Jan 18 23:24:33.433: INFO: Pod "execpod2tqhm" satisfied condition "running"
    Jan 18 23:24:34.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-2371 exec execpod2tqhm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 18 23:24:34.576: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 23:24:34.576: INFO: stdout: ""
    Jan 18 23:24:34.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-2371 exec execpod2tqhm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.231 80'
    Jan 18 23:24:34.706: INFO: stderr: "+ nc -v -z -w 2 10.96.2.231 80\nConnection to 10.96.2.231 80 port [tcp/http] succeeded!\n"
    Jan 18 23:24:34.706: INFO: stdout: ""
    Jan 18 23:24:34.706: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2371" for this suite. 01/18/23 23:24:34.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:34.727
Jan 18 23:24:34.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename subpath 01/18/23 23:24:34.728
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:34.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:34.744
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 23:24:34.748
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-4v5s 01/18/23 23:24:34.756
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:24:34.756
Jan 18 23:24:34.765: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4v5s" in namespace "subpath-3969" to be "Succeeded or Failed"
Jan 18 23:24:34.767: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386097ms
Jan 18 23:24:36.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006314772s
Jan 18 23:24:38.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.00613086s
Jan 18 23:24:40.772: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.006769867s
Jan 18 23:24:42.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.006296278s
Jan 18 23:24:44.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.006036772s
Jan 18 23:24:46.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.006485681s
Jan 18 23:24:48.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.006098493s
Jan 18 23:24:50.770: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.005593727s
Jan 18 23:24:52.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.006042337s
Jan 18 23:24:54.772: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.007300513s
Jan 18 23:24:56.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.005799624s
Jan 18 23:24:58.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006135098s
STEP: Saw pod success 01/18/23 23:24:58.771
Jan 18 23:24:58.771: INFO: Pod "pod-subpath-test-projected-4v5s" satisfied condition "Succeeded or Failed"
Jan 18 23:24:58.774: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-projected-4v5s container test-container-subpath-projected-4v5s: <nil>
STEP: delete the pod 01/18/23 23:24:58.778
Jan 18 23:24:58.789: INFO: Waiting for pod pod-subpath-test-projected-4v5s to disappear
Jan 18 23:24:58.791: INFO: Pod pod-subpath-test-projected-4v5s no longer exists
STEP: Deleting pod pod-subpath-test-projected-4v5s 01/18/23 23:24:58.791
Jan 18 23:24:58.791: INFO: Deleting pod "pod-subpath-test-projected-4v5s" in namespace "subpath-3969"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 23:24:58.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3969" for this suite. 01/18/23 23:24:58.796
------------------------------
â€¢ [SLOW TEST] [24.076 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:34.727
    Jan 18 23:24:34.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename subpath 01/18/23 23:24:34.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:34.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:34.744
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 23:24:34.748
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-4v5s 01/18/23 23:24:34.756
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:24:34.756
    Jan 18 23:24:34.765: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4v5s" in namespace "subpath-3969" to be "Succeeded or Failed"
    Jan 18 23:24:34.767: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386097ms
    Jan 18 23:24:36.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006314772s
    Jan 18 23:24:38.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.00613086s
    Jan 18 23:24:40.772: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.006769867s
    Jan 18 23:24:42.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.006296278s
    Jan 18 23:24:44.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.006036772s
    Jan 18 23:24:46.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.006485681s
    Jan 18 23:24:48.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.006098493s
    Jan 18 23:24:50.770: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.005593727s
    Jan 18 23:24:52.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.006042337s
    Jan 18 23:24:54.772: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.007300513s
    Jan 18 23:24:56.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.005799624s
    Jan 18 23:24:58.771: INFO: Pod "pod-subpath-test-projected-4v5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006135098s
    STEP: Saw pod success 01/18/23 23:24:58.771
    Jan 18 23:24:58.771: INFO: Pod "pod-subpath-test-projected-4v5s" satisfied condition "Succeeded or Failed"
    Jan 18 23:24:58.774: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-projected-4v5s container test-container-subpath-projected-4v5s: <nil>
    STEP: delete the pod 01/18/23 23:24:58.778
    Jan 18 23:24:58.789: INFO: Waiting for pod pod-subpath-test-projected-4v5s to disappear
    Jan 18 23:24:58.791: INFO: Pod pod-subpath-test-projected-4v5s no longer exists
    STEP: Deleting pod pod-subpath-test-projected-4v5s 01/18/23 23:24:58.791
    Jan 18 23:24:58.791: INFO: Deleting pod "pod-subpath-test-projected-4v5s" in namespace "subpath-3969"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:24:58.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3969" for this suite. 01/18/23 23:24:58.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:24:58.804
Jan 18 23:24:58.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-watch 01/18/23 23:24:58.805
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:58.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:58.817
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 18 23:24:58.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Creating first CR  01/18/23 23:25:01.37
Jan 18 23:25:01.376: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:01Z]] name:name1 resourceVersion:24863 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/18/23 23:25:11.378
Jan 18 23:25:11.383: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:11Z]] name:name2 resourceVersion:24887 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/18/23 23:25:21.384
Jan 18 23:25:21.390: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:21Z]] name:name1 resourceVersion:24902 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/18/23 23:25:31.394
Jan 18 23:25:31.404: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:31Z]] name:name2 resourceVersion:24919 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/18/23 23:25:41.407
Jan 18 23:25:41.414: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:21Z]] name:name1 resourceVersion:24934 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/18/23 23:25:51.414
Jan 18 23:25:51.421: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:31Z]] name:name2 resourceVersion:24949 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:26:01.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3930" for this suite. 01/18/23 23:26:01.939
------------------------------
â€¢ [SLOW TEST] [63.141 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:24:58.804
    Jan 18 23:24:58.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-watch 01/18/23 23:24:58.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:24:58.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:24:58.817
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 18 23:24:58.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Creating first CR  01/18/23 23:25:01.37
    Jan 18 23:25:01.376: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:01Z]] name:name1 resourceVersion:24863 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/18/23 23:25:11.378
    Jan 18 23:25:11.383: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:11Z]] name:name2 resourceVersion:24887 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/18/23 23:25:21.384
    Jan 18 23:25:21.390: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:21Z]] name:name1 resourceVersion:24902 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/18/23 23:25:31.394
    Jan 18 23:25:31.404: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:31Z]] name:name2 resourceVersion:24919 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/18/23 23:25:41.407
    Jan 18 23:25:41.414: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:21Z]] name:name1 resourceVersion:24934 uid:5359731b-f1b3-4de1-b205-f12051e35614] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/18/23 23:25:51.414
    Jan 18 23:25:51.421: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T23:25:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T23:25:31Z]] name:name2 resourceVersion:24949 uid:199df994-0a73-45fb-b7ee-7943108cb791] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:26:01.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3930" for this suite. 01/18/23 23:26:01.939
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:26:01.945
Jan 18 23:26:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:26:01.946
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:01.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:01.962
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:26:01.965
Jan 18 23:26:01.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1" in namespace "projected-4927" to be "Succeeded or Failed"
Jan 18 23:26:01.976: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499605ms
Jan 18 23:26:03.979: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005772335s
Jan 18 23:26:05.981: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007192188s
STEP: Saw pod success 01/18/23 23:26:05.981
Jan 18 23:26:05.981: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1" satisfied condition "Succeeded or Failed"
Jan 18 23:26:05.984: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 container client-container: <nil>
STEP: delete the pod 01/18/23 23:26:05.989
Jan 18 23:26:06.000: INFO: Waiting for pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 to disappear
Jan 18 23:26:06.002: INFO: Pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 23:26:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4927" for this suite. 01/18/23 23:26:06.005
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:26:01.945
    Jan 18 23:26:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:26:01.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:01.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:01.962
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:26:01.965
    Jan 18 23:26:01.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1" in namespace "projected-4927" to be "Succeeded or Failed"
    Jan 18 23:26:01.976: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499605ms
    Jan 18 23:26:03.979: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005772335s
    Jan 18 23:26:05.981: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007192188s
    STEP: Saw pod success 01/18/23 23:26:05.981
    Jan 18 23:26:05.981: INFO: Pod "downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1" satisfied condition "Succeeded or Failed"
    Jan 18 23:26:05.984: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:26:05.989
    Jan 18 23:26:06.000: INFO: Waiting for pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 to disappear
    Jan 18 23:26:06.002: INFO: Pod downwardapi-volume-7c07c17c-b871-48de-9fe6-97b8e68f74b1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:26:06.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4927" for this suite. 01/18/23 23:26:06.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:26:06.015
Jan 18 23:26:06.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename dns 01/18/23 23:26:06.016
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:06.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:06.028
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3177.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3177.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/18/23 23:26:06.031
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3177.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3177.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/18/23 23:26:06.031
STEP: creating a pod to probe /etc/hosts 01/18/23 23:26:06.031
STEP: submitting the pod to kubernetes 01/18/23 23:26:06.031
Jan 18 23:26:06.038: INFO: Waiting up to 15m0s for pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876" in namespace "dns-3177" to be "running"
Jan 18 23:26:06.040: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2978ms
Jan 18 23:26:08.044: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876": Phase="Running", Reason="", readiness=true. Elapsed: 2.006516598s
Jan 18 23:26:08.045: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876" satisfied condition "running"
STEP: retrieving the pod 01/18/23 23:26:08.045
STEP: looking for the results for each expected name from probers 01/18/23 23:26:08.047
Jan 18 23:26:08.064: INFO: DNS probes using dns-3177/dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876 succeeded

STEP: deleting the pod 01/18/23 23:26:08.064
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 23:26:08.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3177" for this suite. 01/18/23 23:26:08.079
------------------------------
â€¢ [2.069 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:26:06.015
    Jan 18 23:26:06.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename dns 01/18/23 23:26:06.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:06.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:06.028
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3177.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3177.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/18/23 23:26:06.031
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3177.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3177.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/18/23 23:26:06.031
    STEP: creating a pod to probe /etc/hosts 01/18/23 23:26:06.031
    STEP: submitting the pod to kubernetes 01/18/23 23:26:06.031
    Jan 18 23:26:06.038: INFO: Waiting up to 15m0s for pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876" in namespace "dns-3177" to be "running"
    Jan 18 23:26:06.040: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2978ms
    Jan 18 23:26:08.044: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876": Phase="Running", Reason="", readiness=true. Elapsed: 2.006516598s
    Jan 18 23:26:08.045: INFO: Pod "dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 23:26:08.045
    STEP: looking for the results for each expected name from probers 01/18/23 23:26:08.047
    Jan 18 23:26:08.064: INFO: DNS probes using dns-3177/dns-test-1a2b181d-c326-4bb5-bc3b-6f47f7ece876 succeeded

    STEP: deleting the pod 01/18/23 23:26:08.064
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:26:08.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3177" for this suite. 01/18/23 23:26:08.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:26:08.085
Jan 18 23:26:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:26:08.086
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:08.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:08.099
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1351 01/18/23 23:26:08.102
STEP: creating a selector 01/18/23 23:26:08.102
STEP: Creating the service pods in kubernetes 01/18/23 23:26:08.102
Jan 18 23:26:08.102: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:26:08.123: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1351" to be "running and ready"
Jan 18 23:26:08.125: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744191ms
Jan 18 23:26:08.125: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:26:10.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005881479s
Jan 18 23:26:10.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:26:12.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006621527s
Jan 18 23:26:12.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:26:14.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006999233s
Jan 18 23:26:14.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:26:16.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006589744s
Jan 18 23:26:16.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:26:18.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006585706s
Jan 18 23:26:18.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:26:20.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007581528s
Jan 18 23:26:20.130: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 23:26:20.130: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 23:26:20.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1351" to be "running and ready"
Jan 18 23:26:20.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.952415ms
Jan 18 23:26:20.135: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 23:26:20.135: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 23:26:20.137
Jan 18 23:26:20.143: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1351" to be "running"
Jan 18 23:26:20.145: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240834ms
Jan 18 23:26:22.150: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006836199s
Jan 18 23:26:22.150: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 23:26:22.152: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 23:26:22.152: INFO: Breadth first check of 10.32.0.2 on host 10.128.15.198...
Jan 18 23:26:22.154: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.3:9080/dial?request=hostname&protocol=http&host=10.32.0.2&port=8083&tries=1'] Namespace:pod-network-test-1351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:26:22.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:26:22.155: INFO: ExecWithOptions: Clientset creation
Jan 18 23:26:22.155: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.3%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.32.0.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:26:22.251: INFO: Waiting for responses: map[]
Jan 18 23:26:22.251: INFO: reached 10.32.0.2 after 0/1 tries
Jan 18 23:26:22.251: INFO: Breadth first check of 10.32.12.4 on host 10.128.15.199...
Jan 18 23:26:22.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.3:9080/dial?request=hostname&protocol=http&host=10.32.12.4&port=8083&tries=1'] Namespace:pod-network-test-1351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:26:22.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:26:22.254: INFO: ExecWithOptions: Clientset creation
Jan 18 23:26:22.254: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.3%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.32.12.4%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:26:22.333: INFO: Waiting for responses: map[]
Jan 18 23:26:22.333: INFO: reached 10.32.12.4 after 0/1 tries
Jan 18 23:26:22.333: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 23:26:22.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1351" for this suite. 01/18/23 23:26:22.336
------------------------------
â€¢ [SLOW TEST] [14.257 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:26:08.085
    Jan 18 23:26:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:26:08.086
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:08.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:08.099
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1351 01/18/23 23:26:08.102
    STEP: creating a selector 01/18/23 23:26:08.102
    STEP: Creating the service pods in kubernetes 01/18/23 23:26:08.102
    Jan 18 23:26:08.102: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 23:26:08.123: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1351" to be "running and ready"
    Jan 18 23:26:08.125: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744191ms
    Jan 18 23:26:08.125: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:26:10.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005881479s
    Jan 18 23:26:10.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:26:12.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006621527s
    Jan 18 23:26:12.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:26:14.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006999233s
    Jan 18 23:26:14.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:26:16.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006589744s
    Jan 18 23:26:16.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:26:18.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006585706s
    Jan 18 23:26:18.129: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:26:20.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007581528s
    Jan 18 23:26:20.130: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 23:26:20.130: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 23:26:20.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1351" to be "running and ready"
    Jan 18 23:26:20.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.952415ms
    Jan 18 23:26:20.135: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 23:26:20.135: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 23:26:20.137
    Jan 18 23:26:20.143: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1351" to be "running"
    Jan 18 23:26:20.145: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240834ms
    Jan 18 23:26:22.150: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006836199s
    Jan 18 23:26:22.150: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 23:26:22.152: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 23:26:22.152: INFO: Breadth first check of 10.32.0.2 on host 10.128.15.198...
    Jan 18 23:26:22.154: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.3:9080/dial?request=hostname&protocol=http&host=10.32.0.2&port=8083&tries=1'] Namespace:pod-network-test-1351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:26:22.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:26:22.155: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:26:22.155: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.3%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.32.0.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 23:26:22.251: INFO: Waiting for responses: map[]
    Jan 18 23:26:22.251: INFO: reached 10.32.0.2 after 0/1 tries
    Jan 18 23:26:22.251: INFO: Breadth first check of 10.32.12.4 on host 10.128.15.199...
    Jan 18 23:26:22.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.12.3:9080/dial?request=hostname&protocol=http&host=10.32.12.4&port=8083&tries=1'] Namespace:pod-network-test-1351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:26:22.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:26:22.254: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:26:22.254: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.32.12.3%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.32.12.4%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 23:26:22.333: INFO: Waiting for responses: map[]
    Jan 18 23:26:22.333: INFO: reached 10.32.12.4 after 0/1 tries
    Jan 18 23:26:22.333: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:26:22.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1351" for this suite. 01/18/23 23:26:22.336
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:26:22.342
Jan 18 23:26:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:26:22.343
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:22.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:22.36
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/18/23 23:26:22.363
STEP: waiting for pod running 01/18/23 23:26:22.37
Jan 18 23:26:22.370: INFO: Waiting up to 2m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565" to be "running"
Jan 18 23:26:22.373: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829143ms
Jan 18 23:26:24.376: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904564s
Jan 18 23:26:24.376: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" satisfied condition "running"
STEP: creating a file in subpath 01/18/23 23:26:24.376
Jan 18 23:26:24.379: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9565 PodName:var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:26:24.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:26:24.379: INFO: ExecWithOptions: Clientset creation
Jan 18 23:26:24.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9565/pods/var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/18/23 23:26:24.445
Jan 18 23:26:24.448: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9565 PodName:var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:26:24.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:26:24.449: INFO: ExecWithOptions: Clientset creation
Jan 18 23:26:24.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9565/pods/var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/18/23 23:26:24.523
Jan 18 23:26:25.034: INFO: Successfully updated pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722"
STEP: waiting for annotated pod running 01/18/23 23:26:25.034
Jan 18 23:26:25.034: INFO: Waiting up to 2m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565" to be "running"
Jan 18 23:26:25.037: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Running", Reason="", readiness=true. Elapsed: 2.31447ms
Jan 18 23:26:25.037: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 23:26:25.037
Jan 18 23:26:25.037: INFO: Deleting pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565"
Jan 18 23:26:25.046: INFO: Wait up to 5m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 23:26:59.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9565" for this suite. 01/18/23 23:26:59.055
------------------------------
â€¢ [SLOW TEST] [36.718 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:26:22.342
    Jan 18 23:26:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:26:22.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:22.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:22.36
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/18/23 23:26:22.363
    STEP: waiting for pod running 01/18/23 23:26:22.37
    Jan 18 23:26:22.370: INFO: Waiting up to 2m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565" to be "running"
    Jan 18 23:26:22.373: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829143ms
    Jan 18 23:26:24.376: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904564s
    Jan 18 23:26:24.376: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" satisfied condition "running"
    STEP: creating a file in subpath 01/18/23 23:26:24.376
    Jan 18 23:26:24.379: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9565 PodName:var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:26:24.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:26:24.379: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:26:24.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9565/pods/var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/18/23 23:26:24.445
    Jan 18 23:26:24.448: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9565 PodName:var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:26:24.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:26:24.449: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:26:24.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9565/pods/var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/18/23 23:26:24.523
    Jan 18 23:26:25.034: INFO: Successfully updated pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722"
    STEP: waiting for annotated pod running 01/18/23 23:26:25.034
    Jan 18 23:26:25.034: INFO: Waiting up to 2m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565" to be "running"
    Jan 18 23:26:25.037: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722": Phase="Running", Reason="", readiness=true. Elapsed: 2.31447ms
    Jan 18 23:26:25.037: INFO: Pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 23:26:25.037
    Jan 18 23:26:25.037: INFO: Deleting pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" in namespace "var-expansion-9565"
    Jan 18 23:26:25.046: INFO: Wait up to 5m0s for pod "var-expansion-b1dd95a2-a501-423d-9d51-848d420a2722" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:26:59.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9565" for this suite. 01/18/23 23:26:59.055
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:26:59.061
Jan 18 23:26:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:26:59.062
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:59.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:59.077
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-a97d3597-b49b-481b-9201-c2b82b7aefab 01/18/23 23:26:59.082
STEP: Creating secret with name s-test-opt-upd-6b23cd07-4c48-4a13-aa1c-c766cd421f1c 01/18/23 23:26:59.086
STEP: Creating the pod 01/18/23 23:26:59.09
Jan 18 23:26:59.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476" in namespace "projected-7911" to be "running and ready"
Jan 18 23:26:59.101: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597399ms
Jan 18 23:26:59.101: INFO: The phase of Pod pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:27:01.105: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476": Phase="Running", Reason="", readiness=true. Elapsed: 2.006673937s
Jan 18 23:27:01.105: INFO: The phase of Pod pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476 is Running (Ready = true)
Jan 18 23:27:01.105: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a97d3597-b49b-481b-9201-c2b82b7aefab 01/18/23 23:27:01.121
STEP: Updating secret s-test-opt-upd-6b23cd07-4c48-4a13-aa1c-c766cd421f1c 01/18/23 23:27:01.126
STEP: Creating secret with name s-test-opt-create-bf957e8d-cbde-4875-b47d-9371f7e48c32 01/18/23 23:27:01.13
STEP: waiting to observe update in volume 01/18/23 23:27:01.133
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7911" for this suite. 01/18/23 23:27:03.159
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:26:59.061
    Jan 18 23:26:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:26:59.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:26:59.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:26:59.077
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-a97d3597-b49b-481b-9201-c2b82b7aefab 01/18/23 23:26:59.082
    STEP: Creating secret with name s-test-opt-upd-6b23cd07-4c48-4a13-aa1c-c766cd421f1c 01/18/23 23:26:59.086
    STEP: Creating the pod 01/18/23 23:26:59.09
    Jan 18 23:26:59.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476" in namespace "projected-7911" to be "running and ready"
    Jan 18 23:26:59.101: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597399ms
    Jan 18 23:26:59.101: INFO: The phase of Pod pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:27:01.105: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476": Phase="Running", Reason="", readiness=true. Elapsed: 2.006673937s
    Jan 18 23:27:01.105: INFO: The phase of Pod pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476 is Running (Ready = true)
    Jan 18 23:27:01.105: INFO: Pod "pod-projected-secrets-7944dfe8-38b0-4588-96e3-b1ad96ad7476" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a97d3597-b49b-481b-9201-c2b82b7aefab 01/18/23 23:27:01.121
    STEP: Updating secret s-test-opt-upd-6b23cd07-4c48-4a13-aa1c-c766cd421f1c 01/18/23 23:27:01.126
    STEP: Creating secret with name s-test-opt-create-bf957e8d-cbde-4875-b47d-9371f7e48c32 01/18/23 23:27:01.13
    STEP: waiting to observe update in volume 01/18/23 23:27:01.133
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7911" for this suite. 01/18/23 23:27:03.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:03.165
Jan 18 23:27:03.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:27:03.166
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:03.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:03.18
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/18/23 23:27:03.183
Jan 18 23:27:03.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9309 api-versions'
Jan 18 23:27:03.263: INFO: stderr: ""
Jan 18 23:27:03.263: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.kurl.sh/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:03.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9309" for this suite. 01/18/23 23:27:03.266
------------------------------
â€¢ [0.107 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:03.165
    Jan 18 23:27:03.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:27:03.166
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:03.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:03.18
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/18/23 23:27:03.183
    Jan 18 23:27:03.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-9309 api-versions'
    Jan 18 23:27:03.263: INFO: stderr: ""
    Jan 18 23:27:03.263: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.kurl.sh/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:03.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9309" for this suite. 01/18/23 23:27:03.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:03.273
Jan 18 23:27:03.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:27:03.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:03.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:03.286
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:27:03.3
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:27:03.684
STEP: Deploying the webhook pod 01/18/23 23:27:03.691
STEP: Wait for the deployment to be ready 01/18/23 23:27:03.701
Jan 18 23:27:03.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:27:05.716
STEP: Verifying the service has paired with the endpoint 01/18/23 23:27:05.724
Jan 18 23:27:06.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 23:27:06.727
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 23:27:06.744
STEP: Creating a dummy validating-webhook-configuration object 01/18/23 23:27:06.76
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 23:27:06.769
STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 23:27:06.774
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 23:27:06.783
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7401" for this suite. 01/18/23 23:27:06.825
STEP: Destroying namespace "webhook-7401-markers" for this suite. 01/18/23 23:27:06.831
------------------------------
â€¢ [3.563 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:03.273
    Jan 18 23:27:03.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:27:03.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:03.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:03.286
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:27:03.3
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:27:03.684
    STEP: Deploying the webhook pod 01/18/23 23:27:03.691
    STEP: Wait for the deployment to be ready 01/18/23 23:27:03.701
    Jan 18 23:27:03.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:27:05.716
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:27:05.724
    Jan 18 23:27:06.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 23:27:06.727
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 23:27:06.744
    STEP: Creating a dummy validating-webhook-configuration object 01/18/23 23:27:06.76
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 23:27:06.769
    STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 23:27:06.774
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 23:27:06.783
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7401" for this suite. 01/18/23 23:27:06.825
    STEP: Destroying namespace "webhook-7401-markers" for this suite. 01/18/23 23:27:06.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:06.836
Jan 18 23:27:06.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:27:06.838
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:06.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:06.85
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 18 23:27:06.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 23:27:08.288
Jan 18 23:27:08.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 create -f -'
Jan 18 23:27:08.901: INFO: stderr: ""
Jan 18 23:27:08.901: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:27:08.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 delete e2e-test-crd-publish-openapi-5077-crds test-cr'
Jan 18 23:27:08.975: INFO: stderr: ""
Jan 18 23:27:08.975: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 23:27:08.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 apply -f -'
Jan 18 23:27:09.172: INFO: stderr: ""
Jan 18 23:27:09.172: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:27:09.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 delete e2e-test-crd-publish-openapi-5077-crds test-cr'
Jan 18 23:27:09.251: INFO: stderr: ""
Jan 18 23:27:09.251: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 23:27:09.251
Jan 18 23:27:09.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 explain e2e-test-crd-publish-openapi-5077-crds'
Jan 18 23:27:09.439: INFO: stderr: ""
Jan 18 23:27:09.439: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5077-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:10.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3961" for this suite. 01/18/23 23:27:10.861
------------------------------
â€¢ [4.030 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:06.836
    Jan 18 23:27:06.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:27:06.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:06.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:06.85
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 18 23:27:06.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 23:27:08.288
    Jan 18 23:27:08.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 create -f -'
    Jan 18 23:27:08.901: INFO: stderr: ""
    Jan 18 23:27:08.901: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 23:27:08.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 delete e2e-test-crd-publish-openapi-5077-crds test-cr'
    Jan 18 23:27:08.975: INFO: stderr: ""
    Jan 18 23:27:08.975: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 18 23:27:08.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 apply -f -'
    Jan 18 23:27:09.172: INFO: stderr: ""
    Jan 18 23:27:09.172: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 23:27:09.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 --namespace=crd-publish-openapi-3961 delete e2e-test-crd-publish-openapi-5077-crds test-cr'
    Jan 18 23:27:09.251: INFO: stderr: ""
    Jan 18 23:27:09.251: INFO: stdout: "e2e-test-crd-publish-openapi-5077-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 23:27:09.251
    Jan 18 23:27:09.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=crd-publish-openapi-3961 explain e2e-test-crd-publish-openapi-5077-crds'
    Jan 18 23:27:09.439: INFO: stderr: ""
    Jan 18 23:27:09.439: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5077-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:10.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3961" for this suite. 01/18/23 23:27:10.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:10.867
Jan 18 23:27:10.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:27:10.867
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:10.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:10.882
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-cc184301-6986-4984-9f12-c37fda48fa72 01/18/23 23:27:10.885
STEP: Creating a pod to test consume secrets 01/18/23 23:27:10.888
Jan 18 23:27:10.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7" in namespace "projected-2398" to be "Succeeded or Failed"
Jan 18 23:27:10.899: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226649ms
Jan 18 23:27:12.903: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006342836s
Jan 18 23:27:14.904: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007420804s
STEP: Saw pod success 01/18/23 23:27:14.904
Jan 18 23:27:14.904: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7" satisfied condition "Succeeded or Failed"
Jan 18 23:27:14.907: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:27:14.912
Jan 18 23:27:14.922: INFO: Waiting for pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 to disappear
Jan 18 23:27:14.925: INFO: Pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:14.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2398" for this suite. 01/18/23 23:27:14.928
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:10.867
    Jan 18 23:27:10.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:27:10.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:10.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:10.882
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-cc184301-6986-4984-9f12-c37fda48fa72 01/18/23 23:27:10.885
    STEP: Creating a pod to test consume secrets 01/18/23 23:27:10.888
    Jan 18 23:27:10.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7" in namespace "projected-2398" to be "Succeeded or Failed"
    Jan 18 23:27:10.899: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226649ms
    Jan 18 23:27:12.903: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006342836s
    Jan 18 23:27:14.904: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007420804s
    STEP: Saw pod success 01/18/23 23:27:14.904
    Jan 18 23:27:14.904: INFO: Pod "pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7" satisfied condition "Succeeded or Failed"
    Jan 18 23:27:14.907: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:27:14.912
    Jan 18 23:27:14.922: INFO: Waiting for pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 to disappear
    Jan 18 23:27:14.925: INFO: Pod pod-projected-secrets-f4419e58-4282-4992-a1f4-d2027e4de4e7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:14.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2398" for this suite. 01/18/23 23:27:14.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:14.934
Jan 18 23:27:14.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename job 01/18/23 23:27:14.935
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:14.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:14.948
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/18/23 23:27:14.95
STEP: Ensuring job reaches completions 01/18/23 23:27:14.955
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 23:27:26.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6729" for this suite. 01/18/23 23:27:26.963
------------------------------
â€¢ [SLOW TEST] [12.036 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:14.934
    Jan 18 23:27:14.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename job 01/18/23 23:27:14.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:14.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:14.948
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/18/23 23:27:14.95
    STEP: Ensuring job reaches completions 01/18/23 23:27:14.955
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:27:26.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6729" for this suite. 01/18/23 23:27:26.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:27:26.97
Jan 18 23:27:26.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-probe 01/18/23 23:27:26.971
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:26.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:26.985
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea in namespace container-probe-6216 01/18/23 23:27:26.988
Jan 18 23:27:26.997: INFO: Waiting up to 5m0s for pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea" in namespace "container-probe-6216" to be "not pending"
Jan 18 23:27:27.003: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128351ms
Jan 18 23:27:29.007: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.010384456s
Jan 18 23:27:29.007: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea" satisfied condition "not pending"
Jan 18 23:27:29.007: INFO: Started pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea in namespace container-probe-6216
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 23:27:29.007
Jan 18 23:27:29.009: INFO: Initial restart count of pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea is 0
STEP: deleting the pod 01/18/23 23:31:29.5
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:29.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6216" for this suite. 01/18/23 23:31:29.519
------------------------------
â€¢ [SLOW TEST] [242.555 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:27:26.97
    Jan 18 23:27:26.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-probe 01/18/23 23:27:26.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:27:26.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:27:26.985
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea in namespace container-probe-6216 01/18/23 23:27:26.988
    Jan 18 23:27:26.997: INFO: Waiting up to 5m0s for pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea" in namespace "container-probe-6216" to be "not pending"
    Jan 18 23:27:27.003: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128351ms
    Jan 18 23:27:29.007: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.010384456s
    Jan 18 23:27:29.007: INFO: Pod "liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea" satisfied condition "not pending"
    Jan 18 23:27:29.007: INFO: Started pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea in namespace container-probe-6216
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 23:27:29.007
    Jan 18 23:27:29.009: INFO: Initial restart count of pod liveness-372953ca-3de7-4fa5-abe1-bb257d2433ea is 0
    STEP: deleting the pod 01/18/23 23:31:29.5
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:29.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6216" for this suite. 01/18/23 23:31:29.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:29.526
Jan 18 23:31:29.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:31:29.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:29.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:29.541
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/18/23 23:31:29.544
Jan 18 23:31:29.553: INFO: Waiting up to 5m0s for pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964" in namespace "downward-api-4433" to be "Succeeded or Failed"
Jan 18 23:31:29.555: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399132ms
Jan 18 23:31:31.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006181809s
Jan 18 23:31:33.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006354689s
STEP: Saw pod success 01/18/23 23:31:33.559
Jan 18 23:31:33.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964" satisfied condition "Succeeded or Failed"
Jan 18 23:31:33.561: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:31:33.576
Jan 18 23:31:33.588: INFO: Waiting for pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 to disappear
Jan 18 23:31:33.590: INFO: Pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4433" for this suite. 01/18/23 23:31:33.592
------------------------------
â€¢ [4.071 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:29.526
    Jan 18 23:31:29.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:31:29.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:29.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:29.541
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/18/23 23:31:29.544
    Jan 18 23:31:29.553: INFO: Waiting up to 5m0s for pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964" in namespace "downward-api-4433" to be "Succeeded or Failed"
    Jan 18 23:31:29.555: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399132ms
    Jan 18 23:31:31.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006181809s
    Jan 18 23:31:33.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006354689s
    STEP: Saw pod success 01/18/23 23:31:33.559
    Jan 18 23:31:33.559: INFO: Pod "downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964" satisfied condition "Succeeded or Failed"
    Jan 18 23:31:33.561: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:31:33.576
    Jan 18 23:31:33.588: INFO: Waiting for pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 to disappear
    Jan 18 23:31:33.590: INFO: Pod downward-api-38da7052-ea47-456e-bdf2-6e28daaaa964 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4433" for this suite. 01/18/23 23:31:33.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:33.598
Jan 18 23:31:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:31:33.599
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:33.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:33.612
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-05d491e7-5d23-4dbf-a49e-f89f89dd7a21 01/18/23 23:31:33.615
STEP: Creating a pod to test consume configMaps 01/18/23 23:31:33.619
Jan 18 23:31:33.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c" in namespace "projected-1798" to be "Succeeded or Failed"
Jan 18 23:31:33.628: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427302ms
Jan 18 23:31:35.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006172732s
Jan 18 23:31:37.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006658711s
STEP: Saw pod success 01/18/23 23:31:37.632
Jan 18 23:31:37.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c" satisfied condition "Succeeded or Failed"
Jan 18 23:31:37.635: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:31:37.64
Jan 18 23:31:37.651: INFO: Waiting for pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c to disappear
Jan 18 23:31:37.654: INFO: Pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:37.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1798" for this suite. 01/18/23 23:31:37.657
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:33.598
    Jan 18 23:31:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:31:33.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:33.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:33.612
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-05d491e7-5d23-4dbf-a49e-f89f89dd7a21 01/18/23 23:31:33.615
    STEP: Creating a pod to test consume configMaps 01/18/23 23:31:33.619
    Jan 18 23:31:33.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c" in namespace "projected-1798" to be "Succeeded or Failed"
    Jan 18 23:31:33.628: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427302ms
    Jan 18 23:31:35.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006172732s
    Jan 18 23:31:37.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006658711s
    STEP: Saw pod success 01/18/23 23:31:37.632
    Jan 18 23:31:37.632: INFO: Pod "pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c" satisfied condition "Succeeded or Failed"
    Jan 18 23:31:37.635: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:31:37.64
    Jan 18 23:31:37.651: INFO: Waiting for pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c to disappear
    Jan 18 23:31:37.654: INFO: Pod pod-projected-configmaps-54397212-5997-467d-a00f-80fe5481b78c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:37.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1798" for this suite. 01/18/23 23:31:37.657
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:37.662
Jan 18 23:31:37.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context-test 01/18/23 23:31:37.663
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:37.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:37.676
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 18 23:31:37.687: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0" in namespace "security-context-test-3510" to be "Succeeded or Failed"
Jan 18 23:31:37.689: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.303019ms
Jan 18 23:31:39.693: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005939s
Jan 18 23:31:41.692: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005503407s
Jan 18 23:31:41.692: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:41.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3510" for this suite. 01/18/23 23:31:41.696
------------------------------
â€¢ [4.041 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:37.662
    Jan 18 23:31:37.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 23:31:37.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:37.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:37.676
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 18 23:31:37.687: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0" in namespace "security-context-test-3510" to be "Succeeded or Failed"
    Jan 18 23:31:37.689: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.303019ms
    Jan 18 23:31:39.693: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005939s
    Jan 18 23:31:41.692: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005503407s
    Jan 18 23:31:41.692: INFO: Pod "busybox-readonly-false-36d78a45-a8fe-414c-b89f-f72319f1f5b0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:41.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3510" for this suite. 01/18/23 23:31:41.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:41.704
Jan 18 23:31:41.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:31:41.705
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:41.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:41.718
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/18/23 23:31:41.721
Jan 18 23:31:41.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 create -f -'
Jan 18 23:31:42.275: INFO: stderr: ""
Jan 18 23:31:42.275: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/18/23 23:31:42.275
Jan 18 23:31:42.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 diff -f -'
Jan 18 23:31:42.464: INFO: rc: 1
Jan 18 23:31:42.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 delete -f -'
Jan 18 23:31:42.533: INFO: stderr: ""
Jan 18 23:31:42.533: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:42.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3140" for this suite. 01/18/23 23:31:42.538
------------------------------
â€¢ [0.839 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:41.704
    Jan 18 23:31:41.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:31:41.705
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:41.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:41.718
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/18/23 23:31:41.721
    Jan 18 23:31:41.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 create -f -'
    Jan 18 23:31:42.275: INFO: stderr: ""
    Jan 18 23:31:42.275: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/18/23 23:31:42.275
    Jan 18 23:31:42.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 diff -f -'
    Jan 18 23:31:42.464: INFO: rc: 1
    Jan 18 23:31:42.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-3140 delete -f -'
    Jan 18 23:31:42.533: INFO: stderr: ""
    Jan 18 23:31:42.533: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:42.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3140" for this suite. 01/18/23 23:31:42.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:42.543
Jan 18 23:31:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename lease-test 01/18/23 23:31:42.544
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:42.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:42.557
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:42.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-531" for this suite. 01/18/23 23:31:42.638
------------------------------
â€¢ [0.100 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:42.543
    Jan 18 23:31:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename lease-test 01/18/23 23:31:42.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:42.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:42.557
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:42.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-531" for this suite. 01/18/23 23:31:42.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:42.644
Jan 18 23:31:42.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename watch 01/18/23 23:31:42.645
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:42.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:42.663
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/18/23 23:31:42.666
STEP: creating a new configmap 01/18/23 23:31:42.669
STEP: modifying the configmap once 01/18/23 23:31:42.672
STEP: changing the label value of the configmap 01/18/23 23:31:42.679
STEP: Expecting to observe a delete notification for the watched object 01/18/23 23:31:42.684
Jan 18 23:31:42.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25990 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:31:42.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25991 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:31:42.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25992 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/18/23 23:31:42.685
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 23:31:42.691
STEP: changing the label value of the configmap back 01/18/23 23:31:52.692
STEP: modifying the configmap a third time 01/18/23 23:31:52.699
STEP: deleting the configmap 01/18/23 23:31:52.706
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 23:31:52.711
Jan 18 23:31:52.711: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26043 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:31:52.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26044 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:31:52.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26045 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8908" for this suite. 01/18/23 23:31:52.714
------------------------------
â€¢ [SLOW TEST] [10.075 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:42.644
    Jan 18 23:31:42.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename watch 01/18/23 23:31:42.645
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:42.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:42.663
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/18/23 23:31:42.666
    STEP: creating a new configmap 01/18/23 23:31:42.669
    STEP: modifying the configmap once 01/18/23 23:31:42.672
    STEP: changing the label value of the configmap 01/18/23 23:31:42.679
    STEP: Expecting to observe a delete notification for the watched object 01/18/23 23:31:42.684
    Jan 18 23:31:42.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25990 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:31:42.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25991 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:31:42.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 25992 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/18/23 23:31:42.685
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 23:31:42.691
    STEP: changing the label value of the configmap back 01/18/23 23:31:52.692
    STEP: modifying the configmap a third time 01/18/23 23:31:52.699
    STEP: deleting the configmap 01/18/23 23:31:52.706
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 23:31:52.711
    Jan 18 23:31:52.711: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26043 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:31:52.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26044 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 23:31:52.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8908  f854412f-ace1-435a-ae0a-29f9a915fc7d 26045 0 2023-01-18 23:31:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 23:31:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8908" for this suite. 01/18/23 23:31:52.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:52.719
Jan 18 23:31:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename proxy 01/18/23 23:31:52.72
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:52.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:52.735
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 18 23:31:52.737: INFO: Creating pod...
Jan 18 23:31:52.744: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3187" to be "running"
Jan 18 23:31:52.746: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233019ms
Jan 18 23:31:54.750: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005596982s
Jan 18 23:31:54.750: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 23:31:54.750: INFO: Creating service...
Jan 18 23:31:54.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=DELETE
Jan 18 23:31:54.767: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:31:54.767: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=OPTIONS
Jan 18 23:31:54.771: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:31:54.771: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=PATCH
Jan 18 23:31:54.774: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:31:54.774: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=POST
Jan 18 23:31:54.777: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:31:54.777: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=PUT
Jan 18 23:31:54.780: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:31:54.780: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 23:31:54.784: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:31:54.784: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 23:31:54.788: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:31:54.788: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 23:31:54.792: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:31:54.792: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 23:31:54.795: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:31:54.795: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 23:31:54.801: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:31:54.801: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=GET
Jan 18 23:31:54.803: INFO: http.Client request:GET StatusCode:301
Jan 18 23:31:54.803: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 23:31:54.806: INFO: http.Client request:GET StatusCode:301
Jan 18 23:31:54.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=HEAD
Jan 18 23:31:54.808: INFO: http.Client request:HEAD StatusCode:301
Jan 18 23:31:54.808: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 23:31:54.811: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:54.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3187" for this suite. 01/18/23 23:31:54.814
------------------------------
â€¢ [2.100 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:52.719
    Jan 18 23:31:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename proxy 01/18/23 23:31:52.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:52.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:52.735
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 18 23:31:52.737: INFO: Creating pod...
    Jan 18 23:31:52.744: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3187" to be "running"
    Jan 18 23:31:52.746: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233019ms
    Jan 18 23:31:54.750: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005596982s
    Jan 18 23:31:54.750: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 23:31:54.750: INFO: Creating service...
    Jan 18 23:31:54.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=DELETE
    Jan 18 23:31:54.767: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 23:31:54.767: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=OPTIONS
    Jan 18 23:31:54.771: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 23:31:54.771: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=PATCH
    Jan 18 23:31:54.774: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 23:31:54.774: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=POST
    Jan 18 23:31:54.777: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 23:31:54.777: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=PUT
    Jan 18 23:31:54.780: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 23:31:54.780: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 18 23:31:54.784: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 23:31:54.784: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 18 23:31:54.788: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 23:31:54.788: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 18 23:31:54.792: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 23:31:54.792: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=POST
    Jan 18 23:31:54.795: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 23:31:54.795: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 18 23:31:54.801: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 23:31:54.801: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=GET
    Jan 18 23:31:54.803: INFO: http.Client request:GET StatusCode:301
    Jan 18 23:31:54.803: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=GET
    Jan 18 23:31:54.806: INFO: http.Client request:GET StatusCode:301
    Jan 18 23:31:54.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/pods/agnhost/proxy?method=HEAD
    Jan 18 23:31:54.808: INFO: http.Client request:HEAD StatusCode:301
    Jan 18 23:31:54.808: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3187/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 18 23:31:54.811: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:54.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3187" for this suite. 01/18/23 23:31:54.814
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:54.82
Jan 18 23:31:54.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-runtime 01/18/23 23:31:54.821
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:54.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:54.842
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/18/23 23:31:54.845
STEP: wait for the container to reach Succeeded 01/18/23 23:31:54.854
STEP: get the container status 01/18/23 23:31:58.87
STEP: the container should be terminated 01/18/23 23:31:58.872
STEP: the termination message should be set 01/18/23 23:31:58.872
Jan 18 23:31:58.872: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 23:31:58.872
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:58.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-652" for this suite. 01/18/23 23:31:58.886
------------------------------
â€¢ [4.070 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:54.82
    Jan 18 23:31:54.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 23:31:54.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:54.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:54.842
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/18/23 23:31:54.845
    STEP: wait for the container to reach Succeeded 01/18/23 23:31:54.854
    STEP: get the container status 01/18/23 23:31:58.87
    STEP: the container should be terminated 01/18/23 23:31:58.872
    STEP: the termination message should be set 01/18/23 23:31:58.872
    Jan 18 23:31:58.872: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 23:31:58.872
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:58.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-652" for this suite. 01/18/23 23:31:58.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:58.892
Jan 18 23:31:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename tables 01/18/23 23:31:58.893
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:58.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:58.91
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:58.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-737" for this suite. 01/18/23 23:31:58.917
------------------------------
â€¢ [0.030 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:58.892
    Jan 18 23:31:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename tables 01/18/23 23:31:58.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:58.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:58.91
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:58.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-737" for this suite. 01/18/23 23:31:58.917
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:58.922
Jan 18 23:31:58.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename endpointslice 01/18/23 23:31:58.923
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:58.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:58.937
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/18/23 23:31:58.94
STEP: getting /apis/discovery.k8s.io 01/18/23 23:31:58.942
STEP: getting /apis/discovery.k8s.iov1 01/18/23 23:31:58.943
STEP: creating 01/18/23 23:31:58.944
STEP: getting 01/18/23 23:31:58.956
STEP: listing 01/18/23 23:31:58.958
STEP: watching 01/18/23 23:31:58.962
Jan 18 23:31:58.962: INFO: starting watch
STEP: cluster-wide listing 01/18/23 23:31:58.964
STEP: cluster-wide watching 01/18/23 23:31:58.966
Jan 18 23:31:58.967: INFO: starting watch
STEP: patching 01/18/23 23:31:58.968
STEP: updating 01/18/23 23:31:58.974
Jan 18 23:31:58.980: INFO: waiting for watch events with expected annotations
Jan 18 23:31:58.980: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 23:31:58.98
STEP: deleting a collection 01/18/23 23:31:58.989
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 23:31:58.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7293" for this suite. 01/18/23 23:31:59.002
------------------------------
â€¢ [0.084 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:58.922
    Jan 18 23:31:58.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 23:31:58.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:58.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:58.937
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/18/23 23:31:58.94
    STEP: getting /apis/discovery.k8s.io 01/18/23 23:31:58.942
    STEP: getting /apis/discovery.k8s.iov1 01/18/23 23:31:58.943
    STEP: creating 01/18/23 23:31:58.944
    STEP: getting 01/18/23 23:31:58.956
    STEP: listing 01/18/23 23:31:58.958
    STEP: watching 01/18/23 23:31:58.962
    Jan 18 23:31:58.962: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 23:31:58.964
    STEP: cluster-wide watching 01/18/23 23:31:58.966
    Jan 18 23:31:58.967: INFO: starting watch
    STEP: patching 01/18/23 23:31:58.968
    STEP: updating 01/18/23 23:31:58.974
    Jan 18 23:31:58.980: INFO: waiting for watch events with expected annotations
    Jan 18 23:31:58.980: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 23:31:58.98
    STEP: deleting a collection 01/18/23 23:31:58.989
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:31:58.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7293" for this suite. 01/18/23 23:31:59.002
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:31:59.006
Jan 18 23:31:59.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context 01/18/23 23:31:59.007
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:59.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:59.021
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 23:31:59.023
Jan 18 23:31:59.031: INFO: Waiting up to 5m0s for pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc" in namespace "security-context-1300" to be "Succeeded or Failed"
Jan 18 23:31:59.034: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534775ms
Jan 18 23:32:01.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040868s
Jan 18 23:32:03.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006289775s
STEP: Saw pod success 01/18/23 23:32:03.038
Jan 18 23:32:03.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc" satisfied condition "Succeeded or Failed"
Jan 18 23:32:03.040: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc container test-container: <nil>
STEP: delete the pod 01/18/23 23:32:03.045
Jan 18 23:32:03.053: INFO: Waiting for pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc to disappear
Jan 18 23:32:03.056: INFO: Pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:03.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1300" for this suite. 01/18/23 23:32:03.059
------------------------------
â€¢ [4.057 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:31:59.006
    Jan 18 23:31:59.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context 01/18/23 23:31:59.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:31:59.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:31:59.021
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 23:31:59.023
    Jan 18 23:31:59.031: INFO: Waiting up to 5m0s for pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc" in namespace "security-context-1300" to be "Succeeded or Failed"
    Jan 18 23:31:59.034: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534775ms
    Jan 18 23:32:01.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040868s
    Jan 18 23:32:03.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006289775s
    STEP: Saw pod success 01/18/23 23:32:03.038
    Jan 18 23:32:03.038: INFO: Pod "security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc" satisfied condition "Succeeded or Failed"
    Jan 18 23:32:03.040: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc container test-container: <nil>
    STEP: delete the pod 01/18/23 23:32:03.045
    Jan 18 23:32:03.053: INFO: Waiting for pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc to disappear
    Jan 18 23:32:03.056: INFO: Pod security-context-451d73bd-d7e6-4d64-aac9-401a7b3d8cdc no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:03.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1300" for this suite. 01/18/23 23:32:03.059
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:03.064
Jan 18 23:32:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename security-context-test 01/18/23 23:32:03.064
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:03.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:03.077
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 18 23:32:03.086: INFO: Waiting up to 5m0s for pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68" in namespace "security-context-test-4227" to be "Succeeded or Failed"
Jan 18 23:32:03.088: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330535ms
Jan 18 23:32:05.091: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005086869s
Jan 18 23:32:07.093: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006867127s
Jan 18 23:32:07.093: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:07.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4227" for this suite. 01/18/23 23:32:07.097
------------------------------
â€¢ [4.038 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:03.064
    Jan 18 23:32:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 23:32:03.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:03.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:03.077
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 18 23:32:03.086: INFO: Waiting up to 5m0s for pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68" in namespace "security-context-test-4227" to be "Succeeded or Failed"
    Jan 18 23:32:03.088: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330535ms
    Jan 18 23:32:05.091: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005086869s
    Jan 18 23:32:07.093: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006867127s
    Jan 18 23:32:07.093: INFO: Pod "busybox-user-65534-36af73b9-f8b5-46d3-b3bf-7dad118dfc68" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:07.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4227" for this suite. 01/18/23 23:32:07.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:07.102
Jan 18 23:32:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 23:32:07.103
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:07.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:07.117
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/18/23 23:32:07.12
Jan 18 23:32:07.126: INFO: Waiting up to 5m0s for pod "pod-xglhr" in namespace "pods-9146" to be "running"
Jan 18 23:32:07.128: INFO: Pod "pod-xglhr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426194ms
Jan 18 23:32:09.132: INFO: Pod "pod-xglhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006457276s
Jan 18 23:32:09.132: INFO: Pod "pod-xglhr" satisfied condition "running"
STEP: patching /status 01/18/23 23:32:09.133
Jan 18 23:32:09.142: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:09.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9146" for this suite. 01/18/23 23:32:09.145
------------------------------
â€¢ [2.047 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:07.102
    Jan 18 23:32:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 23:32:07.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:07.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:07.117
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/18/23 23:32:07.12
    Jan 18 23:32:07.126: INFO: Waiting up to 5m0s for pod "pod-xglhr" in namespace "pods-9146" to be "running"
    Jan 18 23:32:07.128: INFO: Pod "pod-xglhr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426194ms
    Jan 18 23:32:09.132: INFO: Pod "pod-xglhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006457276s
    Jan 18 23:32:09.132: INFO: Pod "pod-xglhr" satisfied condition "running"
    STEP: patching /status 01/18/23 23:32:09.133
    Jan 18 23:32:09.142: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:09.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9146" for this suite. 01/18/23 23:32:09.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:09.15
Jan 18 23:32:09.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:32:09.151
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:09.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:09.163
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/18/23 23:32:09.167
Jan 18 23:32:09.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8148 cluster-info'
Jan 18 23:32:09.237: INFO: stderr: ""
Jan 18 23:32:09.237: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8148" for this suite. 01/18/23 23:32:09.24
------------------------------
â€¢ [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:09.15
    Jan 18 23:32:09.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:32:09.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:09.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:09.163
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/18/23 23:32:09.167
    Jan 18 23:32:09.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-8148 cluster-info'
    Jan 18 23:32:09.237: INFO: stderr: ""
    Jan 18 23:32:09.237: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8148" for this suite. 01/18/23 23:32:09.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:09.248
Jan 18 23:32:09.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:32:09.249
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:09.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:09.263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:32:09.277
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:32:10.001
STEP: Deploying the webhook pod 01/18/23 23:32:10.01
STEP: Wait for the deployment to be ready 01/18/23 23:32:10.02
Jan 18 23:32:10.027: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:32:12.037
STEP: Verifying the service has paired with the endpoint 01/18/23 23:32:12.045
Jan 18 23:32:13.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 23:32:13.048
STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 23:32:13.067
Jan 18 23:32:13.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:13.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5716" for this suite. 01/18/23 23:32:13.116
STEP: Destroying namespace "webhook-5716-markers" for this suite. 01/18/23 23:32:13.121
------------------------------
â€¢ [3.881 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:09.248
    Jan 18 23:32:09.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:32:09.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:09.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:09.263
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:32:09.277
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:32:10.001
    STEP: Deploying the webhook pod 01/18/23 23:32:10.01
    STEP: Wait for the deployment to be ready 01/18/23 23:32:10.02
    Jan 18 23:32:10.027: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:32:12.037
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:32:12.045
    Jan 18 23:32:13.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 23:32:13.048
    STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 23:32:13.067
    Jan 18 23:32:13.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:13.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5716" for this suite. 01/18/23 23:32:13.116
    STEP: Destroying namespace "webhook-5716-markers" for this suite. 01/18/23 23:32:13.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:13.13
Jan 18 23:32:13.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 23:32:13.131
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:13.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:13.145
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 18 23:32:13.165: INFO: Waiting up to 5m0s for pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386" in namespace "emptydir-wrapper-2900" to be "running and ready"
Jan 18 23:32:13.168: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.708398ms
Jan 18 23:32:13.168: INFO: The phase of Pod pod-secrets-62deb301-797a-4bfa-8120-bac802a30386 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:32:15.173: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386": Phase="Running", Reason="", readiness=true. Elapsed: 2.007232232s
Jan 18 23:32:15.173: INFO: The phase of Pod pod-secrets-62deb301-797a-4bfa-8120-bac802a30386 is Running (Ready = true)
Jan 18 23:32:15.173: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/18/23 23:32:15.175
STEP: Cleaning up the configmap 01/18/23 23:32:15.182
STEP: Cleaning up the pod 01/18/23 23:32:15.188
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:15.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2900" for this suite. 01/18/23 23:32:15.204
------------------------------
â€¢ [2.078 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:13.13
    Jan 18 23:32:13.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 23:32:13.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:13.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:13.145
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 18 23:32:13.165: INFO: Waiting up to 5m0s for pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386" in namespace "emptydir-wrapper-2900" to be "running and ready"
    Jan 18 23:32:13.168: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.708398ms
    Jan 18 23:32:13.168: INFO: The phase of Pod pod-secrets-62deb301-797a-4bfa-8120-bac802a30386 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:32:15.173: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386": Phase="Running", Reason="", readiness=true. Elapsed: 2.007232232s
    Jan 18 23:32:15.173: INFO: The phase of Pod pod-secrets-62deb301-797a-4bfa-8120-bac802a30386 is Running (Ready = true)
    Jan 18 23:32:15.173: INFO: Pod "pod-secrets-62deb301-797a-4bfa-8120-bac802a30386" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/18/23 23:32:15.175
    STEP: Cleaning up the configmap 01/18/23 23:32:15.182
    STEP: Cleaning up the pod 01/18/23 23:32:15.188
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:15.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2900" for this suite. 01/18/23 23:32:15.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:15.211
Jan 18 23:32:15.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:32:15.212
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:15.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:15.227
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:32:15.233
Jan 18 23:32:15.244: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6391" to be "running and ready"
Jan 18 23:32:15.246: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42681ms
Jan 18 23:32:15.246: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:32:17.250: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828411s
Jan 18 23:32:17.250: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 23:32:17.250: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/18/23 23:32:17.252
Jan 18 23:32:17.259: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6391" to be "running and ready"
Jan 18 23:32:17.262: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349535ms
Jan 18 23:32:17.262: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:32:19.266: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006559686s
Jan 18 23:32:19.266: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 18 23:32:19.266: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 23:32:19.268
STEP: delete the pod with lifecycle hook 01/18/23 23:32:19.283
Jan 18 23:32:19.290: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 23:32:19.293: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 23:32:21.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 23:32:21.297: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:21.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6391" for this suite. 01/18/23 23:32:21.3
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:15.211
    Jan 18 23:32:15.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:32:15.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:15.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:15.227
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:32:15.233
    Jan 18 23:32:15.244: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6391" to be "running and ready"
    Jan 18 23:32:15.246: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42681ms
    Jan 18 23:32:15.246: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:32:17.250: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828411s
    Jan 18 23:32:17.250: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 23:32:17.250: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/18/23 23:32:17.252
    Jan 18 23:32:17.259: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6391" to be "running and ready"
    Jan 18 23:32:17.262: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349535ms
    Jan 18 23:32:17.262: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:32:19.266: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006559686s
    Jan 18 23:32:19.266: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 18 23:32:19.266: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 23:32:19.268
    STEP: delete the pod with lifecycle hook 01/18/23 23:32:19.283
    Jan 18 23:32:19.290: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 23:32:19.293: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 23:32:21.294: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 23:32:21.297: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:21.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6391" for this suite. 01/18/23 23:32:21.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:21.308
Jan 18 23:32:21.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:32:21.309
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:21.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:21.322
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/18/23 23:32:21.325
Jan 18 23:32:21.333: INFO: Waiting up to 5m0s for pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850" in namespace "downward-api-5167" to be "running and ready"
Jan 18 23:32:21.336: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355134ms
Jan 18 23:32:21.336: INFO: The phase of Pod annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:32:23.340: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850": Phase="Running", Reason="", readiness=true. Elapsed: 2.00624172s
Jan 18 23:32:23.340: INFO: The phase of Pod annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850 is Running (Ready = true)
Jan 18 23:32:23.340: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850" satisfied condition "running and ready"
Jan 18 23:32:23.860: INFO: Successfully updated pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5167" for this suite. 01/18/23 23:32:27.881
------------------------------
â€¢ [SLOW TEST] [6.579 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:21.308
    Jan 18 23:32:21.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:32:21.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:21.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:21.322
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/18/23 23:32:21.325
    Jan 18 23:32:21.333: INFO: Waiting up to 5m0s for pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850" in namespace "downward-api-5167" to be "running and ready"
    Jan 18 23:32:21.336: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355134ms
    Jan 18 23:32:21.336: INFO: The phase of Pod annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:32:23.340: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850": Phase="Running", Reason="", readiness=true. Elapsed: 2.00624172s
    Jan 18 23:32:23.340: INFO: The phase of Pod annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850 is Running (Ready = true)
    Jan 18 23:32:23.340: INFO: Pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850" satisfied condition "running and ready"
    Jan 18 23:32:23.860: INFO: Successfully updated pod "annotationupdate822dfb6d-a506-4001-a23e-100c8b6f5850"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5167" for this suite. 01/18/23 23:32:27.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:27.887
Jan 18 23:32:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:32:27.888
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:27.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:27.905
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:32:27.908
Jan 18 23:32:27.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a" in namespace "downward-api-931" to be "Succeeded or Failed"
Jan 18 23:32:27.918: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565878ms
Jan 18 23:32:29.923: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006894295s
Jan 18 23:32:31.922: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006081849s
STEP: Saw pod success 01/18/23 23:32:31.922
Jan 18 23:32:31.922: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a" satisfied condition "Succeeded or Failed"
Jan 18 23:32:31.925: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a container client-container: <nil>
STEP: delete the pod 01/18/23 23:32:31.929
Jan 18 23:32:31.942: INFO: Waiting for pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a to disappear
Jan 18 23:32:31.945: INFO: Pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-931" for this suite. 01/18/23 23:32:31.948
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:27.887
    Jan 18 23:32:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:32:27.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:27.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:27.905
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:32:27.908
    Jan 18 23:32:27.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a" in namespace "downward-api-931" to be "Succeeded or Failed"
    Jan 18 23:32:27.918: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565878ms
    Jan 18 23:32:29.923: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006894295s
    Jan 18 23:32:31.922: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006081849s
    STEP: Saw pod success 01/18/23 23:32:31.922
    Jan 18 23:32:31.922: INFO: Pod "downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a" satisfied condition "Succeeded or Failed"
    Jan 18 23:32:31.925: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a container client-container: <nil>
    STEP: delete the pod 01/18/23 23:32:31.929
    Jan 18 23:32:31.942: INFO: Waiting for pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a to disappear
    Jan 18 23:32:31.945: INFO: Pod downwardapi-volume-f1be3c9c-6994-43a6-a184-7838b9bbd63a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-931" for this suite. 01/18/23 23:32:31.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:31.954
Jan 18 23:32:31.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption 01/18/23 23:32:31.955
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:31.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:31.968
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/18/23 23:32:31.976
STEP: Updating PodDisruptionBudget status 01/18/23 23:32:33.983
STEP: Waiting for all pods to be running 01/18/23 23:32:33.99
Jan 18 23:32:33.992: INFO: running pods: 0 < 1
STEP: locating a running pod 01/18/23 23:32:35.996
STEP: Waiting for the pdb to be processed 01/18/23 23:32:36.006
STEP: Patching PodDisruptionBudget status 01/18/23 23:32:36.012
STEP: Waiting for the pdb to be processed 01/18/23 23:32:36.019
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:36.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6190" for this suite. 01/18/23 23:32:36.026
------------------------------
â€¢ [4.078 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:31.954
    Jan 18 23:32:31.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption 01/18/23 23:32:31.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:31.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:31.968
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/18/23 23:32:31.976
    STEP: Updating PodDisruptionBudget status 01/18/23 23:32:33.983
    STEP: Waiting for all pods to be running 01/18/23 23:32:33.99
    Jan 18 23:32:33.992: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/18/23 23:32:35.996
    STEP: Waiting for the pdb to be processed 01/18/23 23:32:36.006
    STEP: Patching PodDisruptionBudget status 01/18/23 23:32:36.012
    STEP: Waiting for the pdb to be processed 01/18/23 23:32:36.019
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:36.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6190" for this suite. 01/18/23 23:32:36.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:36.033
Jan 18 23:32:36.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:32:36.033
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:36.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:36.05
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9192 01/18/23 23:32:36.053
STEP: creating a selector 01/18/23 23:32:36.053
STEP: Creating the service pods in kubernetes 01/18/23 23:32:36.053
Jan 18 23:32:36.053: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:32:36.071: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9192" to be "running and ready"
Jan 18 23:32:36.074: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904323ms
Jan 18 23:32:36.074: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:32:38.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006458058s
Jan 18 23:32:38.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:32:40.077: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006397736s
Jan 18 23:32:40.077: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:32:42.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007197455s
Jan 18 23:32:42.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:32:44.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006478197s
Jan 18 23:32:44.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:32:46.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006981911s
Jan 18 23:32:46.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 23:32:48.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006520422s
Jan 18 23:32:48.078: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 23:32:48.078: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 23:32:48.080: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9192" to be "running and ready"
Jan 18 23:32:48.082: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.194833ms
Jan 18 23:32:48.082: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 23:32:48.082: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 23:32:48.085
Jan 18 23:32:48.095: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9192" to be "running"
Jan 18 23:32:48.098: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643321ms
Jan 18 23:32:50.102: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006219896s
Jan 18 23:32:50.102: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 23:32:50.104: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9192" to be "running"
Jan 18 23:32:50.107: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.419077ms
Jan 18 23:32:50.107: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 23:32:50.109: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 23:32:50.109: INFO: Going to poll 10.32.0.2 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 23:32:50.111: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:32:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:32:50.112: INFO: ExecWithOptions: Clientset creation
Jan 18 23:32:50.112: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.32.0.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:32:51.184: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 23:32:51.184: INFO: Going to poll 10.32.12.4 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 23:32:51.187: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.12.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:32:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:32:51.188: INFO: ExecWithOptions: Clientset creation
Jan 18 23:32:51.188: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.32.12.4+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:32:52.261: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:52.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9192" for this suite. 01/18/23 23:32:52.264
------------------------------
â€¢ [SLOW TEST] [16.237 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:36.033
    Jan 18 23:32:36.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 23:32:36.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:36.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:36.05
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9192 01/18/23 23:32:36.053
    STEP: creating a selector 01/18/23 23:32:36.053
    STEP: Creating the service pods in kubernetes 01/18/23 23:32:36.053
    Jan 18 23:32:36.053: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 23:32:36.071: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9192" to be "running and ready"
    Jan 18 23:32:36.074: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904323ms
    Jan 18 23:32:36.074: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:32:38.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006458058s
    Jan 18 23:32:38.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:32:40.077: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006397736s
    Jan 18 23:32:40.077: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:32:42.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007197455s
    Jan 18 23:32:42.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:32:44.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006478197s
    Jan 18 23:32:44.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:32:46.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006981911s
    Jan 18 23:32:46.078: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 23:32:48.078: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006520422s
    Jan 18 23:32:48.078: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 23:32:48.078: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 23:32:48.080: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9192" to be "running and ready"
    Jan 18 23:32:48.082: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.194833ms
    Jan 18 23:32:48.082: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 23:32:48.082: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 23:32:48.085
    Jan 18 23:32:48.095: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9192" to be "running"
    Jan 18 23:32:48.098: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643321ms
    Jan 18 23:32:50.102: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006219896s
    Jan 18 23:32:50.102: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 23:32:50.104: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9192" to be "running"
    Jan 18 23:32:50.107: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.419077ms
    Jan 18 23:32:50.107: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 23:32:50.109: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 23:32:50.109: INFO: Going to poll 10.32.0.2 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 23:32:50.111: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:32:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:32:50.112: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:32:50.112: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.32.0.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 23:32:51.184: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 23:32:51.184: INFO: Going to poll 10.32.12.4 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 23:32:51.187: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.12.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 23:32:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:32:51.188: INFO: ExecWithOptions: Clientset creation
    Jan 18 23:32:51.188: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.32.12.4+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 23:32:52.261: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:52.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9192" for this suite. 01/18/23 23:32:52.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:52.27
Jan 18 23:32:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 23:32:52.271
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:52.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:52.285
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/18/23 23:32:52.288
STEP: getting 01/18/23 23:32:52.301
STEP: listing 01/18/23 23:32:52.305
STEP: deleting 01/18/23 23:32:52.307
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:32:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4609" for this suite. 01/18/23 23:32:52.323
------------------------------
â€¢ [0.059 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:52.27
    Jan 18 23:32:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 23:32:52.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:52.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:52.285
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/18/23 23:32:52.288
    STEP: getting 01/18/23 23:32:52.301
    STEP: listing 01/18/23 23:32:52.305
    STEP: deleting 01/18/23 23:32:52.307
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:32:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4609" for this suite. 01/18/23 23:32:52.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:32:52.332
Jan 18 23:32:52.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 23:32:52.333
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:52.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:52.349
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/18/23 23:32:52.354
STEP: delete the rc 01/18/23 23:32:57.367
STEP: wait for the rc to be deleted 01/18/23 23:32:57.378
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 23:33:02.381
STEP: Gathering metrics 01/18/23 23:33:32.393
Jan 18 23:33:32.405: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 23:33:32.407: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.409444ms
Jan 18 23:33:32.408: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 23:33:32.408: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 23:33:32.480: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 23:33:32.480: INFO: Deleting pod "simpletest.rc-27p4n" in namespace "gc-2084"
Jan 18 23:33:32.489: INFO: Deleting pod "simpletest.rc-42dgt" in namespace "gc-2084"
Jan 18 23:33:32.499: INFO: Deleting pod "simpletest.rc-42z77" in namespace "gc-2084"
Jan 18 23:33:32.508: INFO: Deleting pod "simpletest.rc-45hvh" in namespace "gc-2084"
Jan 18 23:33:32.520: INFO: Deleting pod "simpletest.rc-46nbm" in namespace "gc-2084"
Jan 18 23:33:32.529: INFO: Deleting pod "simpletest.rc-48hsw" in namespace "gc-2084"
Jan 18 23:33:32.541: INFO: Deleting pod "simpletest.rc-49cgt" in namespace "gc-2084"
Jan 18 23:33:32.552: INFO: Deleting pod "simpletest.rc-4nzpn" in namespace "gc-2084"
Jan 18 23:33:32.562: INFO: Deleting pod "simpletest.rc-548bb" in namespace "gc-2084"
Jan 18 23:33:32.571: INFO: Deleting pod "simpletest.rc-5smkq" in namespace "gc-2084"
Jan 18 23:33:32.581: INFO: Deleting pod "simpletest.rc-5xf7h" in namespace "gc-2084"
Jan 18 23:33:32.594: INFO: Deleting pod "simpletest.rc-67x7l" in namespace "gc-2084"
Jan 18 23:33:32.603: INFO: Deleting pod "simpletest.rc-6p6qp" in namespace "gc-2084"
Jan 18 23:33:32.615: INFO: Deleting pod "simpletest.rc-6wjz8" in namespace "gc-2084"
Jan 18 23:33:32.629: INFO: Deleting pod "simpletest.rc-6zwqn" in namespace "gc-2084"
Jan 18 23:33:32.638: INFO: Deleting pod "simpletest.rc-6zx8x" in namespace "gc-2084"
Jan 18 23:33:32.652: INFO: Deleting pod "simpletest.rc-7rpj2" in namespace "gc-2084"
Jan 18 23:33:32.661: INFO: Deleting pod "simpletest.rc-7wxq7" in namespace "gc-2084"
Jan 18 23:33:32.672: INFO: Deleting pod "simpletest.rc-7zx9c" in namespace "gc-2084"
Jan 18 23:33:32.689: INFO: Deleting pod "simpletest.rc-87mts" in namespace "gc-2084"
Jan 18 23:33:32.704: INFO: Deleting pod "simpletest.rc-8h2xk" in namespace "gc-2084"
Jan 18 23:33:32.714: INFO: Deleting pod "simpletest.rc-8rhzb" in namespace "gc-2084"
Jan 18 23:33:32.734: INFO: Deleting pod "simpletest.rc-8x5qf" in namespace "gc-2084"
Jan 18 23:33:32.747: INFO: Deleting pod "simpletest.rc-9hv9d" in namespace "gc-2084"
Jan 18 23:33:32.758: INFO: Deleting pod "simpletest.rc-9hz9r" in namespace "gc-2084"
Jan 18 23:33:32.773: INFO: Deleting pod "simpletest.rc-9jqmk" in namespace "gc-2084"
Jan 18 23:33:32.788: INFO: Deleting pod "simpletest.rc-9kwwd" in namespace "gc-2084"
Jan 18 23:33:32.801: INFO: Deleting pod "simpletest.rc-9pmbn" in namespace "gc-2084"
Jan 18 23:33:32.815: INFO: Deleting pod "simpletest.rc-bjtld" in namespace "gc-2084"
Jan 18 23:33:32.828: INFO: Deleting pod "simpletest.rc-cgvnb" in namespace "gc-2084"
Jan 18 23:33:32.841: INFO: Deleting pod "simpletest.rc-ch87g" in namespace "gc-2084"
Jan 18 23:33:32.854: INFO: Deleting pod "simpletest.rc-cmr96" in namespace "gc-2084"
Jan 18 23:33:32.871: INFO: Deleting pod "simpletest.rc-csxdt" in namespace "gc-2084"
Jan 18 23:33:32.882: INFO: Deleting pod "simpletest.rc-ctslt" in namespace "gc-2084"
Jan 18 23:33:32.902: INFO: Deleting pod "simpletest.rc-ctxlj" in namespace "gc-2084"
Jan 18 23:33:32.921: INFO: Deleting pod "simpletest.rc-d45v6" in namespace "gc-2084"
Jan 18 23:33:32.938: INFO: Deleting pod "simpletest.rc-d6b4j" in namespace "gc-2084"
Jan 18 23:33:32.949: INFO: Deleting pod "simpletest.rc-ddmpm" in namespace "gc-2084"
Jan 18 23:33:32.964: INFO: Deleting pod "simpletest.rc-dzsbp" in namespace "gc-2084"
Jan 18 23:33:32.977: INFO: Deleting pod "simpletest.rc-fkq4k" in namespace "gc-2084"
Jan 18 23:33:32.992: INFO: Deleting pod "simpletest.rc-fnvgm" in namespace "gc-2084"
Jan 18 23:33:33.004: INFO: Deleting pod "simpletest.rc-frxjs" in namespace "gc-2084"
Jan 18 23:33:33.031: INFO: Deleting pod "simpletest.rc-g5bl4" in namespace "gc-2084"
Jan 18 23:33:33.044: INFO: Deleting pod "simpletest.rc-gn5qq" in namespace "gc-2084"
Jan 18 23:33:33.064: INFO: Deleting pod "simpletest.rc-gw8j7" in namespace "gc-2084"
Jan 18 23:33:33.074: INFO: Deleting pod "simpletest.rc-gx22z" in namespace "gc-2084"
Jan 18 23:33:33.087: INFO: Deleting pod "simpletest.rc-h64lq" in namespace "gc-2084"
Jan 18 23:33:33.100: INFO: Deleting pod "simpletest.rc-hhs2v" in namespace "gc-2084"
Jan 18 23:33:33.112: INFO: Deleting pod "simpletest.rc-hnhzv" in namespace "gc-2084"
Jan 18 23:33:33.123: INFO: Deleting pod "simpletest.rc-hr422" in namespace "gc-2084"
Jan 18 23:33:33.140: INFO: Deleting pod "simpletest.rc-hsxwd" in namespace "gc-2084"
Jan 18 23:33:33.154: INFO: Deleting pod "simpletest.rc-jk72p" in namespace "gc-2084"
Jan 18 23:33:33.170: INFO: Deleting pod "simpletest.rc-jlmr2" in namespace "gc-2084"
Jan 18 23:33:33.180: INFO: Deleting pod "simpletest.rc-jrzsx" in namespace "gc-2084"
Jan 18 23:33:33.203: INFO: Deleting pod "simpletest.rc-ks84c" in namespace "gc-2084"
Jan 18 23:33:33.219: INFO: Deleting pod "simpletest.rc-ldccg" in namespace "gc-2084"
Jan 18 23:33:33.242: INFO: Deleting pod "simpletest.rc-ldfhj" in namespace "gc-2084"
Jan 18 23:33:33.255: INFO: Deleting pod "simpletest.rc-lhws2" in namespace "gc-2084"
Jan 18 23:33:33.266: INFO: Deleting pod "simpletest.rc-lkqkj" in namespace "gc-2084"
Jan 18 23:33:33.281: INFO: Deleting pod "simpletest.rc-lxlcc" in namespace "gc-2084"
Jan 18 23:33:33.296: INFO: Deleting pod "simpletest.rc-lztls" in namespace "gc-2084"
Jan 18 23:33:33.310: INFO: Deleting pod "simpletest.rc-mgzgg" in namespace "gc-2084"
Jan 18 23:33:33.323: INFO: Deleting pod "simpletest.rc-mvlgh" in namespace "gc-2084"
Jan 18 23:33:33.336: INFO: Deleting pod "simpletest.rc-n4vxc" in namespace "gc-2084"
Jan 18 23:33:33.350: INFO: Deleting pod "simpletest.rc-n5dtl" in namespace "gc-2084"
Jan 18 23:33:33.365: INFO: Deleting pod "simpletest.rc-n96f4" in namespace "gc-2084"
Jan 18 23:33:33.398: INFO: Deleting pod "simpletest.rc-nmdbm" in namespace "gc-2084"
Jan 18 23:33:33.449: INFO: Deleting pod "simpletest.rc-nvg2x" in namespace "gc-2084"
Jan 18 23:33:33.500: INFO: Deleting pod "simpletest.rc-p5g85" in namespace "gc-2084"
Jan 18 23:33:33.547: INFO: Deleting pod "simpletest.rc-px42b" in namespace "gc-2084"
Jan 18 23:33:33.595: INFO: Deleting pod "simpletest.rc-q56ds" in namespace "gc-2084"
Jan 18 23:33:33.642: INFO: Deleting pod "simpletest.rc-q7drk" in namespace "gc-2084"
Jan 18 23:33:33.700: INFO: Deleting pod "simpletest.rc-q8crf" in namespace "gc-2084"
Jan 18 23:33:33.745: INFO: Deleting pod "simpletest.rc-qjkhw" in namespace "gc-2084"
Jan 18 23:33:33.795: INFO: Deleting pod "simpletest.rc-qmf8z" in namespace "gc-2084"
Jan 18 23:33:33.846: INFO: Deleting pod "simpletest.rc-qnts6" in namespace "gc-2084"
Jan 18 23:33:33.894: INFO: Deleting pod "simpletest.rc-qpl2l" in namespace "gc-2084"
Jan 18 23:33:33.944: INFO: Deleting pod "simpletest.rc-r6hbm" in namespace "gc-2084"
Jan 18 23:33:33.996: INFO: Deleting pod "simpletest.rc-r9dw6" in namespace "gc-2084"
Jan 18 23:33:34.051: INFO: Deleting pod "simpletest.rc-rmfvj" in namespace "gc-2084"
Jan 18 23:33:34.096: INFO: Deleting pod "simpletest.rc-rp5r4" in namespace "gc-2084"
Jan 18 23:33:34.145: INFO: Deleting pod "simpletest.rc-rsl4s" in namespace "gc-2084"
Jan 18 23:33:34.194: INFO: Deleting pod "simpletest.rc-rtfmr" in namespace "gc-2084"
Jan 18 23:33:34.244: INFO: Deleting pod "simpletest.rc-rxbcj" in namespace "gc-2084"
Jan 18 23:33:34.297: INFO: Deleting pod "simpletest.rc-s7whg" in namespace "gc-2084"
Jan 18 23:33:34.345: INFO: Deleting pod "simpletest.rc-sjv8h" in namespace "gc-2084"
Jan 18 23:33:34.399: INFO: Deleting pod "simpletest.rc-tpl4h" in namespace "gc-2084"
Jan 18 23:33:34.445: INFO: Deleting pod "simpletest.rc-tqzgp" in namespace "gc-2084"
Jan 18 23:33:34.495: INFO: Deleting pod "simpletest.rc-v92tk" in namespace "gc-2084"
Jan 18 23:33:34.547: INFO: Deleting pod "simpletest.rc-v9ntk" in namespace "gc-2084"
Jan 18 23:33:34.602: INFO: Deleting pod "simpletest.rc-wclhg" in namespace "gc-2084"
Jan 18 23:33:34.646: INFO: Deleting pod "simpletest.rc-wjkcg" in namespace "gc-2084"
Jan 18 23:33:34.696: INFO: Deleting pod "simpletest.rc-wx2sq" in namespace "gc-2084"
Jan 18 23:33:34.743: INFO: Deleting pod "simpletest.rc-wztj2" in namespace "gc-2084"
Jan 18 23:33:34.796: INFO: Deleting pod "simpletest.rc-x2zf8" in namespace "gc-2084"
Jan 18 23:33:34.848: INFO: Deleting pod "simpletest.rc-xqstd" in namespace "gc-2084"
Jan 18 23:33:34.895: INFO: Deleting pod "simpletest.rc-zb2pn" in namespace "gc-2084"
Jan 18 23:33:34.947: INFO: Deleting pod "simpletest.rc-zk96r" in namespace "gc-2084"
Jan 18 23:33:34.996: INFO: Deleting pod "simpletest.rc-zppxj" in namespace "gc-2084"
Jan 18 23:33:35.047: INFO: Deleting pod "simpletest.rc-zxxg7" in namespace "gc-2084"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 23:33:35.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2084" for this suite. 01/18/23 23:33:35.137
------------------------------
â€¢ [SLOW TEST] [42.858 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:32:52.332
    Jan 18 23:32:52.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 23:32:52.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:32:52.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:32:52.349
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/18/23 23:32:52.354
    STEP: delete the rc 01/18/23 23:32:57.367
    STEP: wait for the rc to be deleted 01/18/23 23:32:57.378
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 23:33:02.381
    STEP: Gathering metrics 01/18/23 23:33:32.393
    Jan 18 23:33:32.405: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 23:33:32.407: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.409444ms
    Jan 18 23:33:32.408: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 23:33:32.408: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 23:33:32.480: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 23:33:32.480: INFO: Deleting pod "simpletest.rc-27p4n" in namespace "gc-2084"
    Jan 18 23:33:32.489: INFO: Deleting pod "simpletest.rc-42dgt" in namespace "gc-2084"
    Jan 18 23:33:32.499: INFO: Deleting pod "simpletest.rc-42z77" in namespace "gc-2084"
    Jan 18 23:33:32.508: INFO: Deleting pod "simpletest.rc-45hvh" in namespace "gc-2084"
    Jan 18 23:33:32.520: INFO: Deleting pod "simpletest.rc-46nbm" in namespace "gc-2084"
    Jan 18 23:33:32.529: INFO: Deleting pod "simpletest.rc-48hsw" in namespace "gc-2084"
    Jan 18 23:33:32.541: INFO: Deleting pod "simpletest.rc-49cgt" in namespace "gc-2084"
    Jan 18 23:33:32.552: INFO: Deleting pod "simpletest.rc-4nzpn" in namespace "gc-2084"
    Jan 18 23:33:32.562: INFO: Deleting pod "simpletest.rc-548bb" in namespace "gc-2084"
    Jan 18 23:33:32.571: INFO: Deleting pod "simpletest.rc-5smkq" in namespace "gc-2084"
    Jan 18 23:33:32.581: INFO: Deleting pod "simpletest.rc-5xf7h" in namespace "gc-2084"
    Jan 18 23:33:32.594: INFO: Deleting pod "simpletest.rc-67x7l" in namespace "gc-2084"
    Jan 18 23:33:32.603: INFO: Deleting pod "simpletest.rc-6p6qp" in namespace "gc-2084"
    Jan 18 23:33:32.615: INFO: Deleting pod "simpletest.rc-6wjz8" in namespace "gc-2084"
    Jan 18 23:33:32.629: INFO: Deleting pod "simpletest.rc-6zwqn" in namespace "gc-2084"
    Jan 18 23:33:32.638: INFO: Deleting pod "simpletest.rc-6zx8x" in namespace "gc-2084"
    Jan 18 23:33:32.652: INFO: Deleting pod "simpletest.rc-7rpj2" in namespace "gc-2084"
    Jan 18 23:33:32.661: INFO: Deleting pod "simpletest.rc-7wxq7" in namespace "gc-2084"
    Jan 18 23:33:32.672: INFO: Deleting pod "simpletest.rc-7zx9c" in namespace "gc-2084"
    Jan 18 23:33:32.689: INFO: Deleting pod "simpletest.rc-87mts" in namespace "gc-2084"
    Jan 18 23:33:32.704: INFO: Deleting pod "simpletest.rc-8h2xk" in namespace "gc-2084"
    Jan 18 23:33:32.714: INFO: Deleting pod "simpletest.rc-8rhzb" in namespace "gc-2084"
    Jan 18 23:33:32.734: INFO: Deleting pod "simpletest.rc-8x5qf" in namespace "gc-2084"
    Jan 18 23:33:32.747: INFO: Deleting pod "simpletest.rc-9hv9d" in namespace "gc-2084"
    Jan 18 23:33:32.758: INFO: Deleting pod "simpletest.rc-9hz9r" in namespace "gc-2084"
    Jan 18 23:33:32.773: INFO: Deleting pod "simpletest.rc-9jqmk" in namespace "gc-2084"
    Jan 18 23:33:32.788: INFO: Deleting pod "simpletest.rc-9kwwd" in namespace "gc-2084"
    Jan 18 23:33:32.801: INFO: Deleting pod "simpletest.rc-9pmbn" in namespace "gc-2084"
    Jan 18 23:33:32.815: INFO: Deleting pod "simpletest.rc-bjtld" in namespace "gc-2084"
    Jan 18 23:33:32.828: INFO: Deleting pod "simpletest.rc-cgvnb" in namespace "gc-2084"
    Jan 18 23:33:32.841: INFO: Deleting pod "simpletest.rc-ch87g" in namespace "gc-2084"
    Jan 18 23:33:32.854: INFO: Deleting pod "simpletest.rc-cmr96" in namespace "gc-2084"
    Jan 18 23:33:32.871: INFO: Deleting pod "simpletest.rc-csxdt" in namespace "gc-2084"
    Jan 18 23:33:32.882: INFO: Deleting pod "simpletest.rc-ctslt" in namespace "gc-2084"
    Jan 18 23:33:32.902: INFO: Deleting pod "simpletest.rc-ctxlj" in namespace "gc-2084"
    Jan 18 23:33:32.921: INFO: Deleting pod "simpletest.rc-d45v6" in namespace "gc-2084"
    Jan 18 23:33:32.938: INFO: Deleting pod "simpletest.rc-d6b4j" in namespace "gc-2084"
    Jan 18 23:33:32.949: INFO: Deleting pod "simpletest.rc-ddmpm" in namespace "gc-2084"
    Jan 18 23:33:32.964: INFO: Deleting pod "simpletest.rc-dzsbp" in namespace "gc-2084"
    Jan 18 23:33:32.977: INFO: Deleting pod "simpletest.rc-fkq4k" in namespace "gc-2084"
    Jan 18 23:33:32.992: INFO: Deleting pod "simpletest.rc-fnvgm" in namespace "gc-2084"
    Jan 18 23:33:33.004: INFO: Deleting pod "simpletest.rc-frxjs" in namespace "gc-2084"
    Jan 18 23:33:33.031: INFO: Deleting pod "simpletest.rc-g5bl4" in namespace "gc-2084"
    Jan 18 23:33:33.044: INFO: Deleting pod "simpletest.rc-gn5qq" in namespace "gc-2084"
    Jan 18 23:33:33.064: INFO: Deleting pod "simpletest.rc-gw8j7" in namespace "gc-2084"
    Jan 18 23:33:33.074: INFO: Deleting pod "simpletest.rc-gx22z" in namespace "gc-2084"
    Jan 18 23:33:33.087: INFO: Deleting pod "simpletest.rc-h64lq" in namespace "gc-2084"
    Jan 18 23:33:33.100: INFO: Deleting pod "simpletest.rc-hhs2v" in namespace "gc-2084"
    Jan 18 23:33:33.112: INFO: Deleting pod "simpletest.rc-hnhzv" in namespace "gc-2084"
    Jan 18 23:33:33.123: INFO: Deleting pod "simpletest.rc-hr422" in namespace "gc-2084"
    Jan 18 23:33:33.140: INFO: Deleting pod "simpletest.rc-hsxwd" in namespace "gc-2084"
    Jan 18 23:33:33.154: INFO: Deleting pod "simpletest.rc-jk72p" in namespace "gc-2084"
    Jan 18 23:33:33.170: INFO: Deleting pod "simpletest.rc-jlmr2" in namespace "gc-2084"
    Jan 18 23:33:33.180: INFO: Deleting pod "simpletest.rc-jrzsx" in namespace "gc-2084"
    Jan 18 23:33:33.203: INFO: Deleting pod "simpletest.rc-ks84c" in namespace "gc-2084"
    Jan 18 23:33:33.219: INFO: Deleting pod "simpletest.rc-ldccg" in namespace "gc-2084"
    Jan 18 23:33:33.242: INFO: Deleting pod "simpletest.rc-ldfhj" in namespace "gc-2084"
    Jan 18 23:33:33.255: INFO: Deleting pod "simpletest.rc-lhws2" in namespace "gc-2084"
    Jan 18 23:33:33.266: INFO: Deleting pod "simpletest.rc-lkqkj" in namespace "gc-2084"
    Jan 18 23:33:33.281: INFO: Deleting pod "simpletest.rc-lxlcc" in namespace "gc-2084"
    Jan 18 23:33:33.296: INFO: Deleting pod "simpletest.rc-lztls" in namespace "gc-2084"
    Jan 18 23:33:33.310: INFO: Deleting pod "simpletest.rc-mgzgg" in namespace "gc-2084"
    Jan 18 23:33:33.323: INFO: Deleting pod "simpletest.rc-mvlgh" in namespace "gc-2084"
    Jan 18 23:33:33.336: INFO: Deleting pod "simpletest.rc-n4vxc" in namespace "gc-2084"
    Jan 18 23:33:33.350: INFO: Deleting pod "simpletest.rc-n5dtl" in namespace "gc-2084"
    Jan 18 23:33:33.365: INFO: Deleting pod "simpletest.rc-n96f4" in namespace "gc-2084"
    Jan 18 23:33:33.398: INFO: Deleting pod "simpletest.rc-nmdbm" in namespace "gc-2084"
    Jan 18 23:33:33.449: INFO: Deleting pod "simpletest.rc-nvg2x" in namespace "gc-2084"
    Jan 18 23:33:33.500: INFO: Deleting pod "simpletest.rc-p5g85" in namespace "gc-2084"
    Jan 18 23:33:33.547: INFO: Deleting pod "simpletest.rc-px42b" in namespace "gc-2084"
    Jan 18 23:33:33.595: INFO: Deleting pod "simpletest.rc-q56ds" in namespace "gc-2084"
    Jan 18 23:33:33.642: INFO: Deleting pod "simpletest.rc-q7drk" in namespace "gc-2084"
    Jan 18 23:33:33.700: INFO: Deleting pod "simpletest.rc-q8crf" in namespace "gc-2084"
    Jan 18 23:33:33.745: INFO: Deleting pod "simpletest.rc-qjkhw" in namespace "gc-2084"
    Jan 18 23:33:33.795: INFO: Deleting pod "simpletest.rc-qmf8z" in namespace "gc-2084"
    Jan 18 23:33:33.846: INFO: Deleting pod "simpletest.rc-qnts6" in namespace "gc-2084"
    Jan 18 23:33:33.894: INFO: Deleting pod "simpletest.rc-qpl2l" in namespace "gc-2084"
    Jan 18 23:33:33.944: INFO: Deleting pod "simpletest.rc-r6hbm" in namespace "gc-2084"
    Jan 18 23:33:33.996: INFO: Deleting pod "simpletest.rc-r9dw6" in namespace "gc-2084"
    Jan 18 23:33:34.051: INFO: Deleting pod "simpletest.rc-rmfvj" in namespace "gc-2084"
    Jan 18 23:33:34.096: INFO: Deleting pod "simpletest.rc-rp5r4" in namespace "gc-2084"
    Jan 18 23:33:34.145: INFO: Deleting pod "simpletest.rc-rsl4s" in namespace "gc-2084"
    Jan 18 23:33:34.194: INFO: Deleting pod "simpletest.rc-rtfmr" in namespace "gc-2084"
    Jan 18 23:33:34.244: INFO: Deleting pod "simpletest.rc-rxbcj" in namespace "gc-2084"
    Jan 18 23:33:34.297: INFO: Deleting pod "simpletest.rc-s7whg" in namespace "gc-2084"
    Jan 18 23:33:34.345: INFO: Deleting pod "simpletest.rc-sjv8h" in namespace "gc-2084"
    Jan 18 23:33:34.399: INFO: Deleting pod "simpletest.rc-tpl4h" in namespace "gc-2084"
    Jan 18 23:33:34.445: INFO: Deleting pod "simpletest.rc-tqzgp" in namespace "gc-2084"
    Jan 18 23:33:34.495: INFO: Deleting pod "simpletest.rc-v92tk" in namespace "gc-2084"
    Jan 18 23:33:34.547: INFO: Deleting pod "simpletest.rc-v9ntk" in namespace "gc-2084"
    Jan 18 23:33:34.602: INFO: Deleting pod "simpletest.rc-wclhg" in namespace "gc-2084"
    Jan 18 23:33:34.646: INFO: Deleting pod "simpletest.rc-wjkcg" in namespace "gc-2084"
    Jan 18 23:33:34.696: INFO: Deleting pod "simpletest.rc-wx2sq" in namespace "gc-2084"
    Jan 18 23:33:34.743: INFO: Deleting pod "simpletest.rc-wztj2" in namespace "gc-2084"
    Jan 18 23:33:34.796: INFO: Deleting pod "simpletest.rc-x2zf8" in namespace "gc-2084"
    Jan 18 23:33:34.848: INFO: Deleting pod "simpletest.rc-xqstd" in namespace "gc-2084"
    Jan 18 23:33:34.895: INFO: Deleting pod "simpletest.rc-zb2pn" in namespace "gc-2084"
    Jan 18 23:33:34.947: INFO: Deleting pod "simpletest.rc-zk96r" in namespace "gc-2084"
    Jan 18 23:33:34.996: INFO: Deleting pod "simpletest.rc-zppxj" in namespace "gc-2084"
    Jan 18 23:33:35.047: INFO: Deleting pod "simpletest.rc-zxxg7" in namespace "gc-2084"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:33:35.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2084" for this suite. 01/18/23 23:33:35.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:33:35.193
Jan 18 23:33:35.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename daemonsets 01/18/23 23:33:35.194
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:35.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:35.209
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 23:33:35.228
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:33:35.233
Jan 18 23:33:35.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:33:35.240: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:33:36.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:33:36.247: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:33:37.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:33:37.247: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:33:38.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:33:38.248: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
Jan 18 23:33:39.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:33:39.247: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:33:40.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 23:33:40.247: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
Jan 18 23:33:41.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 23:33:41.249: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 23:33:41.252
Jan 18 23:33:41.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 23:33:41.271: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 23:33:41.271
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 23:33:42.279
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3159, will wait for the garbage collector to delete the pods 01/18/23 23:33:42.279
Jan 18 23:33:42.338: INFO: Deleting DaemonSet.extensions daemon-set took: 5.654927ms
Jan 18 23:33:42.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.309307ms
Jan 18 23:33:43.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:33:43.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 23:33:43.444: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28333"},"items":null}

Jan 18 23:33:43.446: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28333"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:33:43.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3159" for this suite. 01/18/23 23:33:43.457
------------------------------
â€¢ [SLOW TEST] [8.269 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:33:35.193
    Jan 18 23:33:35.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 23:33:35.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:35.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:35.209
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 23:33:35.228
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 23:33:35.233
    Jan 18 23:33:35.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:33:35.240: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:33:36.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:33:36.247: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:33:37.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:33:37.247: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:33:38.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:33:38.248: INFO: Node cncf-conformance-1-26-1 is running 0 daemon pod, expected 1
    Jan 18 23:33:39.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:33:39.247: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:33:40.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 23:33:40.247: INFO: Node cncf-conformance-1-26-2 is running 0 daemon pod, expected 1
    Jan 18 23:33:41.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 23:33:41.249: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 23:33:41.252
    Jan 18 23:33:41.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 23:33:41.271: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 23:33:41.271
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 23:33:42.279
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3159, will wait for the garbage collector to delete the pods 01/18/23 23:33:42.279
    Jan 18 23:33:42.338: INFO: Deleting DaemonSet.extensions daemon-set took: 5.654927ms
    Jan 18 23:33:42.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.309307ms
    Jan 18 23:33:43.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 23:33:43.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 23:33:43.444: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28333"},"items":null}

    Jan 18 23:33:43.446: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28333"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:33:43.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3159" for this suite. 01/18/23 23:33:43.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:33:43.463
Jan 18 23:33:43.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 23:33:43.464
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:43.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:43.478
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 23:33:43.481
Jan 18 23:33:43.488: INFO: Waiting up to 5m0s for pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697" in namespace "emptydir-221" to be "Succeeded or Failed"
Jan 18 23:33:43.491: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370198ms
Jan 18 23:33:45.495: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Running", Reason="", readiness=false. Elapsed: 2.006386899s
Jan 18 23:33:47.494: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005472726s
STEP: Saw pod success 01/18/23 23:33:47.494
Jan 18 23:33:47.494: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697" satisfied condition "Succeeded or Failed"
Jan 18 23:33:47.497: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 container test-container: <nil>
STEP: delete the pod 01/18/23 23:33:47.502
Jan 18 23:33:47.513: INFO: Waiting for pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 to disappear
Jan 18 23:33:47.516: INFO: Pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:33:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-221" for this suite. 01/18/23 23:33:47.518
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:33:43.463
    Jan 18 23:33:43.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 23:33:43.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:43.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:43.478
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 23:33:43.481
    Jan 18 23:33:43.488: INFO: Waiting up to 5m0s for pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697" in namespace "emptydir-221" to be "Succeeded or Failed"
    Jan 18 23:33:43.491: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370198ms
    Jan 18 23:33:45.495: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Running", Reason="", readiness=false. Elapsed: 2.006386899s
    Jan 18 23:33:47.494: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005472726s
    STEP: Saw pod success 01/18/23 23:33:47.494
    Jan 18 23:33:47.494: INFO: Pod "pod-c5e267b3-d85c-47fb-88b9-68d05f416697" satisfied condition "Succeeded or Failed"
    Jan 18 23:33:47.497: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 container test-container: <nil>
    STEP: delete the pod 01/18/23 23:33:47.502
    Jan 18 23:33:47.513: INFO: Waiting for pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 to disappear
    Jan 18 23:33:47.516: INFO: Pod pod-c5e267b3-d85c-47fb-88b9-68d05f416697 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:33:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-221" for this suite. 01/18/23 23:33:47.518
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:33:47.525
Jan 18 23:33:47.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:33:47.526
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:47.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:47.541
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:33:47.548
Jan 18 23:33:47.557: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6594" to be "running and ready"
Jan 18 23:33:47.560: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712904ms
Jan 18 23:33:47.560: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:33:49.564: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0068729s
Jan 18 23:33:49.564: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 23:33:49.564: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/18/23 23:33:49.567
Jan 18 23:33:49.574: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6594" to be "running and ready"
Jan 18 23:33:49.577: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.532037ms
Jan 18 23:33:49.577: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:33:51.580: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005958984s
Jan 18 23:33:51.580: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 18 23:33:51.580: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 23:33:51.582
Jan 18 23:33:51.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 23:33:51.592: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 23:33:53.593: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 23:33:53.596: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 23:33:55.594: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 23:33:55.597: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/18/23 23:33:55.597
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 23:33:55.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6594" for this suite. 01/18/23 23:33:55.604
------------------------------
â€¢ [SLOW TEST] [8.084 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:33:47.525
    Jan 18 23:33:47.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:33:47.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:47.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:47.541
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:33:47.548
    Jan 18 23:33:47.557: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6594" to be "running and ready"
    Jan 18 23:33:47.560: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712904ms
    Jan 18 23:33:47.560: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:33:49.564: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0068729s
    Jan 18 23:33:49.564: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 23:33:49.564: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/18/23 23:33:49.567
    Jan 18 23:33:49.574: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6594" to be "running and ready"
    Jan 18 23:33:49.577: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.532037ms
    Jan 18 23:33:49.577: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:33:51.580: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005958984s
    Jan 18 23:33:51.580: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 18 23:33:51.580: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 23:33:51.582
    Jan 18 23:33:51.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 23:33:51.592: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 23:33:53.593: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 23:33:53.596: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 23:33:55.594: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 23:33:55.597: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/18/23 23:33:55.597
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:33:55.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6594" for this suite. 01/18/23 23:33:55.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:33:55.609
Jan 18 23:33:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename webhook 01/18/23 23:33:55.611
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:55.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:55.622
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 23:33:55.635
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:33:56.318
STEP: Deploying the webhook pod 01/18/23 23:33:56.325
STEP: Wait for the deployment to be ready 01/18/23 23:33:56.335
Jan 18 23:33:56.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:33:58.35
STEP: Verifying the service has paired with the endpoint 01/18/23 23:33:58.358
Jan 18 23:33:59.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/18/23 23:33:59.363
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 23:33:59.384
STEP: Creating a configMap that should not be mutated 01/18/23 23:33:59.391
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 23:33:59.403
STEP: Creating a configMap that should be mutated 01/18/23 23:33:59.41
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:33:59.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8012" for this suite. 01/18/23 23:33:59.463
STEP: Destroying namespace "webhook-8012-markers" for this suite. 01/18/23 23:33:59.468
------------------------------
â€¢ [3.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:33:55.609
    Jan 18 23:33:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename webhook 01/18/23 23:33:55.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:55.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:55.622
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 23:33:55.635
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:33:56.318
    STEP: Deploying the webhook pod 01/18/23 23:33:56.325
    STEP: Wait for the deployment to be ready 01/18/23 23:33:56.335
    Jan 18 23:33:56.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:33:58.35
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:33:58.358
    Jan 18 23:33:59.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/18/23 23:33:59.363
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 23:33:59.384
    STEP: Creating a configMap that should not be mutated 01/18/23 23:33:59.391
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 23:33:59.403
    STEP: Creating a configMap that should be mutated 01/18/23 23:33:59.41
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:33:59.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8012" for this suite. 01/18/23 23:33:59.463
    STEP: Destroying namespace "webhook-8012-markers" for this suite. 01/18/23 23:33:59.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:33:59.473
Jan 18 23:33:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sysctl 01/18/23 23:33:59.474
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:59.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:59.489
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 23:33:59.492
STEP: Watching for error events or started pod 01/18/23 23:33:59.499
STEP: Waiting for pod completion 01/18/23 23:34:01.503
Jan 18 23:34:01.503: INFO: Waiting up to 3m0s for pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7" in namespace "sysctl-9543" to be "completed"
Jan 18 23:34:01.506: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436903ms
Jan 18 23:34:03.509: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005957283s
Jan 18 23:34:03.509: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/18/23 23:34:03.512
STEP: Getting logs from the pod 01/18/23 23:34:03.512
STEP: Checking that the sysctl is actually updated 01/18/23 23:34:03.517
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:03.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9543" for this suite. 01/18/23 23:34:03.52
------------------------------
â€¢ [4.053 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:33:59.473
    Jan 18 23:33:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sysctl 01/18/23 23:33:59.474
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:33:59.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:33:59.489
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 23:33:59.492
    STEP: Watching for error events or started pod 01/18/23 23:33:59.499
    STEP: Waiting for pod completion 01/18/23 23:34:01.503
    Jan 18 23:34:01.503: INFO: Waiting up to 3m0s for pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7" in namespace "sysctl-9543" to be "completed"
    Jan 18 23:34:01.506: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436903ms
    Jan 18 23:34:03.509: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005957283s
    Jan 18 23:34:03.509: INFO: Pod "sysctl-a7b397d5-75e5-4f39-95fa-4b1d4e1021e7" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/18/23 23:34:03.512
    STEP: Getting logs from the pod 01/18/23 23:34:03.512
    STEP: Checking that the sysctl is actually updated 01/18/23 23:34:03.517
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:03.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9543" for this suite. 01/18/23 23:34:03.52
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:03.526
Jan 18 23:34:03.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:34:03.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:03.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:03.544
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/18/23 23:34:03.547
Jan 18 23:34:03.554: INFO: Waiting up to 5m0s for pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c" in namespace "downward-api-1274" to be "Succeeded or Failed"
Jan 18 23:34:03.557: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340276ms
Jan 18 23:34:05.561: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006280496s
Jan 18 23:34:07.560: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005881546s
STEP: Saw pod success 01/18/23 23:34:07.56
Jan 18 23:34:07.560: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c" satisfied condition "Succeeded or Failed"
Jan 18 23:34:07.563: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:34:07.568
Jan 18 23:34:07.582: INFO: Waiting for pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c to disappear
Jan 18 23:34:07.584: INFO: Pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:07.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1274" for this suite. 01/18/23 23:34:07.587
------------------------------
â€¢ [4.066 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:03.526
    Jan 18 23:34:03.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:34:03.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:03.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:03.544
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/18/23 23:34:03.547
    Jan 18 23:34:03.554: INFO: Waiting up to 5m0s for pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c" in namespace "downward-api-1274" to be "Succeeded or Failed"
    Jan 18 23:34:03.557: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340276ms
    Jan 18 23:34:05.561: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006280496s
    Jan 18 23:34:07.560: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005881546s
    STEP: Saw pod success 01/18/23 23:34:07.56
    Jan 18 23:34:07.560: INFO: Pod "downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c" satisfied condition "Succeeded or Failed"
    Jan 18 23:34:07.563: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:34:07.568
    Jan 18 23:34:07.582: INFO: Waiting for pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c to disappear
    Jan 18 23:34:07.584: INFO: Pod downward-api-17c21dd3-90dc-471d-b345-86f3d632a81c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:07.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1274" for this suite. 01/18/23 23:34:07.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:07.593
Jan 18 23:34:07.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename emptydir 01/18/23 23:34:07.594
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:07.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:07.61
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 23:34:07.613
Jan 18 23:34:07.622: INFO: Waiting up to 5m0s for pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34" in namespace "emptydir-2301" to be "Succeeded or Failed"
Jan 18 23:34:07.624: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182066ms
Jan 18 23:34:09.628: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005929452s
Jan 18 23:34:11.629: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007579653s
STEP: Saw pod success 01/18/23 23:34:11.629
Jan 18 23:34:11.629: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34" satisfied condition "Succeeded or Failed"
Jan 18 23:34:11.632: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 container test-container: <nil>
STEP: delete the pod 01/18/23 23:34:11.636
Jan 18 23:34:11.645: INFO: Waiting for pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 to disappear
Jan 18 23:34:11.648: INFO: Pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2301" for this suite. 01/18/23 23:34:11.651
------------------------------
â€¢ [4.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:07.593
    Jan 18 23:34:07.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename emptydir 01/18/23 23:34:07.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:07.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:07.61
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 23:34:07.613
    Jan 18 23:34:07.622: INFO: Waiting up to 5m0s for pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34" in namespace "emptydir-2301" to be "Succeeded or Failed"
    Jan 18 23:34:07.624: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182066ms
    Jan 18 23:34:09.628: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005929452s
    Jan 18 23:34:11.629: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007579653s
    STEP: Saw pod success 01/18/23 23:34:11.629
    Jan 18 23:34:11.629: INFO: Pod "pod-26dd653f-79f0-4de3-8675-0003e77faa34" satisfied condition "Succeeded or Failed"
    Jan 18 23:34:11.632: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 container test-container: <nil>
    STEP: delete the pod 01/18/23 23:34:11.636
    Jan 18 23:34:11.645: INFO: Waiting for pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 to disappear
    Jan 18 23:34:11.648: INFO: Pod pod-26dd653f-79f0-4de3-8675-0003e77faa34 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2301" for this suite. 01/18/23 23:34:11.651
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:11.656
Jan 18 23:34:11.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:34:11.657
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:11.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:11.671
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 23:34:11.674
Jan 18 23:34:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
Jan 18 23:34:13.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:19.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8502" for this suite. 01/18/23 23:34:19.271
------------------------------
â€¢ [SLOW TEST] [7.622 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:11.656
    Jan 18 23:34:11.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:34:11.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:11.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:11.671
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 23:34:11.674
    Jan 18 23:34:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    Jan 18 23:34:13.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:19.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8502" for this suite. 01/18/23 23:34:19.271
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:19.278
Jan 18 23:34:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:34:19.279
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:19.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:19.293
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-33ff9c80-230f-4592-b947-f26453edf8c2 01/18/23 23:34:19.296
STEP: Creating a pod to test consume configMaps 01/18/23 23:34:19.303
Jan 18 23:34:19.311: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414" in namespace "projected-7830" to be "Succeeded or Failed"
Jan 18 23:34:19.314: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.98691ms
Jan 18 23:34:21.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006608824s
Jan 18 23:34:23.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006650822s
STEP: Saw pod success 01/18/23 23:34:23.317
Jan 18 23:34:23.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414" satisfied condition "Succeeded or Failed"
Jan 18 23:34:23.320: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:34:23.325
Jan 18 23:34:23.334: INFO: Waiting for pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 to disappear
Jan 18 23:34:23.336: INFO: Pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:23.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7830" for this suite. 01/18/23 23:34:23.339
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:19.278
    Jan 18 23:34:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:34:19.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:19.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:19.293
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-33ff9c80-230f-4592-b947-f26453edf8c2 01/18/23 23:34:19.296
    STEP: Creating a pod to test consume configMaps 01/18/23 23:34:19.303
    Jan 18 23:34:19.311: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414" in namespace "projected-7830" to be "Succeeded or Failed"
    Jan 18 23:34:19.314: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.98691ms
    Jan 18 23:34:21.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006608824s
    Jan 18 23:34:23.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006650822s
    STEP: Saw pod success 01/18/23 23:34:23.317
    Jan 18 23:34:23.317: INFO: Pod "pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414" satisfied condition "Succeeded or Failed"
    Jan 18 23:34:23.320: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:34:23.325
    Jan 18 23:34:23.334: INFO: Waiting for pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 to disappear
    Jan 18 23:34:23.336: INFO: Pod pod-projected-configmaps-955dc9df-fe4a-4a15-83ea-77cf36f5d414 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:23.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7830" for this suite. 01/18/23 23:34:23.339
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:23.344
Jan 18 23:34:23.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replication-controller 01/18/23 23:34:23.345
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:23.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:23.36
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/18/23 23:34:23.363
STEP: When the matched label of one of its pods change 01/18/23 23:34:23.368
Jan 18 23:34:23.371: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 23:34:28.375: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 23:34:28.384
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9565" for this suite. 01/18/23 23:34:29.393
------------------------------
â€¢ [SLOW TEST] [6.055 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:23.344
    Jan 18 23:34:23.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 23:34:23.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:23.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:23.36
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/18/23 23:34:23.363
    STEP: When the matched label of one of its pods change 01/18/23 23:34:23.368
    Jan 18 23:34:23.371: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 18 23:34:28.375: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 23:34:28.384
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9565" for this suite. 01/18/23 23:34:29.393
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:29.399
Jan 18 23:34:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename ingress 01/18/23 23:34:29.4
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:29.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:29.413
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/18/23 23:34:29.415
STEP: getting /apis/networking.k8s.io 01/18/23 23:34:29.418
STEP: getting /apis/networking.k8s.iov1 01/18/23 23:34:29.419
STEP: creating 01/18/23 23:34:29.42
STEP: getting 01/18/23 23:34:29.434
STEP: listing 01/18/23 23:34:29.436
STEP: watching 01/18/23 23:34:29.438
Jan 18 23:34:29.438: INFO: starting watch
STEP: cluster-wide listing 01/18/23 23:34:29.439
STEP: cluster-wide watching 01/18/23 23:34:29.441
Jan 18 23:34:29.441: INFO: starting watch
STEP: patching 01/18/23 23:34:29.442
STEP: updating 01/18/23 23:34:29.447
Jan 18 23:34:29.453: INFO: waiting for watch events with expected annotations
Jan 18 23:34:29.453: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 23:34:29.453
STEP: updating /status 01/18/23 23:34:29.459
STEP: get /status 01/18/23 23:34:29.466
STEP: deleting 01/18/23 23:34:29.468
STEP: deleting a collection 01/18/23 23:34:29.477
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:29.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-476" for this suite. 01/18/23 23:34:29.49
------------------------------
â€¢ [0.096 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:29.399
    Jan 18 23:34:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename ingress 01/18/23 23:34:29.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:29.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:29.413
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/18/23 23:34:29.415
    STEP: getting /apis/networking.k8s.io 01/18/23 23:34:29.418
    STEP: getting /apis/networking.k8s.iov1 01/18/23 23:34:29.419
    STEP: creating 01/18/23 23:34:29.42
    STEP: getting 01/18/23 23:34:29.434
    STEP: listing 01/18/23 23:34:29.436
    STEP: watching 01/18/23 23:34:29.438
    Jan 18 23:34:29.438: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 23:34:29.439
    STEP: cluster-wide watching 01/18/23 23:34:29.441
    Jan 18 23:34:29.441: INFO: starting watch
    STEP: patching 01/18/23 23:34:29.442
    STEP: updating 01/18/23 23:34:29.447
    Jan 18 23:34:29.453: INFO: waiting for watch events with expected annotations
    Jan 18 23:34:29.453: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 23:34:29.453
    STEP: updating /status 01/18/23 23:34:29.459
    STEP: get /status 01/18/23 23:34:29.466
    STEP: deleting 01/18/23 23:34:29.468
    STEP: deleting a collection 01/18/23 23:34:29.477
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:29.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-476" for this suite. 01/18/23 23:34:29.49
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:29.495
Jan 18 23:34:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:34:29.496
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:29.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:29.511
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-dda9aec8-bdba-4262-8728-44521648d601 01/18/23 23:34:29.518
STEP: Creating the pod 01/18/23 23:34:29.522
Jan 18 23:34:29.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4" in namespace "configmap-838" to be "running"
Jan 18 23:34:29.533: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320141ms
Jan 18 23:34:31.536: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4": Phase="Running", Reason="", readiness=false. Elapsed: 2.005582481s
Jan 18 23:34:31.537: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4" satisfied condition "running"
STEP: Waiting for pod with text data 01/18/23 23:34:31.537
STEP: Waiting for pod with binary data 01/18/23 23:34:31.541
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:31.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-838" for this suite. 01/18/23 23:34:31.549
------------------------------
â€¢ [2.059 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:29.495
    Jan 18 23:34:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:34:29.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:29.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:29.511
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-dda9aec8-bdba-4262-8728-44521648d601 01/18/23 23:34:29.518
    STEP: Creating the pod 01/18/23 23:34:29.522
    Jan 18 23:34:29.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4" in namespace "configmap-838" to be "running"
    Jan 18 23:34:29.533: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320141ms
    Jan 18 23:34:31.536: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4": Phase="Running", Reason="", readiness=false. Elapsed: 2.005582481s
    Jan 18 23:34:31.537: INFO: Pod "pod-configmaps-86e3effa-1fc7-4f0b-87be-ece94533f5b4" satisfied condition "running"
    STEP: Waiting for pod with text data 01/18/23 23:34:31.537
    STEP: Waiting for pod with binary data 01/18/23 23:34:31.541
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:31.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-838" for this suite. 01/18/23 23:34:31.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:31.554
Jan 18 23:34:31.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption 01/18/23 23:34:31.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:31.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:31.59
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:31.593
Jan 18 23:34:31.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption-2 01/18/23 23:34:31.594
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:31.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:31.607
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/18/23 23:34:31.616
STEP: Waiting for the pdb to be processed 01/18/23 23:34:33.627
STEP: Waiting for the pdb to be processed 01/18/23 23:34:35.639
STEP: listing a collection of PDBs across all namespaces 01/18/23 23:34:37.645
STEP: listing a collection of PDBs in namespace disruption-9550 01/18/23 23:34:37.648
STEP: deleting a collection of PDBs 01/18/23 23:34:37.65
STEP: Waiting for the PDB collection to be deleted 01/18/23 23:34:37.658
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:37.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-8576" for this suite. 01/18/23 23:34:37.666
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9550" for this suite. 01/18/23 23:34:37.671
------------------------------
â€¢ [SLOW TEST] [6.121 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:31.554
    Jan 18 23:34:31.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption 01/18/23 23:34:31.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:31.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:31.59
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:31.593
    Jan 18 23:34:31.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption-2 01/18/23 23:34:31.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:31.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:31.607
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/18/23 23:34:31.616
    STEP: Waiting for the pdb to be processed 01/18/23 23:34:33.627
    STEP: Waiting for the pdb to be processed 01/18/23 23:34:35.639
    STEP: listing a collection of PDBs across all namespaces 01/18/23 23:34:37.645
    STEP: listing a collection of PDBs in namespace disruption-9550 01/18/23 23:34:37.648
    STEP: deleting a collection of PDBs 01/18/23 23:34:37.65
    STEP: Waiting for the PDB collection to be deleted 01/18/23 23:34:37.658
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:37.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-8576" for this suite. 01/18/23 23:34:37.666
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9550" for this suite. 01/18/23 23:34:37.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:37.677
Jan 18 23:34:37.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 23:34:37.678
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:37.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:37.69
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 18 23:34:37.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3457" for this suite. 01/18/23 23:34:40.822
------------------------------
â€¢ [3.150 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:37.677
    Jan 18 23:34:37.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 23:34:37.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:37.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:37.69
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 18 23:34:37.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3457" for this suite. 01/18/23 23:34:40.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:40.829
Jan 18 23:34:40.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:34:40.83
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:40.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:40.844
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-afe4f052-cd24-402a-acd5-e159b9c145b5 01/18/23 23:34:40.847
STEP: Creating a pod to test consume secrets 01/18/23 23:34:40.852
Jan 18 23:34:40.859: INFO: Waiting up to 5m0s for pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b" in namespace "secrets-9485" to be "Succeeded or Failed"
Jan 18 23:34:40.862: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918475ms
Jan 18 23:34:42.865: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006460131s
Jan 18 23:34:44.866: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006835397s
STEP: Saw pod success 01/18/23 23:34:44.866
Jan 18 23:34:44.866: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b" satisfied condition "Succeeded or Failed"
Jan 18 23:34:44.869: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b container secret-env-test: <nil>
STEP: delete the pod 01/18/23 23:34:44.874
Jan 18 23:34:44.881: INFO: Waiting for pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b to disappear
Jan 18 23:34:44.883: INFO: Pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:34:44.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9485" for this suite. 01/18/23 23:34:44.886
------------------------------
â€¢ [4.065 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:40.829
    Jan 18 23:34:40.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:34:40.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:40.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:40.844
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-afe4f052-cd24-402a-acd5-e159b9c145b5 01/18/23 23:34:40.847
    STEP: Creating a pod to test consume secrets 01/18/23 23:34:40.852
    Jan 18 23:34:40.859: INFO: Waiting up to 5m0s for pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b" in namespace "secrets-9485" to be "Succeeded or Failed"
    Jan 18 23:34:40.862: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918475ms
    Jan 18 23:34:42.865: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006460131s
    Jan 18 23:34:44.866: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006835397s
    STEP: Saw pod success 01/18/23 23:34:44.866
    Jan 18 23:34:44.866: INFO: Pod "pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b" satisfied condition "Succeeded or Failed"
    Jan 18 23:34:44.869: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b container secret-env-test: <nil>
    STEP: delete the pod 01/18/23 23:34:44.874
    Jan 18 23:34:44.881: INFO: Waiting for pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b to disappear
    Jan 18 23:34:44.883: INFO: Pod pod-secrets-3b32d146-0b22-413c-8df0-a7d108cf503b no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:34:44.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9485" for this suite. 01/18/23 23:34:44.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:34:44.894
Jan 18 23:34:44.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename statefulset 01/18/23 23:34:44.895
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:44.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:44.909
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6518 01/18/23 23:34:44.912
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 23:34:44.916
STEP: Creating stateful set ss in namespace statefulset-6518 01/18/23 23:34:44.918
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6518 01/18/23 23:34:44.923
Jan 18 23:34:44.925: INFO: Found 0 stateful pods, waiting for 1
Jan 18 23:34:54.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 23:34:54.929
Jan 18 23:34:54.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:34:55.077: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:34:55.077: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:34:55.077: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:34:55.080: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 23:35:05.088: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:35:05.088: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:35:05.102: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999804s
Jan 18 23:35:06.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99724206s
Jan 18 23:35:07.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993380027s
Jan 18 23:35:08.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989941466s
Jan 18 23:35:09.116: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986555132s
Jan 18 23:35:10.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9830376s
Jan 18 23:35:11.123: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979773654s
Jan 18 23:35:12.126: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976350581s
Jan 18 23:35:13.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.973033861s
Jan 18 23:35:14.133: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.695137ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6518 01/18/23 23:35:15.133
Jan 18 23:35:15.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:35:15.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:35:15.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:35:15.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:35:15.281: INFO: Found 1 stateful pods, waiting for 3
Jan 18 23:35:25.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:35:25.286: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:35:25.286: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/18/23 23:35:25.286
STEP: Scale down will halt with unhealthy stateful pod 01/18/23 23:35:25.286
Jan 18 23:35:25.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:35:25.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:35:25.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:35:25.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:35:25.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:35:25.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:35:25.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:35:25.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:35:25.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:35:25.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:35:25.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:35:25.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:35:25.692: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:35:25.694: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 18 23:35:35.702: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:35:35.702: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:35:35.702: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:35:35.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999678s
Jan 18 23:35:36.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996840526s
Jan 18 23:35:37.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992750871s
Jan 18 23:35:38.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989173638s
Jan 18 23:35:39.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985738279s
Jan 18 23:35:40.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982075727s
Jan 18 23:35:41.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978346196s
Jan 18 23:35:42.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974038226s
Jan 18 23:35:43.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969974178s
Jan 18 23:35:44.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.157601ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6518 01/18/23 23:35:45.748
Jan 18 23:35:45.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:35:45.887: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:35:45.887: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:35:45.887: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:35:45.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:35:46.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:35:46.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:35:46.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:35:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:35:46.162: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:35:46.162: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:35:46.162: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:35:46.162: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 23:35:56.178
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 23:35:56.178: INFO: Deleting all statefulset in ns statefulset-6518
Jan 18 23:35:56.181: INFO: Scaling statefulset ss to 0
Jan 18 23:35:56.191: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:35:56.193: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:35:56.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6518" for this suite. 01/18/23 23:35:56.208
------------------------------
â€¢ [SLOW TEST] [71.319 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:34:44.894
    Jan 18 23:34:44.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename statefulset 01/18/23 23:34:44.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:34:44.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:34:44.909
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6518 01/18/23 23:34:44.912
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 23:34:44.916
    STEP: Creating stateful set ss in namespace statefulset-6518 01/18/23 23:34:44.918
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6518 01/18/23 23:34:44.923
    Jan 18 23:34:44.925: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 23:34:54.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 23:34:54.929
    Jan 18 23:34:54.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:34:55.077: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:34:55.077: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:34:55.077: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:34:55.080: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 23:35:05.088: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:35:05.088: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:35:05.102: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999804s
    Jan 18 23:35:06.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99724206s
    Jan 18 23:35:07.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993380027s
    Jan 18 23:35:08.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989941466s
    Jan 18 23:35:09.116: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986555132s
    Jan 18 23:35:10.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9830376s
    Jan 18 23:35:11.123: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979773654s
    Jan 18 23:35:12.126: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976350581s
    Jan 18 23:35:13.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.973033861s
    Jan 18 23:35:14.133: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.695137ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6518 01/18/23 23:35:15.133
    Jan 18 23:35:15.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:35:15.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:35:15.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:35:15.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:35:15.281: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 23:35:25.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:35:25.286: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:35:25.286: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/18/23 23:35:25.286
    STEP: Scale down will halt with unhealthy stateful pod 01/18/23 23:35:25.286
    Jan 18 23:35:25.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:35:25.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:35:25.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:35:25.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:35:25.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:35:25.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:35:25.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:35:25.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:35:25.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:35:25.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:35:25.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:35:25.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:35:25.692: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:35:25.694: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 18 23:35:35.702: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:35:35.702: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:35:35.702: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:35:35.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999678s
    Jan 18 23:35:36.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996840526s
    Jan 18 23:35:37.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992750871s
    Jan 18 23:35:38.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989173638s
    Jan 18 23:35:39.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985738279s
    Jan 18 23:35:40.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982075727s
    Jan 18 23:35:41.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978346196s
    Jan 18 23:35:42.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974038226s
    Jan 18 23:35:43.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969974178s
    Jan 18 23:35:44.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.157601ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6518 01/18/23 23:35:45.748
    Jan 18 23:35:45.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:35:45.887: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:35:45.887: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:35:45.887: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:35:45.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:35:46.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:35:46.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:35:46.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:35:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=statefulset-6518 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:35:46.162: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:35:46.162: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:35:46.162: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:35:46.162: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 23:35:56.178
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 23:35:56.178: INFO: Deleting all statefulset in ns statefulset-6518
    Jan 18 23:35:56.181: INFO: Scaling statefulset ss to 0
    Jan 18 23:35:56.191: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:35:56.193: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:35:56.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6518" for this suite. 01/18/23 23:35:56.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:35:56.214
Jan 18 23:35:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename subpath 01/18/23 23:35:56.215
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:35:56.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:35:56.231
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 23:35:56.234
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-4d8s 01/18/23 23:35:56.242
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:35:56.242
Jan 18 23:35:56.250: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4d8s" in namespace "subpath-640" to be "Succeeded or Failed"
Jan 18 23:35:56.253: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.727727ms
Jan 18 23:35:58.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006287412s
Jan 18 23:36:00.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 4.007130342s
Jan 18 23:36:02.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 6.007032476s
Jan 18 23:36:04.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 8.006432589s
Jan 18 23:36:06.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 10.006242127s
Jan 18 23:36:08.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 12.006776955s
Jan 18 23:36:10.259: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 14.008217269s
Jan 18 23:36:12.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 16.007661521s
Jan 18 23:36:14.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 18.006219952s
Jan 18 23:36:16.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 20.008060912s
Jan 18 23:36:18.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=false. Elapsed: 22.006513119s
Jan 18 23:36:20.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00695382s
STEP: Saw pod success 01/18/23 23:36:20.257
Jan 18 23:36:20.257: INFO: Pod "pod-subpath-test-secret-4d8s" satisfied condition "Succeeded or Failed"
Jan 18 23:36:20.260: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-secret-4d8s container test-container-subpath-secret-4d8s: <nil>
STEP: delete the pod 01/18/23 23:36:20.275
Jan 18 23:36:20.287: INFO: Waiting for pod pod-subpath-test-secret-4d8s to disappear
Jan 18 23:36:20.289: INFO: Pod pod-subpath-test-secret-4d8s no longer exists
STEP: Deleting pod pod-subpath-test-secret-4d8s 01/18/23 23:36:20.289
Jan 18 23:36:20.289: INFO: Deleting pod "pod-subpath-test-secret-4d8s" in namespace "subpath-640"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-640" for this suite. 01/18/23 23:36:20.294
------------------------------
â€¢ [SLOW TEST] [24.086 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:35:56.214
    Jan 18 23:35:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename subpath 01/18/23 23:35:56.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:35:56.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:35:56.231
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 23:35:56.234
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-4d8s 01/18/23 23:35:56.242
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:35:56.242
    Jan 18 23:35:56.250: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4d8s" in namespace "subpath-640" to be "Succeeded or Failed"
    Jan 18 23:35:56.253: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.727727ms
    Jan 18 23:35:58.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006287412s
    Jan 18 23:36:00.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 4.007130342s
    Jan 18 23:36:02.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 6.007032476s
    Jan 18 23:36:04.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 8.006432589s
    Jan 18 23:36:06.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 10.006242127s
    Jan 18 23:36:08.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 12.006776955s
    Jan 18 23:36:10.259: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 14.008217269s
    Jan 18 23:36:12.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 16.007661521s
    Jan 18 23:36:14.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 18.006219952s
    Jan 18 23:36:16.258: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=true. Elapsed: 20.008060912s
    Jan 18 23:36:18.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Running", Reason="", readiness=false. Elapsed: 22.006513119s
    Jan 18 23:36:20.257: INFO: Pod "pod-subpath-test-secret-4d8s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00695382s
    STEP: Saw pod success 01/18/23 23:36:20.257
    Jan 18 23:36:20.257: INFO: Pod "pod-subpath-test-secret-4d8s" satisfied condition "Succeeded or Failed"
    Jan 18 23:36:20.260: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-subpath-test-secret-4d8s container test-container-subpath-secret-4d8s: <nil>
    STEP: delete the pod 01/18/23 23:36:20.275
    Jan 18 23:36:20.287: INFO: Waiting for pod pod-subpath-test-secret-4d8s to disappear
    Jan 18 23:36:20.289: INFO: Pod pod-subpath-test-secret-4d8s no longer exists
    STEP: Deleting pod pod-subpath-test-secret-4d8s 01/18/23 23:36:20.289
    Jan 18 23:36:20.289: INFO: Deleting pod "pod-subpath-test-secret-4d8s" in namespace "subpath-640"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-640" for this suite. 01/18/23 23:36:20.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:20.3
Jan 18 23:36:20.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:36:20.301
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:20.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:20.318
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:24.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9185" for this suite. 01/18/23 23:36:24.338
------------------------------
â€¢ [4.042 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:20.3
    Jan 18 23:36:20.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:36:20.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:20.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:20.318
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:24.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9185" for this suite. 01/18/23 23:36:24.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:24.343
Jan 18 23:36:24.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-pred 01/18/23 23:36:24.344
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:24.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:24.36
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 23:36:24.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 23:36:24.367: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 23:36:24.370: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
Jan 18 23:36:24.374: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:36:24.374: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container coredns ready: true, restart count 0
Jan 18 23:36:24.374: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container etcd ready: true, restart count 0
Jan 18 23:36:24.374: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:36:24.374: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:36:24.374: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:36:24.374: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:36:24.374: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container weave ready: true, restart count 1
Jan 18 23:36:24.374: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:36:24.374: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:36:24.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:36:24.374: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:36:24.374: INFO: 
Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
Jan 18 23:36:24.378: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 23:36:24.378: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container weave ready: true, restart count 0
Jan 18 23:36:24.378: INFO: 	Container weave-npc ready: true, restart count 0
Jan 18 23:36:24.378: INFO: bin-false44e99fab-08be-4637-b251-7a95a1306764 from kubelet-test-9185 started at 2023-01-18 23:36:20 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container bin-false44e99fab-08be-4637-b251-7a95a1306764 ready: false, restart count 0
Jan 18 23:36:24.378: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:36:24.378: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:36:24.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:36:24.378: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:36:24.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:36:24.378: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 23:36:24.378
Jan 18 23:36:24.385: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2393" to be "running"
Jan 18 23:36:24.388: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341242ms
Jan 18 23:36:26.392: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007069576s
Jan 18 23:36:26.392: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 23:36:26.395
STEP: Trying to apply a random label on the found node. 01/18/23 23:36:26.402
STEP: verifying the node has the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce 42 01/18/23 23:36:26.413
STEP: Trying to relaunch the pod, now with labels. 01/18/23 23:36:26.416
Jan 18 23:36:26.421: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2393" to be "not pending"
Jan 18 23:36:26.423: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398314ms
Jan 18 23:36:28.427: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005993875s
Jan 18 23:36:28.427: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce off the node cncf-conformance-1-26-2 01/18/23 23:36:28.429
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce 01/18/23 23:36:28.441
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2393" for this suite. 01/18/23 23:36:28.446
------------------------------
â€¢ [4.109 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:24.343
    Jan 18 23:36:24.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 23:36:24.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:24.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:24.36
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 23:36:24.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 23:36:24.367: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 23:36:24.370: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-1 before test
    Jan 18 23:36:24.374: INFO: coredns-787d4945fb-4gpmq from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: coredns-787d4945fb-4xgtd from kube-system started at 2023-01-18 22:02:06 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: etcd-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:49 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container etcd ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: kube-apiserver-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: kube-controller-manager-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: kube-proxy-79fqc from kube-system started at 2023-01-18 22:01:52 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: kube-scheduler-cncf-conformance-1-26-1 from kube-system started at 2023-01-18 22:01:48 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: weave-net-wzqsl from kube-system started at 2023-01-18 22:02:05 +0000 UTC (2 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container weave ready: true, restart count 1
    Jan 18 23:36:24.374: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-v9q7b from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:36:24.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 23:36:24.374: INFO: 
    Logging pods the apiserver thinks is on node cncf-conformance-1-26-2 before test
    Jan 18 23:36:24.378: INFO: kube-proxy-qt8bw from kube-system started at 2023-01-18 22:05:36 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: weave-net-c2v49 from kube-system started at 2023-01-18 22:05:36 +0000 UTC (2 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container weave ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: 	Container weave-npc ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: bin-false44e99fab-08be-4637-b251-7a95a1306764 from kubelet-test-9185 started at 2023-01-18 23:36:20 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container bin-false44e99fab-08be-4637-b251-7a95a1306764 ready: false, restart count 0
    Jan 18 23:36:24.378: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:09:22 +0000 UTC (1 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: sonobuoy-e2e-job-5d5752962aaf4a7e from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: sonobuoy-systemd-logs-daemon-set-9196ceed1d93404b-plthq from sonobuoy started at 2023-01-18 22:09:25 +0000 UTC (2 container statuses recorded)
    Jan 18 23:36:24.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 23:36:24.378: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 23:36:24.378
    Jan 18 23:36:24.385: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2393" to be "running"
    Jan 18 23:36:24.388: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341242ms
    Jan 18 23:36:26.392: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007069576s
    Jan 18 23:36:26.392: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 23:36:26.395
    STEP: Trying to apply a random label on the found node. 01/18/23 23:36:26.402
    STEP: verifying the node has the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce 42 01/18/23 23:36:26.413
    STEP: Trying to relaunch the pod, now with labels. 01/18/23 23:36:26.416
    Jan 18 23:36:26.421: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2393" to be "not pending"
    Jan 18 23:36:26.423: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398314ms
    Jan 18 23:36:28.427: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005993875s
    Jan 18 23:36:28.427: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce off the node cncf-conformance-1-26-2 01/18/23 23:36:28.429
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d7cafb20-62dd-4b1b-862a-7cfc1b7f63ce 01/18/23 23:36:28.441
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2393" for this suite. 01/18/23 23:36:28.446
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:28.453
Jan 18 23:36:28.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename init-container 01/18/23 23:36:28.454
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:28.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:28.468
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/18/23 23:36:28.471
Jan 18 23:36:28.471: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:32.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9515" for this suite. 01/18/23 23:36:32.654
------------------------------
â€¢ [4.208 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:28.453
    Jan 18 23:36:28.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename init-container 01/18/23 23:36:28.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:28.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:28.468
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/18/23 23:36:28.471
    Jan 18 23:36:28.471: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:32.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9515" for this suite. 01/18/23 23:36:32.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:32.663
Jan 18 23:36:32.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:36:32.664
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:32.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:32.678
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/18/23 23:36:32.681
Jan 18 23:36:32.690: INFO: Waiting up to 5m0s for pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3" in namespace "downward-api-855" to be "Succeeded or Failed"
Jan 18 23:36:32.692: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240905ms
Jan 18 23:36:34.695: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005233738s
Jan 18 23:36:36.697: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007424719s
STEP: Saw pod success 01/18/23 23:36:36.698
Jan 18 23:36:36.698: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3" satisfied condition "Succeeded or Failed"
Jan 18 23:36:36.700: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:36:36.706
Jan 18 23:36:36.716: INFO: Waiting for pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 to disappear
Jan 18 23:36:36.718: INFO: Pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:36.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-855" for this suite. 01/18/23 23:36:36.721
------------------------------
â€¢ [4.063 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:32.663
    Jan 18 23:36:32.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:36:32.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:32.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:32.678
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/18/23 23:36:32.681
    Jan 18 23:36:32.690: INFO: Waiting up to 5m0s for pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3" in namespace "downward-api-855" to be "Succeeded or Failed"
    Jan 18 23:36:32.692: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240905ms
    Jan 18 23:36:34.695: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005233738s
    Jan 18 23:36:36.697: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007424719s
    STEP: Saw pod success 01/18/23 23:36:36.698
    Jan 18 23:36:36.698: INFO: Pod "downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3" satisfied condition "Succeeded or Failed"
    Jan 18 23:36:36.700: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:36:36.706
    Jan 18 23:36:36.716: INFO: Waiting for pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 to disappear
    Jan 18 23:36:36.718: INFO: Pod downward-api-a16d246c-d940-4277-a9bd-bde73a63b6a3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:36.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-855" for this suite. 01/18/23 23:36:36.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:36.727
Jan 18 23:36:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename configmap 01/18/23 23:36:36.728
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:36.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:36.742
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1860/configmap-test-83bba700-183f-437f-a2be-d027c4ff213a 01/18/23 23:36:36.745
STEP: Creating a pod to test consume configMaps 01/18/23 23:36:36.749
Jan 18 23:36:36.756: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58" in namespace "configmap-1860" to be "Succeeded or Failed"
Jan 18 23:36:36.759: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437571ms
Jan 18 23:36:38.762: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005704437s
Jan 18 23:36:40.763: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00705346s
STEP: Saw pod success 01/18/23 23:36:40.763
Jan 18 23:36:40.764: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58" satisfied condition "Succeeded or Failed"
Jan 18 23:36:40.766: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 container env-test: <nil>
STEP: delete the pod 01/18/23 23:36:40.772
Jan 18 23:36:40.785: INFO: Waiting for pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 to disappear
Jan 18 23:36:40.787: INFO: Pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:36:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1860" for this suite. 01/18/23 23:36:40.79
------------------------------
â€¢ [4.068 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:36.727
    Jan 18 23:36:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename configmap 01/18/23 23:36:36.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:36.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:36.742
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1860/configmap-test-83bba700-183f-437f-a2be-d027c4ff213a 01/18/23 23:36:36.745
    STEP: Creating a pod to test consume configMaps 01/18/23 23:36:36.749
    Jan 18 23:36:36.756: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58" in namespace "configmap-1860" to be "Succeeded or Failed"
    Jan 18 23:36:36.759: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437571ms
    Jan 18 23:36:38.762: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005704437s
    Jan 18 23:36:40.763: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00705346s
    STEP: Saw pod success 01/18/23 23:36:40.763
    Jan 18 23:36:40.764: INFO: Pod "pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58" satisfied condition "Succeeded or Failed"
    Jan 18 23:36:40.766: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 container env-test: <nil>
    STEP: delete the pod 01/18/23 23:36:40.772
    Jan 18 23:36:40.785: INFO: Waiting for pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 to disappear
    Jan 18 23:36:40.787: INFO: Pod pod-configmaps-0f7fb9d2-f942-42d6-a095-60cc7612da58 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:36:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1860" for this suite. 01/18/23 23:36:40.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:36:40.796
Jan 18 23:36:40.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 23:36:40.797
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:40.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:40.815
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 23:36:40.831: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 23:37:40.850: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:37:40.852
Jan 18 23:37:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 23:37:40.853
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:40.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:40.866
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan 18 23:37:40.883: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 18 23:37:40.885: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 18 23:37:40.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:37:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6225" for this suite. 01/18/23 23:37:40.947
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-567" for this suite. 01/18/23 23:37:40.952
------------------------------
â€¢ [SLOW TEST] [60.162 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:36:40.796
    Jan 18 23:36:40.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 23:36:40.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:36:40.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:36:40.815
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 23:36:40.831: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 23:37:40.850: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:37:40.852
    Jan 18 23:37:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 23:37:40.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:40.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:40.866
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan 18 23:37:40.883: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 18 23:37:40.885: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:37:40.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:37:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6225" for this suite. 01/18/23 23:37:40.947
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-567" for this suite. 01/18/23 23:37:40.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:37:40.961
Jan 18 23:37:40.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 23:37:40.962
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:40.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:40.975
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/18/23 23:37:40.978
Jan 18 23:37:40.986: INFO: created test-pod-1
Jan 18 23:37:40.991: INFO: created test-pod-2
Jan 18 23:37:40.998: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/18/23 23:37:40.998
Jan 18 23:37:40.998: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5770' to be running and ready
Jan 18 23:37:41.009: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 23:37:41.009: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 23:37:41.009: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 23:37:41.009: INFO: 0 / 3 pods in namespace 'pods-5770' are running and ready (0 seconds elapsed)
Jan 18 23:37:41.009: INFO: expected 0 pod replicas in namespace 'pods-5770', 0 are Running and Ready.
Jan 18 23:37:41.009: INFO: POD         NODE                     PHASE    GRACE  CONDITIONS
Jan 18 23:37:41.009: INFO: test-pod-1  cncf-conformance-1-26-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  }]
Jan 18 23:37:41.009: INFO: test-pod-2  cncf-conformance-1-26-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  }]
Jan 18 23:37:41.009: INFO: test-pod-3  cncf-conformance-1-26-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:41 +0000 UTC  }]
Jan 18 23:37:41.009: INFO: 
Jan 18 23:37:43.017: INFO: 3 / 3 pods in namespace 'pods-5770' are running and ready (2 seconds elapsed)
Jan 18 23:37:43.017: INFO: expected 0 pod replicas in namespace 'pods-5770', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/18/23 23:37:43.033
Jan 18 23:37:43.035: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 23:37:44.039: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 23:37:45.040: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 23:37:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5770" for this suite. 01/18/23 23:37:46.043
------------------------------
â€¢ [SLOW TEST] [5.087 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:37:40.961
    Jan 18 23:37:40.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 23:37:40.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:40.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:40.975
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/18/23 23:37:40.978
    Jan 18 23:37:40.986: INFO: created test-pod-1
    Jan 18 23:37:40.991: INFO: created test-pod-2
    Jan 18 23:37:40.998: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/18/23 23:37:40.998
    Jan 18 23:37:40.998: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5770' to be running and ready
    Jan 18 23:37:41.009: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 23:37:41.009: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 23:37:41.009: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 23:37:41.009: INFO: 0 / 3 pods in namespace 'pods-5770' are running and ready (0 seconds elapsed)
    Jan 18 23:37:41.009: INFO: expected 0 pod replicas in namespace 'pods-5770', 0 are Running and Ready.
    Jan 18 23:37:41.009: INFO: POD         NODE                     PHASE    GRACE  CONDITIONS
    Jan 18 23:37:41.009: INFO: test-pod-1  cncf-conformance-1-26-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  }]
    Jan 18 23:37:41.009: INFO: test-pod-2  cncf-conformance-1-26-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:40 +0000 UTC  }]
    Jan 18 23:37:41.009: INFO: test-pod-3  cncf-conformance-1-26-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 23:37:41 +0000 UTC  }]
    Jan 18 23:37:41.009: INFO: 
    Jan 18 23:37:43.017: INFO: 3 / 3 pods in namespace 'pods-5770' are running and ready (2 seconds elapsed)
    Jan 18 23:37:43.017: INFO: expected 0 pod replicas in namespace 'pods-5770', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/18/23 23:37:43.033
    Jan 18 23:37:43.035: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 23:37:44.039: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 23:37:45.040: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:37:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5770" for this suite. 01/18/23 23:37:46.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:37:46.052
Jan 18 23:37:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename replicaset 01/18/23 23:37:46.053
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:46.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:46.065
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/18/23 23:37:46.07
STEP: Verify that the required pods have come up. 01/18/23 23:37:46.075
Jan 18 23:37:46.078: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 23:37:51.081: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 23:37:51.082
STEP: Getting /status 01/18/23 23:37:51.082
Jan 18 23:37:51.085: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/18/23 23:37:51.085
Jan 18 23:37:51.091: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/18/23 23:37:51.091
Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: ADDED
Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.094: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.094: INFO: Found replicaset test-rs in namespace replicaset-3720 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 23:37:51.094: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/18/23 23:37:51.094
Jan 18 23:37:51.094: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 23:37:51.101: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/18/23 23:37:51.101
Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: ADDED
Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.103: INFO: Observed replicaset test-rs in namespace replicaset-3720 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:37:51.104: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 23:37:51.104: INFO: Found replicaset test-rs in namespace replicaset-3720 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 23:37:51.104: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 23:37:51.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3720" for this suite. 01/18/23 23:37:51.106
------------------------------
â€¢ [SLOW TEST] [5.059 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:37:46.052
    Jan 18 23:37:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename replicaset 01/18/23 23:37:46.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:46.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:46.065
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/18/23 23:37:46.07
    STEP: Verify that the required pods have come up. 01/18/23 23:37:46.075
    Jan 18 23:37:46.078: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 23:37:51.081: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 23:37:51.082
    STEP: Getting /status 01/18/23 23:37:51.082
    Jan 18 23:37:51.085: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/18/23 23:37:51.085
    Jan 18 23:37:51.091: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/18/23 23:37:51.091
    Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.093: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.094: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.094: INFO: Found replicaset test-rs in namespace replicaset-3720 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 23:37:51.094: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/18/23 23:37:51.094
    Jan 18 23:37:51.094: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 23:37:51.101: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/18/23 23:37:51.101
    Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.103: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.103: INFO: Observed replicaset test-rs in namespace replicaset-3720 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 23:37:51.104: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 23:37:51.104: INFO: Found replicaset test-rs in namespace replicaset-3720 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 23:37:51.104: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:37:51.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3720" for this suite. 01/18/23 23:37:51.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:37:51.111
Jan 18 23:37:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:37:51.112
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:51.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:51.126
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/18/23 23:37:51.129
Jan 18 23:37:51.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 create -f -'
Jan 18 23:37:51.854: INFO: stderr: ""
Jan 18 23:37:51.854: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:51.854
Jan 18 23:37:51.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:37:51.927: INFO: stderr: ""
Jan 18 23:37:51.927: INFO: stdout: "update-demo-nautilus-4stqm update-demo-nautilus-5nv6c "
Jan 18 23:37:51.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:37:51.994: INFO: stderr: ""
Jan 18 23:37:51.994: INFO: stdout: ""
Jan 18 23:37:51.994: INFO: update-demo-nautilus-4stqm is created but not running
Jan 18 23:37:56.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:37:57.066: INFO: stderr: ""
Jan 18 23:37:57.066: INFO: stdout: "update-demo-nautilus-4stqm update-demo-nautilus-5nv6c "
Jan 18 23:37:57.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:37:57.140: INFO: stderr: ""
Jan 18 23:37:57.140: INFO: stdout: "true"
Jan 18 23:37:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:37:57.213: INFO: stderr: ""
Jan 18 23:37:57.213: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 23:37:57.213: INFO: validating pod update-demo-nautilus-4stqm
Jan 18 23:37:57.218: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:37:57.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:37:57.218: INFO: update-demo-nautilus-4stqm is verified up and running
Jan 18 23:37:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:37:57.285: INFO: stderr: ""
Jan 18 23:37:57.285: INFO: stdout: "true"
Jan 18 23:37:57.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:37:57.358: INFO: stderr: ""
Jan 18 23:37:57.358: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 23:37:57.358: INFO: validating pod update-demo-nautilus-5nv6c
Jan 18 23:37:57.362: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:37:57.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:37:57.362: INFO: update-demo-nautilus-5nv6c is verified up and running
STEP: scaling down the replication controller 01/18/23 23:37:57.362
Jan 18 23:37:57.364: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:37:57.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 23:37:58.450: INFO: stderr: ""
Jan 18 23:37:58.450: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:58.45
Jan 18 23:37:58.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:37:58.521: INFO: stderr: ""
Jan 18 23:37:58.522: INFO: stdout: "update-demo-nautilus-5nv6c "
Jan 18 23:37:58.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:37:58.589: INFO: stderr: ""
Jan 18 23:37:58.589: INFO: stdout: "true"
Jan 18 23:37:58.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:37:58.659: INFO: stderr: ""
Jan 18 23:37:58.659: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 23:37:58.659: INFO: validating pod update-demo-nautilus-5nv6c
Jan 18 23:37:58.662: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:37:58.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:37:58.662: INFO: update-demo-nautilus-5nv6c is verified up and running
STEP: scaling up the replication controller 01/18/23 23:37:58.662
Jan 18 23:37:58.664: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:37:58.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 23:37:59.750: INFO: stderr: ""
Jan 18 23:37:59.750: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:59.75
Jan 18 23:37:59.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:37:59.819: INFO: stderr: ""
Jan 18 23:37:59.819: INFO: stdout: "update-demo-nautilus-5nv6c update-demo-nautilus-fbrb8 "
Jan 18 23:37:59.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:37:59.889: INFO: stderr: ""
Jan 18 23:37:59.889: INFO: stdout: "true"
Jan 18 23:37:59.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:37:59.957: INFO: stderr: ""
Jan 18 23:37:59.957: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 23:37:59.957: INFO: validating pod update-demo-nautilus-5nv6c
Jan 18 23:37:59.960: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:37:59.960: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:37:59.960: INFO: update-demo-nautilus-5nv6c is verified up and running
Jan 18 23:37:59.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-fbrb8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:38:00.028: INFO: stderr: ""
Jan 18 23:38:00.028: INFO: stdout: "true"
Jan 18 23:38:00.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-fbrb8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:38:00.094: INFO: stderr: ""
Jan 18 23:38:00.094: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 23:38:00.094: INFO: validating pod update-demo-nautilus-fbrb8
Jan 18 23:38:00.099: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:38:00.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:38:00.099: INFO: update-demo-nautilus-fbrb8 is verified up and running
STEP: using delete to clean up resources 01/18/23 23:38:00.099
Jan 18 23:38:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 delete --grace-period=0 --force -f -'
Jan 18 23:38:00.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:38:00.170: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 23:38:00.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get rc,svc -l name=update-demo --no-headers'
Jan 18 23:38:00.242: INFO: stderr: "No resources found in kubectl-6294 namespace.\n"
Jan 18 23:38:00.242: INFO: stdout: ""
Jan 18 23:38:00.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:38:00.312: INFO: stderr: ""
Jan 18 23:38:00.312: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:38:00.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6294" for this suite. 01/18/23 23:38:00.316
------------------------------
â€¢ [SLOW TEST] [9.210 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:37:51.111
    Jan 18 23:37:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:37:51.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:37:51.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:37:51.126
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/18/23 23:37:51.129
    Jan 18 23:37:51.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 create -f -'
    Jan 18 23:37:51.854: INFO: stderr: ""
    Jan 18 23:37:51.854: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:51.854
    Jan 18 23:37:51.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:37:51.927: INFO: stderr: ""
    Jan 18 23:37:51.927: INFO: stdout: "update-demo-nautilus-4stqm update-demo-nautilus-5nv6c "
    Jan 18 23:37:51.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:37:51.994: INFO: stderr: ""
    Jan 18 23:37:51.994: INFO: stdout: ""
    Jan 18 23:37:51.994: INFO: update-demo-nautilus-4stqm is created but not running
    Jan 18 23:37:56.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:37:57.066: INFO: stderr: ""
    Jan 18 23:37:57.066: INFO: stdout: "update-demo-nautilus-4stqm update-demo-nautilus-5nv6c "
    Jan 18 23:37:57.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:37:57.140: INFO: stderr: ""
    Jan 18 23:37:57.140: INFO: stdout: "true"
    Jan 18 23:37:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-4stqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:37:57.213: INFO: stderr: ""
    Jan 18 23:37:57.213: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 23:37:57.213: INFO: validating pod update-demo-nautilus-4stqm
    Jan 18 23:37:57.218: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:37:57.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:37:57.218: INFO: update-demo-nautilus-4stqm is verified up and running
    Jan 18 23:37:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:37:57.285: INFO: stderr: ""
    Jan 18 23:37:57.285: INFO: stdout: "true"
    Jan 18 23:37:57.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:37:57.358: INFO: stderr: ""
    Jan 18 23:37:57.358: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 23:37:57.358: INFO: validating pod update-demo-nautilus-5nv6c
    Jan 18 23:37:57.362: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:37:57.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:37:57.362: INFO: update-demo-nautilus-5nv6c is verified up and running
    STEP: scaling down the replication controller 01/18/23 23:37:57.362
    Jan 18 23:37:57.364: INFO: scanned /root for discovery docs: <nil>
    Jan 18 23:37:57.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 18 23:37:58.450: INFO: stderr: ""
    Jan 18 23:37:58.450: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:58.45
    Jan 18 23:37:58.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:37:58.521: INFO: stderr: ""
    Jan 18 23:37:58.522: INFO: stdout: "update-demo-nautilus-5nv6c "
    Jan 18 23:37:58.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:37:58.589: INFO: stderr: ""
    Jan 18 23:37:58.589: INFO: stdout: "true"
    Jan 18 23:37:58.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:37:58.659: INFO: stderr: ""
    Jan 18 23:37:58.659: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 23:37:58.659: INFO: validating pod update-demo-nautilus-5nv6c
    Jan 18 23:37:58.662: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:37:58.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:37:58.662: INFO: update-demo-nautilus-5nv6c is verified up and running
    STEP: scaling up the replication controller 01/18/23 23:37:58.662
    Jan 18 23:37:58.664: INFO: scanned /root for discovery docs: <nil>
    Jan 18 23:37:58.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 18 23:37:59.750: INFO: stderr: ""
    Jan 18 23:37:59.750: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:37:59.75
    Jan 18 23:37:59.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:37:59.819: INFO: stderr: ""
    Jan 18 23:37:59.819: INFO: stdout: "update-demo-nautilus-5nv6c update-demo-nautilus-fbrb8 "
    Jan 18 23:37:59.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:37:59.889: INFO: stderr: ""
    Jan 18 23:37:59.889: INFO: stdout: "true"
    Jan 18 23:37:59.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-5nv6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:37:59.957: INFO: stderr: ""
    Jan 18 23:37:59.957: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 23:37:59.957: INFO: validating pod update-demo-nautilus-5nv6c
    Jan 18 23:37:59.960: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:37:59.960: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:37:59.960: INFO: update-demo-nautilus-5nv6c is verified up and running
    Jan 18 23:37:59.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-fbrb8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:38:00.028: INFO: stderr: ""
    Jan 18 23:38:00.028: INFO: stdout: "true"
    Jan 18 23:38:00.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods update-demo-nautilus-fbrb8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:38:00.094: INFO: stderr: ""
    Jan 18 23:38:00.094: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 23:38:00.094: INFO: validating pod update-demo-nautilus-fbrb8
    Jan 18 23:38:00.099: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:38:00.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:38:00.099: INFO: update-demo-nautilus-fbrb8 is verified up and running
    STEP: using delete to clean up resources 01/18/23 23:38:00.099
    Jan 18 23:38:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 delete --grace-period=0 --force -f -'
    Jan 18 23:38:00.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:38:00.170: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 23:38:00.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get rc,svc -l name=update-demo --no-headers'
    Jan 18 23:38:00.242: INFO: stderr: "No resources found in kubectl-6294 namespace.\n"
    Jan 18 23:38:00.242: INFO: stdout: ""
    Jan 18 23:38:00.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-6294 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 23:38:00.312: INFO: stderr: ""
    Jan 18 23:38:00.312: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:38:00.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6294" for this suite. 01/18/23 23:38:00.316
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:38:00.322
Jan 18 23:38:00.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:38:00.323
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:00.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:00.339
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 18 23:38:00.352: INFO: created pod
Jan 18 23:38:00.352: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8971" to be "Succeeded or Failed"
Jan 18 23:38:00.355: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349776ms
Jan 18 23:38:02.359: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006678847s
Jan 18 23:38:04.360: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007239877s
STEP: Saw pod success 01/18/23 23:38:04.36
Jan 18 23:38:04.360: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 23:38:34.361: INFO: polling logs
Jan 18 23:38:34.374: INFO: Pod logs: 
I0118 23:38:01.078317       1 log.go:198] OK: Got token
I0118 23:38:01.078401       1 log.go:198] validating with in-cluster discovery
I0118 23:38:01.078911       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0118 23:38:01.078943       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8971:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674085680, NotBefore:1674085080, IssuedAt:1674085080, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8971", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"159ce1b7-bd8c-499a-80a3-4bb40f192cbc"}}}
I0118 23:38:01.106149       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0118 23:38:01.113630       1 log.go:198] OK: Validated signature on JWT
I0118 23:38:01.113753       1 log.go:198] OK: Got valid claims from token!
I0118 23:38:01.113792       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8971:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674085680, NotBefore:1674085080, IssuedAt:1674085080, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8971", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"159ce1b7-bd8c-499a-80a3-4bb40f192cbc"}}}

Jan 18 23:38:34.374: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 23:38:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8971" for this suite. 01/18/23 23:38:34.382
------------------------------
â€¢ [SLOW TEST] [34.065 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:38:00.322
    Jan 18 23:38:00.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:38:00.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:00.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:00.339
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 18 23:38:00.352: INFO: created pod
    Jan 18 23:38:00.352: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8971" to be "Succeeded or Failed"
    Jan 18 23:38:00.355: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349776ms
    Jan 18 23:38:02.359: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006678847s
    Jan 18 23:38:04.360: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007239877s
    STEP: Saw pod success 01/18/23 23:38:04.36
    Jan 18 23:38:04.360: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 18 23:38:34.361: INFO: polling logs
    Jan 18 23:38:34.374: INFO: Pod logs: 
    I0118 23:38:01.078317       1 log.go:198] OK: Got token
    I0118 23:38:01.078401       1 log.go:198] validating with in-cluster discovery
    I0118 23:38:01.078911       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0118 23:38:01.078943       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8971:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674085680, NotBefore:1674085080, IssuedAt:1674085080, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8971", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"159ce1b7-bd8c-499a-80a3-4bb40f192cbc"}}}
    I0118 23:38:01.106149       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0118 23:38:01.113630       1 log.go:198] OK: Validated signature on JWT
    I0118 23:38:01.113753       1 log.go:198] OK: Got valid claims from token!
    I0118 23:38:01.113792       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8971:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674085680, NotBefore:1674085080, IssuedAt:1674085080, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8971", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"159ce1b7-bd8c-499a-80a3-4bb40f192cbc"}}}

    Jan 18 23:38:34.374: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:38:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8971" for this suite. 01/18/23 23:38:34.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:38:34.388
Jan 18 23:38:34.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 23:38:34.389
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:34.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:34.405
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/18/23 23:38:34.415
Jan 18 23:38:34.421: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/18/23 23:38:36.425
Jan 18 23:38:36.432: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/18/23 23:38:38.436
Jan 18 23:38:38.443: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 18 23:38:40.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6548" for this suite. 01/18/23 23:38:40.449
------------------------------
â€¢ [SLOW TEST] [6.066 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:38:34.388
    Jan 18 23:38:34.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 23:38:34.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:34.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:34.405
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/18/23 23:38:34.415
    Jan 18 23:38:34.421: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/18/23 23:38:36.425
    Jan 18 23:38:36.432: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/18/23 23:38:38.436
    Jan 18 23:38:38.443: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:38:40.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6548" for this suite. 01/18/23 23:38:40.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:38:40.455
Jan 18 23:38:40.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:38:40.456
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:40.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:40.469
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/18/23 23:38:40.472
STEP: watching for the ServiceAccount to be added 01/18/23 23:38:40.478
STEP: patching the ServiceAccount 01/18/23 23:38:40.479
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 23:38:40.485
STEP: deleting the ServiceAccount 01/18/23 23:38:40.488
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 23:38:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5039" for this suite. 01/18/23 23:38:40.5
------------------------------
â€¢ [0.049 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:38:40.455
    Jan 18 23:38:40.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 23:38:40.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:40.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:40.469
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/18/23 23:38:40.472
    STEP: watching for the ServiceAccount to be added 01/18/23 23:38:40.478
    STEP: patching the ServiceAccount 01/18/23 23:38:40.479
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 23:38:40.485
    STEP: deleting the ServiceAccount 01/18/23 23:38:40.488
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:38:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5039" for this suite. 01/18/23 23:38:40.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:38:40.505
Jan 18 23:38:40.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename gc 01/18/23 23:38:40.506
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:40.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:40.521
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/18/23 23:38:40.525
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 23:38:40.53
STEP: delete the deployment 01/18/23 23:38:41.039
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 23:38:41.047
STEP: Gathering metrics 01/18/23 23:38:41.563
Jan 18 23:38:41.581: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
Jan 18 23:38:41.584: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.739342ms
Jan 18 23:38:41.584: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
Jan 18 23:38:41.584: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
Jan 18 23:38:41.646: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 23:38:41.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1165" for this suite. 01/18/23 23:38:41.649
------------------------------
â€¢ [1.149 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:38:40.505
    Jan 18 23:38:40.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename gc 01/18/23 23:38:40.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:40.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:40.521
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/18/23 23:38:40.525
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 23:38:40.53
    STEP: delete the deployment 01/18/23 23:38:41.039
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 23:38:41.047
    STEP: Gathering metrics 01/18/23 23:38:41.563
    Jan 18 23:38:41.581: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-conformance-1-26-1" in namespace "kube-system" to be "running and ready"
    Jan 18 23:38:41.584: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.739342ms
    Jan 18 23:38:41.584: INFO: The phase of Pod kube-controller-manager-cncf-conformance-1-26-1 is Running (Ready = true)
    Jan 18 23:38:41.584: INFO: Pod "kube-controller-manager-cncf-conformance-1-26-1" satisfied condition "running and ready"
    Jan 18 23:38:41.646: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:38:41.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1165" for this suite. 01/18/23 23:38:41.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:38:41.654
Jan 18 23:38:41.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:38:41.655
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:41.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:41.672
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/18/23 23:38:41.675
Jan 18 23:38:41.682: INFO: Waiting up to 2m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208" to be "running"
Jan 18 23:38:41.685: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821664ms
Jan 18 23:38:43.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006168694s
Jan 18 23:38:45.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00715032s
Jan 18 23:38:47.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006919697s
Jan 18 23:38:49.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008221915s
Jan 18 23:38:51.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006704358s
Jan 18 23:38:53.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007023287s
Jan 18 23:38:55.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007166949s
Jan 18 23:38:57.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00610371s
Jan 18 23:38:59.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007386138s
Jan 18 23:39:01.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007161045s
Jan 18 23:39:03.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007075087s
Jan 18 23:39:05.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006066497s
Jan 18 23:39:07.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007202881s
Jan 18 23:39:09.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007161956s
Jan 18 23:39:11.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006482485s
Jan 18 23:39:13.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006597514s
Jan 18 23:39:15.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007557769s
Jan 18 23:39:17.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00649089s
Jan 18 23:39:19.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007673129s
Jan 18 23:39:21.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00673631s
Jan 18 23:39:23.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 42.00690202s
Jan 18 23:39:25.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008144271s
Jan 18 23:39:27.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006872384s
Jan 18 23:39:29.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006581175s
Jan 18 23:39:31.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007369141s
Jan 18 23:39:33.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006690296s
Jan 18 23:39:35.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007704107s
Jan 18 23:39:37.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006963315s
Jan 18 23:39:39.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00814979s
Jan 18 23:39:41.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006183246s
Jan 18 23:39:43.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006388038s
Jan 18 23:39:45.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007693858s
Jan 18 23:39:47.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007390676s
Jan 18 23:39:49.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007513867s
Jan 18 23:39:51.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006396141s
Jan 18 23:39:53.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007156482s
Jan 18 23:39:55.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007066338s
Jan 18 23:39:57.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006779894s
Jan 18 23:39:59.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007710065s
Jan 18 23:40:01.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005957216s
Jan 18 23:40:03.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006146712s
Jan 18 23:40:05.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007468003s
Jan 18 23:40:07.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006241988s
Jan 18 23:40:09.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007558115s
Jan 18 23:40:11.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007269092s
Jan 18 23:40:13.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006643709s
Jan 18 23:40:15.691: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008562862s
Jan 18 23:40:17.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006330606s
Jan 18 23:40:19.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006102093s
Jan 18 23:40:21.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005990921s
Jan 18 23:40:23.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006193491s
Jan 18 23:40:25.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007266896s
Jan 18 23:40:27.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006786317s
Jan 18 23:40:29.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007492848s
Jan 18 23:40:31.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006526824s
Jan 18 23:40:33.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006514024s
Jan 18 23:40:35.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006942613s
Jan 18 23:40:37.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006819652s
Jan 18 23:40:39.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007901758s
Jan 18 23:40:41.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008280048s
Jan 18 23:40:41.693: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011167298s
STEP: updating the pod 01/18/23 23:40:41.693
Jan 18 23:40:42.205: INFO: Successfully updated pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242"
STEP: waiting for pod running 01/18/23 23:40:42.205
Jan 18 23:40:42.205: INFO: Waiting up to 2m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208" to be "running"
Jan 18 23:40:42.208: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412117ms
Jan 18 23:40:44.211: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Running", Reason="", readiness=true. Elapsed: 2.005963048s
Jan 18 23:40:44.211: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 23:40:44.211
Jan 18 23:40:44.212: INFO: Deleting pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208"
Jan 18 23:40:44.218: INFO: Wait up to 5m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:16.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9208" for this suite. 01/18/23 23:41:16.229
------------------------------
â€¢ [SLOW TEST] [154.580 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:38:41.654
    Jan 18 23:38:41.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:38:41.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:38:41.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:38:41.672
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/18/23 23:38:41.675
    Jan 18 23:38:41.682: INFO: Waiting up to 2m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208" to be "running"
    Jan 18 23:38:41.685: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821664ms
    Jan 18 23:38:43.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006168694s
    Jan 18 23:38:45.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00715032s
    Jan 18 23:38:47.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006919697s
    Jan 18 23:38:49.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008221915s
    Jan 18 23:38:51.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006704358s
    Jan 18 23:38:53.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007023287s
    Jan 18 23:38:55.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007166949s
    Jan 18 23:38:57.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00610371s
    Jan 18 23:38:59.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007386138s
    Jan 18 23:39:01.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007161045s
    Jan 18 23:39:03.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007075087s
    Jan 18 23:39:05.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006066497s
    Jan 18 23:39:07.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007202881s
    Jan 18 23:39:09.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007161956s
    Jan 18 23:39:11.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006482485s
    Jan 18 23:39:13.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006597514s
    Jan 18 23:39:15.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007557769s
    Jan 18 23:39:17.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00649089s
    Jan 18 23:39:19.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007673129s
    Jan 18 23:39:21.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00673631s
    Jan 18 23:39:23.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 42.00690202s
    Jan 18 23:39:25.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008144271s
    Jan 18 23:39:27.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006872384s
    Jan 18 23:39:29.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006581175s
    Jan 18 23:39:31.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007369141s
    Jan 18 23:39:33.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006690296s
    Jan 18 23:39:35.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007704107s
    Jan 18 23:39:37.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006963315s
    Jan 18 23:39:39.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00814979s
    Jan 18 23:39:41.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006183246s
    Jan 18 23:39:43.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006388038s
    Jan 18 23:39:45.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007693858s
    Jan 18 23:39:47.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007390676s
    Jan 18 23:39:49.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007513867s
    Jan 18 23:39:51.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006396141s
    Jan 18 23:39:53.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007156482s
    Jan 18 23:39:55.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007066338s
    Jan 18 23:39:57.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006779894s
    Jan 18 23:39:59.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007710065s
    Jan 18 23:40:01.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005957216s
    Jan 18 23:40:03.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006146712s
    Jan 18 23:40:05.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007468003s
    Jan 18 23:40:07.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006241988s
    Jan 18 23:40:09.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007558115s
    Jan 18 23:40:11.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007269092s
    Jan 18 23:40:13.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006643709s
    Jan 18 23:40:15.691: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008562862s
    Jan 18 23:40:17.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006330606s
    Jan 18 23:40:19.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006102093s
    Jan 18 23:40:21.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005990921s
    Jan 18 23:40:23.688: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006193491s
    Jan 18 23:40:25.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007266896s
    Jan 18 23:40:27.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006786317s
    Jan 18 23:40:29.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007492848s
    Jan 18 23:40:31.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006526824s
    Jan 18 23:40:33.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006514024s
    Jan 18 23:40:35.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006942613s
    Jan 18 23:40:37.689: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006819652s
    Jan 18 23:40:39.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007901758s
    Jan 18 23:40:41.690: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008280048s
    Jan 18 23:40:41.693: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011167298s
    STEP: updating the pod 01/18/23 23:40:41.693
    Jan 18 23:40:42.205: INFO: Successfully updated pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242"
    STEP: waiting for pod running 01/18/23 23:40:42.205
    Jan 18 23:40:42.205: INFO: Waiting up to 2m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208" to be "running"
    Jan 18 23:40:42.208: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412117ms
    Jan 18 23:40:44.211: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242": Phase="Running", Reason="", readiness=true. Elapsed: 2.005963048s
    Jan 18 23:40:44.211: INFO: Pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 23:40:44.211
    Jan 18 23:40:44.212: INFO: Deleting pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" in namespace "var-expansion-9208"
    Jan 18 23:40:44.218: INFO: Wait up to 5m0s for pod "var-expansion-ee2f150e-dca1-4bda-ad5b-f2e2ba2d6242" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:16.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9208" for this suite. 01/18/23 23:41:16.229
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:16.235
Jan 18 23:41:16.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename kubectl 01/18/23 23:41:16.236
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.262
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 18 23:41:16.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-2001 version'
Jan 18 23:41:16.330: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 23:41:16.330: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:16.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2001" for this suite. 01/18/23 23:41:16.334
------------------------------
â€¢ [0.104 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:16.235
    Jan 18 23:41:16.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:41:16.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.262
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 18 23:41:16.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=kubectl-2001 version'
    Jan 18 23:41:16.330: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 18 23:41:16.330: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:16.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2001" for this suite. 01/18/23 23:41:16.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:16.34
Jan 18 23:41:16.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 23:41:16.341
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.356
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/18/23 23:41:16.359
STEP: getting /apis/node.k8s.io 01/18/23 23:41:16.362
STEP: getting /apis/node.k8s.io/v1 01/18/23 23:41:16.363
STEP: creating 01/18/23 23:41:16.364
STEP: watching 01/18/23 23:41:16.376
Jan 18 23:41:16.376: INFO: starting watch
STEP: getting 01/18/23 23:41:16.383
STEP: listing 01/18/23 23:41:16.386
STEP: patching 01/18/23 23:41:16.388
STEP: updating 01/18/23 23:41:16.392
Jan 18 23:41:16.396: INFO: waiting for watch events with expected annotations
STEP: deleting 01/18/23 23:41:16.396
STEP: deleting a collection 01/18/23 23:41:16.406
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:16.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4907" for this suite. 01/18/23 23:41:16.419
------------------------------
â€¢ [0.084 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:16.34
    Jan 18 23:41:16.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 23:41:16.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.356
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/18/23 23:41:16.359
    STEP: getting /apis/node.k8s.io 01/18/23 23:41:16.362
    STEP: getting /apis/node.k8s.io/v1 01/18/23 23:41:16.363
    STEP: creating 01/18/23 23:41:16.364
    STEP: watching 01/18/23 23:41:16.376
    Jan 18 23:41:16.376: INFO: starting watch
    STEP: getting 01/18/23 23:41:16.383
    STEP: listing 01/18/23 23:41:16.386
    STEP: patching 01/18/23 23:41:16.388
    STEP: updating 01/18/23 23:41:16.392
    Jan 18 23:41:16.396: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/18/23 23:41:16.396
    STEP: deleting a collection 01/18/23 23:41:16.406
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:16.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4907" for this suite. 01/18/23 23:41:16.419
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:16.424
Jan 18 23:41:16.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename services 01/18/23 23:41:16.425
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.441
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-4091 01/18/23 23:41:16.444
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[] 01/18/23 23:41:16.451
Jan 18 23:41:16.454: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 18 23:41:17.460: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4091 01/18/23 23:41:17.46
Jan 18 23:41:17.468: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4091" to be "running and ready"
Jan 18 23:41:17.471: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533271ms
Jan 18 23:41:17.471: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:41:19.475: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006310889s
Jan 18 23:41:19.475: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 23:41:19.475: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod1:[80]] 01/18/23 23:41:19.477
Jan 18 23:41:19.485: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/18/23 23:41:19.485
Jan 18 23:41:19.485: INFO: Creating new exec pod
Jan 18 23:41:19.491: INFO: Waiting up to 5m0s for pod "execpod4q6fm" in namespace "services-4091" to be "running"
Jan 18 23:41:19.493: INFO: Pod "execpod4q6fm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336008ms
Jan 18 23:41:21.497: INFO: Pod "execpod4q6fm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005749063s
Jan 18 23:41:21.497: INFO: Pod "execpod4q6fm" satisfied condition "running"
Jan 18 23:41:22.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 23:41:22.631: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:22.631: INFO: stdout: ""
Jan 18 23:41:22.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
Jan 18 23:41:22.758: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:22.758: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-4091 01/18/23 23:41:22.758
Jan 18 23:41:22.763: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4091" to be "running and ready"
Jan 18 23:41:22.766: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07655ms
Jan 18 23:41:22.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:41:24.770: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006562741s
Jan 18 23:41:24.770: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 23:41:24.770: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 23:41:24.772
Jan 18 23:41:24.783: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 23:41:24.783
Jan 18 23:41:25.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 23:41:25.920: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:25.920: INFO: stdout: ""
Jan 18 23:41:25.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
Jan 18 23:41:26.055: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:26.055: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4091 01/18/23 23:41:26.055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod2:[80]] 01/18/23 23:41:26.065
Jan 18 23:41:26.075: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/18/23 23:41:26.075
Jan 18 23:41:27.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 23:41:27.211: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:27.211: INFO: stdout: ""
Jan 18 23:41:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
Jan 18 23:41:27.332: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
Jan 18 23:41:27.332: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-4091 01/18/23 23:41:27.332
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[] 01/18/23 23:41:27.346
Jan 18 23:41:27.353: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:27.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4091" for this suite. 01/18/23 23:41:27.37
------------------------------
â€¢ [SLOW TEST] [10.952 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:16.424
    Jan 18 23:41:16.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename services 01/18/23 23:41:16.425
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:16.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:16.441
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-4091 01/18/23 23:41:16.444
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[] 01/18/23 23:41:16.451
    Jan 18 23:41:16.454: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 18 23:41:17.460: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4091 01/18/23 23:41:17.46
    Jan 18 23:41:17.468: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4091" to be "running and ready"
    Jan 18 23:41:17.471: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533271ms
    Jan 18 23:41:17.471: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:41:19.475: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006310889s
    Jan 18 23:41:19.475: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 23:41:19.475: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod1:[80]] 01/18/23 23:41:19.477
    Jan 18 23:41:19.485: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/18/23 23:41:19.485
    Jan 18 23:41:19.485: INFO: Creating new exec pod
    Jan 18 23:41:19.491: INFO: Waiting up to 5m0s for pod "execpod4q6fm" in namespace "services-4091" to be "running"
    Jan 18 23:41:19.493: INFO: Pod "execpod4q6fm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336008ms
    Jan 18 23:41:21.497: INFO: Pod "execpod4q6fm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005749063s
    Jan 18 23:41:21.497: INFO: Pod "execpod4q6fm" satisfied condition "running"
    Jan 18 23:41:22.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 23:41:22.631: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:22.631: INFO: stdout: ""
    Jan 18 23:41:22.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
    Jan 18 23:41:22.758: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:22.758: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-4091 01/18/23 23:41:22.758
    Jan 18 23:41:22.763: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4091" to be "running and ready"
    Jan 18 23:41:22.766: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07655ms
    Jan 18 23:41:22.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:41:24.770: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006562741s
    Jan 18 23:41:24.770: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 23:41:24.770: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 23:41:24.772
    Jan 18 23:41:24.783: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 23:41:24.783
    Jan 18 23:41:25.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 23:41:25.920: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:25.920: INFO: stdout: ""
    Jan 18 23:41:25.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
    Jan 18 23:41:26.055: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:26.055: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4091 01/18/23 23:41:26.055
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[pod2:[80]] 01/18/23 23:41:26.065
    Jan 18 23:41:26.075: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/18/23 23:41:26.075
    Jan 18 23:41:27.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 23:41:27.211: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:27.211: INFO: stdout: ""
    Jan 18 23:41:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2359182000 --namespace=services-4091 exec execpod4q6fm -- /bin/sh -x -c nc -v -z -w 2 10.96.2.10 80'
    Jan 18 23:41:27.332: INFO: stderr: "+ nc -v -z -w 2 10.96.2.10 80\nConnection to 10.96.2.10 80 port [tcp/http] succeeded!\n"
    Jan 18 23:41:27.332: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-4091 01/18/23 23:41:27.332
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4091 to expose endpoints map[] 01/18/23 23:41:27.346
    Jan 18 23:41:27.353: INFO: successfully validated that service endpoint-test2 in namespace services-4091 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:27.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4091" for this suite. 01/18/23 23:41:27.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:27.377
Jan 18 23:41:27.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:41:27.379
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:27.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:27.401
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-fbf9a967-ab08-4328-9163-b1d9aaa864ea 01/18/23 23:41:27.404
STEP: Creating a pod to test consume configMaps 01/18/23 23:41:27.412
Jan 18 23:41:27.421: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66" in namespace "projected-5266" to be "Succeeded or Failed"
Jan 18 23:41:27.424: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.745285ms
Jan 18 23:41:29.428: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006450369s
Jan 18 23:41:31.429: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007378427s
STEP: Saw pod success 01/18/23 23:41:31.429
Jan 18 23:41:31.429: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66" satisfied condition "Succeeded or Failed"
Jan 18 23:41:31.431: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:41:31.444
Jan 18 23:41:31.457: INFO: Waiting for pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 to disappear
Jan 18 23:41:31.460: INFO: Pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:31.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5266" for this suite. 01/18/23 23:41:31.463
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:27.377
    Jan 18 23:41:27.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:41:27.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:27.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:27.401
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-fbf9a967-ab08-4328-9163-b1d9aaa864ea 01/18/23 23:41:27.404
    STEP: Creating a pod to test consume configMaps 01/18/23 23:41:27.412
    Jan 18 23:41:27.421: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66" in namespace "projected-5266" to be "Succeeded or Failed"
    Jan 18 23:41:27.424: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.745285ms
    Jan 18 23:41:29.428: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006450369s
    Jan 18 23:41:31.429: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007378427s
    STEP: Saw pod success 01/18/23 23:41:31.429
    Jan 18 23:41:31.429: INFO: Pod "pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66" satisfied condition "Succeeded or Failed"
    Jan 18 23:41:31.431: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:41:31.444
    Jan 18 23:41:31.457: INFO: Waiting for pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 to disappear
    Jan 18 23:41:31.460: INFO: Pod pod-projected-configmaps-2e46d740-6dd2-4341-a29f-34b394603b66 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:31.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5266" for this suite. 01/18/23 23:41:31.463
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:31.468
Jan 18 23:41:31.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename projected 01/18/23 23:41:31.469
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:31.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:31.483
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:41:31.485
Jan 18 23:41:31.494: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60" in namespace "projected-8597" to be "Succeeded or Failed"
Jan 18 23:41:31.497: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556636ms
Jan 18 23:41:33.500: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005890106s
Jan 18 23:41:35.501: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00691595s
STEP: Saw pod success 01/18/23 23:41:35.501
Jan 18 23:41:35.501: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60" satisfied condition "Succeeded or Failed"
Jan 18 23:41:35.504: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 container client-container: <nil>
STEP: delete the pod 01/18/23 23:41:35.509
Jan 18 23:41:35.520: INFO: Waiting for pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 to disappear
Jan 18 23:41:35.522: INFO: Pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:35.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8597" for this suite. 01/18/23 23:41:35.525
------------------------------
â€¢ [4.062 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:31.468
    Jan 18 23:41:31.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename projected 01/18/23 23:41:31.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:31.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:31.483
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:41:31.485
    Jan 18 23:41:31.494: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60" in namespace "projected-8597" to be "Succeeded or Failed"
    Jan 18 23:41:31.497: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556636ms
    Jan 18 23:41:33.500: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005890106s
    Jan 18 23:41:35.501: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00691595s
    STEP: Saw pod success 01/18/23 23:41:35.501
    Jan 18 23:41:35.501: INFO: Pod "downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60" satisfied condition "Succeeded or Failed"
    Jan 18 23:41:35.504: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:41:35.509
    Jan 18 23:41:35.520: INFO: Waiting for pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 to disappear
    Jan 18 23:41:35.522: INFO: Pod downwardapi-volume-73ee850c-9f95-417b-b236-535cbfa4bf60 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:35.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8597" for this suite. 01/18/23 23:41:35.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:35.531
Jan 18 23:41:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename disruption 01/18/23 23:41:35.532
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:35.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:35.548
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 23:41:35.551
STEP: Waiting for the pdb to be processed 01/18/23 23:41:35.555
STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 23:41:37.567
STEP: Waiting for all pods to be running 01/18/23 23:41:37.567
Jan 18 23:41:37.569: INFO: pods: 0 < 3
STEP: locating a running pod 01/18/23 23:41:39.573
STEP: Updating the pdb to allow a pod to be evicted 01/18/23 23:41:39.584
STEP: Waiting for the pdb to be processed 01/18/23 23:41:39.591
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 23:41:41.597
STEP: Waiting for all pods to be running 01/18/23 23:41:41.597
STEP: Waiting for the pdb to observed all healthy pods 01/18/23 23:41:41.6
STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 23:41:41.621
STEP: Waiting for the pdb to be processed 01/18/23 23:41:41.631
STEP: Waiting for all pods to be running 01/18/23 23:41:43.639
STEP: locating a running pod 01/18/23 23:41:43.643
STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 23:41:43.65
STEP: Waiting for the pdb to be deleted 01/18/23 23:41:43.655
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 23:41:43.657
STEP: Waiting for all pods to be running 01/18/23 23:41:43.657
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:43.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1993" for this suite. 01/18/23 23:41:43.68
------------------------------
â€¢ [SLOW TEST] [8.165 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:35.531
    Jan 18 23:41:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename disruption 01/18/23 23:41:35.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:35.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:35.548
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 23:41:35.551
    STEP: Waiting for the pdb to be processed 01/18/23 23:41:35.555
    STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 23:41:37.567
    STEP: Waiting for all pods to be running 01/18/23 23:41:37.567
    Jan 18 23:41:37.569: INFO: pods: 0 < 3
    STEP: locating a running pod 01/18/23 23:41:39.573
    STEP: Updating the pdb to allow a pod to be evicted 01/18/23 23:41:39.584
    STEP: Waiting for the pdb to be processed 01/18/23 23:41:39.591
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 23:41:41.597
    STEP: Waiting for all pods to be running 01/18/23 23:41:41.597
    STEP: Waiting for the pdb to observed all healthy pods 01/18/23 23:41:41.6
    STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 23:41:41.621
    STEP: Waiting for the pdb to be processed 01/18/23 23:41:41.631
    STEP: Waiting for all pods to be running 01/18/23 23:41:43.639
    STEP: locating a running pod 01/18/23 23:41:43.643
    STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 23:41:43.65
    STEP: Waiting for the pdb to be deleted 01/18/23 23:41:43.655
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 23:41:43.657
    STEP: Waiting for all pods to be running 01/18/23 23:41:43.657
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:43.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1993" for this suite. 01/18/23 23:41:43.68
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:43.696
Jan 18 23:41:43.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename init-container 01/18/23 23:41:43.698
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:43.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:43.725
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/18/23 23:41:43.728
Jan 18 23:41:43.728: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:47.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3011" for this suite. 01/18/23 23:41:47.361
------------------------------
â€¢ [3.669 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:43.696
    Jan 18 23:41:43.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename init-container 01/18/23 23:41:43.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:43.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:43.725
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/18/23 23:41:43.728
    Jan 18 23:41:43.728: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:47.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3011" for this suite. 01/18/23 23:41:47.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:47.366
Jan 18 23:41:47.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:41:47.367
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:47.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:47.379
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:41:47.382
Jan 18 23:41:47.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b" in namespace "downward-api-1141" to be "Succeeded or Failed"
Jan 18 23:41:47.391: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26579ms
Jan 18 23:41:49.397: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008640399s
Jan 18 23:41:51.398: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009161214s
STEP: Saw pod success 01/18/23 23:41:51.398
Jan 18 23:41:51.398: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b" satisfied condition "Succeeded or Failed"
Jan 18 23:41:51.400: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b container client-container: <nil>
STEP: delete the pod 01/18/23 23:41:51.407
Jan 18 23:41:51.416: INFO: Waiting for pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b to disappear
Jan 18 23:41:51.419: INFO: Pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:51.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1141" for this suite. 01/18/23 23:41:51.422
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:47.366
    Jan 18 23:41:47.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:41:47.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:47.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:47.379
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:41:47.382
    Jan 18 23:41:47.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b" in namespace "downward-api-1141" to be "Succeeded or Failed"
    Jan 18 23:41:47.391: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26579ms
    Jan 18 23:41:49.397: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008640399s
    Jan 18 23:41:51.398: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009161214s
    STEP: Saw pod success 01/18/23 23:41:51.398
    Jan 18 23:41:51.398: INFO: Pod "downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b" satisfied condition "Succeeded or Failed"
    Jan 18 23:41:51.400: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b container client-container: <nil>
    STEP: delete the pod 01/18/23 23:41:51.407
    Jan 18 23:41:51.416: INFO: Waiting for pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b to disappear
    Jan 18 23:41:51.419: INFO: Pod downwardapi-volume-020500f2-a33b-4c3e-b260-734c0b2e478b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:51.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1141" for this suite. 01/18/23 23:41:51.422
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:51.427
Jan 18 23:41:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename downward-api 01/18/23 23:41:51.428
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:51.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:51.44
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/18/23 23:41:51.443
Jan 18 23:41:51.450: INFO: Waiting up to 5m0s for pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143" in namespace "downward-api-3877" to be "Succeeded or Failed"
Jan 18 23:41:51.452: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365037ms
Jan 18 23:41:53.456: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00581978s
Jan 18 23:41:55.457: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007331837s
STEP: Saw pod success 01/18/23 23:41:55.457
Jan 18 23:41:55.457: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143" satisfied condition "Succeeded or Failed"
Jan 18 23:41:55.460: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:41:55.465
Jan 18 23:41:55.476: INFO: Waiting for pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 to disappear
Jan 18 23:41:55.478: INFO: Pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3877" for this suite. 01/18/23 23:41:55.481
------------------------------
â€¢ [4.061 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:51.427
    Jan 18 23:41:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:41:51.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:51.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:51.44
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/18/23 23:41:51.443
    Jan 18 23:41:51.450: INFO: Waiting up to 5m0s for pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143" in namespace "downward-api-3877" to be "Succeeded or Failed"
    Jan 18 23:41:51.452: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365037ms
    Jan 18 23:41:53.456: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00581978s
    Jan 18 23:41:55.457: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007331837s
    STEP: Saw pod success 01/18/23 23:41:55.457
    Jan 18 23:41:55.457: INFO: Pod "downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143" satisfied condition "Succeeded or Failed"
    Jan 18 23:41:55.460: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:41:55.465
    Jan 18 23:41:55.476: INFO: Waiting for pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 to disappear
    Jan 18 23:41:55.478: INFO: Pod downward-api-b0ccc3af-aaf9-44e9-bc91-01058d765143 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3877" for this suite. 01/18/23 23:41:55.481
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:55.488
Jan 18 23:41:55.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:41:55.489
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:55.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:55.504
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-4520eda0-fda3-46c0-8ff6-86bc3e973f6d 01/18/23 23:41:55.507
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:55.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9111" for this suite. 01/18/23 23:41:55.512
------------------------------
â€¢ [0.030 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:55.488
    Jan 18 23:41:55.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:41:55.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:55.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:55.504
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-4520eda0-fda3-46c0-8ff6-86bc3e973f6d 01/18/23 23:41:55.507
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:55.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9111" for this suite. 01/18/23 23:41:55.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:55.518
Jan 18 23:41:55.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 23:41:55.519
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:55.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:55.535
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 18 23:41:55.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 23:41:56.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7586" for this suite. 01/18/23 23:41:56.108
------------------------------
â€¢ [0.595 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:55.518
    Jan 18 23:41:55.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 23:41:55.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:55.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:55.535
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 18 23:41:55.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:41:56.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7586" for this suite. 01/18/23 23:41:56.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:41:56.114
Jan 18 23:41:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:41:56.115
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:56.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:56.13
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:41:56.136
Jan 18 23:41:56.143: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3497" to be "running and ready"
Jan 18 23:41:56.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669169ms
Jan 18 23:41:56.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:41:58.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006458985s
Jan 18 23:41:58.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 23:41:58.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/18/23 23:41:58.152
Jan 18 23:41:58.157: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3497" to be "running and ready"
Jan 18 23:41:58.159: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419995ms
Jan 18 23:41:58.159: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:42:00.162: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005674802s
Jan 18 23:42:00.162: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 18 23:42:00.162: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 23:42:00.165
Jan 18 23:42:00.170: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 23:42:00.173: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 23:42:02.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 23:42:02.177: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 23:42:04.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 23:42:04.177: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/18/23 23:42:04.177
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 23:42:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3497" for this suite. 01/18/23 23:42:04.185
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:41:56.114
    Jan 18 23:41:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 23:41:56.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:41:56.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:41:56.13
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 23:41:56.136
    Jan 18 23:41:56.143: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3497" to be "running and ready"
    Jan 18 23:41:56.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669169ms
    Jan 18 23:41:56.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:41:58.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006458985s
    Jan 18 23:41:58.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 23:41:58.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/18/23 23:41:58.152
    Jan 18 23:41:58.157: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3497" to be "running and ready"
    Jan 18 23:41:58.159: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419995ms
    Jan 18 23:41:58.159: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:42:00.162: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005674802s
    Jan 18 23:42:00.162: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 18 23:42:00.162: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 23:42:00.165
    Jan 18 23:42:00.170: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 23:42:00.173: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 23:42:02.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 23:42:02.177: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 23:42:04.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 23:42:04.177: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/18/23 23:42:04.177
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:42:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3497" for this suite. 01/18/23 23:42:04.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:42:04.19
Jan 18 23:42:04.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename secrets 01/18/23 23:42:04.191
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:42:04.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:42:04.207
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-8678ef78-1e4b-4284-9203-f01c6e538e91 01/18/23 23:42:04.21
STEP: Creating a pod to test consume secrets 01/18/23 23:42:04.216
Jan 18 23:42:04.225: INFO: Waiting up to 5m0s for pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec" in namespace "secrets-2088" to be "Succeeded or Failed"
Jan 18 23:42:04.228: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750992ms
Jan 18 23:42:06.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007282379s
Jan 18 23:42:08.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00688844s
STEP: Saw pod success 01/18/23 23:42:08.232
Jan 18 23:42:08.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec" satisfied condition "Succeeded or Failed"
Jan 18 23:42:08.235: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:42:08.24
Jan 18 23:42:08.252: INFO: Waiting for pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec to disappear
Jan 18 23:42:08.255: INFO: Pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 23:42:08.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2088" for this suite. 01/18/23 23:42:08.258
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:42:04.19
    Jan 18 23:42:04.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename secrets 01/18/23 23:42:04.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:42:04.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:42:04.207
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-8678ef78-1e4b-4284-9203-f01c6e538e91 01/18/23 23:42:04.21
    STEP: Creating a pod to test consume secrets 01/18/23 23:42:04.216
    Jan 18 23:42:04.225: INFO: Waiting up to 5m0s for pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec" in namespace "secrets-2088" to be "Succeeded or Failed"
    Jan 18 23:42:04.228: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750992ms
    Jan 18 23:42:06.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007282379s
    Jan 18 23:42:08.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00688844s
    STEP: Saw pod success 01/18/23 23:42:08.232
    Jan 18 23:42:08.232: INFO: Pod "pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec" satisfied condition "Succeeded or Failed"
    Jan 18 23:42:08.235: INFO: Trying to get logs from node cncf-conformance-1-26-2 pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:42:08.24
    Jan 18 23:42:08.252: INFO: Waiting for pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec to disappear
    Jan 18 23:42:08.255: INFO: Pod pod-secrets-0e905f15-2fd4-4850-9fcd-78de3d514fec no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:42:08.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2088" for this suite. 01/18/23 23:42:08.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 23:42:08.274
Jan 18 23:42:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
STEP: Building a namespace api object, basename pods 01/18/23 23:42:08.275
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:42:08.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:42:08.29
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/18/23 23:42:08.293
STEP: setting up watch 01/18/23 23:42:08.293
STEP: submitting the pod to kubernetes 01/18/23 23:42:08.396
STEP: verifying the pod is in kubernetes 01/18/23 23:42:08.406
STEP: verifying pod creation was observed 01/18/23 23:42:08.409
Jan 18 23:42:08.409: INFO: Waiting up to 5m0s for pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a" in namespace "pods-1148" to be "running"
Jan 18 23:42:08.411: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266659ms
Jan 18 23:42:10.415: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006118023s
Jan 18 23:42:10.415: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 23:42:10.418
STEP: verifying pod deletion was observed 01/18/23 23:42:10.427
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 23:42:12.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1148" for this suite. 01/18/23 23:42:12.468
------------------------------
â€¢ [4.199 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 23:42:08.274
    Jan 18 23:42:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2359182000
    STEP: Building a namespace api object, basename pods 01/18/23 23:42:08.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:42:08.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:42:08.29
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/18/23 23:42:08.293
    STEP: setting up watch 01/18/23 23:42:08.293
    STEP: submitting the pod to kubernetes 01/18/23 23:42:08.396
    STEP: verifying the pod is in kubernetes 01/18/23 23:42:08.406
    STEP: verifying pod creation was observed 01/18/23 23:42:08.409
    Jan 18 23:42:08.409: INFO: Waiting up to 5m0s for pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a" in namespace "pods-1148" to be "running"
    Jan 18 23:42:08.411: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266659ms
    Jan 18 23:42:10.415: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006118023s
    Jan 18 23:42:10.415: INFO: Pod "pod-submit-remove-141a8a7c-bbb6-4ce3-9ece-e6ca5486b16a" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 23:42:10.418
    STEP: verifying pod deletion was observed 01/18/23 23:42:10.427
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 23:42:12.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1148" for this suite. 01/18/23 23:42:12.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 18 23:42:12.474: INFO: Running AfterSuite actions on node 1
Jan 18 23:42:12.474: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 18 23:42:12.474: INFO: Running AfterSuite actions on node 1
    Jan 18 23:42:12.474: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.095 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5554.736 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h32m35.096223909s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

