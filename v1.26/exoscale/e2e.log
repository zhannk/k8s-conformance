I1216 13:11:05.658343      21 e2e.go:126] Starting e2e run "6ab7a2a6-4f2a-43ab-a7a0-8623e2091669" on Ginkgo node 1
Dec 16 13:11:05.673: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1671196265 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec 16 13:11:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:11:05.784: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 16 13:11:05.804: INFO: Condition Ready of node pool-a3802-fsxxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-12-16 13:10:47 +0000 UTC}]. Failure
Dec 16 13:11:05.804: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Dec 16 13:11:05.804: INFO: 	-> Node pool-a3802-fsxxd [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-12-16 13:10:47 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Dec 16 13:11:05.804: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Dec 16 13:11:35.813: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 16 13:11:35.834: INFO: The status of Pod calico-node-5m9wp is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 16 13:11:35.834: INFO: 10 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 16 13:11:35.834: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 16 13:11:35.834: INFO: POD                NODE              PHASE    GRACE  CONDITIONS
Dec 16 13:11:35.834: INFO: calico-node-5m9wp  pool-a3802-fsxxd  Pending         [{Initialized False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotInitialized containers with incomplete status: [mount-bpffs]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:43 +0000 UTC  }]
Dec 16 13:11:35.834: INFO: 
Dec 16 13:11:37.856: INFO: The status of Pod calico-node-5m9wp is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 16 13:11:37.856: INFO: 10 / 11 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Dec 16 13:11:37.856: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 16 13:11:37.856: INFO: POD                NODE              PHASE    GRACE  CONDITIONS
Dec 16 13:11:37.856: INFO: calico-node-5m9wp  pool-a3802-fsxxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:11:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:43 +0000 UTC  }]
Dec 16 13:11:37.856: INFO: 
Dec 16 13:11:39.860: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Dec 16 13:11:39.860: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 16 13:11:39.860: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 16 13:11:39.866: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 16 13:11:39.866: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 16 13:11:39.866: INFO: e2e test version: v1.26.0
Dec 16 13:11:39.867: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec 16 13:11:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:11:39.872: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [34.090 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec 16 13:11:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:11:05.784: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Dec 16 13:11:05.804: INFO: Condition Ready of node pool-a3802-fsxxd is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-12-16 13:10:47 +0000 UTC}]. Failure
    Dec 16 13:11:05.804: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Dec 16 13:11:05.804: INFO: 	-> Node pool-a3802-fsxxd [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-12-16 13:10:47 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Dec 16 13:11:05.804: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Dec 16 13:11:35.813: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Dec 16 13:11:35.834: INFO: The status of Pod calico-node-5m9wp is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 16 13:11:35.834: INFO: 10 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec 16 13:11:35.834: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Dec 16 13:11:35.834: INFO: POD                NODE              PHASE    GRACE  CONDITIONS
    Dec 16 13:11:35.834: INFO: calico-node-5m9wp  pool-a3802-fsxxd  Pending         [{Initialized False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotInitialized containers with incomplete status: [mount-bpffs]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:43 +0000 UTC  }]
    Dec 16 13:11:35.834: INFO: 
    Dec 16 13:11:37.856: INFO: The status of Pod calico-node-5m9wp is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 16 13:11:37.856: INFO: 10 / 11 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
    Dec 16 13:11:37.856: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Dec 16 13:11:37.856: INFO: POD                NODE              PHASE    GRACE  CONDITIONS
    Dec 16 13:11:37.856: INFO: calico-node-5m9wp  pool-a3802-fsxxd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:11:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:45 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:10:43 +0000 UTC  }]
    Dec 16 13:11:37.856: INFO: 
    Dec 16 13:11:39.860: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
    Dec 16 13:11:39.860: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Dec 16 13:11:39.860: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec 16 13:11:39.866: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Dec 16 13:11:39.866: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Dec 16 13:11:39.866: INFO: e2e test version: v1.26.0
    Dec 16 13:11:39.867: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec 16 13:11:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:11:39.872: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:11:39.894
Dec 16 13:11:39.894: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption 12/16/22 13:11:39.894
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:39.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:39.916
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 12/16/22 13:11:39.924
STEP: Waiting for all pods to be running 12/16/22 13:11:45.754
Dec 16 13:11:45.786: INFO: running pods: 0 < 3
Dec 16 13:11:47.793: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:11:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2513" for this suite. 12/16/22 13:11:49.798
------------------------------
• [SLOW TEST] [9.910 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:11:39.894
    Dec 16 13:11:39.894: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption 12/16/22 13:11:39.894
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:39.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:39.916
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 12/16/22 13:11:39.924
    STEP: Waiting for all pods to be running 12/16/22 13:11:45.754
    Dec 16 13:11:45.786: INFO: running pods: 0 < 3
    Dec 16 13:11:47.793: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:11:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2513" for this suite. 12/16/22 13:11:49.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:11:49.804
Dec 16 13:11:49.804: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename watch 12/16/22 13:11:49.805
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:49.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:49.824
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 12/16/22 13:11:49.827
STEP: starting a background goroutine to produce watch events 12/16/22 13:11:49.83
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/16/22 13:11:49.83
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 16 13:11:52.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1419" for this suite. 12/16/22 13:11:52.66
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:11:49.804
    Dec 16 13:11:49.804: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename watch 12/16/22 13:11:49.805
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:49.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:49.824
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 12/16/22 13:11:49.827
    STEP: starting a background goroutine to produce watch events 12/16/22 13:11:49.83
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/16/22 13:11:49.83
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:11:52.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1419" for this suite. 12/16/22 13:11:52.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:11:52.714
Dec 16 13:11:52.714: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context-test 12/16/22 13:11:52.715
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:52.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:52.784
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Dec 16 13:11:52.794: INFO: Waiting up to 5m0s for pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180" in namespace "security-context-test-8879" to be "Succeeded or Failed"
Dec 16 13:11:52.797: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033038ms
Dec 16 13:11:54.802: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007629901s
Dec 16 13:11:56.804: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010314605s
Dec 16 13:11:58.803: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009073217s
Dec 16 13:11:58.803: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 13:11:58.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8879" for this suite. 12/16/22 13:11:58.807
------------------------------
• [SLOW TEST] [6.098 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:11:52.714
    Dec 16 13:11:52.714: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context-test 12/16/22 13:11:52.715
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:52.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:52.784
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Dec 16 13:11:52.794: INFO: Waiting up to 5m0s for pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180" in namespace "security-context-test-8879" to be "Succeeded or Failed"
    Dec 16 13:11:52.797: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033038ms
    Dec 16 13:11:54.802: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007629901s
    Dec 16 13:11:56.804: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010314605s
    Dec 16 13:11:58.803: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009073217s
    Dec 16 13:11:58.803: INFO: Pod "busybox-user-65534-51d317cc-ab06-4e27-a5a6-e5c1c444a180" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:11:58.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8879" for this suite. 12/16/22 13:11:58.807
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:11:58.813
Dec 16 13:11:58.813: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir-wrapper 12/16/22 13:11:58.814
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:58.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:58.837
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/16/22 13:11:58.839
STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:11:59.068
Dec 16 13:11:59.174: INFO: Pod name wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2: Found 3 pods out of 5
Dec 16 13:12:04.184: INFO: Pod name wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/16/22 13:12:04.185
Dec 16 13:12:04.185: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:04.188: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832979ms
Dec 16 13:12:06.195: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010097998s
Dec 16 13:12:08.196: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011470324s
Dec 16 13:12:10.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00974033s
Dec 16 13:12:12.193: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008429633s
Dec 16 13:12:14.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Running", Reason="", readiness=true. Elapsed: 10.00951652s
Dec 16 13:12:14.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k" satisfied condition "running"
Dec 16 13:12:14.194: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:14.198: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn": Phase="Running", Reason="", readiness=true. Elapsed: 3.743071ms
Dec 16 13:12:14.198: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn" satisfied condition "running"
Dec 16 13:12:14.198: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:14.202: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv": Phase="Running", Reason="", readiness=true. Elapsed: 3.965167ms
Dec 16 13:12:14.202: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv" satisfied condition "running"
Dec 16 13:12:14.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:14.206: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp": Phase="Running", Reason="", readiness=true. Elapsed: 3.824399ms
Dec 16 13:12:14.206: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp" satisfied condition "running"
Dec 16 13:12:14.206: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:14.209: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.951539ms
Dec 16 13:12:14.209: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:14.209
Dec 16 13:12:14.272: INFO: Deleting ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 took: 9.066515ms
Dec 16 13:12:14.372: INFO: Terminating ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 pods took: 100.452641ms
STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:12:16.878
Dec 16 13:12:16.890: INFO: Pod name wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9: Found 0 pods out of 5
Dec 16 13:12:21.900: INFO: Pod name wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/16/22 13:12:21.9
Dec 16 13:12:21.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:21.903: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428246ms
Dec 16 13:12:23.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010068285s
Dec 16 13:12:25.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010212602s
Dec 16 13:12:27.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010521345s
Dec 16 13:12:29.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010703926s
Dec 16 13:12:31.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Running", Reason="", readiness=true. Elapsed: 10.010433557s
Dec 16 13:12:31.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8" satisfied condition "running"
Dec 16 13:12:31.910: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:32.103: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9": Phase="Running", Reason="", readiness=true. Elapsed: 193.279414ms
Dec 16 13:12:32.103: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9" satisfied condition "running"
Dec 16 13:12:32.103: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:32.107: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc": Phase="Running", Reason="", readiness=true. Elapsed: 3.682338ms
Dec 16 13:12:32.107: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc" satisfied condition "running"
Dec 16 13:12:32.107: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:32.111: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz": Phase="Running", Reason="", readiness=true. Elapsed: 3.668785ms
Dec 16 13:12:32.111: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz" satisfied condition "running"
Dec 16 13:12:32.111: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:32.120: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt": Phase="Running", Reason="", readiness=true. Elapsed: 8.683301ms
Dec 16 13:12:32.120: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:32.12
Dec 16 13:12:32.182: INFO: Deleting ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 took: 7.928343ms
Dec 16 13:12:32.282: INFO: Terminating ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 pods took: 100.223313ms
STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:12:35.987
Dec 16 13:12:36.002: INFO: Pod name wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79: Found 0 pods out of 5
Dec 16 13:12:41.011: INFO: Pod name wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/16/22 13:12:41.011
Dec 16 13:12:41.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:41.016: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80027ms
Dec 16 13:12:43.022: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010499025s
Dec 16 13:12:45.023: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011888406s
Dec 16 13:12:47.025: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013256151s
Dec 16 13:12:49.023: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011937066s
Dec 16 13:12:51.024: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Running", Reason="", readiness=true. Elapsed: 10.012494456s
Dec 16 13:12:51.024: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j" satisfied condition "running"
Dec 16 13:12:51.024: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:51.028: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.772473ms
Dec 16 13:12:53.035: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.011049342s
Dec 16 13:12:53.035: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w" satisfied condition "running"
Dec 16 13:12:53.035: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:53.039: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn": Phase="Running", Reason="", readiness=true. Elapsed: 3.831631ms
Dec 16 13:12:53.039: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn" satisfied condition "running"
Dec 16 13:12:53.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:53.043: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj": Phase="Running", Reason="", readiness=true. Elapsed: 3.923773ms
Dec 16 13:12:53.043: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj" satisfied condition "running"
Dec 16 13:12:53.043: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f" in namespace "emptydir-wrapper-6376" to be "running"
Dec 16 13:12:53.047: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f": Phase="Running", Reason="", readiness=true. Elapsed: 3.655609ms
Dec 16 13:12:53.047: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:53.047
Dec 16 13:12:53.111: INFO: Deleting ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 took: 10.058214ms
Dec 16 13:12:53.212: INFO: Terminating ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 pods took: 101.396959ms
STEP: Cleaning up the configMaps 12/16/22 13:12:56.613
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:12:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6376" for this suite. 12/16/22 13:12:56.888
------------------------------
• [SLOW TEST] [58.081 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:11:58.813
    Dec 16 13:11:58.813: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir-wrapper 12/16/22 13:11:58.814
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:11:58.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:11:58.837
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/16/22 13:11:58.839
    STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:11:59.068
    Dec 16 13:11:59.174: INFO: Pod name wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2: Found 3 pods out of 5
    Dec 16 13:12:04.184: INFO: Pod name wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/16/22 13:12:04.185
    Dec 16 13:12:04.185: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:04.188: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832979ms
    Dec 16 13:12:06.195: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010097998s
    Dec 16 13:12:08.196: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011470324s
    Dec 16 13:12:10.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00974033s
    Dec 16 13:12:12.193: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008429633s
    Dec 16 13:12:14.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k": Phase="Running", Reason="", readiness=true. Elapsed: 10.00951652s
    Dec 16 13:12:14.194: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-5zl7k" satisfied condition "running"
    Dec 16 13:12:14.194: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:14.198: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn": Phase="Running", Reason="", readiness=true. Elapsed: 3.743071ms
    Dec 16 13:12:14.198: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-fsxkn" satisfied condition "running"
    Dec 16 13:12:14.198: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:14.202: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv": Phase="Running", Reason="", readiness=true. Elapsed: 3.965167ms
    Dec 16 13:12:14.202: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-p7njv" satisfied condition "running"
    Dec 16 13:12:14.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:14.206: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp": Phase="Running", Reason="", readiness=true. Elapsed: 3.824399ms
    Dec 16 13:12:14.206: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-rqcfp" satisfied condition "running"
    Dec 16 13:12:14.206: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:14.209: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.951539ms
    Dec 16 13:12:14.209: INFO: Pod "wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2-whjpk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:14.209
    Dec 16 13:12:14.272: INFO: Deleting ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 took: 9.066515ms
    Dec 16 13:12:14.372: INFO: Terminating ReplicationController wrapped-volume-race-29b1bec7-7279-4082-803b-e09e1f6f05b2 pods took: 100.452641ms
    STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:12:16.878
    Dec 16 13:12:16.890: INFO: Pod name wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9: Found 0 pods out of 5
    Dec 16 13:12:21.900: INFO: Pod name wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/16/22 13:12:21.9
    Dec 16 13:12:21.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:21.903: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428246ms
    Dec 16 13:12:23.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010068285s
    Dec 16 13:12:25.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010212602s
    Dec 16 13:12:27.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010521345s
    Dec 16 13:12:29.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010703926s
    Dec 16 13:12:31.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8": Phase="Running", Reason="", readiness=true. Elapsed: 10.010433557s
    Dec 16 13:12:31.910: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-6ngf8" satisfied condition "running"
    Dec 16 13:12:31.910: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:32.103: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9": Phase="Running", Reason="", readiness=true. Elapsed: 193.279414ms
    Dec 16 13:12:32.103: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-cphs9" satisfied condition "running"
    Dec 16 13:12:32.103: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:32.107: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc": Phase="Running", Reason="", readiness=true. Elapsed: 3.682338ms
    Dec 16 13:12:32.107: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-hqscc" satisfied condition "running"
    Dec 16 13:12:32.107: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:32.111: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz": Phase="Running", Reason="", readiness=true. Elapsed: 3.668785ms
    Dec 16 13:12:32.111: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-kdklz" satisfied condition "running"
    Dec 16 13:12:32.111: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:32.120: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt": Phase="Running", Reason="", readiness=true. Elapsed: 8.683301ms
    Dec 16 13:12:32.120: INFO: Pod "wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9-rksxt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:32.12
    Dec 16 13:12:32.182: INFO: Deleting ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 took: 7.928343ms
    Dec 16 13:12:32.282: INFO: Terminating ReplicationController wrapped-volume-race-18778859-8d8b-419b-9217-fd083c0da7a9 pods took: 100.223313ms
    STEP: Creating RC which spawns configmap-volume pods 12/16/22 13:12:35.987
    Dec 16 13:12:36.002: INFO: Pod name wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79: Found 0 pods out of 5
    Dec 16 13:12:41.011: INFO: Pod name wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/16/22 13:12:41.011
    Dec 16 13:12:41.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:41.016: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80027ms
    Dec 16 13:12:43.022: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010499025s
    Dec 16 13:12:45.023: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011888406s
    Dec 16 13:12:47.025: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013256151s
    Dec 16 13:12:49.023: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011937066s
    Dec 16 13:12:51.024: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j": Phase="Running", Reason="", readiness=true. Elapsed: 10.012494456s
    Dec 16 13:12:51.024: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-5d48j" satisfied condition "running"
    Dec 16 13:12:51.024: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:51.028: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.772473ms
    Dec 16 13:12:53.035: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.011049342s
    Dec 16 13:12:53.035: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-7jd2w" satisfied condition "running"
    Dec 16 13:12:53.035: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:53.039: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn": Phase="Running", Reason="", readiness=true. Elapsed: 3.831631ms
    Dec 16 13:12:53.039: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-fgggn" satisfied condition "running"
    Dec 16 13:12:53.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:53.043: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj": Phase="Running", Reason="", readiness=true. Elapsed: 3.923773ms
    Dec 16 13:12:53.043: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-j7nnj" satisfied condition "running"
    Dec 16 13:12:53.043: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f" in namespace "emptydir-wrapper-6376" to be "running"
    Dec 16 13:12:53.047: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f": Phase="Running", Reason="", readiness=true. Elapsed: 3.655609ms
    Dec 16 13:12:53.047: INFO: Pod "wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79-qt25f" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 in namespace emptydir-wrapper-6376, will wait for the garbage collector to delete the pods 12/16/22 13:12:53.047
    Dec 16 13:12:53.111: INFO: Deleting ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 took: 10.058214ms
    Dec 16 13:12:53.212: INFO: Terminating ReplicationController wrapped-volume-race-7e37eb80-b3d5-4c11-be53-3e24d378be79 pods took: 101.396959ms
    STEP: Cleaning up the configMaps 12/16/22 13:12:56.613
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:12:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6376" for this suite. 12/16/22 13:12:56.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:12:56.895
Dec 16 13:12:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:12:56.896
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:12:56.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:12:56.913
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:12:56.916
Dec 16 13:12:56.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4" in namespace "projected-823" to be "Succeeded or Failed"
Dec 16 13:12:56.927: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950016ms
Dec 16 13:12:58.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553751s
Dec 16 13:13:00.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008582303s
Dec 16 13:13:02.934: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010135108s
Dec 16 13:13:04.931: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007634525s
STEP: Saw pod success 12/16/22 13:13:04.931
Dec 16 13:13:04.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4" satisfied condition "Succeeded or Failed"
Dec 16 13:13:04.935: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 container client-container: <nil>
STEP: delete the pod 12/16/22 13:13:04.993
Dec 16 13:13:05.006: INFO: Waiting for pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 to disappear
Dec 16 13:13:05.009: INFO: Pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 13:13:05.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-823" for this suite. 12/16/22 13:13:05.012
------------------------------
• [SLOW TEST] [8.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:12:56.895
    Dec 16 13:12:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:12:56.896
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:12:56.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:12:56.913
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:12:56.916
    Dec 16 13:12:56.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4" in namespace "projected-823" to be "Succeeded or Failed"
    Dec 16 13:12:56.927: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950016ms
    Dec 16 13:12:58.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553751s
    Dec 16 13:13:00.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008582303s
    Dec 16 13:13:02.934: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010135108s
    Dec 16 13:13:04.931: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007634525s
    STEP: Saw pod success 12/16/22 13:13:04.931
    Dec 16 13:13:04.932: INFO: Pod "downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4" satisfied condition "Succeeded or Failed"
    Dec 16 13:13:04.935: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:13:04.993
    Dec 16 13:13:05.006: INFO: Waiting for pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 to disappear
    Dec 16 13:13:05.009: INFO: Pod downwardapi-volume-853a9570-4efe-40f4-9f18-23ff385297b4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:13:05.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-823" for this suite. 12/16/22 13:13:05.012
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:13:05.019
Dec 16 13:13:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 13:13:05.02
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:13:05.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:13:05.037
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Dec 16 13:13:05.056: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:13:05.061
Dec 16 13:13:05.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:05.067: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:06.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:06.075: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:07.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:07.077: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:08.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:08.078: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:09.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:09.079: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:10.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 13:13:10.077: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:11.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 13:13:11.076: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 12/16/22 13:13:11.093
STEP: Check that daemon pods images are updated. 12/16/22 13:13:11.106
Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-kdtxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:12.121: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:12.121: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:13.122: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:13.122: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:14.119: INFO: Pod daemon-set-6p87m is not available
Dec 16 13:13:14.119: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:14.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:15.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:16.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:16.120: INFO: Pod daemon-set-wnrb2 is not available
Dec 16 13:13:17.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:17.119: INFO: Pod daemon-set-wnrb2 is not available
Dec 16 13:13:18.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:18.120: INFO: Pod daemon-set-wnrb2 is not available
Dec 16 13:13:19.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 16 13:13:19.120: INFO: Pod daemon-set-wnrb2 is not available
Dec 16 13:13:21.119: INFO: Pod daemon-set-hdxjs is not available
STEP: Check that daemon pods are still running on every node of the cluster. 12/16/22 13:13:21.123
Dec 16 13:13:21.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 13:13:21.129: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:22.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 13:13:22.140: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:23.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 13:13:23.141: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:24.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 13:13:24.138: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:13:25.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 13:13:25.140: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:13:25.158
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5594, will wait for the garbage collector to delete the pods 12/16/22 13:13:25.158
Dec 16 13:13:25.220: INFO: Deleting DaemonSet.extensions daemon-set took: 8.187478ms
Dec 16 13:13:25.320: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116312ms
Dec 16 13:13:27.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:13:27.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 13:13:27.427: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680866928"},"items":null}

Dec 16 13:13:27.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680866928"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:13:27.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5594" for this suite. 12/16/22 13:13:27.45
------------------------------
• [SLOW TEST] [22.438 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:13:05.019
    Dec 16 13:13:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 13:13:05.02
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:13:05.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:13:05.037
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Dec 16 13:13:05.056: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:13:05.061
    Dec 16 13:13:05.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:05.067: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:06.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:06.075: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:07.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:07.077: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:08.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:08.078: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:09.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:09.079: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:10.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 13:13:10.077: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:11.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 13:13:11.076: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 12/16/22 13:13:11.093
    STEP: Check that daemon pods images are updated. 12/16/22 13:13:11.106
    Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:11.109: INFO: Wrong image for pod: daemon-set-kdtxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:12.121: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:12.121: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:13.122: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:13.122: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:14.119: INFO: Pod daemon-set-6p87m is not available
    Dec 16 13:13:14.119: INFO: Wrong image for pod: daemon-set-8srkf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:14.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:15.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:16.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:16.120: INFO: Pod daemon-set-wnrb2 is not available
    Dec 16 13:13:17.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:17.119: INFO: Pod daemon-set-wnrb2 is not available
    Dec 16 13:13:18.120: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:18.120: INFO: Pod daemon-set-wnrb2 is not available
    Dec 16 13:13:19.119: INFO: Wrong image for pod: daemon-set-fgcmd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 16 13:13:19.120: INFO: Pod daemon-set-wnrb2 is not available
    Dec 16 13:13:21.119: INFO: Pod daemon-set-hdxjs is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 12/16/22 13:13:21.123
    Dec 16 13:13:21.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 13:13:21.129: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:22.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 13:13:22.140: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:23.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 13:13:23.141: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:24.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 13:13:24.138: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:13:25.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 13:13:25.140: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:13:25.158
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5594, will wait for the garbage collector to delete the pods 12/16/22 13:13:25.158
    Dec 16 13:13:25.220: INFO: Deleting DaemonSet.extensions daemon-set took: 8.187478ms
    Dec 16 13:13:25.320: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116312ms
    Dec 16 13:13:27.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:13:27.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 13:13:27.427: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680866928"},"items":null}

    Dec 16 13:13:27.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680866928"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:13:27.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5594" for this suite. 12/16/22 13:13:27.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:13:27.458
Dec 16 13:13:27.458: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename cronjob 12/16/22 13:13:27.459
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:13:27.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:13:27.48
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 12/16/22 13:13:27.483
STEP: Ensuring a job is scheduled 12/16/22 13:13:27.49
STEP: Ensuring exactly one is scheduled 12/16/22 13:14:01.498
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/16/22 13:14:01.502
STEP: Ensuring the job is replaced with a new one 12/16/22 13:14:01.506
STEP: Removing cronjob 12/16/22 13:15:01.511
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7560" for this suite. 12/16/22 13:15:01.523
------------------------------
• [SLOW TEST] [94.076 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:13:27.458
    Dec 16 13:13:27.458: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename cronjob 12/16/22 13:13:27.459
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:13:27.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:13:27.48
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 12/16/22 13:13:27.483
    STEP: Ensuring a job is scheduled 12/16/22 13:13:27.49
    STEP: Ensuring exactly one is scheduled 12/16/22 13:14:01.498
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/16/22 13:14:01.502
    STEP: Ensuring the job is replaced with a new one 12/16/22 13:14:01.506
    STEP: Removing cronjob 12/16/22 13:15:01.511
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7560" for this suite. 12/16/22 13:15:01.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:01.536
Dec 16 13:15:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename ingressclass 12/16/22 13:15:01.537
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:01.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:01.558
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 12/16/22 13:15:01.561
STEP: getting /apis/networking.k8s.io 12/16/22 13:15:01.564
STEP: getting /apis/networking.k8s.iov1 12/16/22 13:15:01.565
STEP: creating 12/16/22 13:15:01.566
STEP: getting 12/16/22 13:15:01.579
STEP: listing 12/16/22 13:15:01.583
STEP: watching 12/16/22 13:15:01.586
Dec 16 13:15:01.586: INFO: starting watch
STEP: patching 12/16/22 13:15:01.588
STEP: updating 12/16/22 13:15:01.593
Dec 16 13:15:01.598: INFO: waiting for watch events with expected annotations
Dec 16 13:15:01.598: INFO: saw patched and updated annotations
STEP: deleting 12/16/22 13:15:01.598
STEP: deleting a collection 12/16/22 13:15:01.61
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-7658" for this suite. 12/16/22 13:15:01.629
------------------------------
• [0.100 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:01.536
    Dec 16 13:15:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename ingressclass 12/16/22 13:15:01.537
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:01.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:01.558
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 12/16/22 13:15:01.561
    STEP: getting /apis/networking.k8s.io 12/16/22 13:15:01.564
    STEP: getting /apis/networking.k8s.iov1 12/16/22 13:15:01.565
    STEP: creating 12/16/22 13:15:01.566
    STEP: getting 12/16/22 13:15:01.579
    STEP: listing 12/16/22 13:15:01.583
    STEP: watching 12/16/22 13:15:01.586
    Dec 16 13:15:01.586: INFO: starting watch
    STEP: patching 12/16/22 13:15:01.588
    STEP: updating 12/16/22 13:15:01.593
    Dec 16 13:15:01.598: INFO: waiting for watch events with expected annotations
    Dec 16 13:15:01.598: INFO: saw patched and updated annotations
    STEP: deleting 12/16/22 13:15:01.598
    STEP: deleting a collection 12/16/22 13:15:01.61
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-7658" for this suite. 12/16/22 13:15:01.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:01.637
Dec 16 13:15:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 13:15:01.638
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:01.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:01.655
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/16/22 13:15:01.662
Dec 16 13:15:01.672: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5546" to be "running and ready"
Dec 16 13:15:01.675: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158052ms
Dec 16 13:15:01.675: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:15:03.682: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010034107s
Dec 16 13:15:03.682: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 16 13:15:03.682: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 12/16/22 13:15:03.688
Dec 16 13:15:03.693: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5546" to be "running and ready"
Dec 16 13:15:03.696: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889246ms
Dec 16 13:15:03.696: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:15:05.702: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008819148s
Dec 16 13:15:05.702: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:15:07.701: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.007693867s
Dec 16 13:15:07.701: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Dec 16 13:15:07.701: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/16/22 13:15:07.704
STEP: delete the pod with lifecycle hook 12/16/22 13:15:07.758
Dec 16 13:15:07.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 13:15:07.770: INFO: Pod pod-with-poststart-http-hook still exists
Dec 16 13:15:09.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 16 13:15:09.775: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:09.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5546" for this suite. 12/16/22 13:15:09.779
------------------------------
• [SLOW TEST] [8.148 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:01.637
    Dec 16 13:15:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 13:15:01.638
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:01.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:01.655
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/16/22 13:15:01.662
    Dec 16 13:15:01.672: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5546" to be "running and ready"
    Dec 16 13:15:01.675: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158052ms
    Dec 16 13:15:01.675: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:15:03.682: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010034107s
    Dec 16 13:15:03.682: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 16 13:15:03.682: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 12/16/22 13:15:03.688
    Dec 16 13:15:03.693: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5546" to be "running and ready"
    Dec 16 13:15:03.696: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889246ms
    Dec 16 13:15:03.696: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:15:05.702: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008819148s
    Dec 16 13:15:05.702: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:15:07.701: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.007693867s
    Dec 16 13:15:07.701: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Dec 16 13:15:07.701: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/16/22 13:15:07.704
    STEP: delete the pod with lifecycle hook 12/16/22 13:15:07.758
    Dec 16 13:15:07.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 16 13:15:07.770: INFO: Pod pod-with-poststart-http-hook still exists
    Dec 16 13:15:09.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 16 13:15:09.775: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:09.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5546" for this suite. 12/16/22 13:15:09.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:09.786
Dec 16 13:15:09.786: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:15:09.787
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:09.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:09.804
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:15:09.807
Dec 16 13:15:09.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c" in namespace "projected-8234" to be "Succeeded or Failed"
Dec 16 13:15:09.819: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608902ms
Dec 16 13:15:11.825: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008450684s
Dec 16 13:15:13.826: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009790391s
STEP: Saw pod success 12/16/22 13:15:13.826
Dec 16 13:15:13.826: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c" satisfied condition "Succeeded or Failed"
Dec 16 13:15:13.830: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c container client-container: <nil>
STEP: delete the pod 12/16/22 13:15:13.883
Dec 16 13:15:13.896: INFO: Waiting for pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c to disappear
Dec 16 13:15:13.899: INFO: Pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8234" for this suite. 12/16/22 13:15:13.903
------------------------------
• [4.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:09.786
    Dec 16 13:15:09.786: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:15:09.787
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:09.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:09.804
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:15:09.807
    Dec 16 13:15:09.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c" in namespace "projected-8234" to be "Succeeded or Failed"
    Dec 16 13:15:09.819: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608902ms
    Dec 16 13:15:11.825: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008450684s
    Dec 16 13:15:13.826: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009790391s
    STEP: Saw pod success 12/16/22 13:15:13.826
    Dec 16 13:15:13.826: INFO: Pod "downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c" satisfied condition "Succeeded or Failed"
    Dec 16 13:15:13.830: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c container client-container: <nil>
    STEP: delete the pod 12/16/22 13:15:13.883
    Dec 16 13:15:13.896: INFO: Waiting for pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c to disappear
    Dec 16 13:15:13.899: INFO: Pod downwardapi-volume-ed95a554-0701-45d5-aa19-967f289dcc4c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8234" for this suite. 12/16/22 13:15:13.903
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:13.91
Dec 16 13:15:13.910: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:15:13.911
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:13.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:13.934
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 12/16/22 13:15:13.936
Dec 16 13:15:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-7521 api-versions'
Dec 16 13:15:14.012: INFO: stderr: ""
Dec 16 13:15:14.012: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:14.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7521" for this suite. 12/16/22 13:15:14.018
------------------------------
• [0.114 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:13.91
    Dec 16 13:15:13.910: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:15:13.911
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:13.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:13.934
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 12/16/22 13:15:13.936
    Dec 16 13:15:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-7521 api-versions'
    Dec 16 13:15:14.012: INFO: stderr: ""
    Dec 16 13:15:14.012: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:14.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7521" for this suite. 12/16/22 13:15:14.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:14.026
Dec 16 13:15:14.026: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:15:14.026
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:14.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:14.044
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-4bd97f23-d342-40fe-bd4b-5c87e1a84ac9 12/16/22 13:15:14.05
STEP: Creating secret with name s-test-opt-upd-b9a2f49e-55d8-49b6-9a78-5956f5050f33 12/16/22 13:15:14.055
STEP: Creating the pod 12/16/22 13:15:14.06
Dec 16 13:15:14.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc" in namespace "projected-3945" to be "running and ready"
Dec 16 13:15:14.071: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158798ms
Dec 16 13:15:14.071: INFO: The phase of Pod pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:15:16.076: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988032s
Dec 16 13:15:16.076: INFO: The phase of Pod pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc is Running (Ready = true)
Dec 16 13:15:16.076: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-4bd97f23-d342-40fe-bd4b-5c87e1a84ac9 12/16/22 13:15:16.104
STEP: Updating secret s-test-opt-upd-b9a2f49e-55d8-49b6-9a78-5956f5050f33 12/16/22 13:15:16.11
STEP: Creating secret with name s-test-opt-create-b4dfe52d-321f-49dd-a82d-32faf95a6833 12/16/22 13:15:16.115
STEP: waiting to observe update in volume 12/16/22 13:15:16.119
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:18.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3945" for this suite. 12/16/22 13:15:18.155
------------------------------
• [4.135 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:14.026
    Dec 16 13:15:14.026: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:15:14.026
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:14.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:14.044
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-4bd97f23-d342-40fe-bd4b-5c87e1a84ac9 12/16/22 13:15:14.05
    STEP: Creating secret with name s-test-opt-upd-b9a2f49e-55d8-49b6-9a78-5956f5050f33 12/16/22 13:15:14.055
    STEP: Creating the pod 12/16/22 13:15:14.06
    Dec 16 13:15:14.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc" in namespace "projected-3945" to be "running and ready"
    Dec 16 13:15:14.071: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158798ms
    Dec 16 13:15:14.071: INFO: The phase of Pod pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:15:16.076: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988032s
    Dec 16 13:15:16.076: INFO: The phase of Pod pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc is Running (Ready = true)
    Dec 16 13:15:16.076: INFO: Pod "pod-projected-secrets-e134c192-4255-436f-872b-fc55fe75d7cc" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-4bd97f23-d342-40fe-bd4b-5c87e1a84ac9 12/16/22 13:15:16.104
    STEP: Updating secret s-test-opt-upd-b9a2f49e-55d8-49b6-9a78-5956f5050f33 12/16/22 13:15:16.11
    STEP: Creating secret with name s-test-opt-create-b4dfe52d-321f-49dd-a82d-32faf95a6833 12/16/22 13:15:16.115
    STEP: waiting to observe update in volume 12/16/22 13:15:16.119
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:18.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3945" for this suite. 12/16/22 13:15:18.155
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:18.161
Dec 16 13:15:18.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:15:18.162
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:18.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:18.18
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-2868f578-1f38-4a12-84f7-107cf866d096 12/16/22 13:15:18.182
STEP: Creating a pod to test consume secrets 12/16/22 13:15:18.187
Dec 16 13:15:18.196: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7" in namespace "projected-2368" to be "Succeeded or Failed"
Dec 16 13:15:18.199: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200432ms
Dec 16 13:15:20.204: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823738s
Dec 16 13:15:22.206: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010099757s
STEP: Saw pod success 12/16/22 13:15:22.206
Dec 16 13:15:22.206: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7" satisfied condition "Succeeded or Failed"
Dec 16 13:15:22.209: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:15:22.218
Dec 16 13:15:22.229: INFO: Waiting for pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 to disappear
Dec 16 13:15:22.232: INFO: Pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:22.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2368" for this suite. 12/16/22 13:15:22.24
------------------------------
• [4.085 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:18.161
    Dec 16 13:15:18.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:15:18.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:18.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:18.18
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-2868f578-1f38-4a12-84f7-107cf866d096 12/16/22 13:15:18.182
    STEP: Creating a pod to test consume secrets 12/16/22 13:15:18.187
    Dec 16 13:15:18.196: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7" in namespace "projected-2368" to be "Succeeded or Failed"
    Dec 16 13:15:18.199: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200432ms
    Dec 16 13:15:20.204: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823738s
    Dec 16 13:15:22.206: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010099757s
    STEP: Saw pod success 12/16/22 13:15:22.206
    Dec 16 13:15:22.206: INFO: Pod "pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7" satisfied condition "Succeeded or Failed"
    Dec 16 13:15:22.209: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:15:22.218
    Dec 16 13:15:22.229: INFO: Waiting for pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 to disappear
    Dec 16 13:15:22.232: INFO: Pod pod-projected-secrets-6533d97a-2276-4321-9556-cddc1e7d6dc7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:22.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2368" for this suite. 12/16/22 13:15:22.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:22.247
Dec 16 13:15:22.247: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 13:15:22.248
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:22.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:22.273
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Dec 16 13:15:22.275: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/16/22 13:15:23.288
STEP: Checking rc "condition-test" has the desired failure condition set 12/16/22 13:15:23.293
STEP: Scaling down rc "condition-test" to satisfy pod quota 12/16/22 13:15:24.302
Dec 16 13:15:24.409: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 12/16/22 13:15:24.409
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:25.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9849" for this suite. 12/16/22 13:15:25.782
------------------------------
• [SLOW TEST] [5.224 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:22.247
    Dec 16 13:15:22.247: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 13:15:22.248
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:22.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:22.273
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Dec 16 13:15:22.275: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/16/22 13:15:23.288
    STEP: Checking rc "condition-test" has the desired failure condition set 12/16/22 13:15:23.293
    STEP: Scaling down rc "condition-test" to satisfy pod quota 12/16/22 13:15:24.302
    Dec 16 13:15:24.409: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 12/16/22 13:15:24.409
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:25.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9849" for this suite. 12/16/22 13:15:25.782
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:27.471
Dec 16 13:15:27.471: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 13:15:27.472
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:29.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:29.217
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2005 12/16/22 13:15:29.22
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Dec 16 13:15:29.796: INFO: Found 0 stateful pods, waiting for 1
Dec 16 13:15:39.804: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 12/16/22 13:15:39.811
W1216 13:15:39.821338      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 16 13:15:39.827: INFO: Found 1 stateful pods, waiting for 2
Dec 16 13:15:49.832: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:15:49.832: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 12/16/22 13:15:49.84
STEP: Delete all of the StatefulSets 12/16/22 13:15:49.844
STEP: Verify that StatefulSets have been deleted 12/16/22 13:15:49.851
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 13:15:49.855: INFO: Deleting all statefulset in ns statefulset-2005
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:49.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2005" for this suite. 12/16/22 13:15:49.868
------------------------------
• [SLOW TEST] [22.403 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:27.471
    Dec 16 13:15:27.471: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 13:15:27.472
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:29.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:29.217
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2005 12/16/22 13:15:29.22
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Dec 16 13:15:29.796: INFO: Found 0 stateful pods, waiting for 1
    Dec 16 13:15:39.804: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 12/16/22 13:15:39.811
    W1216 13:15:39.821338      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 16 13:15:39.827: INFO: Found 1 stateful pods, waiting for 2
    Dec 16 13:15:49.832: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:15:49.832: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 12/16/22 13:15:49.84
    STEP: Delete all of the StatefulSets 12/16/22 13:15:49.844
    STEP: Verify that StatefulSets have been deleted 12/16/22 13:15:49.851
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 13:15:49.855: INFO: Deleting all statefulset in ns statefulset-2005
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:49.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2005" for this suite. 12/16/22 13:15:49.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:49.875
Dec 16 13:15:49.875: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context 12/16/22 13:15:49.876
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:49.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:49.893
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/16/22 13:15:49.895
Dec 16 13:15:49.905: INFO: Waiting up to 5m0s for pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c" in namespace "security-context-6324" to be "Succeeded or Failed"
Dec 16 13:15:49.908: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252015ms
Dec 16 13:15:51.912: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007580134s
Dec 16 13:15:53.914: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009110693s
STEP: Saw pod success 12/16/22 13:15:53.914
Dec 16 13:15:53.914: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c" satisfied condition "Succeeded or Failed"
Dec 16 13:15:53.918: INFO: Trying to get logs from node pool-a3802-fsxxd pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c container test-container: <nil>
STEP: delete the pod 12/16/22 13:15:53.931
Dec 16 13:15:53.945: INFO: Waiting for pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c to disappear
Dec 16 13:15:53.948: INFO: Pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:53.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6324" for this suite. 12/16/22 13:15:53.959
------------------------------
• [4.091 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:49.875
    Dec 16 13:15:49.875: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context 12/16/22 13:15:49.876
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:49.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:49.893
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/16/22 13:15:49.895
    Dec 16 13:15:49.905: INFO: Waiting up to 5m0s for pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c" in namespace "security-context-6324" to be "Succeeded or Failed"
    Dec 16 13:15:49.908: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252015ms
    Dec 16 13:15:51.912: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007580134s
    Dec 16 13:15:53.914: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009110693s
    STEP: Saw pod success 12/16/22 13:15:53.914
    Dec 16 13:15:53.914: INFO: Pod "security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c" satisfied condition "Succeeded or Failed"
    Dec 16 13:15:53.918: INFO: Trying to get logs from node pool-a3802-fsxxd pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c container test-container: <nil>
    STEP: delete the pod 12/16/22 13:15:53.931
    Dec 16 13:15:53.945: INFO: Waiting for pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c to disappear
    Dec 16 13:15:53.948: INFO: Pod security-context-5df985d6-c73d-468b-ba36-c3d0a4078c3c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:53.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6324" for this suite. 12/16/22 13:15:53.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:53.967
Dec 16 13:15:53.967: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:15:53.968
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:53.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:53.987
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6473/secret-test-2b2078de-e9ca-4dae-a2c2-341ef5d74382 12/16/22 13:15:53.989
STEP: Creating a pod to test consume secrets 12/16/22 13:15:53.994
Dec 16 13:15:54.001: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4" in namespace "secrets-6473" to be "Succeeded or Failed"
Dec 16 13:15:54.004: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00633ms
Dec 16 13:15:56.008: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006892434s
Dec 16 13:15:58.011: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01022096s
STEP: Saw pod success 12/16/22 13:15:58.011
Dec 16 13:15:58.011: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4" satisfied condition "Succeeded or Failed"
Dec 16 13:15:58.015: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 container env-test: <nil>
STEP: delete the pod 12/16/22 13:15:58.023
Dec 16 13:15:58.032: INFO: Waiting for pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 to disappear
Dec 16 13:15:58.034: INFO: Pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:15:58.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6473" for this suite. 12/16/22 13:15:58.039
------------------------------
• [4.077 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:53.967
    Dec 16 13:15:53.967: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:15:53.968
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:53.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:53.987
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6473/secret-test-2b2078de-e9ca-4dae-a2c2-341ef5d74382 12/16/22 13:15:53.989
    STEP: Creating a pod to test consume secrets 12/16/22 13:15:53.994
    Dec 16 13:15:54.001: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4" in namespace "secrets-6473" to be "Succeeded or Failed"
    Dec 16 13:15:54.004: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00633ms
    Dec 16 13:15:56.008: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006892434s
    Dec 16 13:15:58.011: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01022096s
    STEP: Saw pod success 12/16/22 13:15:58.011
    Dec 16 13:15:58.011: INFO: Pod "pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4" satisfied condition "Succeeded or Failed"
    Dec 16 13:15:58.015: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 container env-test: <nil>
    STEP: delete the pod 12/16/22 13:15:58.023
    Dec 16 13:15:58.032: INFO: Waiting for pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 to disappear
    Dec 16 13:15:58.034: INFO: Pod pod-configmaps-fcd083d6-eaa9-439d-94b2-665391f485e4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:15:58.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6473" for this suite. 12/16/22 13:15:58.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:15:58.045
Dec 16 13:15:58.045: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:15:58.045
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:58.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:58.063
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-e8474415-7b52-48fb-93c7-bfb08eb9f818 12/16/22 13:15:58.065
STEP: Creating a pod to test consume configMaps 12/16/22 13:15:58.069
Dec 16 13:15:58.075: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1" in namespace "projected-8833" to be "Succeeded or Failed"
Dec 16 13:15:58.078: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880353ms
Dec 16 13:16:00.083: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753799s
Dec 16 13:16:02.084: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00837013s
STEP: Saw pod success 12/16/22 13:16:02.084
Dec 16 13:16:02.084: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1" satisfied condition "Succeeded or Failed"
Dec 16 13:16:02.089: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:16:02.096
Dec 16 13:16:02.107: INFO: Waiting for pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 to disappear
Dec 16 13:16:02.111: INFO: Pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8833" for this suite. 12/16/22 13:16:02.115
------------------------------
• [4.076 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:15:58.045
    Dec 16 13:15:58.045: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:15:58.045
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:15:58.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:15:58.063
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-e8474415-7b52-48fb-93c7-bfb08eb9f818 12/16/22 13:15:58.065
    STEP: Creating a pod to test consume configMaps 12/16/22 13:15:58.069
    Dec 16 13:15:58.075: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1" in namespace "projected-8833" to be "Succeeded or Failed"
    Dec 16 13:15:58.078: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880353ms
    Dec 16 13:16:00.083: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753799s
    Dec 16 13:16:02.084: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00837013s
    STEP: Saw pod success 12/16/22 13:16:02.084
    Dec 16 13:16:02.084: INFO: Pod "pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1" satisfied condition "Succeeded or Failed"
    Dec 16 13:16:02.089: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:16:02.096
    Dec 16 13:16:02.107: INFO: Waiting for pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 to disappear
    Dec 16 13:16:02.111: INFO: Pod pod-projected-configmaps-b020b039-a53b-4c9f-ba85-424af2db1dd1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8833" for this suite. 12/16/22 13:16:02.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:02.121
Dec 16 13:16:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:16:02.122
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:02.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:02.142
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Dec 16 13:16:02.145: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/16/22 13:16:03.965
Dec 16 13:16:03.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
Dec 16 13:16:04.643: INFO: stderr: ""
Dec 16 13:16:04.643: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 16 13:16:04.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 delete e2e-test-crd-publish-openapi-2243-crds test-foo'
Dec 16 13:16:04.719: INFO: stderr: ""
Dec 16 13:16:04.719: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 16 13:16:04.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
Dec 16 13:16:04.934: INFO: stderr: ""
Dec 16 13:16:04.934: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 16 13:16:04.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 delete e2e-test-crd-publish-openapi-2243-crds test-foo'
Dec 16 13:16:05.005: INFO: stderr: ""
Dec 16 13:16:05.005: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/16/22 13:16:05.005
Dec 16 13:16:05.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
Dec 16 13:16:05.195: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/16/22 13:16:05.195
Dec 16 13:16:05.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
Dec 16 13:16:05.726: INFO: rc: 1
Dec 16 13:16:05.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
Dec 16 13:16:05.930: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/16/22 13:16:05.931
Dec 16 13:16:05.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
Dec 16 13:16:06.130: INFO: rc: 1
Dec 16 13:16:06.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
Dec 16 13:16:06.341: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 12/16/22 13:16:06.341
Dec 16 13:16:06.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds'
Dec 16 13:16:06.535: INFO: stderr: ""
Dec 16 13:16:06.535: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 12/16/22 13:16:06.535
Dec 16 13:16:06.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.metadata'
Dec 16 13:16:06.730: INFO: stderr: ""
Dec 16 13:16:06.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 16 13:16:06.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec'
Dec 16 13:16:06.922: INFO: stderr: ""
Dec 16 13:16:06.922: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 16 13:16:06.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec.bars'
Dec 16 13:16:07.121: INFO: stderr: ""
Dec 16 13:16:07.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/16/22 13:16:07.121
Dec 16 13:16:07.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec.bars2'
Dec 16 13:16:07.305: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:09.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2173" for this suite. 12/16/22 13:16:09.082
------------------------------
• [SLOW TEST] [6.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:02.121
    Dec 16 13:16:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:16:02.122
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:02.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:02.142
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Dec 16 13:16:02.145: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/16/22 13:16:03.965
    Dec 16 13:16:03.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
    Dec 16 13:16:04.643: INFO: stderr: ""
    Dec 16 13:16:04.643: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 16 13:16:04.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 delete e2e-test-crd-publish-openapi-2243-crds test-foo'
    Dec 16 13:16:04.719: INFO: stderr: ""
    Dec 16 13:16:04.719: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Dec 16 13:16:04.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
    Dec 16 13:16:04.934: INFO: stderr: ""
    Dec 16 13:16:04.934: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 16 13:16:04.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 delete e2e-test-crd-publish-openapi-2243-crds test-foo'
    Dec 16 13:16:05.005: INFO: stderr: ""
    Dec 16 13:16:05.005: INFO: stdout: "e2e-test-crd-publish-openapi-2243-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/16/22 13:16:05.005
    Dec 16 13:16:05.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
    Dec 16 13:16:05.195: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/16/22 13:16:05.195
    Dec 16 13:16:05.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
    Dec 16 13:16:05.726: INFO: rc: 1
    Dec 16 13:16:05.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
    Dec 16 13:16:05.930: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/16/22 13:16:05.931
    Dec 16 13:16:05.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 create -f -'
    Dec 16 13:16:06.130: INFO: rc: 1
    Dec 16 13:16:06.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 --namespace=crd-publish-openapi-2173 apply -f -'
    Dec 16 13:16:06.341: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 12/16/22 13:16:06.341
    Dec 16 13:16:06.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds'
    Dec 16 13:16:06.535: INFO: stderr: ""
    Dec 16 13:16:06.535: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 12/16/22 13:16:06.535
    Dec 16 13:16:06.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.metadata'
    Dec 16 13:16:06.730: INFO: stderr: ""
    Dec 16 13:16:06.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Dec 16 13:16:06.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec'
    Dec 16 13:16:06.922: INFO: stderr: ""
    Dec 16 13:16:06.922: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Dec 16 13:16:06.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec.bars'
    Dec 16 13:16:07.121: INFO: stderr: ""
    Dec 16 13:16:07.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2243-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/16/22 13:16:07.121
    Dec 16 13:16:07.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2173 explain e2e-test-crd-publish-openapi-2243-crds.spec.bars2'
    Dec 16 13:16:07.305: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:09.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2173" for this suite. 12/16/22 13:16:09.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:09.089
Dec 16 13:16:09.089: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:16:09.089
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:09.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:09.103
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:16:09.106
Dec 16 13:16:09.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e" in namespace "downward-api-8193" to be "Succeeded or Failed"
Dec 16 13:16:09.118: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818091ms
Dec 16 13:16:11.122: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006739644s
Dec 16 13:16:13.123: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748258s
STEP: Saw pod success 12/16/22 13:16:13.123
Dec 16 13:16:13.123: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e" satisfied condition "Succeeded or Failed"
Dec 16 13:16:13.126: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e container client-container: <nil>
STEP: delete the pod 12/16/22 13:16:13.174
Dec 16 13:16:13.185: INFO: Waiting for pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e to disappear
Dec 16 13:16:13.188: INFO: Pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:13.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8193" for this suite. 12/16/22 13:16:13.191
------------------------------
• [4.108 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:09.089
    Dec 16 13:16:09.089: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:16:09.089
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:09.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:09.103
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:16:09.106
    Dec 16 13:16:09.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e" in namespace "downward-api-8193" to be "Succeeded or Failed"
    Dec 16 13:16:09.118: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818091ms
    Dec 16 13:16:11.122: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006739644s
    Dec 16 13:16:13.123: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748258s
    STEP: Saw pod success 12/16/22 13:16:13.123
    Dec 16 13:16:13.123: INFO: Pod "downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e" satisfied condition "Succeeded or Failed"
    Dec 16 13:16:13.126: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e container client-container: <nil>
    STEP: delete the pod 12/16/22 13:16:13.174
    Dec 16 13:16:13.185: INFO: Waiting for pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e to disappear
    Dec 16 13:16:13.188: INFO: Pod downwardapi-volume-0330a8b4-36cb-4af2-abba-4df94251c80e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:13.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8193" for this suite. 12/16/22 13:16:13.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:13.197
Dec 16 13:16:13.197: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename runtimeclass 12/16/22 13:16:13.198
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:13.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:13.212
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6456" for this suite. 12/16/22 13:16:13.225
------------------------------
• [0.033 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:13.197
    Dec 16 13:16:13.197: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename runtimeclass 12/16/22 13:16:13.198
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:13.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:13.212
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6456" for this suite. 12/16/22 13:16:13.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:13.231
Dec 16 13:16:13.231: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svc-latency 12/16/22 13:16:13.232
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:13.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:13.248
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Dec 16 13:16:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8834 12/16/22 13:16:13.251
I1216 13:16:13.257201      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8834, replica count: 1
I1216 13:16:14.308368      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1216 13:16:15.308534      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 13:16:15.430: INFO: Created: latency-svc-thdzq
Dec 16 13:16:15.437: INFO: Got endpoints: latency-svc-thdzq [28.31765ms]
Dec 16 13:16:15.459: INFO: Created: latency-svc-fzjkc
Dec 16 13:16:15.466: INFO: Got endpoints: latency-svc-fzjkc [28.351528ms]
Dec 16 13:16:15.470: INFO: Created: latency-svc-trr4b
Dec 16 13:16:15.474: INFO: Got endpoints: latency-svc-trr4b [36.556554ms]
Dec 16 13:16:15.481: INFO: Created: latency-svc-fvd74
Dec 16 13:16:15.486: INFO: Got endpoints: latency-svc-fvd74 [48.92558ms]
Dec 16 13:16:15.492: INFO: Created: latency-svc-h6wfl
Dec 16 13:16:15.499: INFO: Got endpoints: latency-svc-h6wfl [61.941549ms]
Dec 16 13:16:15.502: INFO: Created: latency-svc-2rgxv
Dec 16 13:16:15.509: INFO: Got endpoints: latency-svc-2rgxv [71.13677ms]
Dec 16 13:16:15.513: INFO: Created: latency-svc-fmm8g
Dec 16 13:16:15.520: INFO: Got endpoints: latency-svc-fmm8g [82.411845ms]
Dec 16 13:16:15.523: INFO: Created: latency-svc-vd2cw
Dec 16 13:16:15.531: INFO: Got endpoints: latency-svc-vd2cw [93.769363ms]
Dec 16 13:16:15.532: INFO: Created: latency-svc-85282
Dec 16 13:16:15.538: INFO: Got endpoints: latency-svc-85282 [100.577383ms]
Dec 16 13:16:15.542: INFO: Created: latency-svc-9pvv8
Dec 16 13:16:15.550: INFO: Got endpoints: latency-svc-9pvv8 [112.537581ms]
Dec 16 13:16:15.553: INFO: Created: latency-svc-q67xm
Dec 16 13:16:15.558: INFO: Got endpoints: latency-svc-q67xm [121.009715ms]
Dec 16 13:16:15.561: INFO: Created: latency-svc-tscds
Dec 16 13:16:15.573: INFO: Got endpoints: latency-svc-tscds [135.050449ms]
Dec 16 13:16:15.579: INFO: Created: latency-svc-r5qrl
Dec 16 13:16:15.582: INFO: Got endpoints: latency-svc-r5qrl [143.97166ms]
Dec 16 13:16:15.587: INFO: Created: latency-svc-7df5w
Dec 16 13:16:15.595: INFO: Got endpoints: latency-svc-7df5w [156.937619ms]
Dec 16 13:16:15.598: INFO: Created: latency-svc-gsxq7
Dec 16 13:16:15.603: INFO: Got endpoints: latency-svc-gsxq7 [165.009872ms]
Dec 16 13:16:15.610: INFO: Created: latency-svc-xt4hv
Dec 16 13:16:15.616: INFO: Got endpoints: latency-svc-xt4hv [178.529192ms]
Dec 16 13:16:15.619: INFO: Created: latency-svc-8gkpc
Dec 16 13:16:15.627: INFO: Got endpoints: latency-svc-8gkpc [160.973007ms]
Dec 16 13:16:15.628: INFO: Created: latency-svc-kvg99
Dec 16 13:16:15.635: INFO: Got endpoints: latency-svc-kvg99 [160.722168ms]
Dec 16 13:16:15.639: INFO: Created: latency-svc-b4bb2
Dec 16 13:16:15.646: INFO: Got endpoints: latency-svc-b4bb2 [159.113851ms]
Dec 16 13:16:15.648: INFO: Created: latency-svc-lddf4
Dec 16 13:16:15.657: INFO: Got endpoints: latency-svc-lddf4 [157.801503ms]
Dec 16 13:16:15.657: INFO: Created: latency-svc-v54c6
Dec 16 13:16:15.661: INFO: Got endpoints: latency-svc-v54c6 [151.939714ms]
Dec 16 13:16:15.668: INFO: Created: latency-svc-pwxrz
Dec 16 13:16:15.681: INFO: Got endpoints: latency-svc-pwxrz [160.983811ms]
Dec 16 13:16:15.687: INFO: Created: latency-svc-q8jh2
Dec 16 13:16:15.694: INFO: Got endpoints: latency-svc-q8jh2 [162.038175ms]
Dec 16 13:16:15.695: INFO: Created: latency-svc-cb2sv
Dec 16 13:16:15.700: INFO: Got endpoints: latency-svc-cb2sv [161.387626ms]
Dec 16 13:16:15.706: INFO: Created: latency-svc-dc5kt
Dec 16 13:16:15.713: INFO: Got endpoints: latency-svc-dc5kt [162.863112ms]
Dec 16 13:16:15.715: INFO: Created: latency-svc-zsqhx
Dec 16 13:16:15.722: INFO: Got endpoints: latency-svc-zsqhx [163.345874ms]
Dec 16 13:16:15.726: INFO: Created: latency-svc-m99pd
Dec 16 13:16:15.731: INFO: Got endpoints: latency-svc-m99pd [157.831047ms]
Dec 16 13:16:15.737: INFO: Created: latency-svc-92pgq
Dec 16 13:16:15.744: INFO: Got endpoints: latency-svc-92pgq [161.988806ms]
Dec 16 13:16:15.750: INFO: Created: latency-svc-r9sxh
Dec 16 13:16:15.754: INFO: Created: latency-svc-j6vq6
Dec 16 13:16:15.763: INFO: Got endpoints: latency-svc-r9sxh [168.800238ms]
Dec 16 13:16:15.764: INFO: Got endpoints: latency-svc-j6vq6 [160.810635ms]
Dec 16 13:16:15.768: INFO: Created: latency-svc-mn2lk
Dec 16 13:16:15.775: INFO: Got endpoints: latency-svc-mn2lk [158.397671ms]
Dec 16 13:16:15.778: INFO: Created: latency-svc-pl8wg
Dec 16 13:16:15.784: INFO: Got endpoints: latency-svc-pl8wg [157.179724ms]
Dec 16 13:16:15.792: INFO: Created: latency-svc-d9gj7
Dec 16 13:16:15.797: INFO: Got endpoints: latency-svc-d9gj7 [161.971712ms]
Dec 16 13:16:15.806: INFO: Created: latency-svc-p4ss9
Dec 16 13:16:15.813: INFO: Got endpoints: latency-svc-p4ss9 [167.050945ms]
Dec 16 13:16:15.816: INFO: Created: latency-svc-xxn99
Dec 16 13:16:15.822: INFO: Got endpoints: latency-svc-xxn99 [164.684654ms]
Dec 16 13:16:15.827: INFO: Created: latency-svc-rxcpp
Dec 16 13:16:15.831: INFO: Got endpoints: latency-svc-rxcpp [170.560915ms]
Dec 16 13:16:15.840: INFO: Created: latency-svc-qg2ws
Dec 16 13:16:15.848: INFO: Got endpoints: latency-svc-qg2ws [167.086241ms]
Dec 16 13:16:15.877: INFO: Created: latency-svc-s6b7j
Dec 16 13:16:15.883: INFO: Got endpoints: latency-svc-s6b7j [189.118241ms]
Dec 16 13:16:15.894: INFO: Created: latency-svc-h72sn
Dec 16 13:16:15.898: INFO: Got endpoints: latency-svc-h72sn [198.732756ms]
Dec 16 13:16:15.939: INFO: Created: latency-svc-k8cps
Dec 16 13:16:15.941: INFO: Got endpoints: latency-svc-k8cps [228.494543ms]
Dec 16 13:16:15.948: INFO: Created: latency-svc-d742n
Dec 16 13:16:15.960: INFO: Created: latency-svc-vpcg7
Dec 16 13:16:15.980: INFO: Created: latency-svc-pkn2n
Dec 16 13:16:15.987: INFO: Got endpoints: latency-svc-d742n [264.774253ms]
Dec 16 13:16:15.990: INFO: Created: latency-svc-fpgr6
Dec 16 13:16:16.003: INFO: Created: latency-svc-gmx5c
Dec 16 13:16:16.016: INFO: Created: latency-svc-76n4t
Dec 16 13:16:16.024: INFO: Created: latency-svc-mjdpk
Dec 16 13:16:16.034: INFO: Got endpoints: latency-svc-vpcg7 [303.000579ms]
Dec 16 13:16:16.034: INFO: Created: latency-svc-6fjkn
Dec 16 13:16:16.048: INFO: Created: latency-svc-2wrl2
Dec 16 13:16:16.083: INFO: Got endpoints: latency-svc-pkn2n [339.760139ms]
Dec 16 13:16:16.084: INFO: Created: latency-svc-ccp65
Dec 16 13:16:16.095: INFO: Created: latency-svc-j66gr
Dec 16 13:16:16.107: INFO: Created: latency-svc-4zngj
Dec 16 13:16:16.125: INFO: Created: latency-svc-4m5sg
Dec 16 13:16:16.134: INFO: Got endpoints: latency-svc-fpgr6 [370.082491ms]
Dec 16 13:16:16.135: INFO: Created: latency-svc-vmwjf
Dec 16 13:16:16.147: INFO: Created: latency-svc-cdl7l
Dec 16 13:16:16.159: INFO: Created: latency-svc-76k9r
Dec 16 13:16:16.170: INFO: Created: latency-svc-85c4l
Dec 16 13:16:16.183: INFO: Created: latency-svc-cjrh9
Dec 16 13:16:16.190: INFO: Got endpoints: latency-svc-gmx5c [426.727472ms]
Dec 16 13:16:16.192: INFO: Created: latency-svc-kzjjp
Dec 16 13:16:16.206: INFO: Created: latency-svc-jcfpt
Dec 16 13:16:16.239: INFO: Got endpoints: latency-svc-76n4t [464.060077ms]
Dec 16 13:16:16.278: INFO: Created: latency-svc-fz59x
Dec 16 13:16:16.295: INFO: Got endpoints: latency-svc-mjdpk [510.727964ms]
Dec 16 13:16:16.319: INFO: Created: latency-svc-nw9f4
Dec 16 13:16:16.335: INFO: Got endpoints: latency-svc-6fjkn [538.061381ms]
Dec 16 13:16:16.347: INFO: Created: latency-svc-m9mqp
Dec 16 13:16:16.385: INFO: Got endpoints: latency-svc-2wrl2 [571.946116ms]
Dec 16 13:16:16.402: INFO: Created: latency-svc-mz2c4
Dec 16 13:16:16.434: INFO: Got endpoints: latency-svc-ccp65 [612.445921ms]
Dec 16 13:16:16.449: INFO: Created: latency-svc-klx5k
Dec 16 13:16:16.485: INFO: Got endpoints: latency-svc-j66gr [653.357891ms]
Dec 16 13:16:16.500: INFO: Created: latency-svc-rqxr5
Dec 16 13:16:16.535: INFO: Got endpoints: latency-svc-4zngj [686.452415ms]
Dec 16 13:16:16.554: INFO: Created: latency-svc-9lrtw
Dec 16 13:16:16.584: INFO: Got endpoints: latency-svc-4m5sg [700.791127ms]
Dec 16 13:16:16.599: INFO: Created: latency-svc-hppmm
Dec 16 13:16:16.634: INFO: Got endpoints: latency-svc-vmwjf [735.457539ms]
Dec 16 13:16:16.649: INFO: Created: latency-svc-v9twv
Dec 16 13:16:16.684: INFO: Got endpoints: latency-svc-cdl7l [742.817969ms]
Dec 16 13:16:16.701: INFO: Created: latency-svc-mdz9c
Dec 16 13:16:16.735: INFO: Got endpoints: latency-svc-76k9r [748.057968ms]
Dec 16 13:16:16.750: INFO: Created: latency-svc-g4dpz
Dec 16 13:16:16.783: INFO: Got endpoints: latency-svc-85c4l [749.133134ms]
Dec 16 13:16:16.795: INFO: Created: latency-svc-qw5nb
Dec 16 13:16:16.834: INFO: Got endpoints: latency-svc-cjrh9 [750.556877ms]
Dec 16 13:16:16.859: INFO: Created: latency-svc-4fplc
Dec 16 13:16:16.884: INFO: Got endpoints: latency-svc-kzjjp [750.456531ms]
Dec 16 13:16:16.899: INFO: Created: latency-svc-gzhfv
Dec 16 13:16:16.933: INFO: Got endpoints: latency-svc-jcfpt [742.874612ms]
Dec 16 13:16:16.946: INFO: Created: latency-svc-cxc9k
Dec 16 13:16:16.984: INFO: Got endpoints: latency-svc-fz59x [745.344874ms]
Dec 16 13:16:17.000: INFO: Created: latency-svc-tmlrt
Dec 16 13:16:17.035: INFO: Got endpoints: latency-svc-nw9f4 [740.419008ms]
Dec 16 13:16:17.052: INFO: Created: latency-svc-94cr8
Dec 16 13:16:17.084: INFO: Got endpoints: latency-svc-m9mqp [749.422148ms]
Dec 16 13:16:17.098: INFO: Created: latency-svc-c2fsk
Dec 16 13:16:17.134: INFO: Got endpoints: latency-svc-mz2c4 [749.539999ms]
Dec 16 13:16:17.151: INFO: Created: latency-svc-6qk4x
Dec 16 13:16:17.191: INFO: Got endpoints: latency-svc-klx5k [756.492209ms]
Dec 16 13:16:17.226: INFO: Created: latency-svc-g5w7v
Dec 16 13:16:17.236: INFO: Got endpoints: latency-svc-rqxr5 [750.751228ms]
Dec 16 13:16:17.275: INFO: Created: latency-svc-6vqtv
Dec 16 13:16:17.294: INFO: Got endpoints: latency-svc-9lrtw [759.111971ms]
Dec 16 13:16:17.312: INFO: Created: latency-svc-vl2g5
Dec 16 13:16:17.336: INFO: Got endpoints: latency-svc-hppmm [752.620765ms]
Dec 16 13:16:17.351: INFO: Created: latency-svc-mnm7c
Dec 16 13:16:17.384: INFO: Got endpoints: latency-svc-v9twv [749.716538ms]
Dec 16 13:16:17.400: INFO: Created: latency-svc-q5zcb
Dec 16 13:16:17.434: INFO: Got endpoints: latency-svc-mdz9c [749.169014ms]
Dec 16 13:16:17.447: INFO: Created: latency-svc-dfx4w
Dec 16 13:16:17.493: INFO: Got endpoints: latency-svc-g4dpz [758.373968ms]
Dec 16 13:16:17.508: INFO: Created: latency-svc-2dq4m
Dec 16 13:16:17.535: INFO: Got endpoints: latency-svc-qw5nb [751.893515ms]
Dec 16 13:16:17.548: INFO: Created: latency-svc-ndc59
Dec 16 13:16:17.584: INFO: Got endpoints: latency-svc-4fplc [750.035051ms]
Dec 16 13:16:17.604: INFO: Created: latency-svc-vhmdz
Dec 16 13:16:17.634: INFO: Got endpoints: latency-svc-gzhfv [750.239638ms]
Dec 16 13:16:17.651: INFO: Created: latency-svc-tvqtk
Dec 16 13:16:17.685: INFO: Got endpoints: latency-svc-cxc9k [751.226625ms]
Dec 16 13:16:17.698: INFO: Created: latency-svc-95fpx
Dec 16 13:16:17.734: INFO: Got endpoints: latency-svc-tmlrt [750.190584ms]
Dec 16 13:16:17.752: INFO: Created: latency-svc-qptrd
Dec 16 13:16:17.785: INFO: Got endpoints: latency-svc-94cr8 [749.896515ms]
Dec 16 13:16:17.801: INFO: Created: latency-svc-v4ztq
Dec 16 13:16:17.834: INFO: Got endpoints: latency-svc-c2fsk [749.771771ms]
Dec 16 13:16:17.847: INFO: Created: latency-svc-9r62b
Dec 16 13:16:17.885: INFO: Got endpoints: latency-svc-6qk4x [750.661173ms]
Dec 16 13:16:17.903: INFO: Created: latency-svc-zbj5d
Dec 16 13:16:17.934: INFO: Got endpoints: latency-svc-g5w7v [742.636407ms]
Dec 16 13:16:17.952: INFO: Created: latency-svc-ld8sg
Dec 16 13:16:17.985: INFO: Got endpoints: latency-svc-6vqtv [748.936677ms]
Dec 16 13:16:17.999: INFO: Created: latency-svc-7cb98
Dec 16 13:16:18.034: INFO: Got endpoints: latency-svc-vl2g5 [739.45328ms]
Dec 16 13:16:18.048: INFO: Created: latency-svc-x24dc
Dec 16 13:16:18.084: INFO: Got endpoints: latency-svc-mnm7c [747.498424ms]
Dec 16 13:16:18.101: INFO: Created: latency-svc-vft62
Dec 16 13:16:18.134: INFO: Got endpoints: latency-svc-q5zcb [750.498289ms]
Dec 16 13:16:18.148: INFO: Created: latency-svc-bx7h2
Dec 16 13:16:18.184: INFO: Got endpoints: latency-svc-dfx4w [750.350436ms]
Dec 16 13:16:18.200: INFO: Created: latency-svc-rl2d8
Dec 16 13:16:18.234: INFO: Got endpoints: latency-svc-2dq4m [740.563669ms]
Dec 16 13:16:18.250: INFO: Created: latency-svc-hljj8
Dec 16 13:16:18.285: INFO: Got endpoints: latency-svc-ndc59 [749.955676ms]
Dec 16 13:16:18.297: INFO: Created: latency-svc-g9q9p
Dec 16 13:16:18.334: INFO: Got endpoints: latency-svc-vhmdz [749.903813ms]
Dec 16 13:16:18.347: INFO: Created: latency-svc-qfr4m
Dec 16 13:16:18.383: INFO: Got endpoints: latency-svc-tvqtk [748.934989ms]
Dec 16 13:16:18.398: INFO: Created: latency-svc-2hq8h
Dec 16 13:16:18.434: INFO: Got endpoints: latency-svc-95fpx [749.792137ms]
Dec 16 13:16:18.447: INFO: Created: latency-svc-pfl8x
Dec 16 13:16:18.482: INFO: Got endpoints: latency-svc-qptrd [747.866848ms]
Dec 16 13:16:18.497: INFO: Created: latency-svc-zz568
Dec 16 13:16:18.534: INFO: Got endpoints: latency-svc-v4ztq [749.200994ms]
Dec 16 13:16:18.551: INFO: Created: latency-svc-5ht7j
Dec 16 13:16:18.585: INFO: Got endpoints: latency-svc-9r62b [750.448858ms]
Dec 16 13:16:18.598: INFO: Created: latency-svc-v7c45
Dec 16 13:16:18.635: INFO: Got endpoints: latency-svc-zbj5d [749.851363ms]
Dec 16 13:16:18.650: INFO: Created: latency-svc-hfzlb
Dec 16 13:16:18.683: INFO: Got endpoints: latency-svc-ld8sg [749.517879ms]
Dec 16 13:16:18.700: INFO: Created: latency-svc-sw7qz
Dec 16 13:16:18.734: INFO: Got endpoints: latency-svc-7cb98 [749.639234ms]
Dec 16 13:16:18.753: INFO: Created: latency-svc-t7cgz
Dec 16 13:16:18.785: INFO: Got endpoints: latency-svc-x24dc [751.222564ms]
Dec 16 13:16:18.799: INFO: Created: latency-svc-jc7hs
Dec 16 13:16:18.835: INFO: Got endpoints: latency-svc-vft62 [751.021418ms]
Dec 16 13:16:18.852: INFO: Created: latency-svc-mc2tw
Dec 16 13:16:18.884: INFO: Got endpoints: latency-svc-bx7h2 [749.832674ms]
Dec 16 13:16:18.898: INFO: Created: latency-svc-h67sd
Dec 16 13:16:18.935: INFO: Got endpoints: latency-svc-rl2d8 [750.778561ms]
Dec 16 13:16:18.948: INFO: Created: latency-svc-ns2fr
Dec 16 13:16:18.984: INFO: Got endpoints: latency-svc-hljj8 [750.444006ms]
Dec 16 13:16:18.999: INFO: Created: latency-svc-56tps
Dec 16 13:16:19.035: INFO: Got endpoints: latency-svc-g9q9p [749.797122ms]
Dec 16 13:16:19.048: INFO: Created: latency-svc-frh6k
Dec 16 13:16:19.083: INFO: Got endpoints: latency-svc-qfr4m [749.152886ms]
Dec 16 13:16:19.097: INFO: Created: latency-svc-m8t69
Dec 16 13:16:19.134: INFO: Got endpoints: latency-svc-2hq8h [750.585762ms]
Dec 16 13:16:19.148: INFO: Created: latency-svc-9bvxl
Dec 16 13:16:19.185: INFO: Got endpoints: latency-svc-pfl8x [750.905778ms]
Dec 16 13:16:19.198: INFO: Created: latency-svc-m7wbs
Dec 16 13:16:19.234: INFO: Got endpoints: latency-svc-zz568 [751.227222ms]
Dec 16 13:16:19.251: INFO: Created: latency-svc-vmnmb
Dec 16 13:16:19.295: INFO: Got endpoints: latency-svc-5ht7j [760.434137ms]
Dec 16 13:16:19.308: INFO: Created: latency-svc-jdtqx
Dec 16 13:16:19.334: INFO: Got endpoints: latency-svc-v7c45 [749.568016ms]
Dec 16 13:16:19.348: INFO: Created: latency-svc-xz767
Dec 16 13:16:19.383: INFO: Got endpoints: latency-svc-hfzlb [748.286945ms]
Dec 16 13:16:19.397: INFO: Created: latency-svc-4g7nw
Dec 16 13:16:19.434: INFO: Got endpoints: latency-svc-sw7qz [751.079125ms]
Dec 16 13:16:19.449: INFO: Created: latency-svc-mdlgh
Dec 16 13:16:19.483: INFO: Got endpoints: latency-svc-t7cgz [748.799201ms]
Dec 16 13:16:19.497: INFO: Created: latency-svc-5gllg
Dec 16 13:16:19.534: INFO: Got endpoints: latency-svc-jc7hs [749.363329ms]
Dec 16 13:16:19.549: INFO: Created: latency-svc-fjcnm
Dec 16 13:16:19.585: INFO: Got endpoints: latency-svc-mc2tw [750.032604ms]
Dec 16 13:16:19.601: INFO: Created: latency-svc-b9nmm
Dec 16 13:16:19.635: INFO: Got endpoints: latency-svc-h67sd [750.961416ms]
Dec 16 13:16:19.649: INFO: Created: latency-svc-5k8zp
Dec 16 13:16:19.684: INFO: Got endpoints: latency-svc-ns2fr [749.404622ms]
Dec 16 13:16:19.701: INFO: Created: latency-svc-thxm8
Dec 16 13:16:19.734: INFO: Got endpoints: latency-svc-56tps [750.172456ms]
Dec 16 13:16:19.751: INFO: Created: latency-svc-59v7z
Dec 16 13:16:19.786: INFO: Got endpoints: latency-svc-frh6k [751.180995ms]
Dec 16 13:16:19.802: INFO: Created: latency-svc-rtrkw
Dec 16 13:16:19.834: INFO: Got endpoints: latency-svc-m8t69 [750.862487ms]
Dec 16 13:16:19.848: INFO: Created: latency-svc-lf2c7
Dec 16 13:16:19.884: INFO: Got endpoints: latency-svc-9bvxl [750.23281ms]
Dec 16 13:16:19.907: INFO: Created: latency-svc-l5gnt
Dec 16 13:16:19.936: INFO: Got endpoints: latency-svc-m7wbs [750.257245ms]
Dec 16 13:16:19.951: INFO: Created: latency-svc-25bkf
Dec 16 13:16:19.985: INFO: Got endpoints: latency-svc-vmnmb [750.984756ms]
Dec 16 13:16:19.998: INFO: Created: latency-svc-zzl64
Dec 16 13:16:20.033: INFO: Got endpoints: latency-svc-jdtqx [738.500106ms]
Dec 16 13:16:20.048: INFO: Created: latency-svc-nwwr2
Dec 16 13:16:20.085: INFO: Got endpoints: latency-svc-xz767 [750.162215ms]
Dec 16 13:16:20.102: INFO: Created: latency-svc-nsjjn
Dec 16 13:16:20.135: INFO: Got endpoints: latency-svc-4g7nw [751.324836ms]
Dec 16 13:16:20.149: INFO: Created: latency-svc-9ztqm
Dec 16 13:16:20.184: INFO: Got endpoints: latency-svc-mdlgh [749.696241ms]
Dec 16 13:16:20.199: INFO: Created: latency-svc-c68rl
Dec 16 13:16:20.235: INFO: Got endpoints: latency-svc-5gllg [751.530774ms]
Dec 16 13:16:20.249: INFO: Created: latency-svc-htwfm
Dec 16 13:16:20.284: INFO: Got endpoints: latency-svc-fjcnm [749.784681ms]
Dec 16 13:16:20.297: INFO: Created: latency-svc-p2v7x
Dec 16 13:16:20.333: INFO: Got endpoints: latency-svc-b9nmm [747.86383ms]
Dec 16 13:16:20.348: INFO: Created: latency-svc-dzzk7
Dec 16 13:16:20.383: INFO: Got endpoints: latency-svc-5k8zp [747.881509ms]
Dec 16 13:16:20.397: INFO: Created: latency-svc-jvrxk
Dec 16 13:16:20.434: INFO: Got endpoints: latency-svc-thxm8 [750.007006ms]
Dec 16 13:16:20.449: INFO: Created: latency-svc-p65cr
Dec 16 13:16:20.484: INFO: Got endpoints: latency-svc-59v7z [749.829026ms]
Dec 16 13:16:20.499: INFO: Created: latency-svc-6rr6j
Dec 16 13:16:20.533: INFO: Got endpoints: latency-svc-rtrkw [747.417576ms]
Dec 16 13:16:20.547: INFO: Created: latency-svc-2ftxh
Dec 16 13:16:20.584: INFO: Got endpoints: latency-svc-lf2c7 [750.005395ms]
Dec 16 13:16:20.600: INFO: Created: latency-svc-qsj8n
Dec 16 13:16:20.635: INFO: Got endpoints: latency-svc-l5gnt [750.330324ms]
Dec 16 13:16:20.649: INFO: Created: latency-svc-97pvz
Dec 16 13:16:20.683: INFO: Got endpoints: latency-svc-25bkf [747.491171ms]
Dec 16 13:16:20.702: INFO: Created: latency-svc-xs6gh
Dec 16 13:16:20.734: INFO: Got endpoints: latency-svc-zzl64 [749.433758ms]
Dec 16 13:16:20.751: INFO: Created: latency-svc-hqt5z
Dec 16 13:16:20.783: INFO: Got endpoints: latency-svc-nwwr2 [749.561528ms]
Dec 16 13:16:20.802: INFO: Created: latency-svc-9c2bf
Dec 16 13:16:20.834: INFO: Got endpoints: latency-svc-nsjjn [749.29639ms]
Dec 16 13:16:20.847: INFO: Created: latency-svc-g9lht
Dec 16 13:16:20.885: INFO: Got endpoints: latency-svc-9ztqm [750.108723ms]
Dec 16 13:16:20.901: INFO: Created: latency-svc-pgxnd
Dec 16 13:16:20.933: INFO: Got endpoints: latency-svc-c68rl [748.992798ms]
Dec 16 13:16:20.946: INFO: Created: latency-svc-m5rq9
Dec 16 13:16:20.985: INFO: Got endpoints: latency-svc-htwfm [750.318868ms]
Dec 16 13:16:21.001: INFO: Created: latency-svc-6l9n4
Dec 16 13:16:21.035: INFO: Got endpoints: latency-svc-p2v7x [750.579134ms]
Dec 16 13:16:21.052: INFO: Created: latency-svc-2v8g6
Dec 16 13:16:21.083: INFO: Got endpoints: latency-svc-dzzk7 [749.514704ms]
Dec 16 13:16:21.097: INFO: Created: latency-svc-gphfg
Dec 16 13:16:21.135: INFO: Got endpoints: latency-svc-jvrxk [751.80028ms]
Dec 16 13:16:21.148: INFO: Created: latency-svc-29thn
Dec 16 13:16:21.184: INFO: Got endpoints: latency-svc-p65cr [750.12841ms]
Dec 16 13:16:21.217: INFO: Created: latency-svc-p8kkh
Dec 16 13:16:21.244: INFO: Got endpoints: latency-svc-6rr6j [759.574136ms]
Dec 16 13:16:21.275: INFO: Created: latency-svc-5wc9l
Dec 16 13:16:21.285: INFO: Got endpoints: latency-svc-2ftxh [751.293447ms]
Dec 16 13:16:21.308: INFO: Created: latency-svc-lk8j9
Dec 16 13:16:21.337: INFO: Got endpoints: latency-svc-qsj8n [753.137839ms]
Dec 16 13:16:21.353: INFO: Created: latency-svc-kv4wc
Dec 16 13:16:21.384: INFO: Got endpoints: latency-svc-97pvz [748.976694ms]
Dec 16 13:16:21.398: INFO: Created: latency-svc-7spzd
Dec 16 13:16:21.434: INFO: Got endpoints: latency-svc-xs6gh [750.605131ms]
Dec 16 13:16:21.447: INFO: Created: latency-svc-rktzk
Dec 16 13:16:21.484: INFO: Got endpoints: latency-svc-hqt5z [750.172892ms]
Dec 16 13:16:21.501: INFO: Created: latency-svc-kgf44
Dec 16 13:16:21.535: INFO: Got endpoints: latency-svc-9c2bf [751.89611ms]
Dec 16 13:16:21.549: INFO: Created: latency-svc-6gw8n
Dec 16 13:16:21.583: INFO: Got endpoints: latency-svc-g9lht [748.984116ms]
Dec 16 13:16:21.596: INFO: Created: latency-svc-dzj95
Dec 16 13:16:21.635: INFO: Got endpoints: latency-svc-pgxnd [749.865198ms]
Dec 16 13:16:21.654: INFO: Created: latency-svc-mh4pm
Dec 16 13:16:21.684: INFO: Got endpoints: latency-svc-m5rq9 [750.806307ms]
Dec 16 13:16:21.696: INFO: Created: latency-svc-pzj6h
Dec 16 13:16:21.736: INFO: Got endpoints: latency-svc-6l9n4 [751.410924ms]
Dec 16 13:16:21.750: INFO: Created: latency-svc-z2sw7
Dec 16 13:16:21.785: INFO: Got endpoints: latency-svc-2v8g6 [749.662086ms]
Dec 16 13:16:21.800: INFO: Created: latency-svc-ltt52
Dec 16 13:16:21.834: INFO: Got endpoints: latency-svc-gphfg [750.867325ms]
Dec 16 13:16:21.847: INFO: Created: latency-svc-7ggzv
Dec 16 13:16:21.884: INFO: Got endpoints: latency-svc-29thn [749.161516ms]
Dec 16 13:16:21.897: INFO: Created: latency-svc-dhbbd
Dec 16 13:16:21.934: INFO: Got endpoints: latency-svc-p8kkh [749.480949ms]
Dec 16 13:16:21.948: INFO: Created: latency-svc-vdgwj
Dec 16 13:16:21.985: INFO: Got endpoints: latency-svc-5wc9l [741.095643ms]
Dec 16 13:16:21.999: INFO: Created: latency-svc-8r9g9
Dec 16 13:16:22.035: INFO: Got endpoints: latency-svc-lk8j9 [750.309529ms]
Dec 16 13:16:22.049: INFO: Created: latency-svc-58xgt
Dec 16 13:16:22.084: INFO: Got endpoints: latency-svc-kv4wc [746.593431ms]
Dec 16 13:16:22.099: INFO: Created: latency-svc-bgm8c
Dec 16 13:16:22.134: INFO: Got endpoints: latency-svc-7spzd [750.621736ms]
Dec 16 13:16:22.149: INFO: Created: latency-svc-mg87h
Dec 16 13:16:22.184: INFO: Got endpoints: latency-svc-rktzk [749.685549ms]
Dec 16 13:16:22.197: INFO: Created: latency-svc-lm775
Dec 16 13:16:22.235: INFO: Got endpoints: latency-svc-kgf44 [750.336091ms]
Dec 16 13:16:22.250: INFO: Created: latency-svc-j44nd
Dec 16 13:16:22.287: INFO: Got endpoints: latency-svc-6gw8n [751.661676ms]
Dec 16 13:16:22.301: INFO: Created: latency-svc-5pskr
Dec 16 13:16:22.334: INFO: Got endpoints: latency-svc-dzj95 [751.499738ms]
Dec 16 13:16:22.349: INFO: Created: latency-svc-8tgc6
Dec 16 13:16:22.386: INFO: Got endpoints: latency-svc-mh4pm [750.65111ms]
Dec 16 13:16:22.411: INFO: Created: latency-svc-kmr46
Dec 16 13:16:22.435: INFO: Got endpoints: latency-svc-pzj6h [750.828965ms]
Dec 16 13:16:22.449: INFO: Created: latency-svc-js8px
Dec 16 13:16:22.484: INFO: Got endpoints: latency-svc-z2sw7 [747.220949ms]
Dec 16 13:16:22.498: INFO: Created: latency-svc-ttm6w
Dec 16 13:16:22.534: INFO: Got endpoints: latency-svc-ltt52 [749.343588ms]
Dec 16 13:16:22.549: INFO: Created: latency-svc-g8t5t
Dec 16 13:16:22.585: INFO: Got endpoints: latency-svc-7ggzv [750.859546ms]
Dec 16 13:16:22.607: INFO: Created: latency-svc-thhwm
Dec 16 13:16:22.633: INFO: Got endpoints: latency-svc-dhbbd [748.809029ms]
Dec 16 13:16:22.660: INFO: Created: latency-svc-66nz6
Dec 16 13:16:22.691: INFO: Got endpoints: latency-svc-vdgwj [756.753086ms]
Dec 16 13:16:22.728: INFO: Created: latency-svc-z78sx
Dec 16 13:16:22.733: INFO: Got endpoints: latency-svc-8r9g9 [748.20955ms]
Dec 16 13:16:22.747: INFO: Created: latency-svc-k48jv
Dec 16 13:16:22.784: INFO: Got endpoints: latency-svc-58xgt [748.569866ms]
Dec 16 13:16:22.796: INFO: Created: latency-svc-k9nr5
Dec 16 13:16:22.835: INFO: Got endpoints: latency-svc-bgm8c [751.102984ms]
Dec 16 13:16:22.848: INFO: Created: latency-svc-gck8f
Dec 16 13:16:22.884: INFO: Got endpoints: latency-svc-mg87h [749.610921ms]
Dec 16 13:16:22.897: INFO: Created: latency-svc-2mf6g
Dec 16 13:16:22.934: INFO: Got endpoints: latency-svc-lm775 [750.18431ms]
Dec 16 13:16:22.946: INFO: Created: latency-svc-q7m4k
Dec 16 13:16:22.984: INFO: Got endpoints: latency-svc-j44nd [749.257901ms]
Dec 16 13:16:22.998: INFO: Created: latency-svc-bg7x2
Dec 16 13:16:23.034: INFO: Got endpoints: latency-svc-5pskr [746.797628ms]
Dec 16 13:16:23.046: INFO: Created: latency-svc-klvvt
Dec 16 13:16:23.084: INFO: Got endpoints: latency-svc-8tgc6 [749.932766ms]
Dec 16 13:16:23.098: INFO: Created: latency-svc-4zzf2
Dec 16 13:16:23.135: INFO: Got endpoints: latency-svc-kmr46 [749.124059ms]
Dec 16 13:16:23.147: INFO: Created: latency-svc-8gvw5
Dec 16 13:16:23.184: INFO: Got endpoints: latency-svc-js8px [748.635427ms]
Dec 16 13:16:23.196: INFO: Created: latency-svc-ghn2k
Dec 16 13:16:23.233: INFO: Got endpoints: latency-svc-ttm6w [749.010126ms]
Dec 16 13:16:23.247: INFO: Created: latency-svc-lgnms
Dec 16 13:16:23.284: INFO: Got endpoints: latency-svc-g8t5t [750.107219ms]
Dec 16 13:16:23.333: INFO: Got endpoints: latency-svc-thhwm [748.410096ms]
Dec 16 13:16:23.384: INFO: Got endpoints: latency-svc-66nz6 [751.085972ms]
Dec 16 13:16:23.434: INFO: Got endpoints: latency-svc-z78sx [743.157828ms]
Dec 16 13:16:23.483: INFO: Got endpoints: latency-svc-k48jv [750.190061ms]
Dec 16 13:16:23.533: INFO: Got endpoints: latency-svc-k9nr5 [749.016807ms]
Dec 16 13:16:23.585: INFO: Got endpoints: latency-svc-gck8f [749.496235ms]
Dec 16 13:16:23.634: INFO: Got endpoints: latency-svc-2mf6g [750.146194ms]
Dec 16 13:16:23.685: INFO: Got endpoints: latency-svc-q7m4k [750.752144ms]
Dec 16 13:16:23.734: INFO: Got endpoints: latency-svc-bg7x2 [750.159807ms]
Dec 16 13:16:23.784: INFO: Got endpoints: latency-svc-klvvt [750.126066ms]
Dec 16 13:16:23.835: INFO: Got endpoints: latency-svc-4zzf2 [750.422979ms]
Dec 16 13:16:23.885: INFO: Got endpoints: latency-svc-8gvw5 [750.163846ms]
Dec 16 13:16:23.935: INFO: Got endpoints: latency-svc-ghn2k [751.666314ms]
Dec 16 13:16:23.985: INFO: Got endpoints: latency-svc-lgnms [752.278993ms]
Dec 16 13:16:23.985: INFO: Latencies: [28.351528ms 36.556554ms 48.92558ms 61.941549ms 71.13677ms 82.411845ms 93.769363ms 100.577383ms 112.537581ms 121.009715ms 135.050449ms 143.97166ms 151.939714ms 156.937619ms 157.179724ms 157.801503ms 157.831047ms 158.397671ms 159.113851ms 160.722168ms 160.810635ms 160.973007ms 160.983811ms 161.387626ms 161.971712ms 161.988806ms 162.038175ms 162.863112ms 163.345874ms 164.684654ms 165.009872ms 167.050945ms 167.086241ms 168.800238ms 170.560915ms 178.529192ms 189.118241ms 198.732756ms 228.494543ms 264.774253ms 303.000579ms 339.760139ms 370.082491ms 426.727472ms 464.060077ms 510.727964ms 538.061381ms 571.946116ms 612.445921ms 653.357891ms 686.452415ms 700.791127ms 735.457539ms 738.500106ms 739.45328ms 740.419008ms 740.563669ms 741.095643ms 742.636407ms 742.817969ms 742.874612ms 743.157828ms 745.344874ms 746.593431ms 746.797628ms 747.220949ms 747.417576ms 747.491171ms 747.498424ms 747.86383ms 747.866848ms 747.881509ms 748.057968ms 748.20955ms 748.286945ms 748.410096ms 748.569866ms 748.635427ms 748.799201ms 748.809029ms 748.934989ms 748.936677ms 748.976694ms 748.984116ms 748.992798ms 749.010126ms 749.016807ms 749.124059ms 749.133134ms 749.152886ms 749.161516ms 749.169014ms 749.200994ms 749.257901ms 749.29639ms 749.343588ms 749.363329ms 749.404622ms 749.422148ms 749.433758ms 749.480949ms 749.496235ms 749.514704ms 749.517879ms 749.539999ms 749.561528ms 749.568016ms 749.610921ms 749.639234ms 749.662086ms 749.685549ms 749.696241ms 749.716538ms 749.771771ms 749.784681ms 749.792137ms 749.797122ms 749.829026ms 749.832674ms 749.851363ms 749.865198ms 749.896515ms 749.903813ms 749.932766ms 749.955676ms 750.005395ms 750.007006ms 750.032604ms 750.035051ms 750.107219ms 750.108723ms 750.126066ms 750.12841ms 750.146194ms 750.159807ms 750.162215ms 750.163846ms 750.172456ms 750.172892ms 750.18431ms 750.190061ms 750.190584ms 750.23281ms 750.239638ms 750.257245ms 750.309529ms 750.318868ms 750.330324ms 750.336091ms 750.350436ms 750.422979ms 750.444006ms 750.448858ms 750.456531ms 750.498289ms 750.556877ms 750.579134ms 750.585762ms 750.605131ms 750.621736ms 750.65111ms 750.661173ms 750.751228ms 750.752144ms 750.778561ms 750.806307ms 750.828965ms 750.859546ms 750.862487ms 750.867325ms 750.905778ms 750.961416ms 750.984756ms 751.021418ms 751.079125ms 751.085972ms 751.102984ms 751.180995ms 751.222564ms 751.226625ms 751.227222ms 751.293447ms 751.324836ms 751.410924ms 751.499738ms 751.530774ms 751.661676ms 751.666314ms 751.80028ms 751.893515ms 751.89611ms 752.278993ms 752.620765ms 753.137839ms 756.492209ms 756.753086ms 758.373968ms 759.111971ms 759.574136ms 760.434137ms]
Dec 16 13:16:23.985: INFO: 50 %ile: 749.480949ms
Dec 16 13:16:23.985: INFO: 90 %ile: 751.227222ms
Dec 16 13:16:23.985: INFO: 99 %ile: 759.574136ms
Dec 16 13:16:23.985: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-8834" for this suite. 12/16/22 13:16:23.99
------------------------------
• [SLOW TEST] [10.766 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:13.231
    Dec 16 13:16:13.231: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svc-latency 12/16/22 13:16:13.232
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:13.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:13.248
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Dec 16 13:16:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8834 12/16/22 13:16:13.251
    I1216 13:16:13.257201      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8834, replica count: 1
    I1216 13:16:14.308368      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1216 13:16:15.308534      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 13:16:15.430: INFO: Created: latency-svc-thdzq
    Dec 16 13:16:15.437: INFO: Got endpoints: latency-svc-thdzq [28.31765ms]
    Dec 16 13:16:15.459: INFO: Created: latency-svc-fzjkc
    Dec 16 13:16:15.466: INFO: Got endpoints: latency-svc-fzjkc [28.351528ms]
    Dec 16 13:16:15.470: INFO: Created: latency-svc-trr4b
    Dec 16 13:16:15.474: INFO: Got endpoints: latency-svc-trr4b [36.556554ms]
    Dec 16 13:16:15.481: INFO: Created: latency-svc-fvd74
    Dec 16 13:16:15.486: INFO: Got endpoints: latency-svc-fvd74 [48.92558ms]
    Dec 16 13:16:15.492: INFO: Created: latency-svc-h6wfl
    Dec 16 13:16:15.499: INFO: Got endpoints: latency-svc-h6wfl [61.941549ms]
    Dec 16 13:16:15.502: INFO: Created: latency-svc-2rgxv
    Dec 16 13:16:15.509: INFO: Got endpoints: latency-svc-2rgxv [71.13677ms]
    Dec 16 13:16:15.513: INFO: Created: latency-svc-fmm8g
    Dec 16 13:16:15.520: INFO: Got endpoints: latency-svc-fmm8g [82.411845ms]
    Dec 16 13:16:15.523: INFO: Created: latency-svc-vd2cw
    Dec 16 13:16:15.531: INFO: Got endpoints: latency-svc-vd2cw [93.769363ms]
    Dec 16 13:16:15.532: INFO: Created: latency-svc-85282
    Dec 16 13:16:15.538: INFO: Got endpoints: latency-svc-85282 [100.577383ms]
    Dec 16 13:16:15.542: INFO: Created: latency-svc-9pvv8
    Dec 16 13:16:15.550: INFO: Got endpoints: latency-svc-9pvv8 [112.537581ms]
    Dec 16 13:16:15.553: INFO: Created: latency-svc-q67xm
    Dec 16 13:16:15.558: INFO: Got endpoints: latency-svc-q67xm [121.009715ms]
    Dec 16 13:16:15.561: INFO: Created: latency-svc-tscds
    Dec 16 13:16:15.573: INFO: Got endpoints: latency-svc-tscds [135.050449ms]
    Dec 16 13:16:15.579: INFO: Created: latency-svc-r5qrl
    Dec 16 13:16:15.582: INFO: Got endpoints: latency-svc-r5qrl [143.97166ms]
    Dec 16 13:16:15.587: INFO: Created: latency-svc-7df5w
    Dec 16 13:16:15.595: INFO: Got endpoints: latency-svc-7df5w [156.937619ms]
    Dec 16 13:16:15.598: INFO: Created: latency-svc-gsxq7
    Dec 16 13:16:15.603: INFO: Got endpoints: latency-svc-gsxq7 [165.009872ms]
    Dec 16 13:16:15.610: INFO: Created: latency-svc-xt4hv
    Dec 16 13:16:15.616: INFO: Got endpoints: latency-svc-xt4hv [178.529192ms]
    Dec 16 13:16:15.619: INFO: Created: latency-svc-8gkpc
    Dec 16 13:16:15.627: INFO: Got endpoints: latency-svc-8gkpc [160.973007ms]
    Dec 16 13:16:15.628: INFO: Created: latency-svc-kvg99
    Dec 16 13:16:15.635: INFO: Got endpoints: latency-svc-kvg99 [160.722168ms]
    Dec 16 13:16:15.639: INFO: Created: latency-svc-b4bb2
    Dec 16 13:16:15.646: INFO: Got endpoints: latency-svc-b4bb2 [159.113851ms]
    Dec 16 13:16:15.648: INFO: Created: latency-svc-lddf4
    Dec 16 13:16:15.657: INFO: Got endpoints: latency-svc-lddf4 [157.801503ms]
    Dec 16 13:16:15.657: INFO: Created: latency-svc-v54c6
    Dec 16 13:16:15.661: INFO: Got endpoints: latency-svc-v54c6 [151.939714ms]
    Dec 16 13:16:15.668: INFO: Created: latency-svc-pwxrz
    Dec 16 13:16:15.681: INFO: Got endpoints: latency-svc-pwxrz [160.983811ms]
    Dec 16 13:16:15.687: INFO: Created: latency-svc-q8jh2
    Dec 16 13:16:15.694: INFO: Got endpoints: latency-svc-q8jh2 [162.038175ms]
    Dec 16 13:16:15.695: INFO: Created: latency-svc-cb2sv
    Dec 16 13:16:15.700: INFO: Got endpoints: latency-svc-cb2sv [161.387626ms]
    Dec 16 13:16:15.706: INFO: Created: latency-svc-dc5kt
    Dec 16 13:16:15.713: INFO: Got endpoints: latency-svc-dc5kt [162.863112ms]
    Dec 16 13:16:15.715: INFO: Created: latency-svc-zsqhx
    Dec 16 13:16:15.722: INFO: Got endpoints: latency-svc-zsqhx [163.345874ms]
    Dec 16 13:16:15.726: INFO: Created: latency-svc-m99pd
    Dec 16 13:16:15.731: INFO: Got endpoints: latency-svc-m99pd [157.831047ms]
    Dec 16 13:16:15.737: INFO: Created: latency-svc-92pgq
    Dec 16 13:16:15.744: INFO: Got endpoints: latency-svc-92pgq [161.988806ms]
    Dec 16 13:16:15.750: INFO: Created: latency-svc-r9sxh
    Dec 16 13:16:15.754: INFO: Created: latency-svc-j6vq6
    Dec 16 13:16:15.763: INFO: Got endpoints: latency-svc-r9sxh [168.800238ms]
    Dec 16 13:16:15.764: INFO: Got endpoints: latency-svc-j6vq6 [160.810635ms]
    Dec 16 13:16:15.768: INFO: Created: latency-svc-mn2lk
    Dec 16 13:16:15.775: INFO: Got endpoints: latency-svc-mn2lk [158.397671ms]
    Dec 16 13:16:15.778: INFO: Created: latency-svc-pl8wg
    Dec 16 13:16:15.784: INFO: Got endpoints: latency-svc-pl8wg [157.179724ms]
    Dec 16 13:16:15.792: INFO: Created: latency-svc-d9gj7
    Dec 16 13:16:15.797: INFO: Got endpoints: latency-svc-d9gj7 [161.971712ms]
    Dec 16 13:16:15.806: INFO: Created: latency-svc-p4ss9
    Dec 16 13:16:15.813: INFO: Got endpoints: latency-svc-p4ss9 [167.050945ms]
    Dec 16 13:16:15.816: INFO: Created: latency-svc-xxn99
    Dec 16 13:16:15.822: INFO: Got endpoints: latency-svc-xxn99 [164.684654ms]
    Dec 16 13:16:15.827: INFO: Created: latency-svc-rxcpp
    Dec 16 13:16:15.831: INFO: Got endpoints: latency-svc-rxcpp [170.560915ms]
    Dec 16 13:16:15.840: INFO: Created: latency-svc-qg2ws
    Dec 16 13:16:15.848: INFO: Got endpoints: latency-svc-qg2ws [167.086241ms]
    Dec 16 13:16:15.877: INFO: Created: latency-svc-s6b7j
    Dec 16 13:16:15.883: INFO: Got endpoints: latency-svc-s6b7j [189.118241ms]
    Dec 16 13:16:15.894: INFO: Created: latency-svc-h72sn
    Dec 16 13:16:15.898: INFO: Got endpoints: latency-svc-h72sn [198.732756ms]
    Dec 16 13:16:15.939: INFO: Created: latency-svc-k8cps
    Dec 16 13:16:15.941: INFO: Got endpoints: latency-svc-k8cps [228.494543ms]
    Dec 16 13:16:15.948: INFO: Created: latency-svc-d742n
    Dec 16 13:16:15.960: INFO: Created: latency-svc-vpcg7
    Dec 16 13:16:15.980: INFO: Created: latency-svc-pkn2n
    Dec 16 13:16:15.987: INFO: Got endpoints: latency-svc-d742n [264.774253ms]
    Dec 16 13:16:15.990: INFO: Created: latency-svc-fpgr6
    Dec 16 13:16:16.003: INFO: Created: latency-svc-gmx5c
    Dec 16 13:16:16.016: INFO: Created: latency-svc-76n4t
    Dec 16 13:16:16.024: INFO: Created: latency-svc-mjdpk
    Dec 16 13:16:16.034: INFO: Got endpoints: latency-svc-vpcg7 [303.000579ms]
    Dec 16 13:16:16.034: INFO: Created: latency-svc-6fjkn
    Dec 16 13:16:16.048: INFO: Created: latency-svc-2wrl2
    Dec 16 13:16:16.083: INFO: Got endpoints: latency-svc-pkn2n [339.760139ms]
    Dec 16 13:16:16.084: INFO: Created: latency-svc-ccp65
    Dec 16 13:16:16.095: INFO: Created: latency-svc-j66gr
    Dec 16 13:16:16.107: INFO: Created: latency-svc-4zngj
    Dec 16 13:16:16.125: INFO: Created: latency-svc-4m5sg
    Dec 16 13:16:16.134: INFO: Got endpoints: latency-svc-fpgr6 [370.082491ms]
    Dec 16 13:16:16.135: INFO: Created: latency-svc-vmwjf
    Dec 16 13:16:16.147: INFO: Created: latency-svc-cdl7l
    Dec 16 13:16:16.159: INFO: Created: latency-svc-76k9r
    Dec 16 13:16:16.170: INFO: Created: latency-svc-85c4l
    Dec 16 13:16:16.183: INFO: Created: latency-svc-cjrh9
    Dec 16 13:16:16.190: INFO: Got endpoints: latency-svc-gmx5c [426.727472ms]
    Dec 16 13:16:16.192: INFO: Created: latency-svc-kzjjp
    Dec 16 13:16:16.206: INFO: Created: latency-svc-jcfpt
    Dec 16 13:16:16.239: INFO: Got endpoints: latency-svc-76n4t [464.060077ms]
    Dec 16 13:16:16.278: INFO: Created: latency-svc-fz59x
    Dec 16 13:16:16.295: INFO: Got endpoints: latency-svc-mjdpk [510.727964ms]
    Dec 16 13:16:16.319: INFO: Created: latency-svc-nw9f4
    Dec 16 13:16:16.335: INFO: Got endpoints: latency-svc-6fjkn [538.061381ms]
    Dec 16 13:16:16.347: INFO: Created: latency-svc-m9mqp
    Dec 16 13:16:16.385: INFO: Got endpoints: latency-svc-2wrl2 [571.946116ms]
    Dec 16 13:16:16.402: INFO: Created: latency-svc-mz2c4
    Dec 16 13:16:16.434: INFO: Got endpoints: latency-svc-ccp65 [612.445921ms]
    Dec 16 13:16:16.449: INFO: Created: latency-svc-klx5k
    Dec 16 13:16:16.485: INFO: Got endpoints: latency-svc-j66gr [653.357891ms]
    Dec 16 13:16:16.500: INFO: Created: latency-svc-rqxr5
    Dec 16 13:16:16.535: INFO: Got endpoints: latency-svc-4zngj [686.452415ms]
    Dec 16 13:16:16.554: INFO: Created: latency-svc-9lrtw
    Dec 16 13:16:16.584: INFO: Got endpoints: latency-svc-4m5sg [700.791127ms]
    Dec 16 13:16:16.599: INFO: Created: latency-svc-hppmm
    Dec 16 13:16:16.634: INFO: Got endpoints: latency-svc-vmwjf [735.457539ms]
    Dec 16 13:16:16.649: INFO: Created: latency-svc-v9twv
    Dec 16 13:16:16.684: INFO: Got endpoints: latency-svc-cdl7l [742.817969ms]
    Dec 16 13:16:16.701: INFO: Created: latency-svc-mdz9c
    Dec 16 13:16:16.735: INFO: Got endpoints: latency-svc-76k9r [748.057968ms]
    Dec 16 13:16:16.750: INFO: Created: latency-svc-g4dpz
    Dec 16 13:16:16.783: INFO: Got endpoints: latency-svc-85c4l [749.133134ms]
    Dec 16 13:16:16.795: INFO: Created: latency-svc-qw5nb
    Dec 16 13:16:16.834: INFO: Got endpoints: latency-svc-cjrh9 [750.556877ms]
    Dec 16 13:16:16.859: INFO: Created: latency-svc-4fplc
    Dec 16 13:16:16.884: INFO: Got endpoints: latency-svc-kzjjp [750.456531ms]
    Dec 16 13:16:16.899: INFO: Created: latency-svc-gzhfv
    Dec 16 13:16:16.933: INFO: Got endpoints: latency-svc-jcfpt [742.874612ms]
    Dec 16 13:16:16.946: INFO: Created: latency-svc-cxc9k
    Dec 16 13:16:16.984: INFO: Got endpoints: latency-svc-fz59x [745.344874ms]
    Dec 16 13:16:17.000: INFO: Created: latency-svc-tmlrt
    Dec 16 13:16:17.035: INFO: Got endpoints: latency-svc-nw9f4 [740.419008ms]
    Dec 16 13:16:17.052: INFO: Created: latency-svc-94cr8
    Dec 16 13:16:17.084: INFO: Got endpoints: latency-svc-m9mqp [749.422148ms]
    Dec 16 13:16:17.098: INFO: Created: latency-svc-c2fsk
    Dec 16 13:16:17.134: INFO: Got endpoints: latency-svc-mz2c4 [749.539999ms]
    Dec 16 13:16:17.151: INFO: Created: latency-svc-6qk4x
    Dec 16 13:16:17.191: INFO: Got endpoints: latency-svc-klx5k [756.492209ms]
    Dec 16 13:16:17.226: INFO: Created: latency-svc-g5w7v
    Dec 16 13:16:17.236: INFO: Got endpoints: latency-svc-rqxr5 [750.751228ms]
    Dec 16 13:16:17.275: INFO: Created: latency-svc-6vqtv
    Dec 16 13:16:17.294: INFO: Got endpoints: latency-svc-9lrtw [759.111971ms]
    Dec 16 13:16:17.312: INFO: Created: latency-svc-vl2g5
    Dec 16 13:16:17.336: INFO: Got endpoints: latency-svc-hppmm [752.620765ms]
    Dec 16 13:16:17.351: INFO: Created: latency-svc-mnm7c
    Dec 16 13:16:17.384: INFO: Got endpoints: latency-svc-v9twv [749.716538ms]
    Dec 16 13:16:17.400: INFO: Created: latency-svc-q5zcb
    Dec 16 13:16:17.434: INFO: Got endpoints: latency-svc-mdz9c [749.169014ms]
    Dec 16 13:16:17.447: INFO: Created: latency-svc-dfx4w
    Dec 16 13:16:17.493: INFO: Got endpoints: latency-svc-g4dpz [758.373968ms]
    Dec 16 13:16:17.508: INFO: Created: latency-svc-2dq4m
    Dec 16 13:16:17.535: INFO: Got endpoints: latency-svc-qw5nb [751.893515ms]
    Dec 16 13:16:17.548: INFO: Created: latency-svc-ndc59
    Dec 16 13:16:17.584: INFO: Got endpoints: latency-svc-4fplc [750.035051ms]
    Dec 16 13:16:17.604: INFO: Created: latency-svc-vhmdz
    Dec 16 13:16:17.634: INFO: Got endpoints: latency-svc-gzhfv [750.239638ms]
    Dec 16 13:16:17.651: INFO: Created: latency-svc-tvqtk
    Dec 16 13:16:17.685: INFO: Got endpoints: latency-svc-cxc9k [751.226625ms]
    Dec 16 13:16:17.698: INFO: Created: latency-svc-95fpx
    Dec 16 13:16:17.734: INFO: Got endpoints: latency-svc-tmlrt [750.190584ms]
    Dec 16 13:16:17.752: INFO: Created: latency-svc-qptrd
    Dec 16 13:16:17.785: INFO: Got endpoints: latency-svc-94cr8 [749.896515ms]
    Dec 16 13:16:17.801: INFO: Created: latency-svc-v4ztq
    Dec 16 13:16:17.834: INFO: Got endpoints: latency-svc-c2fsk [749.771771ms]
    Dec 16 13:16:17.847: INFO: Created: latency-svc-9r62b
    Dec 16 13:16:17.885: INFO: Got endpoints: latency-svc-6qk4x [750.661173ms]
    Dec 16 13:16:17.903: INFO: Created: latency-svc-zbj5d
    Dec 16 13:16:17.934: INFO: Got endpoints: latency-svc-g5w7v [742.636407ms]
    Dec 16 13:16:17.952: INFO: Created: latency-svc-ld8sg
    Dec 16 13:16:17.985: INFO: Got endpoints: latency-svc-6vqtv [748.936677ms]
    Dec 16 13:16:17.999: INFO: Created: latency-svc-7cb98
    Dec 16 13:16:18.034: INFO: Got endpoints: latency-svc-vl2g5 [739.45328ms]
    Dec 16 13:16:18.048: INFO: Created: latency-svc-x24dc
    Dec 16 13:16:18.084: INFO: Got endpoints: latency-svc-mnm7c [747.498424ms]
    Dec 16 13:16:18.101: INFO: Created: latency-svc-vft62
    Dec 16 13:16:18.134: INFO: Got endpoints: latency-svc-q5zcb [750.498289ms]
    Dec 16 13:16:18.148: INFO: Created: latency-svc-bx7h2
    Dec 16 13:16:18.184: INFO: Got endpoints: latency-svc-dfx4w [750.350436ms]
    Dec 16 13:16:18.200: INFO: Created: latency-svc-rl2d8
    Dec 16 13:16:18.234: INFO: Got endpoints: latency-svc-2dq4m [740.563669ms]
    Dec 16 13:16:18.250: INFO: Created: latency-svc-hljj8
    Dec 16 13:16:18.285: INFO: Got endpoints: latency-svc-ndc59 [749.955676ms]
    Dec 16 13:16:18.297: INFO: Created: latency-svc-g9q9p
    Dec 16 13:16:18.334: INFO: Got endpoints: latency-svc-vhmdz [749.903813ms]
    Dec 16 13:16:18.347: INFO: Created: latency-svc-qfr4m
    Dec 16 13:16:18.383: INFO: Got endpoints: latency-svc-tvqtk [748.934989ms]
    Dec 16 13:16:18.398: INFO: Created: latency-svc-2hq8h
    Dec 16 13:16:18.434: INFO: Got endpoints: latency-svc-95fpx [749.792137ms]
    Dec 16 13:16:18.447: INFO: Created: latency-svc-pfl8x
    Dec 16 13:16:18.482: INFO: Got endpoints: latency-svc-qptrd [747.866848ms]
    Dec 16 13:16:18.497: INFO: Created: latency-svc-zz568
    Dec 16 13:16:18.534: INFO: Got endpoints: latency-svc-v4ztq [749.200994ms]
    Dec 16 13:16:18.551: INFO: Created: latency-svc-5ht7j
    Dec 16 13:16:18.585: INFO: Got endpoints: latency-svc-9r62b [750.448858ms]
    Dec 16 13:16:18.598: INFO: Created: latency-svc-v7c45
    Dec 16 13:16:18.635: INFO: Got endpoints: latency-svc-zbj5d [749.851363ms]
    Dec 16 13:16:18.650: INFO: Created: latency-svc-hfzlb
    Dec 16 13:16:18.683: INFO: Got endpoints: latency-svc-ld8sg [749.517879ms]
    Dec 16 13:16:18.700: INFO: Created: latency-svc-sw7qz
    Dec 16 13:16:18.734: INFO: Got endpoints: latency-svc-7cb98 [749.639234ms]
    Dec 16 13:16:18.753: INFO: Created: latency-svc-t7cgz
    Dec 16 13:16:18.785: INFO: Got endpoints: latency-svc-x24dc [751.222564ms]
    Dec 16 13:16:18.799: INFO: Created: latency-svc-jc7hs
    Dec 16 13:16:18.835: INFO: Got endpoints: latency-svc-vft62 [751.021418ms]
    Dec 16 13:16:18.852: INFO: Created: latency-svc-mc2tw
    Dec 16 13:16:18.884: INFO: Got endpoints: latency-svc-bx7h2 [749.832674ms]
    Dec 16 13:16:18.898: INFO: Created: latency-svc-h67sd
    Dec 16 13:16:18.935: INFO: Got endpoints: latency-svc-rl2d8 [750.778561ms]
    Dec 16 13:16:18.948: INFO: Created: latency-svc-ns2fr
    Dec 16 13:16:18.984: INFO: Got endpoints: latency-svc-hljj8 [750.444006ms]
    Dec 16 13:16:18.999: INFO: Created: latency-svc-56tps
    Dec 16 13:16:19.035: INFO: Got endpoints: latency-svc-g9q9p [749.797122ms]
    Dec 16 13:16:19.048: INFO: Created: latency-svc-frh6k
    Dec 16 13:16:19.083: INFO: Got endpoints: latency-svc-qfr4m [749.152886ms]
    Dec 16 13:16:19.097: INFO: Created: latency-svc-m8t69
    Dec 16 13:16:19.134: INFO: Got endpoints: latency-svc-2hq8h [750.585762ms]
    Dec 16 13:16:19.148: INFO: Created: latency-svc-9bvxl
    Dec 16 13:16:19.185: INFO: Got endpoints: latency-svc-pfl8x [750.905778ms]
    Dec 16 13:16:19.198: INFO: Created: latency-svc-m7wbs
    Dec 16 13:16:19.234: INFO: Got endpoints: latency-svc-zz568 [751.227222ms]
    Dec 16 13:16:19.251: INFO: Created: latency-svc-vmnmb
    Dec 16 13:16:19.295: INFO: Got endpoints: latency-svc-5ht7j [760.434137ms]
    Dec 16 13:16:19.308: INFO: Created: latency-svc-jdtqx
    Dec 16 13:16:19.334: INFO: Got endpoints: latency-svc-v7c45 [749.568016ms]
    Dec 16 13:16:19.348: INFO: Created: latency-svc-xz767
    Dec 16 13:16:19.383: INFO: Got endpoints: latency-svc-hfzlb [748.286945ms]
    Dec 16 13:16:19.397: INFO: Created: latency-svc-4g7nw
    Dec 16 13:16:19.434: INFO: Got endpoints: latency-svc-sw7qz [751.079125ms]
    Dec 16 13:16:19.449: INFO: Created: latency-svc-mdlgh
    Dec 16 13:16:19.483: INFO: Got endpoints: latency-svc-t7cgz [748.799201ms]
    Dec 16 13:16:19.497: INFO: Created: latency-svc-5gllg
    Dec 16 13:16:19.534: INFO: Got endpoints: latency-svc-jc7hs [749.363329ms]
    Dec 16 13:16:19.549: INFO: Created: latency-svc-fjcnm
    Dec 16 13:16:19.585: INFO: Got endpoints: latency-svc-mc2tw [750.032604ms]
    Dec 16 13:16:19.601: INFO: Created: latency-svc-b9nmm
    Dec 16 13:16:19.635: INFO: Got endpoints: latency-svc-h67sd [750.961416ms]
    Dec 16 13:16:19.649: INFO: Created: latency-svc-5k8zp
    Dec 16 13:16:19.684: INFO: Got endpoints: latency-svc-ns2fr [749.404622ms]
    Dec 16 13:16:19.701: INFO: Created: latency-svc-thxm8
    Dec 16 13:16:19.734: INFO: Got endpoints: latency-svc-56tps [750.172456ms]
    Dec 16 13:16:19.751: INFO: Created: latency-svc-59v7z
    Dec 16 13:16:19.786: INFO: Got endpoints: latency-svc-frh6k [751.180995ms]
    Dec 16 13:16:19.802: INFO: Created: latency-svc-rtrkw
    Dec 16 13:16:19.834: INFO: Got endpoints: latency-svc-m8t69 [750.862487ms]
    Dec 16 13:16:19.848: INFO: Created: latency-svc-lf2c7
    Dec 16 13:16:19.884: INFO: Got endpoints: latency-svc-9bvxl [750.23281ms]
    Dec 16 13:16:19.907: INFO: Created: latency-svc-l5gnt
    Dec 16 13:16:19.936: INFO: Got endpoints: latency-svc-m7wbs [750.257245ms]
    Dec 16 13:16:19.951: INFO: Created: latency-svc-25bkf
    Dec 16 13:16:19.985: INFO: Got endpoints: latency-svc-vmnmb [750.984756ms]
    Dec 16 13:16:19.998: INFO: Created: latency-svc-zzl64
    Dec 16 13:16:20.033: INFO: Got endpoints: latency-svc-jdtqx [738.500106ms]
    Dec 16 13:16:20.048: INFO: Created: latency-svc-nwwr2
    Dec 16 13:16:20.085: INFO: Got endpoints: latency-svc-xz767 [750.162215ms]
    Dec 16 13:16:20.102: INFO: Created: latency-svc-nsjjn
    Dec 16 13:16:20.135: INFO: Got endpoints: latency-svc-4g7nw [751.324836ms]
    Dec 16 13:16:20.149: INFO: Created: latency-svc-9ztqm
    Dec 16 13:16:20.184: INFO: Got endpoints: latency-svc-mdlgh [749.696241ms]
    Dec 16 13:16:20.199: INFO: Created: latency-svc-c68rl
    Dec 16 13:16:20.235: INFO: Got endpoints: latency-svc-5gllg [751.530774ms]
    Dec 16 13:16:20.249: INFO: Created: latency-svc-htwfm
    Dec 16 13:16:20.284: INFO: Got endpoints: latency-svc-fjcnm [749.784681ms]
    Dec 16 13:16:20.297: INFO: Created: latency-svc-p2v7x
    Dec 16 13:16:20.333: INFO: Got endpoints: latency-svc-b9nmm [747.86383ms]
    Dec 16 13:16:20.348: INFO: Created: latency-svc-dzzk7
    Dec 16 13:16:20.383: INFO: Got endpoints: latency-svc-5k8zp [747.881509ms]
    Dec 16 13:16:20.397: INFO: Created: latency-svc-jvrxk
    Dec 16 13:16:20.434: INFO: Got endpoints: latency-svc-thxm8 [750.007006ms]
    Dec 16 13:16:20.449: INFO: Created: latency-svc-p65cr
    Dec 16 13:16:20.484: INFO: Got endpoints: latency-svc-59v7z [749.829026ms]
    Dec 16 13:16:20.499: INFO: Created: latency-svc-6rr6j
    Dec 16 13:16:20.533: INFO: Got endpoints: latency-svc-rtrkw [747.417576ms]
    Dec 16 13:16:20.547: INFO: Created: latency-svc-2ftxh
    Dec 16 13:16:20.584: INFO: Got endpoints: latency-svc-lf2c7 [750.005395ms]
    Dec 16 13:16:20.600: INFO: Created: latency-svc-qsj8n
    Dec 16 13:16:20.635: INFO: Got endpoints: latency-svc-l5gnt [750.330324ms]
    Dec 16 13:16:20.649: INFO: Created: latency-svc-97pvz
    Dec 16 13:16:20.683: INFO: Got endpoints: latency-svc-25bkf [747.491171ms]
    Dec 16 13:16:20.702: INFO: Created: latency-svc-xs6gh
    Dec 16 13:16:20.734: INFO: Got endpoints: latency-svc-zzl64 [749.433758ms]
    Dec 16 13:16:20.751: INFO: Created: latency-svc-hqt5z
    Dec 16 13:16:20.783: INFO: Got endpoints: latency-svc-nwwr2 [749.561528ms]
    Dec 16 13:16:20.802: INFO: Created: latency-svc-9c2bf
    Dec 16 13:16:20.834: INFO: Got endpoints: latency-svc-nsjjn [749.29639ms]
    Dec 16 13:16:20.847: INFO: Created: latency-svc-g9lht
    Dec 16 13:16:20.885: INFO: Got endpoints: latency-svc-9ztqm [750.108723ms]
    Dec 16 13:16:20.901: INFO: Created: latency-svc-pgxnd
    Dec 16 13:16:20.933: INFO: Got endpoints: latency-svc-c68rl [748.992798ms]
    Dec 16 13:16:20.946: INFO: Created: latency-svc-m5rq9
    Dec 16 13:16:20.985: INFO: Got endpoints: latency-svc-htwfm [750.318868ms]
    Dec 16 13:16:21.001: INFO: Created: latency-svc-6l9n4
    Dec 16 13:16:21.035: INFO: Got endpoints: latency-svc-p2v7x [750.579134ms]
    Dec 16 13:16:21.052: INFO: Created: latency-svc-2v8g6
    Dec 16 13:16:21.083: INFO: Got endpoints: latency-svc-dzzk7 [749.514704ms]
    Dec 16 13:16:21.097: INFO: Created: latency-svc-gphfg
    Dec 16 13:16:21.135: INFO: Got endpoints: latency-svc-jvrxk [751.80028ms]
    Dec 16 13:16:21.148: INFO: Created: latency-svc-29thn
    Dec 16 13:16:21.184: INFO: Got endpoints: latency-svc-p65cr [750.12841ms]
    Dec 16 13:16:21.217: INFO: Created: latency-svc-p8kkh
    Dec 16 13:16:21.244: INFO: Got endpoints: latency-svc-6rr6j [759.574136ms]
    Dec 16 13:16:21.275: INFO: Created: latency-svc-5wc9l
    Dec 16 13:16:21.285: INFO: Got endpoints: latency-svc-2ftxh [751.293447ms]
    Dec 16 13:16:21.308: INFO: Created: latency-svc-lk8j9
    Dec 16 13:16:21.337: INFO: Got endpoints: latency-svc-qsj8n [753.137839ms]
    Dec 16 13:16:21.353: INFO: Created: latency-svc-kv4wc
    Dec 16 13:16:21.384: INFO: Got endpoints: latency-svc-97pvz [748.976694ms]
    Dec 16 13:16:21.398: INFO: Created: latency-svc-7spzd
    Dec 16 13:16:21.434: INFO: Got endpoints: latency-svc-xs6gh [750.605131ms]
    Dec 16 13:16:21.447: INFO: Created: latency-svc-rktzk
    Dec 16 13:16:21.484: INFO: Got endpoints: latency-svc-hqt5z [750.172892ms]
    Dec 16 13:16:21.501: INFO: Created: latency-svc-kgf44
    Dec 16 13:16:21.535: INFO: Got endpoints: latency-svc-9c2bf [751.89611ms]
    Dec 16 13:16:21.549: INFO: Created: latency-svc-6gw8n
    Dec 16 13:16:21.583: INFO: Got endpoints: latency-svc-g9lht [748.984116ms]
    Dec 16 13:16:21.596: INFO: Created: latency-svc-dzj95
    Dec 16 13:16:21.635: INFO: Got endpoints: latency-svc-pgxnd [749.865198ms]
    Dec 16 13:16:21.654: INFO: Created: latency-svc-mh4pm
    Dec 16 13:16:21.684: INFO: Got endpoints: latency-svc-m5rq9 [750.806307ms]
    Dec 16 13:16:21.696: INFO: Created: latency-svc-pzj6h
    Dec 16 13:16:21.736: INFO: Got endpoints: latency-svc-6l9n4 [751.410924ms]
    Dec 16 13:16:21.750: INFO: Created: latency-svc-z2sw7
    Dec 16 13:16:21.785: INFO: Got endpoints: latency-svc-2v8g6 [749.662086ms]
    Dec 16 13:16:21.800: INFO: Created: latency-svc-ltt52
    Dec 16 13:16:21.834: INFO: Got endpoints: latency-svc-gphfg [750.867325ms]
    Dec 16 13:16:21.847: INFO: Created: latency-svc-7ggzv
    Dec 16 13:16:21.884: INFO: Got endpoints: latency-svc-29thn [749.161516ms]
    Dec 16 13:16:21.897: INFO: Created: latency-svc-dhbbd
    Dec 16 13:16:21.934: INFO: Got endpoints: latency-svc-p8kkh [749.480949ms]
    Dec 16 13:16:21.948: INFO: Created: latency-svc-vdgwj
    Dec 16 13:16:21.985: INFO: Got endpoints: latency-svc-5wc9l [741.095643ms]
    Dec 16 13:16:21.999: INFO: Created: latency-svc-8r9g9
    Dec 16 13:16:22.035: INFO: Got endpoints: latency-svc-lk8j9 [750.309529ms]
    Dec 16 13:16:22.049: INFO: Created: latency-svc-58xgt
    Dec 16 13:16:22.084: INFO: Got endpoints: latency-svc-kv4wc [746.593431ms]
    Dec 16 13:16:22.099: INFO: Created: latency-svc-bgm8c
    Dec 16 13:16:22.134: INFO: Got endpoints: latency-svc-7spzd [750.621736ms]
    Dec 16 13:16:22.149: INFO: Created: latency-svc-mg87h
    Dec 16 13:16:22.184: INFO: Got endpoints: latency-svc-rktzk [749.685549ms]
    Dec 16 13:16:22.197: INFO: Created: latency-svc-lm775
    Dec 16 13:16:22.235: INFO: Got endpoints: latency-svc-kgf44 [750.336091ms]
    Dec 16 13:16:22.250: INFO: Created: latency-svc-j44nd
    Dec 16 13:16:22.287: INFO: Got endpoints: latency-svc-6gw8n [751.661676ms]
    Dec 16 13:16:22.301: INFO: Created: latency-svc-5pskr
    Dec 16 13:16:22.334: INFO: Got endpoints: latency-svc-dzj95 [751.499738ms]
    Dec 16 13:16:22.349: INFO: Created: latency-svc-8tgc6
    Dec 16 13:16:22.386: INFO: Got endpoints: latency-svc-mh4pm [750.65111ms]
    Dec 16 13:16:22.411: INFO: Created: latency-svc-kmr46
    Dec 16 13:16:22.435: INFO: Got endpoints: latency-svc-pzj6h [750.828965ms]
    Dec 16 13:16:22.449: INFO: Created: latency-svc-js8px
    Dec 16 13:16:22.484: INFO: Got endpoints: latency-svc-z2sw7 [747.220949ms]
    Dec 16 13:16:22.498: INFO: Created: latency-svc-ttm6w
    Dec 16 13:16:22.534: INFO: Got endpoints: latency-svc-ltt52 [749.343588ms]
    Dec 16 13:16:22.549: INFO: Created: latency-svc-g8t5t
    Dec 16 13:16:22.585: INFO: Got endpoints: latency-svc-7ggzv [750.859546ms]
    Dec 16 13:16:22.607: INFO: Created: latency-svc-thhwm
    Dec 16 13:16:22.633: INFO: Got endpoints: latency-svc-dhbbd [748.809029ms]
    Dec 16 13:16:22.660: INFO: Created: latency-svc-66nz6
    Dec 16 13:16:22.691: INFO: Got endpoints: latency-svc-vdgwj [756.753086ms]
    Dec 16 13:16:22.728: INFO: Created: latency-svc-z78sx
    Dec 16 13:16:22.733: INFO: Got endpoints: latency-svc-8r9g9 [748.20955ms]
    Dec 16 13:16:22.747: INFO: Created: latency-svc-k48jv
    Dec 16 13:16:22.784: INFO: Got endpoints: latency-svc-58xgt [748.569866ms]
    Dec 16 13:16:22.796: INFO: Created: latency-svc-k9nr5
    Dec 16 13:16:22.835: INFO: Got endpoints: latency-svc-bgm8c [751.102984ms]
    Dec 16 13:16:22.848: INFO: Created: latency-svc-gck8f
    Dec 16 13:16:22.884: INFO: Got endpoints: latency-svc-mg87h [749.610921ms]
    Dec 16 13:16:22.897: INFO: Created: latency-svc-2mf6g
    Dec 16 13:16:22.934: INFO: Got endpoints: latency-svc-lm775 [750.18431ms]
    Dec 16 13:16:22.946: INFO: Created: latency-svc-q7m4k
    Dec 16 13:16:22.984: INFO: Got endpoints: latency-svc-j44nd [749.257901ms]
    Dec 16 13:16:22.998: INFO: Created: latency-svc-bg7x2
    Dec 16 13:16:23.034: INFO: Got endpoints: latency-svc-5pskr [746.797628ms]
    Dec 16 13:16:23.046: INFO: Created: latency-svc-klvvt
    Dec 16 13:16:23.084: INFO: Got endpoints: latency-svc-8tgc6 [749.932766ms]
    Dec 16 13:16:23.098: INFO: Created: latency-svc-4zzf2
    Dec 16 13:16:23.135: INFO: Got endpoints: latency-svc-kmr46 [749.124059ms]
    Dec 16 13:16:23.147: INFO: Created: latency-svc-8gvw5
    Dec 16 13:16:23.184: INFO: Got endpoints: latency-svc-js8px [748.635427ms]
    Dec 16 13:16:23.196: INFO: Created: latency-svc-ghn2k
    Dec 16 13:16:23.233: INFO: Got endpoints: latency-svc-ttm6w [749.010126ms]
    Dec 16 13:16:23.247: INFO: Created: latency-svc-lgnms
    Dec 16 13:16:23.284: INFO: Got endpoints: latency-svc-g8t5t [750.107219ms]
    Dec 16 13:16:23.333: INFO: Got endpoints: latency-svc-thhwm [748.410096ms]
    Dec 16 13:16:23.384: INFO: Got endpoints: latency-svc-66nz6 [751.085972ms]
    Dec 16 13:16:23.434: INFO: Got endpoints: latency-svc-z78sx [743.157828ms]
    Dec 16 13:16:23.483: INFO: Got endpoints: latency-svc-k48jv [750.190061ms]
    Dec 16 13:16:23.533: INFO: Got endpoints: latency-svc-k9nr5 [749.016807ms]
    Dec 16 13:16:23.585: INFO: Got endpoints: latency-svc-gck8f [749.496235ms]
    Dec 16 13:16:23.634: INFO: Got endpoints: latency-svc-2mf6g [750.146194ms]
    Dec 16 13:16:23.685: INFO: Got endpoints: latency-svc-q7m4k [750.752144ms]
    Dec 16 13:16:23.734: INFO: Got endpoints: latency-svc-bg7x2 [750.159807ms]
    Dec 16 13:16:23.784: INFO: Got endpoints: latency-svc-klvvt [750.126066ms]
    Dec 16 13:16:23.835: INFO: Got endpoints: latency-svc-4zzf2 [750.422979ms]
    Dec 16 13:16:23.885: INFO: Got endpoints: latency-svc-8gvw5 [750.163846ms]
    Dec 16 13:16:23.935: INFO: Got endpoints: latency-svc-ghn2k [751.666314ms]
    Dec 16 13:16:23.985: INFO: Got endpoints: latency-svc-lgnms [752.278993ms]
    Dec 16 13:16:23.985: INFO: Latencies: [28.351528ms 36.556554ms 48.92558ms 61.941549ms 71.13677ms 82.411845ms 93.769363ms 100.577383ms 112.537581ms 121.009715ms 135.050449ms 143.97166ms 151.939714ms 156.937619ms 157.179724ms 157.801503ms 157.831047ms 158.397671ms 159.113851ms 160.722168ms 160.810635ms 160.973007ms 160.983811ms 161.387626ms 161.971712ms 161.988806ms 162.038175ms 162.863112ms 163.345874ms 164.684654ms 165.009872ms 167.050945ms 167.086241ms 168.800238ms 170.560915ms 178.529192ms 189.118241ms 198.732756ms 228.494543ms 264.774253ms 303.000579ms 339.760139ms 370.082491ms 426.727472ms 464.060077ms 510.727964ms 538.061381ms 571.946116ms 612.445921ms 653.357891ms 686.452415ms 700.791127ms 735.457539ms 738.500106ms 739.45328ms 740.419008ms 740.563669ms 741.095643ms 742.636407ms 742.817969ms 742.874612ms 743.157828ms 745.344874ms 746.593431ms 746.797628ms 747.220949ms 747.417576ms 747.491171ms 747.498424ms 747.86383ms 747.866848ms 747.881509ms 748.057968ms 748.20955ms 748.286945ms 748.410096ms 748.569866ms 748.635427ms 748.799201ms 748.809029ms 748.934989ms 748.936677ms 748.976694ms 748.984116ms 748.992798ms 749.010126ms 749.016807ms 749.124059ms 749.133134ms 749.152886ms 749.161516ms 749.169014ms 749.200994ms 749.257901ms 749.29639ms 749.343588ms 749.363329ms 749.404622ms 749.422148ms 749.433758ms 749.480949ms 749.496235ms 749.514704ms 749.517879ms 749.539999ms 749.561528ms 749.568016ms 749.610921ms 749.639234ms 749.662086ms 749.685549ms 749.696241ms 749.716538ms 749.771771ms 749.784681ms 749.792137ms 749.797122ms 749.829026ms 749.832674ms 749.851363ms 749.865198ms 749.896515ms 749.903813ms 749.932766ms 749.955676ms 750.005395ms 750.007006ms 750.032604ms 750.035051ms 750.107219ms 750.108723ms 750.126066ms 750.12841ms 750.146194ms 750.159807ms 750.162215ms 750.163846ms 750.172456ms 750.172892ms 750.18431ms 750.190061ms 750.190584ms 750.23281ms 750.239638ms 750.257245ms 750.309529ms 750.318868ms 750.330324ms 750.336091ms 750.350436ms 750.422979ms 750.444006ms 750.448858ms 750.456531ms 750.498289ms 750.556877ms 750.579134ms 750.585762ms 750.605131ms 750.621736ms 750.65111ms 750.661173ms 750.751228ms 750.752144ms 750.778561ms 750.806307ms 750.828965ms 750.859546ms 750.862487ms 750.867325ms 750.905778ms 750.961416ms 750.984756ms 751.021418ms 751.079125ms 751.085972ms 751.102984ms 751.180995ms 751.222564ms 751.226625ms 751.227222ms 751.293447ms 751.324836ms 751.410924ms 751.499738ms 751.530774ms 751.661676ms 751.666314ms 751.80028ms 751.893515ms 751.89611ms 752.278993ms 752.620765ms 753.137839ms 756.492209ms 756.753086ms 758.373968ms 759.111971ms 759.574136ms 760.434137ms]
    Dec 16 13:16:23.985: INFO: 50 %ile: 749.480949ms
    Dec 16 13:16:23.985: INFO: 90 %ile: 751.227222ms
    Dec 16 13:16:23.985: INFO: 99 %ile: 759.574136ms
    Dec 16 13:16:23.985: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-8834" for this suite. 12/16/22 13:16:23.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:23.999
Dec 16 13:16:23.999: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:16:23.999
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:24.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:24.015
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 12/16/22 13:16:24.018
Dec 16 13:16:24.018: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: mark a version not serverd 12/16/22 13:16:28.103
STEP: check the unserved version gets removed 12/16/22 13:16:28.131
STEP: check the other version is not changed 12/16/22 13:16:29.393
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:32.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7159" for this suite. 12/16/22 13:16:32.77
------------------------------
• [SLOW TEST] [8.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:23.999
    Dec 16 13:16:23.999: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:16:23.999
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:24.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:24.015
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 12/16/22 13:16:24.018
    Dec 16 13:16:24.018: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: mark a version not serverd 12/16/22 13:16:28.103
    STEP: check the unserved version gets removed 12/16/22 13:16:28.131
    STEP: check the other version is not changed 12/16/22 13:16:29.393
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:32.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7159" for this suite. 12/16/22 13:16:32.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:32.781
Dec 16 13:16:32.781: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:16:32.782
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:32.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:32.798
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:16:32.814
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:16:33.387
STEP: Deploying the webhook pod 12/16/22 13:16:33.395
STEP: Wait for the deployment to be ready 12/16/22 13:16:33.41
Dec 16 13:16:33.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:16:35.43
STEP: Verifying the service has paired with the endpoint 12/16/22 13:16:35.447
Dec 16 13:16:36.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Dec 16 13:16:37.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Dec 16 13:16:38.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Dec 16 13:16:39.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/16/22 13:16:39.452
STEP: create a namespace for the webhook 12/16/22 13:16:39.497
STEP: create a configmap should be unconditionally rejected by the webhook 12/16/22 13:16:39.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-55" for this suite. 12/16/22 13:16:39.596
STEP: Destroying namespace "webhook-55-markers" for this suite. 12/16/22 13:16:39.614
------------------------------
• [SLOW TEST] [6.839 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:32.781
    Dec 16 13:16:32.781: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:16:32.782
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:32.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:32.798
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:16:32.814
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:16:33.387
    STEP: Deploying the webhook pod 12/16/22 13:16:33.395
    STEP: Wait for the deployment to be ready 12/16/22 13:16:33.41
    Dec 16 13:16:33.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:16:35.43
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:16:35.447
    Dec 16 13:16:36.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Dec 16 13:16:37.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Dec 16 13:16:38.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Dec 16 13:16:39.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/16/22 13:16:39.452
    STEP: create a namespace for the webhook 12/16/22 13:16:39.497
    STEP: create a configmap should be unconditionally rejected by the webhook 12/16/22 13:16:39.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-55" for this suite. 12/16/22 13:16:39.596
    STEP: Destroying namespace "webhook-55-markers" for this suite. 12/16/22 13:16:39.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:39.621
Dec 16 13:16:39.621: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:16:39.621
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:39.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:39.636
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-9699 12/16/22 13:16:39.639
STEP: creating service affinity-clusterip-transition in namespace services-9699 12/16/22 13:16:39.639
STEP: creating replication controller affinity-clusterip-transition in namespace services-9699 12/16/22 13:16:39.655
I1216 13:16:39.660906      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9699, replica count: 3
I1216 13:16:42.712068      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 13:16:42.721: INFO: Creating new exec pod
Dec 16 13:16:42.726: INFO: Waiting up to 5m0s for pod "execpod-affinity6kn6b" in namespace "services-9699" to be "running"
Dec 16 13:16:42.730: INFO: Pod "execpod-affinity6kn6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034671ms
Dec 16 13:16:44.734: INFO: Pod "execpod-affinity6kn6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008077574s
Dec 16 13:16:44.734: INFO: Pod "execpod-affinity6kn6b" satisfied condition "running"
Dec 16 13:16:45.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Dec 16 13:16:45.922: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 16 13:16:45.922: INFO: stdout: ""
Dec 16 13:16:45.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c nc -v -z -w 2 10.107.190.41 80'
Dec 16 13:16:46.108: INFO: stderr: "+ nc -v -z -w 2 10.107.190.41 80\nConnection to 10.107.190.41 80 port [tcp/http] succeeded!\n"
Dec 16 13:16:46.108: INFO: stdout: ""
Dec 16 13:16:46.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.190.41:80/ ; done'
Dec 16 13:16:46.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n"
Dec 16 13:16:46.383: INFO: stdout: "\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-47cbn"
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-47cbn
Dec 16 13:16:46.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.190.41:80/ ; done'
Dec 16 13:16:46.643: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n"
Dec 16 13:16:46.643: INFO: stdout: "\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr"
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
Dec 16 13:16:46.643: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9699, will wait for the garbage collector to delete the pods 12/16/22 13:16:46.654
Dec 16 13:16:46.717: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.874894ms
Dec 16 13:16:46.818: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.866449ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:16:48.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9699" for this suite. 12/16/22 13:16:48.942
------------------------------
• [SLOW TEST] [9.328 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:39.621
    Dec 16 13:16:39.621: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:16:39.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:39.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:39.636
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-9699 12/16/22 13:16:39.639
    STEP: creating service affinity-clusterip-transition in namespace services-9699 12/16/22 13:16:39.639
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9699 12/16/22 13:16:39.655
    I1216 13:16:39.660906      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9699, replica count: 3
    I1216 13:16:42.712068      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 13:16:42.721: INFO: Creating new exec pod
    Dec 16 13:16:42.726: INFO: Waiting up to 5m0s for pod "execpod-affinity6kn6b" in namespace "services-9699" to be "running"
    Dec 16 13:16:42.730: INFO: Pod "execpod-affinity6kn6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034671ms
    Dec 16 13:16:44.734: INFO: Pod "execpod-affinity6kn6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008077574s
    Dec 16 13:16:44.734: INFO: Pod "execpod-affinity6kn6b" satisfied condition "running"
    Dec 16 13:16:45.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Dec 16 13:16:45.922: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Dec 16 13:16:45.922: INFO: stdout: ""
    Dec 16 13:16:45.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c nc -v -z -w 2 10.107.190.41 80'
    Dec 16 13:16:46.108: INFO: stderr: "+ nc -v -z -w 2 10.107.190.41 80\nConnection to 10.107.190.41 80 port [tcp/http] succeeded!\n"
    Dec 16 13:16:46.108: INFO: stdout: ""
    Dec 16 13:16:46.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.190.41:80/ ; done'
    Dec 16 13:16:46.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n"
    Dec 16 13:16:46.383: INFO: stdout: "\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-47cbn\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-grjfg\naffinity-clusterip-transition-47cbn"
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.383: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-grjfg
    Dec 16 13:16:46.384: INFO: Received response from host: affinity-clusterip-transition-47cbn
    Dec 16 13:16:46.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9699 exec execpod-affinity6kn6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.190.41:80/ ; done'
    Dec 16 13:16:46.643: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.190.41:80/\n"
    Dec 16 13:16:46.643: INFO: stdout: "\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr\naffinity-clusterip-transition-jbgsr"
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Received response from host: affinity-clusterip-transition-jbgsr
    Dec 16 13:16:46.643: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9699, will wait for the garbage collector to delete the pods 12/16/22 13:16:46.654
    Dec 16 13:16:46.717: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.874894ms
    Dec 16 13:16:46.818: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.866449ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:16:48.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9699" for this suite. 12/16/22 13:16:48.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:16:48.949
Dec 16 13:16:48.949: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:16:48.95
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:48.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:48.964
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 16 13:16:48.980: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:17:49.002: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 12/16/22 13:17:49.006
Dec 16 13:17:49.024: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 16 13:17:49.029: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 16 13:17:49.043: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 16 13:17:49.050: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 16 13:17:49.062: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 16 13:17:49.066: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/16/22 13:17:49.066
Dec 16 13:17:49.066: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:49.069: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87241ms
Dec 16 13:17:51.074: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942849s
Dec 16 13:17:53.075: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008750096s
Dec 16 13:17:55.074: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007371344s
Dec 16 13:17:57.076: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00957188s
Dec 16 13:17:59.076: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.010083701s
Dec 16 13:17:59.076: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 16 13:17:59.076: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:59.079: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.893328ms
Dec 16 13:17:59.079: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:17:59.079: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:59.082: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.005617ms
Dec 16 13:17:59.082: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:17:59.082: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:59.086: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.977219ms
Dec 16 13:17:59.086: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:17:59.086: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:59.089: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.781398ms
Dec 16 13:17:59.089: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:17:59.089: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
Dec 16 13:17:59.092: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.225512ms
Dec 16 13:17:59.092: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/16/22 13:17:59.092
Dec 16 13:17:59.104: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec 16 13:17:59.107: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05297ms
Dec 16 13:18:01.112: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008610653s
Dec 16 13:18:03.113: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008764981s
Dec 16 13:18:05.113: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00949329s
Dec 16 13:18:05.113: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:18:05.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9569" for this suite. 12/16/22 13:18:05.198
------------------------------
• [SLOW TEST] [76.255 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:16:48.949
    Dec 16 13:16:48.949: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:16:48.95
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:16:48.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:16:48.964
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 16 13:16:48.980: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:17:49.002: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 12/16/22 13:17:49.006
    Dec 16 13:17:49.024: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 16 13:17:49.029: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 16 13:17:49.043: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 16 13:17:49.050: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 16 13:17:49.062: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 16 13:17:49.066: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/16/22 13:17:49.066
    Dec 16 13:17:49.066: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:49.069: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87241ms
    Dec 16 13:17:51.074: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942849s
    Dec 16 13:17:53.075: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008750096s
    Dec 16 13:17:55.074: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007371344s
    Dec 16 13:17:57.076: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00957188s
    Dec 16 13:17:59.076: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.010083701s
    Dec 16 13:17:59.076: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 16 13:17:59.076: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:59.079: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.893328ms
    Dec 16 13:17:59.079: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:17:59.079: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:59.082: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.005617ms
    Dec 16 13:17:59.082: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:17:59.082: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:59.086: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.977219ms
    Dec 16 13:17:59.086: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:17:59.086: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:59.089: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.781398ms
    Dec 16 13:17:59.089: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:17:59.089: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9569" to be "running"
    Dec 16 13:17:59.092: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.225512ms
    Dec 16 13:17:59.092: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/16/22 13:17:59.092
    Dec 16 13:17:59.104: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec 16 13:17:59.107: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05297ms
    Dec 16 13:18:01.112: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008610653s
    Dec 16 13:18:03.113: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008764981s
    Dec 16 13:18:05.113: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00949329s
    Dec 16 13:18:05.113: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:18:05.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9569" for this suite. 12/16/22 13:18:05.198
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:18:05.204
Dec 16 13:18:05.204: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:18:05.205
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:05.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:05.22
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 12/16/22 13:18:05.222
STEP: Getting a ResourceQuota 12/16/22 13:18:05.226
STEP: Listing all ResourceQuotas with LabelSelector 12/16/22 13:18:05.23
STEP: Patching the ResourceQuota 12/16/22 13:18:05.233
STEP: Deleting a Collection of ResourceQuotas 12/16/22 13:18:05.24
STEP: Verifying the deleted ResourceQuota 12/16/22 13:18:05.248
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:18:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8650" for this suite. 12/16/22 13:18:05.255
------------------------------
• [0.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:18:05.204
    Dec 16 13:18:05.204: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:18:05.205
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:05.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:05.22
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 12/16/22 13:18:05.222
    STEP: Getting a ResourceQuota 12/16/22 13:18:05.226
    STEP: Listing all ResourceQuotas with LabelSelector 12/16/22 13:18:05.23
    STEP: Patching the ResourceQuota 12/16/22 13:18:05.233
    STEP: Deleting a Collection of ResourceQuotas 12/16/22 13:18:05.24
    STEP: Verifying the deleted ResourceQuota 12/16/22 13:18:05.248
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:18:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8650" for this suite. 12/16/22 13:18:05.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:18:05.261
Dec 16 13:18:05.261: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-runtime 12/16/22 13:18:05.262
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:05.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:05.278
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 12/16/22 13:18:05.28
STEP: wait for the container to reach Succeeded 12/16/22 13:18:05.288
STEP: get the container status 12/16/22 13:18:09.312
STEP: the container should be terminated 12/16/22 13:18:09.316
STEP: the termination message should be set 12/16/22 13:18:09.316
Dec 16 13:18:09.316: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 12/16/22 13:18:09.316
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 16 13:18:09.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-389" for this suite. 12/16/22 13:18:09.332
------------------------------
• [4.077 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:18:05.261
    Dec 16 13:18:05.261: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-runtime 12/16/22 13:18:05.262
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:05.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:05.278
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 12/16/22 13:18:05.28
    STEP: wait for the container to reach Succeeded 12/16/22 13:18:05.288
    STEP: get the container status 12/16/22 13:18:09.312
    STEP: the container should be terminated 12/16/22 13:18:09.316
    STEP: the termination message should be set 12/16/22 13:18:09.316
    Dec 16 13:18:09.316: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 12/16/22 13:18:09.316
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:18:09.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-389" for this suite. 12/16/22 13:18:09.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:18:09.339
Dec 16 13:18:09.339: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:18:09.34
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:09.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:09.355
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 12/16/22 13:18:09.358
STEP: submitting the pod to kubernetes 12/16/22 13:18:09.358
Dec 16 13:18:09.366: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" in namespace "pods-2966" to be "running and ready"
Dec 16 13:18:09.371: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Pending", Reason="", readiness=false. Elapsed: 5.026662ms
Dec 16 13:18:09.371: INFO: The phase of Pod pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:18:11.375: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009274766s
Dec 16 13:18:11.375: INFO: The phase of Pod pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83 is Running (Ready = true)
Dec 16 13:18:11.375: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/16/22 13:18:11.378
STEP: updating the pod 12/16/22 13:18:11.381
Dec 16 13:18:11.891: INFO: Successfully updated pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83"
Dec 16 13:18:11.891: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" in namespace "pods-2966" to be "terminated with reason DeadlineExceeded"
Dec 16 13:18:11.894: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.830317ms
Dec 16 13:18:13.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009817802s
Dec 16 13:18:15.899: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=false. Elapsed: 4.007883079s
Dec 16 13:18:17.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009958931s
Dec 16 13:18:17.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:18:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2966" for this suite. 12/16/22 13:18:17.906
------------------------------
• [SLOW TEST] [8.574 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:18:09.339
    Dec 16 13:18:09.339: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:18:09.34
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:09.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:09.355
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 12/16/22 13:18:09.358
    STEP: submitting the pod to kubernetes 12/16/22 13:18:09.358
    Dec 16 13:18:09.366: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" in namespace "pods-2966" to be "running and ready"
    Dec 16 13:18:09.371: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Pending", Reason="", readiness=false. Elapsed: 5.026662ms
    Dec 16 13:18:09.371: INFO: The phase of Pod pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:18:11.375: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009274766s
    Dec 16 13:18:11.375: INFO: The phase of Pod pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83 is Running (Ready = true)
    Dec 16 13:18:11.375: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/16/22 13:18:11.378
    STEP: updating the pod 12/16/22 13:18:11.381
    Dec 16 13:18:11.891: INFO: Successfully updated pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83"
    Dec 16 13:18:11.891: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" in namespace "pods-2966" to be "terminated with reason DeadlineExceeded"
    Dec 16 13:18:11.894: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.830317ms
    Dec 16 13:18:13.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009817802s
    Dec 16 13:18:15.899: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Running", Reason="", readiness=false. Elapsed: 4.007883079s
    Dec 16 13:18:17.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009958931s
    Dec 16 13:18:17.901: INFO: Pod "pod-update-activedeadlineseconds-77377742-5041-4023-9c07-4b7f93022c83" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:18:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2966" for this suite. 12/16/22 13:18:17.906
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:18:17.913
Dec 16 13:18:17.913: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:18:17.913
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:17.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:17.981
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:18:17.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1068" for this suite. 12/16/22 13:18:17.99
------------------------------
• [0.084 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:18:17.913
    Dec 16 13:18:17.913: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:18:17.913
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:17.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:17.981
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:18:17.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1068" for this suite. 12/16/22 13:18:17.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:18:17.997
Dec 16 13:18:17.997: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:18:17.998
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:18.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:18.012
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 16 13:18:18.026: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:19:18.045: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:19:18.049
Dec 16 13:19:18.049: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption-path 12/16/22 13:19:18.049
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:18.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:18.064
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 12/16/22 13:19:18.066
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 13:19:18.067
Dec 16 13:19:18.075: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3256" to be "running"
Dec 16 13:19:18.080: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243799ms
Dec 16 13:19:20.085: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01027787s
Dec 16 13:19:20.085: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 13:19:20.088
Dec 16 13:19:20.098: INFO: found a healthy node: pool-a3802-fsxxd
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Dec 16 13:19:30.166: INFO: pods created so far: [1 1 1]
Dec 16 13:19:30.166: INFO: length of pods created so far: 3
Dec 16 13:19:32.177: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Dec 16 13:19:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:19:39.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3256" for this suite. 12/16/22 13:19:39.265
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4813" for this suite. 12/16/22 13:19:39.271
------------------------------
• [SLOW TEST] [81.289 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:18:17.997
    Dec 16 13:18:17.997: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:18:17.998
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:18:18.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:18:18.012
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 16 13:18:18.026: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:19:18.045: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:19:18.049
    Dec 16 13:19:18.049: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption-path 12/16/22 13:19:18.049
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:18.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:18.064
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 12/16/22 13:19:18.066
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 13:19:18.067
    Dec 16 13:19:18.075: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3256" to be "running"
    Dec 16 13:19:18.080: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243799ms
    Dec 16 13:19:20.085: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01027787s
    Dec 16 13:19:20.085: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 13:19:20.088
    Dec 16 13:19:20.098: INFO: found a healthy node: pool-a3802-fsxxd
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Dec 16 13:19:30.166: INFO: pods created so far: [1 1 1]
    Dec 16 13:19:30.166: INFO: length of pods created so far: 3
    Dec 16 13:19:32.177: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:19:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:19:39.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3256" for this suite. 12/16/22 13:19:39.265
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4813" for this suite. 12/16/22 13:19:39.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:19:39.287
Dec 16 13:19:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:19:39.288
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:39.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:39.304
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 12/16/22 13:19:39.307
STEP: Counting existing ResourceQuota 12/16/22 13:19:44.311
STEP: Creating a ResourceQuota 12/16/22 13:19:49.316
STEP: Ensuring resource quota status is calculated 12/16/22 13:19:49.322
STEP: Creating a Secret 12/16/22 13:19:51.328
STEP: Ensuring resource quota status captures secret creation 12/16/22 13:19:51.34
STEP: Deleting a secret 12/16/22 13:19:53.345
STEP: Ensuring resource quota status released usage 12/16/22 13:19:53.35
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:19:55.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9213" for this suite. 12/16/22 13:19:55.363
------------------------------
• [SLOW TEST] [16.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:19:39.287
    Dec 16 13:19:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:19:39.288
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:39.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:39.304
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 12/16/22 13:19:39.307
    STEP: Counting existing ResourceQuota 12/16/22 13:19:44.311
    STEP: Creating a ResourceQuota 12/16/22 13:19:49.316
    STEP: Ensuring resource quota status is calculated 12/16/22 13:19:49.322
    STEP: Creating a Secret 12/16/22 13:19:51.328
    STEP: Ensuring resource quota status captures secret creation 12/16/22 13:19:51.34
    STEP: Deleting a secret 12/16/22 13:19:53.345
    STEP: Ensuring resource quota status released usage 12/16/22 13:19:53.35
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:19:55.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9213" for this suite. 12/16/22 13:19:55.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:19:55.371
Dec 16 13:19:55.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:19:55.372
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:55.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:55.39
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 12/16/22 13:19:55.392
Dec 16 13:19:55.393: INFO: namespace kubectl-6701
Dec 16 13:19:55.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 create -f -'
Dec 16 13:19:55.621: INFO: stderr: ""
Dec 16 13:19:55.621: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/16/22 13:19:55.621
Dec 16 13:19:56.626: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:19:56.626: INFO: Found 0 / 1
Dec 16 13:19:57.627: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:19:57.627: INFO: Found 1 / 1
Dec 16 13:19:57.627: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 16 13:19:57.630: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:19:57.630: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 13:19:57.630: INFO: wait on agnhost-primary startup in kubectl-6701 
Dec 16 13:19:57.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 logs agnhost-primary-ft6fd agnhost-primary'
Dec 16 13:19:57.750: INFO: stderr: ""
Dec 16 13:19:57.750: INFO: stdout: "Paused\n"
STEP: exposing RC 12/16/22 13:19:57.75
Dec 16 13:19:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 16 13:19:57.838: INFO: stderr: ""
Dec 16 13:19:57.838: INFO: stdout: "service/rm2 exposed\n"
Dec 16 13:19:57.843: INFO: Service rm2 in namespace kubectl-6701 found.
STEP: exposing service 12/16/22 13:19:59.851
Dec 16 13:19:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 16 13:19:59.932: INFO: stderr: ""
Dec 16 13:19:59.932: INFO: stdout: "service/rm3 exposed\n"
Dec 16 13:19:59.937: INFO: Service rm3 in namespace kubectl-6701 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:20:01.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6701" for this suite. 12/16/22 13:20:01.949
------------------------------
• [SLOW TEST] [6.585 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:19:55.371
    Dec 16 13:19:55.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:19:55.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:19:55.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:19:55.39
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 12/16/22 13:19:55.392
    Dec 16 13:19:55.393: INFO: namespace kubectl-6701
    Dec 16 13:19:55.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 create -f -'
    Dec 16 13:19:55.621: INFO: stderr: ""
    Dec 16 13:19:55.621: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/16/22 13:19:55.621
    Dec 16 13:19:56.626: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:19:56.626: INFO: Found 0 / 1
    Dec 16 13:19:57.627: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:19:57.627: INFO: Found 1 / 1
    Dec 16 13:19:57.627: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 16 13:19:57.630: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:19:57.630: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 16 13:19:57.630: INFO: wait on agnhost-primary startup in kubectl-6701 
    Dec 16 13:19:57.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 logs agnhost-primary-ft6fd agnhost-primary'
    Dec 16 13:19:57.750: INFO: stderr: ""
    Dec 16 13:19:57.750: INFO: stdout: "Paused\n"
    STEP: exposing RC 12/16/22 13:19:57.75
    Dec 16 13:19:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Dec 16 13:19:57.838: INFO: stderr: ""
    Dec 16 13:19:57.838: INFO: stdout: "service/rm2 exposed\n"
    Dec 16 13:19:57.843: INFO: Service rm2 in namespace kubectl-6701 found.
    STEP: exposing service 12/16/22 13:19:59.851
    Dec 16 13:19:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6701 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Dec 16 13:19:59.932: INFO: stderr: ""
    Dec 16 13:19:59.932: INFO: stdout: "service/rm3 exposed\n"
    Dec 16 13:19:59.937: INFO: Service rm3 in namespace kubectl-6701 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:20:01.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6701" for this suite. 12/16/22 13:20:01.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:20:01.958
Dec 16 13:20:01.958: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:20:01.958
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:01.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:01.974
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-8fb6a24b-f6f1-4cb1-aa58-b03f6979615c 12/16/22 13:20:01.977
STEP: Creating a pod to test consume secrets 12/16/22 13:20:01.982
Dec 16 13:20:01.990: INFO: Waiting up to 5m0s for pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d" in namespace "secrets-7190" to be "Succeeded or Failed"
Dec 16 13:20:01.993: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.390184ms
Dec 16 13:20:04.000: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009789851s
Dec 16 13:20:05.999: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008941309s
STEP: Saw pod success 12/16/22 13:20:05.999
Dec 16 13:20:05.999: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d" satisfied condition "Succeeded or Failed"
Dec 16 13:20:06.003: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:20:06.011
Dec 16 13:20:06.020: INFO: Waiting for pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d to disappear
Dec 16 13:20:06.024: INFO: Pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:20:06.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7190" for this suite. 12/16/22 13:20:06.028
------------------------------
• [4.077 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:20:01.958
    Dec 16 13:20:01.958: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:20:01.958
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:01.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:01.974
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-8fb6a24b-f6f1-4cb1-aa58-b03f6979615c 12/16/22 13:20:01.977
    STEP: Creating a pod to test consume secrets 12/16/22 13:20:01.982
    Dec 16 13:20:01.990: INFO: Waiting up to 5m0s for pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d" in namespace "secrets-7190" to be "Succeeded or Failed"
    Dec 16 13:20:01.993: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.390184ms
    Dec 16 13:20:04.000: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009789851s
    Dec 16 13:20:05.999: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008941309s
    STEP: Saw pod success 12/16/22 13:20:05.999
    Dec 16 13:20:05.999: INFO: Pod "pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d" satisfied condition "Succeeded or Failed"
    Dec 16 13:20:06.003: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:20:06.011
    Dec 16 13:20:06.020: INFO: Waiting for pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d to disappear
    Dec 16 13:20:06.024: INFO: Pod pod-secrets-92e605e5-b249-4e48-a5e9-c8b5b0d8b59d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:20:06.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7190" for this suite. 12/16/22 13:20:06.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:20:06.035
Dec 16 13:20:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename conformance-tests 12/16/22 13:20:06.036
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:06.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:06.05
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 12/16/22 13:20:06.053
Dec 16 13:20:06.053: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Dec 16 13:20:06.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-5120" for this suite. 12/16/22 13:20:06.064
------------------------------
• [0.035 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:20:06.035
    Dec 16 13:20:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename conformance-tests 12/16/22 13:20:06.036
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:06.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:06.05
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 12/16/22 13:20:06.053
    Dec 16 13:20:06.053: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:20:06.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-5120" for this suite. 12/16/22 13:20:06.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:20:06.071
Dec 16 13:20:06.071: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:20:06.072
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:06.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:06.087
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 16 13:20:06.104: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:21:06.127: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 12/16/22 13:21:06.13
Dec 16 13:21:06.158: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 16 13:21:06.164: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 16 13:21:06.177: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 16 13:21:06.184: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 16 13:21:06.197: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 16 13:21:06.203: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/16/22 13:21:06.203
Dec 16 13:21:06.203: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:06.206: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935092ms
Dec 16 13:21:08.211: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00828535s
Dec 16 13:21:10.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008526861s
Dec 16 13:21:12.213: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009438442s
Dec 16 13:21:14.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00924627s
Dec 16 13:21:14.212: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 16 13:21:14.212: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.216: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.279348ms
Dec 16 13:21:14.216: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:21:14.216: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.219: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.758284ms
Dec 16 13:21:14.219: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:21:14.219: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.222: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.547116ms
Dec 16 13:21:14.222: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:21:14.222: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.226: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.62244ms
Dec 16 13:21:14.226: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 16 13:21:14.226: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.228: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.546258ms
Dec 16 13:21:14.228: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/16/22 13:21:14.228
Dec 16 13:21:14.234: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1051" to be "running"
Dec 16 13:21:14.237: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750708ms
Dec 16 13:21:16.242: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007872427s
Dec 16 13:21:18.246: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011930144s
Dec 16 13:21:20.241: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007031823s
Dec 16 13:21:20.241: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:21:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1051" for this suite. 12/16/22 13:21:20.299
------------------------------
• [SLOW TEST] [74.234 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:20:06.071
    Dec 16 13:20:06.071: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:20:06.072
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:20:06.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:20:06.087
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 16 13:20:06.104: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:21:06.127: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 12/16/22 13:21:06.13
    Dec 16 13:21:06.158: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 16 13:21:06.164: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 16 13:21:06.177: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 16 13:21:06.184: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 16 13:21:06.197: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 16 13:21:06.203: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/16/22 13:21:06.203
    Dec 16 13:21:06.203: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:06.206: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935092ms
    Dec 16 13:21:08.211: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00828535s
    Dec 16 13:21:10.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008526861s
    Dec 16 13:21:12.213: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009438442s
    Dec 16 13:21:14.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00924627s
    Dec 16 13:21:14.212: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 16 13:21:14.212: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.216: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.279348ms
    Dec 16 13:21:14.216: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:21:14.216: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.219: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.758284ms
    Dec 16 13:21:14.219: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:21:14.219: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.222: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.547116ms
    Dec 16 13:21:14.222: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:21:14.222: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.226: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.62244ms
    Dec 16 13:21:14.226: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 16 13:21:14.226: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.228: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.546258ms
    Dec 16 13:21:14.228: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/16/22 13:21:14.228
    Dec 16 13:21:14.234: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1051" to be "running"
    Dec 16 13:21:14.237: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750708ms
    Dec 16 13:21:16.242: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007872427s
    Dec 16 13:21:18.246: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011930144s
    Dec 16 13:21:20.241: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007031823s
    Dec 16 13:21:20.241: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:21:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1051" for this suite. 12/16/22 13:21:20.299
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:21:20.305
Dec 16 13:21:20.305: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 13:21:20.306
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:20.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:20.319
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 12/16/22 13:21:20.322
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:20.332
STEP: Creating a service in the namespace 12/16/22 13:21:20.335
STEP: Deleting the namespace 12/16/22 13:21:20.346
STEP: Waiting for the namespace to be removed. 12/16/22 13:21:20.352
STEP: Recreating the namespace 12/16/22 13:21:26.356
STEP: Verifying there is no service in the namespace 12/16/22 13:21:26.37
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:21:26.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3798" for this suite. 12/16/22 13:21:26.378
STEP: Destroying namespace "nsdeletetest-7659" for this suite. 12/16/22 13:21:26.384
Dec 16 13:21:26.388: INFO: Namespace nsdeletetest-7659 was already deleted
STEP: Destroying namespace "nsdeletetest-4482" for this suite. 12/16/22 13:21:26.388
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:21:20.305
    Dec 16 13:21:20.305: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 13:21:20.306
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:20.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:20.319
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 12/16/22 13:21:20.322
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:20.332
    STEP: Creating a service in the namespace 12/16/22 13:21:20.335
    STEP: Deleting the namespace 12/16/22 13:21:20.346
    STEP: Waiting for the namespace to be removed. 12/16/22 13:21:20.352
    STEP: Recreating the namespace 12/16/22 13:21:26.356
    STEP: Verifying there is no service in the namespace 12/16/22 13:21:26.37
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:21:26.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3798" for this suite. 12/16/22 13:21:26.378
    STEP: Destroying namespace "nsdeletetest-7659" for this suite. 12/16/22 13:21:26.384
    Dec 16 13:21:26.388: INFO: Namespace nsdeletetest-7659 was already deleted
    STEP: Destroying namespace "nsdeletetest-4482" for this suite. 12/16/22 13:21:26.388
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:21:26.394
Dec 16 13:21:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:21:26.394
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:26.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:26.409
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-2093 12/16/22 13:21:26.411
STEP: creating a selector 12/16/22 13:21:26.412
STEP: Creating the service pods in kubernetes 12/16/22 13:21:26.412
Dec 16 13:21:26.412: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 13:21:26.436: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2093" to be "running and ready"
Dec 16 13:21:26.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328165ms
Dec 16 13:21:26.441: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:21:28.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009603775s
Dec 16 13:21:28.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:30.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009957829s
Dec 16 13:21:30.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:32.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011092072s
Dec 16 13:21:32.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:34.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009923084s
Dec 16 13:21:34.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:36.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009129239s
Dec 16 13:21:36.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:38.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011242418s
Dec 16 13:21:38.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:40.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011313045s
Dec 16 13:21:40.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:42.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010368211s
Dec 16 13:21:42.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:44.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011442404s
Dec 16 13:21:44.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:46.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009465399s
Dec 16 13:21:46.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:21:48.445: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008208759s
Dec 16 13:21:48.445: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 16 13:21:48.445: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 16 13:21:48.449: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2093" to be "running and ready"
Dec 16 13:21:48.452: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.532448ms
Dec 16 13:21:48.452: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 16 13:21:48.452: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 16 13:21:48.455: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2093" to be "running and ready"
Dec 16 13:21:48.458: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.921373ms
Dec 16 13:21:48.458: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 16 13:21:48.458: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/16/22 13:21:48.462
Dec 16 13:21:48.467: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2093" to be "running"
Dec 16 13:21:48.470: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093916ms
Dec 16 13:21:50.474: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492533s
Dec 16 13:21:50.474: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 16 13:21:50.477: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 16 13:21:50.477: INFO: Breadth first check of 192.168.156.146 on host 85.217.161.222...
Dec 16 13:21:50.480: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.156.146&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:21:50.480: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:21:50.481: INFO: ExecWithOptions: Clientset creation
Dec 16 13:21:50.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.156.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:21:50.609: INFO: Waiting for responses: map[]
Dec 16 13:21:50.609: INFO: reached 192.168.156.146 after 0/1 tries
Dec 16 13:21:50.609: INFO: Breadth first check of 192.168.189.41 on host 85.217.161.242...
Dec 16 13:21:50.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.189.41&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:21:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:21:50.615: INFO: ExecWithOptions: Clientset creation
Dec 16 13:21:50.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.189.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:21:50.733: INFO: Waiting for responses: map[]
Dec 16 13:21:50.733: INFO: reached 192.168.189.41 after 0/1 tries
Dec 16 13:21:50.733: INFO: Breadth first check of 192.168.189.214 on host 85.217.161.213...
Dec 16 13:21:50.738: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.189.214&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:21:50.738: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:21:50.739: INFO: ExecWithOptions: Clientset creation
Dec 16 13:21:50.739: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.189.214%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:21:50.866: INFO: Waiting for responses: map[]
Dec 16 13:21:50.866: INFO: reached 192.168.189.214 after 0/1 tries
Dec 16 13:21:50.866: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 16 13:21:50.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2093" for this suite. 12/16/22 13:21:50.873
------------------------------
• [SLOW TEST] [24.486 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:21:26.394
    Dec 16 13:21:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:21:26.394
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:26.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:26.409
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-2093 12/16/22 13:21:26.411
    STEP: creating a selector 12/16/22 13:21:26.412
    STEP: Creating the service pods in kubernetes 12/16/22 13:21:26.412
    Dec 16 13:21:26.412: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 16 13:21:26.436: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2093" to be "running and ready"
    Dec 16 13:21:26.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328165ms
    Dec 16 13:21:26.441: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:21:28.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009603775s
    Dec 16 13:21:28.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:30.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009957829s
    Dec 16 13:21:30.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:32.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011092072s
    Dec 16 13:21:32.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:34.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009923084s
    Dec 16 13:21:34.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:36.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009129239s
    Dec 16 13:21:36.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:38.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011242418s
    Dec 16 13:21:38.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:40.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011313045s
    Dec 16 13:21:40.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:42.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010368211s
    Dec 16 13:21:42.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:44.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011442404s
    Dec 16 13:21:44.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:46.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009465399s
    Dec 16 13:21:46.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:21:48.445: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008208759s
    Dec 16 13:21:48.445: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 16 13:21:48.445: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 16 13:21:48.449: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2093" to be "running and ready"
    Dec 16 13:21:48.452: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.532448ms
    Dec 16 13:21:48.452: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 16 13:21:48.452: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 16 13:21:48.455: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2093" to be "running and ready"
    Dec 16 13:21:48.458: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.921373ms
    Dec 16 13:21:48.458: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 16 13:21:48.458: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/16/22 13:21:48.462
    Dec 16 13:21:48.467: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2093" to be "running"
    Dec 16 13:21:48.470: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093916ms
    Dec 16 13:21:50.474: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492533s
    Dec 16 13:21:50.474: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 16 13:21:50.477: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 16 13:21:50.477: INFO: Breadth first check of 192.168.156.146 on host 85.217.161.222...
    Dec 16 13:21:50.480: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.156.146&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:21:50.480: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:21:50.481: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:21:50.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.156.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:21:50.609: INFO: Waiting for responses: map[]
    Dec 16 13:21:50.609: INFO: reached 192.168.156.146 after 0/1 tries
    Dec 16 13:21:50.609: INFO: Breadth first check of 192.168.189.41 on host 85.217.161.242...
    Dec 16 13:21:50.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.189.41&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:21:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:21:50.615: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:21:50.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.189.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:21:50.733: INFO: Waiting for responses: map[]
    Dec 16 13:21:50.733: INFO: reached 192.168.189.41 after 0/1 tries
    Dec 16 13:21:50.733: INFO: Breadth first check of 192.168.189.214 on host 85.217.161.213...
    Dec 16 13:21:50.738: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.42:9080/dial?request=hostname&protocol=http&host=192.168.189.214&port=8083&tries=1'] Namespace:pod-network-test-2093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:21:50.738: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:21:50.739: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:21:50.739: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.189.214%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:21:50.866: INFO: Waiting for responses: map[]
    Dec 16 13:21:50.866: INFO: reached 192.168.189.214 after 0/1 tries
    Dec 16 13:21:50.866: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:21:50.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2093" for this suite. 12/16/22 13:21:50.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:21:50.88
Dec 16 13:21:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 13:21:50.881
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:50.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:50.896
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7226 12/16/22 13:21:50.899
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 12/16/22 13:21:50.905
Dec 16 13:21:50.913: INFO: Found 0 stateful pods, waiting for 3
Dec 16 13:22:00.919: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:22:00.919: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:22:00.919: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/16/22 13:22:00.931
Dec 16 13:22:00.952: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/16/22 13:22:00.952
STEP: Not applying an update when the partition is greater than the number of replicas 12/16/22 13:22:11.263
STEP: Performing a canary update 12/16/22 13:22:11.263
Dec 16 13:22:12.346: INFO: Updating stateful set ss2
Dec 16 13:22:12.352: INFO: Waiting for Pod statefulset-7226/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 12/16/22 13:22:22.362
Dec 16 13:22:22.389: INFO: Found 2 stateful pods, waiting for 3
Dec 16 13:22:32.396: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:22:32.396: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:22:32.396: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 12/16/22 13:22:32.404
Dec 16 13:22:32.425: INFO: Updating stateful set ss2
Dec 16 13:22:32.433: INFO: Waiting for Pod statefulset-7226/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Dec 16 13:22:42.464: INFO: Updating stateful set ss2
Dec 16 13:22:42.469: INFO: Waiting for StatefulSet statefulset-7226/ss2 to complete update
Dec 16 13:22:42.469: INFO: Waiting for Pod statefulset-7226/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 13:22:52.480: INFO: Deleting all statefulset in ns statefulset-7226
Dec 16 13:22:52.483: INFO: Scaling statefulset ss2 to 0
Dec 16 13:23:02.500: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 13:23:02.502: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:02.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7226" for this suite. 12/16/22 13:23:02.517
------------------------------
• [SLOW TEST] [71.642 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:21:50.88
    Dec 16 13:21:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 13:21:50.881
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:21:50.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:21:50.896
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7226 12/16/22 13:21:50.899
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 12/16/22 13:21:50.905
    Dec 16 13:21:50.913: INFO: Found 0 stateful pods, waiting for 3
    Dec 16 13:22:00.919: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:22:00.919: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:22:00.919: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/16/22 13:22:00.931
    Dec 16 13:22:00.952: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/16/22 13:22:00.952
    STEP: Not applying an update when the partition is greater than the number of replicas 12/16/22 13:22:11.263
    STEP: Performing a canary update 12/16/22 13:22:11.263
    Dec 16 13:22:12.346: INFO: Updating stateful set ss2
    Dec 16 13:22:12.352: INFO: Waiting for Pod statefulset-7226/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 12/16/22 13:22:22.362
    Dec 16 13:22:22.389: INFO: Found 2 stateful pods, waiting for 3
    Dec 16 13:22:32.396: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:22:32.396: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:22:32.396: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 12/16/22 13:22:32.404
    Dec 16 13:22:32.425: INFO: Updating stateful set ss2
    Dec 16 13:22:32.433: INFO: Waiting for Pod statefulset-7226/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Dec 16 13:22:42.464: INFO: Updating stateful set ss2
    Dec 16 13:22:42.469: INFO: Waiting for StatefulSet statefulset-7226/ss2 to complete update
    Dec 16 13:22:42.469: INFO: Waiting for Pod statefulset-7226/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 13:22:52.480: INFO: Deleting all statefulset in ns statefulset-7226
    Dec 16 13:22:52.483: INFO: Scaling statefulset ss2 to 0
    Dec 16 13:23:02.500: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 13:23:02.502: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:02.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7226" for this suite. 12/16/22 13:23:02.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:02.523
Dec 16 13:23:02.523: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:23:02.524
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:02.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:02.538
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Dec 16 13:23:02.557: INFO: created pod pod-service-account-defaultsa
Dec 16 13:23:02.557: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 16 13:23:02.562: INFO: created pod pod-service-account-mountsa
Dec 16 13:23:02.562: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 16 13:23:02.568: INFO: created pod pod-service-account-nomountsa
Dec 16 13:23:02.568: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 16 13:23:02.572: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 16 13:23:02.572: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 16 13:23:02.577: INFO: created pod pod-service-account-mountsa-mountspec
Dec 16 13:23:02.577: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 16 13:23:02.587: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 16 13:23:02.587: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 16 13:23:02.592: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 16 13:23:02.592: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 16 13:23:02.596: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 16 13:23:02.596: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 16 13:23:02.604: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 16 13:23:02.604: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-485" for this suite. 12/16/22 13:23:02.609
------------------------------
• [0.093 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:02.523
    Dec 16 13:23:02.523: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:23:02.524
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:02.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:02.538
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Dec 16 13:23:02.557: INFO: created pod pod-service-account-defaultsa
    Dec 16 13:23:02.557: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Dec 16 13:23:02.562: INFO: created pod pod-service-account-mountsa
    Dec 16 13:23:02.562: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Dec 16 13:23:02.568: INFO: created pod pod-service-account-nomountsa
    Dec 16 13:23:02.568: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Dec 16 13:23:02.572: INFO: created pod pod-service-account-defaultsa-mountspec
    Dec 16 13:23:02.572: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Dec 16 13:23:02.577: INFO: created pod pod-service-account-mountsa-mountspec
    Dec 16 13:23:02.577: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Dec 16 13:23:02.587: INFO: created pod pod-service-account-nomountsa-mountspec
    Dec 16 13:23:02.587: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Dec 16 13:23:02.592: INFO: created pod pod-service-account-defaultsa-nomountspec
    Dec 16 13:23:02.592: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Dec 16 13:23:02.596: INFO: created pod pod-service-account-mountsa-nomountspec
    Dec 16 13:23:02.596: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Dec 16 13:23:02.604: INFO: created pod pod-service-account-nomountsa-nomountspec
    Dec 16 13:23:02.604: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-485" for this suite. 12/16/22 13:23:02.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:02.616
Dec 16 13:23:02.616: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:02.617
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:02.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:02.632
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/16/22 13:23:02.634
Dec 16 13:23:02.642: INFO: Waiting up to 5m0s for pod "pod-6bc07309-6334-47b5-8923-759895ed8414" in namespace "emptydir-9000" to be "Succeeded or Failed"
Dec 16 13:23:02.646: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13905ms
Dec 16 13:23:04.655: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012466668s
Dec 16 13:23:06.651: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762152s
STEP: Saw pod success 12/16/22 13:23:06.651
Dec 16 13:23:06.651: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414" satisfied condition "Succeeded or Failed"
Dec 16 13:23:06.654: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-6bc07309-6334-47b5-8923-759895ed8414 container test-container: <nil>
STEP: delete the pod 12/16/22 13:23:06.702
Dec 16 13:23:06.713: INFO: Waiting for pod pod-6bc07309-6334-47b5-8923-759895ed8414 to disappear
Dec 16 13:23:06.716: INFO: Pod pod-6bc07309-6334-47b5-8923-759895ed8414 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:06.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9000" for this suite. 12/16/22 13:23:06.72
------------------------------
• [4.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:02.616
    Dec 16 13:23:02.616: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:02.617
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:02.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:02.632
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/16/22 13:23:02.634
    Dec 16 13:23:02.642: INFO: Waiting up to 5m0s for pod "pod-6bc07309-6334-47b5-8923-759895ed8414" in namespace "emptydir-9000" to be "Succeeded or Failed"
    Dec 16 13:23:02.646: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13905ms
    Dec 16 13:23:04.655: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012466668s
    Dec 16 13:23:06.651: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762152s
    STEP: Saw pod success 12/16/22 13:23:06.651
    Dec 16 13:23:06.651: INFO: Pod "pod-6bc07309-6334-47b5-8923-759895ed8414" satisfied condition "Succeeded or Failed"
    Dec 16 13:23:06.654: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-6bc07309-6334-47b5-8923-759895ed8414 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:23:06.702
    Dec 16 13:23:06.713: INFO: Waiting for pod pod-6bc07309-6334-47b5-8923-759895ed8414 to disappear
    Dec 16 13:23:06.716: INFO: Pod pod-6bc07309-6334-47b5-8923-759895ed8414 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:06.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9000" for this suite. 12/16/22 13:23:06.72
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:06.726
Dec 16 13:23:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:06.726
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:06.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:06.74
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 12/16/22 13:23:06.742
Dec 16 13:23:06.750: INFO: Waiting up to 5m0s for pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3" in namespace "emptydir-4510" to be "Succeeded or Failed"
Dec 16 13:23:06.753: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487922ms
Dec 16 13:23:08.758: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008133546s
Dec 16 13:23:10.759: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009163361s
STEP: Saw pod success 12/16/22 13:23:10.759
Dec 16 13:23:10.759: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3" satisfied condition "Succeeded or Failed"
Dec 16 13:23:10.763: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 container test-container: <nil>
STEP: delete the pod 12/16/22 13:23:10.769
Dec 16 13:23:10.781: INFO: Waiting for pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 to disappear
Dec 16 13:23:10.784: INFO: Pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:10.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4510" for this suite. 12/16/22 13:23:10.788
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:06.726
    Dec 16 13:23:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:06.726
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:06.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:06.74
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/16/22 13:23:06.742
    Dec 16 13:23:06.750: INFO: Waiting up to 5m0s for pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3" in namespace "emptydir-4510" to be "Succeeded or Failed"
    Dec 16 13:23:06.753: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487922ms
    Dec 16 13:23:08.758: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008133546s
    Dec 16 13:23:10.759: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009163361s
    STEP: Saw pod success 12/16/22 13:23:10.759
    Dec 16 13:23:10.759: INFO: Pod "pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3" satisfied condition "Succeeded or Failed"
    Dec 16 13:23:10.763: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:23:10.769
    Dec 16 13:23:10.781: INFO: Waiting for pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 to disappear
    Dec 16 13:23:10.784: INFO: Pod pod-58cc88c6-16d7-41df-8c2c-222eca28b4e3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:10.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4510" for this suite. 12/16/22 13:23:10.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:10.795
Dec 16 13:23:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:10.796
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:10.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:10.812
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Dec 16 13:23:10.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8518 version'
Dec 16 13:23:10.874: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec 16 13:23:10.874: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:10.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8518" for this suite. 12/16/22 13:23:10.879
------------------------------
• [0.091 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:10.795
    Dec 16 13:23:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:10.796
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:10.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:10.812
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Dec 16 13:23:10.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8518 version'
    Dec 16 13:23:10.874: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Dec 16 13:23:10.874: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:10.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8518" for this suite. 12/16/22 13:23:10.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:10.887
Dec 16 13:23:10.887: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:10.887
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:10.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:10.903
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 12/16/22 13:23:10.905
Dec 16 13:23:10.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 16 13:23:10.986: INFO: stderr: ""
Dec 16 13:23:10.986: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 12/16/22 13:23:10.986
Dec 16 13:23:10.986: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 16 13:23:10.986: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9595" to be "running and ready, or succeeded"
Dec 16 13:23:10.990: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906443ms
Dec 16 13:23:10.990: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Dec 16 13:23:12.994: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007971653s
Dec 16 13:23:12.994: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pool-a3802-fsxxd' to be 'Running' but was 'Pending'
Dec 16 13:23:14.995: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.009045545s
Dec 16 13:23:14.995: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 16 13:23:14.995: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 12/16/22 13:23:14.995
Dec 16 13:23:14.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator'
Dec 16 13:23:15.111: INFO: stderr: ""
Dec 16 13:23:15.111: INFO: stdout: "I1216 13:23:12.310516       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2kn 305\nI1216 13:23:12.511085       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/5ntp 347\nI1216 13:23:12.711444       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rbkc 417\nI1216 13:23:12.911000       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/x6kn 203\nI1216 13:23:13.111382       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/m6f 586\nI1216 13:23:13.310757       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/c9w 353\nI1216 13:23:13.511244       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4wf5 212\nI1216 13:23:13.710692       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/r27q 558\nI1216 13:23:13.911146       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sk26 384\nI1216 13:23:14.111627       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/65r2 234\nI1216 13:23:14.311048       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/l85 261\nI1216 13:23:14.511582       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/c2f 526\nI1216 13:23:14.710953       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/rqvs 576\nI1216 13:23:14.911443       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/mvt 560\n"
STEP: limiting log lines 12/16/22 13:23:15.111
Dec 16 13:23:15.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --tail=1'
Dec 16 13:23:15.230: INFO: stderr: ""
Dec 16 13:23:15.230: INFO: stdout: "I1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\n"
Dec 16 13:23:15.230: INFO: got output "I1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\n"
STEP: limiting log bytes 12/16/22 13:23:15.23
Dec 16 13:23:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --limit-bytes=1'
Dec 16 13:23:15.305: INFO: stderr: ""
Dec 16 13:23:15.305: INFO: stdout: "I"
Dec 16 13:23:15.305: INFO: got output "I"
STEP: exposing timestamps 12/16/22 13:23:15.305
Dec 16 13:23:15.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 16 13:23:15.382: INFO: stderr: ""
Dec 16 13:23:15.382: INFO: stdout: "2022-12-16T13:23:15.311394026Z I1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\n"
Dec 16 13:23:15.382: INFO: got output "2022-12-16T13:23:15.311394026Z I1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\n"
STEP: restricting to a time range 12/16/22 13:23:15.382
Dec 16 13:23:17.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --since=1s'
Dec 16 13:23:17.959: INFO: stderr: ""
Dec 16 13:23:17.959: INFO: stdout: "I1216 13:23:17.111004       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/jmq 270\nI1216 13:23:17.311361       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8ph 491\nI1216 13:23:17.510801       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/q6f 578\nI1216 13:23:17.711146       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/nnft 253\nI1216 13:23:17.911508       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/94w 526\n"
Dec 16 13:23:17.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --since=24h'
Dec 16 13:23:18.038: INFO: stderr: ""
Dec 16 13:23:18.038: INFO: stdout: "I1216 13:23:12.310516       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2kn 305\nI1216 13:23:12.511085       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/5ntp 347\nI1216 13:23:12.711444       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rbkc 417\nI1216 13:23:12.911000       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/x6kn 203\nI1216 13:23:13.111382       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/m6f 586\nI1216 13:23:13.310757       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/c9w 353\nI1216 13:23:13.511244       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4wf5 212\nI1216 13:23:13.710692       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/r27q 558\nI1216 13:23:13.911146       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sk26 384\nI1216 13:23:14.111627       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/65r2 234\nI1216 13:23:14.311048       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/l85 261\nI1216 13:23:14.511582       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/c2f 526\nI1216 13:23:14.710953       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/rqvs 576\nI1216 13:23:14.911443       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/mvt 560\nI1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\nI1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\nI1216 13:23:15.511604       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/952 559\nI1216 13:23:15.711034       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/fth 341\nI1216 13:23:15.911402       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/k5d 394\nI1216 13:23:16.110797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/q46q 563\nI1216 13:23:16.311201       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hh9b 396\nI1216 13:23:16.510615       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/x9r 449\nI1216 13:23:16.711025       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/tdb 219\nI1216 13:23:16.911504       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/kxs8 257\nI1216 13:23:17.111004       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/jmq 270\nI1216 13:23:17.311361       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8ph 491\nI1216 13:23:17.510801       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/q6f 578\nI1216 13:23:17.711146       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/nnft 253\nI1216 13:23:17.911508       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/94w 526\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Dec 16 13:23:18.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 delete pod logs-generator'
Dec 16 13:23:18.670: INFO: stderr: ""
Dec 16 13:23:18.670: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:18.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9595" for this suite. 12/16/22 13:23:18.675
------------------------------
• [SLOW TEST] [7.795 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:10.887
    Dec 16 13:23:10.887: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:10.887
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:10.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:10.903
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 12/16/22 13:23:10.905
    Dec 16 13:23:10.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Dec 16 13:23:10.986: INFO: stderr: ""
    Dec 16 13:23:10.986: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 12/16/22 13:23:10.986
    Dec 16 13:23:10.986: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Dec 16 13:23:10.986: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9595" to be "running and ready, or succeeded"
    Dec 16 13:23:10.990: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906443ms
    Dec 16 13:23:10.990: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Dec 16 13:23:12.994: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007971653s
    Dec 16 13:23:12.994: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pool-a3802-fsxxd' to be 'Running' but was 'Pending'
    Dec 16 13:23:14.995: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.009045545s
    Dec 16 13:23:14.995: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Dec 16 13:23:14.995: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 12/16/22 13:23:14.995
    Dec 16 13:23:14.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator'
    Dec 16 13:23:15.111: INFO: stderr: ""
    Dec 16 13:23:15.111: INFO: stdout: "I1216 13:23:12.310516       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2kn 305\nI1216 13:23:12.511085       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/5ntp 347\nI1216 13:23:12.711444       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rbkc 417\nI1216 13:23:12.911000       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/x6kn 203\nI1216 13:23:13.111382       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/m6f 586\nI1216 13:23:13.310757       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/c9w 353\nI1216 13:23:13.511244       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4wf5 212\nI1216 13:23:13.710692       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/r27q 558\nI1216 13:23:13.911146       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sk26 384\nI1216 13:23:14.111627       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/65r2 234\nI1216 13:23:14.311048       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/l85 261\nI1216 13:23:14.511582       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/c2f 526\nI1216 13:23:14.710953       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/rqvs 576\nI1216 13:23:14.911443       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/mvt 560\n"
    STEP: limiting log lines 12/16/22 13:23:15.111
    Dec 16 13:23:15.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --tail=1'
    Dec 16 13:23:15.230: INFO: stderr: ""
    Dec 16 13:23:15.230: INFO: stdout: "I1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\n"
    Dec 16 13:23:15.230: INFO: got output "I1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\n"
    STEP: limiting log bytes 12/16/22 13:23:15.23
    Dec 16 13:23:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --limit-bytes=1'
    Dec 16 13:23:15.305: INFO: stderr: ""
    Dec 16 13:23:15.305: INFO: stdout: "I"
    Dec 16 13:23:15.305: INFO: got output "I"
    STEP: exposing timestamps 12/16/22 13:23:15.305
    Dec 16 13:23:15.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --tail=1 --timestamps'
    Dec 16 13:23:15.382: INFO: stderr: ""
    Dec 16 13:23:15.382: INFO: stdout: "2022-12-16T13:23:15.311394026Z I1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\n"
    Dec 16 13:23:15.382: INFO: got output "2022-12-16T13:23:15.311394026Z I1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\n"
    STEP: restricting to a time range 12/16/22 13:23:15.382
    Dec 16 13:23:17.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --since=1s'
    Dec 16 13:23:17.959: INFO: stderr: ""
    Dec 16 13:23:17.959: INFO: stdout: "I1216 13:23:17.111004       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/jmq 270\nI1216 13:23:17.311361       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8ph 491\nI1216 13:23:17.510801       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/q6f 578\nI1216 13:23:17.711146       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/nnft 253\nI1216 13:23:17.911508       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/94w 526\n"
    Dec 16 13:23:17.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 logs logs-generator logs-generator --since=24h'
    Dec 16 13:23:18.038: INFO: stderr: ""
    Dec 16 13:23:18.038: INFO: stdout: "I1216 13:23:12.310516       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2kn 305\nI1216 13:23:12.511085       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/5ntp 347\nI1216 13:23:12.711444       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rbkc 417\nI1216 13:23:12.911000       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/x6kn 203\nI1216 13:23:13.111382       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/m6f 586\nI1216 13:23:13.310757       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/c9w 353\nI1216 13:23:13.511244       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4wf5 212\nI1216 13:23:13.710692       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/r27q 558\nI1216 13:23:13.911146       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sk26 384\nI1216 13:23:14.111627       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/65r2 234\nI1216 13:23:14.311048       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/l85 261\nI1216 13:23:14.511582       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/c2f 526\nI1216 13:23:14.710953       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/rqvs 576\nI1216 13:23:14.911443       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/mvt 560\nI1216 13:23:15.110742       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/9l2f 512\nI1216 13:23:15.311130       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/mks 567\nI1216 13:23:15.511604       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/952 559\nI1216 13:23:15.711034       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/fth 341\nI1216 13:23:15.911402       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/k5d 394\nI1216 13:23:16.110797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/q46q 563\nI1216 13:23:16.311201       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hh9b 396\nI1216 13:23:16.510615       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/x9r 449\nI1216 13:23:16.711025       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/tdb 219\nI1216 13:23:16.911504       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/kxs8 257\nI1216 13:23:17.111004       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/jmq 270\nI1216 13:23:17.311361       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8ph 491\nI1216 13:23:17.510801       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/q6f 578\nI1216 13:23:17.711146       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/nnft 253\nI1216 13:23:17.911508       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/94w 526\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Dec 16 13:23:18.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-9595 delete pod logs-generator'
    Dec 16 13:23:18.670: INFO: stderr: ""
    Dec 16 13:23:18.670: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:18.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9595" for this suite. 12/16/22 13:23:18.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:18.684
Dec 16 13:23:18.684: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:23:18.684
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:18.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:18.699
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1217" for this suite. 12/16/22 13:23:18.728
------------------------------
• [0.050 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:18.684
    Dec 16 13:23:18.684: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:23:18.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:18.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:18.699
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1217" for this suite. 12/16/22 13:23:18.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:18.734
Dec 16 13:23:18.734: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sysctl 12/16/22 13:23:18.735
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:18.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:18.75
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/16/22 13:23:18.752
STEP: Watching for error events or started pod 12/16/22 13:23:18.759
STEP: Waiting for pod completion 12/16/22 13:23:20.764
Dec 16 13:23:20.765: INFO: Waiting up to 3m0s for pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a" in namespace "sysctl-5195" to be "completed"
Dec 16 13:23:20.768: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105136ms
Dec 16 13:23:22.772: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007427209s
Dec 16 13:23:22.772: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a" satisfied condition "completed"
STEP: Checking that the pod succeeded 12/16/22 13:23:22.776
STEP: Getting logs from the pod 12/16/22 13:23:22.776
STEP: Checking that the sysctl is actually updated 12/16/22 13:23:22.784
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:22.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5195" for this suite. 12/16/22 13:23:22.789
------------------------------
• [4.061 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:18.734
    Dec 16 13:23:18.734: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sysctl 12/16/22 13:23:18.735
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:18.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:18.75
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/16/22 13:23:18.752
    STEP: Watching for error events or started pod 12/16/22 13:23:18.759
    STEP: Waiting for pod completion 12/16/22 13:23:20.764
    Dec 16 13:23:20.765: INFO: Waiting up to 3m0s for pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a" in namespace "sysctl-5195" to be "completed"
    Dec 16 13:23:20.768: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105136ms
    Dec 16 13:23:22.772: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007427209s
    Dec 16 13:23:22.772: INFO: Pod "sysctl-ffb88682-c9ac-46d9-bf07-d893a58e3c3a" satisfied condition "completed"
    STEP: Checking that the pod succeeded 12/16/22 13:23:22.776
    STEP: Getting logs from the pod 12/16/22 13:23:22.776
    STEP: Checking that the sysctl is actually updated 12/16/22 13:23:22.784
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:22.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5195" for this suite. 12/16/22 13:23:22.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:22.797
Dec 16 13:23:22.797: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:22.798
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:22.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:22.813
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 12/16/22 13:23:22.815
Dec 16 13:23:22.823: INFO: Waiting up to 5m0s for pod "pod-81b08455-0650-4831-8a29-694345f77ab7" in namespace "emptydir-7123" to be "Succeeded or Failed"
Dec 16 13:23:22.827: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.839564ms
Dec 16 13:23:24.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009774777s
Dec 16 13:23:26.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009535089s
STEP: Saw pod success 12/16/22 13:23:26.833
Dec 16 13:23:26.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7" satisfied condition "Succeeded or Failed"
Dec 16 13:23:26.836: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-81b08455-0650-4831-8a29-694345f77ab7 container test-container: <nil>
STEP: delete the pod 12/16/22 13:23:26.845
Dec 16 13:23:26.859: INFO: Waiting for pod pod-81b08455-0650-4831-8a29-694345f77ab7 to disappear
Dec 16 13:23:26.862: INFO: Pod pod-81b08455-0650-4831-8a29-694345f77ab7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:26.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7123" for this suite. 12/16/22 13:23:26.866
------------------------------
• [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:22.797
    Dec 16 13:23:22.797: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:23:22.798
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:22.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:22.813
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/16/22 13:23:22.815
    Dec 16 13:23:22.823: INFO: Waiting up to 5m0s for pod "pod-81b08455-0650-4831-8a29-694345f77ab7" in namespace "emptydir-7123" to be "Succeeded or Failed"
    Dec 16 13:23:22.827: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.839564ms
    Dec 16 13:23:24.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009774777s
    Dec 16 13:23:26.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009535089s
    STEP: Saw pod success 12/16/22 13:23:26.833
    Dec 16 13:23:26.833: INFO: Pod "pod-81b08455-0650-4831-8a29-694345f77ab7" satisfied condition "Succeeded or Failed"
    Dec 16 13:23:26.836: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-81b08455-0650-4831-8a29-694345f77ab7 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:23:26.845
    Dec 16 13:23:26.859: INFO: Waiting for pod pod-81b08455-0650-4831-8a29-694345f77ab7 to disappear
    Dec 16 13:23:26.862: INFO: Pod pod-81b08455-0650-4831-8a29-694345f77ab7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:26.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7123" for this suite. 12/16/22 13:23:26.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:26.873
Dec 16 13:23:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename discovery 12/16/22 13:23:26.874
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:26.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:26.889
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 12/16/22 13:23:26.892
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Dec 16 13:23:27.476: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 16 13:23:27.477: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 16 13:23:27.477: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec 16 13:23:27.477: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 16 13:23:27.477: INFO: Checking APIGroup: apps
Dec 16 13:23:27.478: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 16 13:23:27.478: INFO: Versions found [{apps/v1 v1}]
Dec 16 13:23:27.478: INFO: apps/v1 matches apps/v1
Dec 16 13:23:27.478: INFO: Checking APIGroup: events.k8s.io
Dec 16 13:23:27.479: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 16 13:23:27.479: INFO: Versions found [{events.k8s.io/v1 v1}]
Dec 16 13:23:27.479: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 16 13:23:27.479: INFO: Checking APIGroup: authentication.k8s.io
Dec 16 13:23:27.480: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 16 13:23:27.480: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec 16 13:23:27.480: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 16 13:23:27.480: INFO: Checking APIGroup: authorization.k8s.io
Dec 16 13:23:27.481: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 16 13:23:27.481: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec 16 13:23:27.481: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 16 13:23:27.481: INFO: Checking APIGroup: autoscaling
Dec 16 13:23:27.482: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec 16 13:23:27.482: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Dec 16 13:23:27.482: INFO: autoscaling/v2 matches autoscaling/v2
Dec 16 13:23:27.482: INFO: Checking APIGroup: batch
Dec 16 13:23:27.483: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 16 13:23:27.483: INFO: Versions found [{batch/v1 v1}]
Dec 16 13:23:27.483: INFO: batch/v1 matches batch/v1
Dec 16 13:23:27.483: INFO: Checking APIGroup: certificates.k8s.io
Dec 16 13:23:27.484: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 16 13:23:27.484: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec 16 13:23:27.484: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 16 13:23:27.484: INFO: Checking APIGroup: networking.k8s.io
Dec 16 13:23:27.485: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 16 13:23:27.485: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Dec 16 13:23:27.485: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 16 13:23:27.485: INFO: Checking APIGroup: policy
Dec 16 13:23:27.486: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 16 13:23:27.486: INFO: Versions found [{policy/v1 v1}]
Dec 16 13:23:27.486: INFO: policy/v1 matches policy/v1
Dec 16 13:23:27.486: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 16 13:23:27.487: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 16 13:23:27.487: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec 16 13:23:27.487: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 16 13:23:27.487: INFO: Checking APIGroup: storage.k8s.io
Dec 16 13:23:27.488: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 16 13:23:27.488: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 16 13:23:27.488: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 16 13:23:27.488: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 16 13:23:27.489: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 16 13:23:27.489: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
Dec 16 13:23:27.489: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 16 13:23:27.489: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 16 13:23:27.490: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 16 13:23:27.490: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec 16 13:23:27.490: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 16 13:23:27.490: INFO: Checking APIGroup: scheduling.k8s.io
Dec 16 13:23:27.491: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 16 13:23:27.491: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec 16 13:23:27.491: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 16 13:23:27.491: INFO: Checking APIGroup: coordination.k8s.io
Dec 16 13:23:27.492: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 16 13:23:27.492: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec 16 13:23:27.492: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 16 13:23:27.492: INFO: Checking APIGroup: node.k8s.io
Dec 16 13:23:27.493: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 16 13:23:27.493: INFO: Versions found [{node.k8s.io/v1 v1}]
Dec 16 13:23:27.493: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 16 13:23:27.493: INFO: Checking APIGroup: discovery.k8s.io
Dec 16 13:23:27.494: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 16 13:23:27.494: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Dec 16 13:23:27.494: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 16 13:23:27.494: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 16 13:23:27.494: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Dec 16 13:23:27.494: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Dec 16 13:23:27.494: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Dec 16 13:23:27.494: INFO: Checking APIGroup: internal.apiserver.k8s.io
Dec 16 13:23:27.495: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Dec 16 13:23:27.495: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Dec 16 13:23:27.495: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Dec 16 13:23:27.495: INFO: Checking APIGroup: resource.k8s.io
Dec 16 13:23:27.496: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
Dec 16 13:23:27.496: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
Dec 16 13:23:27.496: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
Dec 16 13:23:27.496: INFO: Checking APIGroup: crd.projectcalico.org
Dec 16 13:23:27.497: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 16 13:23:27.497: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 16 13:23:27.497: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:27.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1122" for this suite. 12/16/22 13:23:27.501
------------------------------
• [0.634 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:26.873
    Dec 16 13:23:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename discovery 12/16/22 13:23:26.874
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:26.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:26.889
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 12/16/22 13:23:26.892
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Dec 16 13:23:27.476: INFO: Checking APIGroup: apiregistration.k8s.io
    Dec 16 13:23:27.477: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Dec 16 13:23:27.477: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Dec 16 13:23:27.477: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Dec 16 13:23:27.477: INFO: Checking APIGroup: apps
    Dec 16 13:23:27.478: INFO: PreferredVersion.GroupVersion: apps/v1
    Dec 16 13:23:27.478: INFO: Versions found [{apps/v1 v1}]
    Dec 16 13:23:27.478: INFO: apps/v1 matches apps/v1
    Dec 16 13:23:27.478: INFO: Checking APIGroup: events.k8s.io
    Dec 16 13:23:27.479: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Dec 16 13:23:27.479: INFO: Versions found [{events.k8s.io/v1 v1}]
    Dec 16 13:23:27.479: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Dec 16 13:23:27.479: INFO: Checking APIGroup: authentication.k8s.io
    Dec 16 13:23:27.480: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Dec 16 13:23:27.480: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Dec 16 13:23:27.480: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Dec 16 13:23:27.480: INFO: Checking APIGroup: authorization.k8s.io
    Dec 16 13:23:27.481: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Dec 16 13:23:27.481: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Dec 16 13:23:27.481: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Dec 16 13:23:27.481: INFO: Checking APIGroup: autoscaling
    Dec 16 13:23:27.482: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Dec 16 13:23:27.482: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Dec 16 13:23:27.482: INFO: autoscaling/v2 matches autoscaling/v2
    Dec 16 13:23:27.482: INFO: Checking APIGroup: batch
    Dec 16 13:23:27.483: INFO: PreferredVersion.GroupVersion: batch/v1
    Dec 16 13:23:27.483: INFO: Versions found [{batch/v1 v1}]
    Dec 16 13:23:27.483: INFO: batch/v1 matches batch/v1
    Dec 16 13:23:27.483: INFO: Checking APIGroup: certificates.k8s.io
    Dec 16 13:23:27.484: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Dec 16 13:23:27.484: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Dec 16 13:23:27.484: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Dec 16 13:23:27.484: INFO: Checking APIGroup: networking.k8s.io
    Dec 16 13:23:27.485: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Dec 16 13:23:27.485: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Dec 16 13:23:27.485: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Dec 16 13:23:27.485: INFO: Checking APIGroup: policy
    Dec 16 13:23:27.486: INFO: PreferredVersion.GroupVersion: policy/v1
    Dec 16 13:23:27.486: INFO: Versions found [{policy/v1 v1}]
    Dec 16 13:23:27.486: INFO: policy/v1 matches policy/v1
    Dec 16 13:23:27.486: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Dec 16 13:23:27.487: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Dec 16 13:23:27.487: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Dec 16 13:23:27.487: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Dec 16 13:23:27.487: INFO: Checking APIGroup: storage.k8s.io
    Dec 16 13:23:27.488: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Dec 16 13:23:27.488: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Dec 16 13:23:27.488: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Dec 16 13:23:27.488: INFO: Checking APIGroup: admissionregistration.k8s.io
    Dec 16 13:23:27.489: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Dec 16 13:23:27.489: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
    Dec 16 13:23:27.489: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Dec 16 13:23:27.489: INFO: Checking APIGroup: apiextensions.k8s.io
    Dec 16 13:23:27.490: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Dec 16 13:23:27.490: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Dec 16 13:23:27.490: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Dec 16 13:23:27.490: INFO: Checking APIGroup: scheduling.k8s.io
    Dec 16 13:23:27.491: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Dec 16 13:23:27.491: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Dec 16 13:23:27.491: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Dec 16 13:23:27.491: INFO: Checking APIGroup: coordination.k8s.io
    Dec 16 13:23:27.492: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Dec 16 13:23:27.492: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Dec 16 13:23:27.492: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Dec 16 13:23:27.492: INFO: Checking APIGroup: node.k8s.io
    Dec 16 13:23:27.493: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Dec 16 13:23:27.493: INFO: Versions found [{node.k8s.io/v1 v1}]
    Dec 16 13:23:27.493: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Dec 16 13:23:27.493: INFO: Checking APIGroup: discovery.k8s.io
    Dec 16 13:23:27.494: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Dec 16 13:23:27.494: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Dec 16 13:23:27.494: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Dec 16 13:23:27.494: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Dec 16 13:23:27.494: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Dec 16 13:23:27.494: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Dec 16 13:23:27.494: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Dec 16 13:23:27.494: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Dec 16 13:23:27.495: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Dec 16 13:23:27.495: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Dec 16 13:23:27.495: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Dec 16 13:23:27.495: INFO: Checking APIGroup: resource.k8s.io
    Dec 16 13:23:27.496: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
    Dec 16 13:23:27.496: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
    Dec 16 13:23:27.496: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
    Dec 16 13:23:27.496: INFO: Checking APIGroup: crd.projectcalico.org
    Dec 16 13:23:27.497: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Dec 16 13:23:27.497: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Dec 16 13:23:27.497: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:27.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1122" for this suite. 12/16/22 13:23:27.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:27.507
Dec 16 13:23:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:27.508
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:27.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:27.522
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 12/16/22 13:23:27.525
Dec 16 13:23:27.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-7502 cluster-info'
Dec 16 13:23:27.591: INFO: stderr: ""
Dec 16 13:23:27.592: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:27.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7502" for this suite. 12/16/22 13:23:27.597
------------------------------
• [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:27.507
    Dec 16 13:23:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:27.508
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:27.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:27.522
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 12/16/22 13:23:27.525
    Dec 16 13:23:27.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-7502 cluster-info'
    Dec 16 13:23:27.591: INFO: stderr: ""
    Dec 16 13:23:27.592: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:27.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7502" for this suite. 12/16/22 13:23:27.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:27.604
Dec 16 13:23:27.604: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:23:27.605
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:27.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:27.619
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Dec 16 13:23:27.622: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9815" for this suite. 12/16/22 13:23:35.603
------------------------------
• [SLOW TEST] [8.004 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:27.604
    Dec 16 13:23:27.604: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:23:27.605
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:27.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:27.619
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Dec 16 13:23:27.622: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9815" for this suite. 12/16/22 13:23:35.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:35.608
Dec 16 13:23:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:23:35.609
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:35.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:35.627
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 12/16/22 13:23:35.637
Dec 16 13:23:35.637: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9" in namespace "kubelet-test-6270" to be "completed"
Dec 16 13:23:35.640: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662115ms
Dec 16 13:23:37.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007967404s
Dec 16 13:23:39.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007485413s
Dec 16 13:23:39.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:39.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6270" for this suite. 12/16/22 13:23:39.657
------------------------------
• [4.056 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:35.608
    Dec 16 13:23:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:23:35.609
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:35.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:35.627
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 12/16/22 13:23:35.637
    Dec 16 13:23:35.637: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9" in namespace "kubelet-test-6270" to be "completed"
    Dec 16 13:23:35.640: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662115ms
    Dec 16 13:23:37.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007967404s
    Dec 16 13:23:39.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007485413s
    Dec 16 13:23:39.645: INFO: Pod "agnhost-host-aliases34f04035-26ef-4b42-a536-d64b1b1206f9" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:39.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6270" for this suite. 12/16/22 13:23:39.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:39.664
Dec 16 13:23:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:23:39.665
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:39.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:39.681
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 12/16/22 13:23:39.683
Dec 16 13:23:39.690: INFO: Waiting up to 5m0s for pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7" in namespace "downward-api-7809" to be "Succeeded or Failed"
Dec 16 13:23:39.693: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.74885ms
Dec 16 13:23:41.698: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007597127s
Dec 16 13:23:43.700: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010019242s
STEP: Saw pod success 12/16/22 13:23:43.7
Dec 16 13:23:43.701: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7" satisfied condition "Succeeded or Failed"
Dec 16 13:23:43.709: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:23:43.717
Dec 16 13:23:43.730: INFO: Waiting for pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 to disappear
Dec 16 13:23:43.733: INFO: Pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7809" for this suite. 12/16/22 13:23:43.738
------------------------------
• [4.079 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:39.664
    Dec 16 13:23:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:23:39.665
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:39.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:39.681
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 12/16/22 13:23:39.683
    Dec 16 13:23:39.690: INFO: Waiting up to 5m0s for pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7" in namespace "downward-api-7809" to be "Succeeded or Failed"
    Dec 16 13:23:39.693: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.74885ms
    Dec 16 13:23:41.698: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007597127s
    Dec 16 13:23:43.700: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010019242s
    STEP: Saw pod success 12/16/22 13:23:43.7
    Dec 16 13:23:43.701: INFO: Pod "downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7" satisfied condition "Succeeded or Failed"
    Dec 16 13:23:43.709: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:23:43.717
    Dec 16 13:23:43.730: INFO: Waiting for pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 to disappear
    Dec 16 13:23:43.733: INFO: Pod downward-api-f8210585-96ac-4f04-b78b-4e1789936ab7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7809" for this suite. 12/16/22 13:23:43.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:43.744
Dec 16 13:23:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:23:43.745
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:43.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:43.761
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 12/16/22 13:23:43.763
STEP: listing secrets in all namespaces to ensure that there are more than zero 12/16/22 13:23:43.767
STEP: patching the secret 12/16/22 13:23:43.771
STEP: deleting the secret using a LabelSelector 12/16/22 13:23:43.779
STEP: listing secrets in all namespaces, searching for label name and value in patch 12/16/22 13:23:43.785
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:43.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7187" for this suite. 12/16/22 13:23:43.792
------------------------------
• [0.054 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:43.744
    Dec 16 13:23:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:23:43.745
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:43.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:43.761
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 12/16/22 13:23:43.763
    STEP: listing secrets in all namespaces to ensure that there are more than zero 12/16/22 13:23:43.767
    STEP: patching the secret 12/16/22 13:23:43.771
    STEP: deleting the secret using a LabelSelector 12/16/22 13:23:43.779
    STEP: listing secrets in all namespaces, searching for label name and value in patch 12/16/22 13:23:43.785
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:43.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7187" for this suite. 12/16/22 13:23:43.792
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:43.799
Dec 16 13:23:43.799: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:43.799
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:43.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:43.815
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 12/16/22 13:23:43.817
Dec 16 13:23:43.817: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 16 13:23:43.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:44.619: INFO: stderr: ""
Dec 16 13:23:44.619: INFO: stdout: "service/agnhost-replica created\n"
Dec 16 13:23:44.619: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 16 13:23:44.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:45.381: INFO: stderr: ""
Dec 16 13:23:45.381: INFO: stdout: "service/agnhost-primary created\n"
Dec 16 13:23:45.381: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 16 13:23:45.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:45.580: INFO: stderr: ""
Dec 16 13:23:45.580: INFO: stdout: "service/frontend created\n"
Dec 16 13:23:45.580: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 16 13:23:45.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:45.791: INFO: stderr: ""
Dec 16 13:23:45.791: INFO: stdout: "deployment.apps/frontend created\n"
Dec 16 13:23:45.792: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 16 13:23:45.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:45.983: INFO: stderr: ""
Dec 16 13:23:45.983: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 16 13:23:45.983: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 16 13:23:45.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
Dec 16 13:23:46.169: INFO: stderr: ""
Dec 16 13:23:46.169: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 12/16/22 13:23:46.169
Dec 16 13:23:46.170: INFO: Waiting for all frontend pods to be Running.
Dec 16 13:23:51.220: INFO: Waiting for frontend to serve content.
Dec 16 13:23:51.259: INFO: Trying to add a new entry to the guestbook.
Dec 16 13:23:51.272: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 12/16/22 13:23:51.308
Dec 16 13:23:51.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.392: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.392: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 12/16/22 13:23:51.392
Dec 16 13:23:51.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.477: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.477: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/16/22 13:23:51.477
Dec 16 13:23:51.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.560: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.560: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/16/22 13:23:51.56
Dec 16 13:23:51.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.633: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.633: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/16/22 13:23:51.633
Dec 16 13:23:51.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.714: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.714: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/16/22 13:23:51.714
Dec 16 13:23:51.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
Dec 16 13:23:51.793: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:23:51.793: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:51.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8213" for this suite. 12/16/22 13:23:51.797
------------------------------
• [SLOW TEST] [8.004 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:43.799
    Dec 16 13:23:43.799: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:23:43.799
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:43.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:43.815
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 12/16/22 13:23:43.817
    Dec 16 13:23:43.817: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Dec 16 13:23:43.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:44.619: INFO: stderr: ""
    Dec 16 13:23:44.619: INFO: stdout: "service/agnhost-replica created\n"
    Dec 16 13:23:44.619: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Dec 16 13:23:44.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:45.381: INFO: stderr: ""
    Dec 16 13:23:45.381: INFO: stdout: "service/agnhost-primary created\n"
    Dec 16 13:23:45.381: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Dec 16 13:23:45.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:45.580: INFO: stderr: ""
    Dec 16 13:23:45.580: INFO: stdout: "service/frontend created\n"
    Dec 16 13:23:45.580: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Dec 16 13:23:45.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:45.791: INFO: stderr: ""
    Dec 16 13:23:45.791: INFO: stdout: "deployment.apps/frontend created\n"
    Dec 16 13:23:45.792: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 16 13:23:45.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:45.983: INFO: stderr: ""
    Dec 16 13:23:45.983: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Dec 16 13:23:45.983: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 16 13:23:45.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 create -f -'
    Dec 16 13:23:46.169: INFO: stderr: ""
    Dec 16 13:23:46.169: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 12/16/22 13:23:46.169
    Dec 16 13:23:46.170: INFO: Waiting for all frontend pods to be Running.
    Dec 16 13:23:51.220: INFO: Waiting for frontend to serve content.
    Dec 16 13:23:51.259: INFO: Trying to add a new entry to the guestbook.
    Dec 16 13:23:51.272: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 12/16/22 13:23:51.308
    Dec 16 13:23:51.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.392: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.392: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 12/16/22 13:23:51.392
    Dec 16 13:23:51.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.477: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.477: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/16/22 13:23:51.477
    Dec 16 13:23:51.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.560: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.560: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/16/22 13:23:51.56
    Dec 16 13:23:51.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.633: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.633: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/16/22 13:23:51.633
    Dec 16 13:23:51.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.714: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.714: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/16/22 13:23:51.714
    Dec 16 13:23:51.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8213 delete --grace-period=0 --force -f -'
    Dec 16 13:23:51.793: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:23:51.793: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:51.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8213" for this suite. 12/16/22 13:23:51.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:51.804
Dec 16 13:23:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename events 12/16/22 13:23:51.805
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:51.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:51.82
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 12/16/22 13:23:51.822
STEP: listing all events in all namespaces 12/16/22 13:23:51.826
STEP: patching the test event 12/16/22 13:23:51.835
STEP: fetching the test event 12/16/22 13:23:51.855
STEP: updating the test event 12/16/22 13:23:51.858
STEP: getting the test event 12/16/22 13:23:51.882
STEP: deleting the test event 12/16/22 13:23:51.885
STEP: listing all events in all namespaces 12/16/22 13:23:51.892
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:51.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-100" for this suite. 12/16/22 13:23:51.904
------------------------------
• [0.105 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:51.804
    Dec 16 13:23:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename events 12/16/22 13:23:51.805
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:51.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:51.82
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 12/16/22 13:23:51.822
    STEP: listing all events in all namespaces 12/16/22 13:23:51.826
    STEP: patching the test event 12/16/22 13:23:51.835
    STEP: fetching the test event 12/16/22 13:23:51.855
    STEP: updating the test event 12/16/22 13:23:51.858
    STEP: getting the test event 12/16/22 13:23:51.882
    STEP: deleting the test event 12/16/22 13:23:51.885
    STEP: listing all events in all namespaces 12/16/22 13:23:51.892
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:51.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-100" for this suite. 12/16/22 13:23:51.904
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:51.91
Dec 16 13:23:51.910: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:23:51.91
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:51.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:51.925
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-33b4370b-7696-4c7f-a8b3-4c235a84b91f 12/16/22 13:23:51.928
STEP: Creating a pod to test consume secrets 12/16/22 13:23:51.949
Dec 16 13:23:51.959: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99" in namespace "projected-7131" to be "Succeeded or Failed"
Dec 16 13:23:51.968: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Pending", Reason="", readiness=false. Elapsed: 9.503642ms
Dec 16 13:23:53.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014630988s
Dec 16 13:23:55.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014291728s
STEP: Saw pod success 12/16/22 13:23:55.973
Dec 16 13:23:55.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99" satisfied condition "Succeeded or Failed"
Dec 16 13:23:55.978: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:23:55.985
Dec 16 13:23:55.997: INFO: Waiting for pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 to disappear
Dec 16 13:23:56.000: INFO: Pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 13:23:56.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7131" for this suite. 12/16/22 13:23:56.004
------------------------------
• [4.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:51.91
    Dec 16 13:23:51.910: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:23:51.91
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:51.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:51.925
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-33b4370b-7696-4c7f-a8b3-4c235a84b91f 12/16/22 13:23:51.928
    STEP: Creating a pod to test consume secrets 12/16/22 13:23:51.949
    Dec 16 13:23:51.959: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99" in namespace "projected-7131" to be "Succeeded or Failed"
    Dec 16 13:23:51.968: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Pending", Reason="", readiness=false. Elapsed: 9.503642ms
    Dec 16 13:23:53.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014630988s
    Dec 16 13:23:55.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014291728s
    STEP: Saw pod success 12/16/22 13:23:55.973
    Dec 16 13:23:55.973: INFO: Pod "pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99" satisfied condition "Succeeded or Failed"
    Dec 16 13:23:55.978: INFO: Trying to get logs from node pool-a3802-oewtd pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:23:55.985
    Dec 16 13:23:55.997: INFO: Waiting for pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 to disappear
    Dec 16 13:23:56.000: INFO: Pod pod-projected-secrets-98e00844-ffd7-4a56-8f9d-6d2bc7b03b99 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:23:56.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7131" for this suite. 12/16/22 13:23:56.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:23:56.01
Dec 16 13:23:56.010: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:23:56.01
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:56.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:56.026
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-6cf73b9b-4442-484b-b036-2c1c5c2ca5e4 12/16/22 13:23:56.029
STEP: Creating a pod to test consume configMaps 12/16/22 13:23:56.033
Dec 16 13:23:56.042: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709" in namespace "projected-9571" to be "Succeeded or Failed"
Dec 16 13:23:56.046: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465338ms
Dec 16 13:23:58.052: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009491658s
Dec 16 13:24:00.051: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008760194s
STEP: Saw pod success 12/16/22 13:24:00.051
Dec 16 13:24:00.051: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709" satisfied condition "Succeeded or Failed"
Dec 16 13:24:00.056: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 container projected-configmap-volume-test: <nil>
STEP: delete the pod 12/16/22 13:24:00.064
Dec 16 13:24:00.074: INFO: Waiting for pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 to disappear
Dec 16 13:24:00.077: INFO: Pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:00.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9571" for this suite. 12/16/22 13:24:00.081
------------------------------
• [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:23:56.01
    Dec 16 13:23:56.010: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:23:56.01
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:23:56.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:23:56.026
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-6cf73b9b-4442-484b-b036-2c1c5c2ca5e4 12/16/22 13:23:56.029
    STEP: Creating a pod to test consume configMaps 12/16/22 13:23:56.033
    Dec 16 13:23:56.042: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709" in namespace "projected-9571" to be "Succeeded or Failed"
    Dec 16 13:23:56.046: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465338ms
    Dec 16 13:23:58.052: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009491658s
    Dec 16 13:24:00.051: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008760194s
    STEP: Saw pod success 12/16/22 13:24:00.051
    Dec 16 13:24:00.051: INFO: Pod "pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709" satisfied condition "Succeeded or Failed"
    Dec 16 13:24:00.056: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:24:00.064
    Dec 16 13:24:00.074: INFO: Waiting for pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 to disappear
    Dec 16 13:24:00.077: INFO: Pod pod-projected-configmaps-a14f43ae-e097-4480-969e-4ce7763de709 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:00.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9571" for this suite. 12/16/22 13:24:00.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:00.087
Dec 16 13:24:00.087: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:24:00.088
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:00.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:00.101
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-7625f771-77e6-43b2-8d51-8c45375c0226 12/16/22 13:24:00.119
STEP: Creating a pod to test consume secrets 12/16/22 13:24:00.123
Dec 16 13:24:00.131: INFO: Waiting up to 5m0s for pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554" in namespace "secrets-4155" to be "Succeeded or Failed"
Dec 16 13:24:00.134: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904955ms
Dec 16 13:24:02.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009366742s
Dec 16 13:24:04.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009265955s
STEP: Saw pod success 12/16/22 13:24:04.14
Dec 16 13:24:04.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554" satisfied condition "Succeeded or Failed"
Dec 16 13:24:04.145: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:24:04.154
Dec 16 13:24:04.166: INFO: Waiting for pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 to disappear
Dec 16 13:24:04.169: INFO: Pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:04.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4155" for this suite. 12/16/22 13:24:04.174
STEP: Destroying namespace "secret-namespace-7845" for this suite. 12/16/22 13:24:04.181
------------------------------
• [4.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:00.087
    Dec 16 13:24:00.087: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:24:00.088
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:00.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:00.101
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-7625f771-77e6-43b2-8d51-8c45375c0226 12/16/22 13:24:00.119
    STEP: Creating a pod to test consume secrets 12/16/22 13:24:00.123
    Dec 16 13:24:00.131: INFO: Waiting up to 5m0s for pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554" in namespace "secrets-4155" to be "Succeeded or Failed"
    Dec 16 13:24:00.134: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904955ms
    Dec 16 13:24:02.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009366742s
    Dec 16 13:24:04.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009265955s
    STEP: Saw pod success 12/16/22 13:24:04.14
    Dec 16 13:24:04.140: INFO: Pod "pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554" satisfied condition "Succeeded or Failed"
    Dec 16 13:24:04.145: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:24:04.154
    Dec 16 13:24:04.166: INFO: Waiting for pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 to disappear
    Dec 16 13:24:04.169: INFO: Pod pod-secrets-1d424f1b-f42b-47c4-a0dc-b75415259554 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:04.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4155" for this suite. 12/16/22 13:24:04.174
    STEP: Destroying namespace "secret-namespace-7845" for this suite. 12/16/22 13:24:04.181
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:04.187
Dec 16 13:24:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:24:04.188
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:04.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:04.203
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 12/16/22 13:24:04.205
STEP: Creating a ResourceQuota 12/16/22 13:24:09.209
STEP: Ensuring resource quota status is calculated 12/16/22 13:24:09.213
STEP: Creating a ReplicaSet 12/16/22 13:24:11.218
STEP: Ensuring resource quota status captures replicaset creation 12/16/22 13:24:11.229
STEP: Deleting a ReplicaSet 12/16/22 13:24:13.234
STEP: Ensuring resource quota status released usage 12/16/22 13:24:13.241
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:15.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1421" for this suite. 12/16/22 13:24:15.251
------------------------------
• [SLOW TEST] [11.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:04.187
    Dec 16 13:24:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:24:04.188
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:04.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:04.203
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 12/16/22 13:24:04.205
    STEP: Creating a ResourceQuota 12/16/22 13:24:09.209
    STEP: Ensuring resource quota status is calculated 12/16/22 13:24:09.213
    STEP: Creating a ReplicaSet 12/16/22 13:24:11.218
    STEP: Ensuring resource quota status captures replicaset creation 12/16/22 13:24:11.229
    STEP: Deleting a ReplicaSet 12/16/22 13:24:13.234
    STEP: Ensuring resource quota status released usage 12/16/22 13:24:13.241
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:15.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1421" for this suite. 12/16/22 13:24:15.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:15.259
Dec 16 13:24:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-webhook 12/16/22 13:24:15.26
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:15.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:15.275
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/16/22 13:24:15.278
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/16/22 13:24:15.577
STEP: Deploying the custom resource conversion webhook pod 12/16/22 13:24:15.585
STEP: Wait for the deployment to be ready 12/16/22 13:24:15.594
Dec 16 13:24:15.601: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:24:17.615
STEP: Verifying the service has paired with the endpoint 12/16/22 13:24:17.63
Dec 16 13:24:18.630: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Dec 16 13:24:18.635: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Creating a v1 custom resource 12/16/22 13:24:21.239
STEP: v2 custom resource should be converted 12/16/22 13:24:21.244
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8938" for this suite. 12/16/22 13:24:21.809
------------------------------
• [SLOW TEST] [6.555 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:15.259
    Dec 16 13:24:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-webhook 12/16/22 13:24:15.26
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:15.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:15.275
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/16/22 13:24:15.278
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/16/22 13:24:15.577
    STEP: Deploying the custom resource conversion webhook pod 12/16/22 13:24:15.585
    STEP: Wait for the deployment to be ready 12/16/22 13:24:15.594
    Dec 16 13:24:15.601: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:24:17.615
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:24:17.63
    Dec 16 13:24:18.630: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Dec 16 13:24:18.635: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Creating a v1 custom resource 12/16/22 13:24:21.239
    STEP: v2 custom resource should be converted 12/16/22 13:24:21.244
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8938" for this suite. 12/16/22 13:24:21.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:21.815
Dec 16 13:24:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:24:21.816
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:21.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:21.827
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 12/16/22 13:24:21.83
Dec 16 13:24:21.837: INFO: Waiting up to 5m0s for pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178" in namespace "emptydir-3275" to be "Succeeded or Failed"
Dec 16 13:24:21.841: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491063ms
Dec 16 13:24:23.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0085937s
Dec 16 13:24:25.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644971s
STEP: Saw pod success 12/16/22 13:24:25.846
Dec 16 13:24:25.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178" satisfied condition "Succeeded or Failed"
Dec 16 13:24:25.850: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 container test-container: <nil>
STEP: delete the pod 12/16/22 13:24:25.857
Dec 16 13:24:25.867: INFO: Waiting for pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 to disappear
Dec 16 13:24:25.871: INFO: Pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:25.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3275" for this suite. 12/16/22 13:24:25.875
------------------------------
• [4.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:21.815
    Dec 16 13:24:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:24:21.816
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:21.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:21.827
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/16/22 13:24:21.83
    Dec 16 13:24:21.837: INFO: Waiting up to 5m0s for pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178" in namespace "emptydir-3275" to be "Succeeded or Failed"
    Dec 16 13:24:21.841: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491063ms
    Dec 16 13:24:23.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0085937s
    Dec 16 13:24:25.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644971s
    STEP: Saw pod success 12/16/22 13:24:25.846
    Dec 16 13:24:25.846: INFO: Pod "pod-54d36fc7-67e5-47cc-843c-05fc8789a178" satisfied condition "Succeeded or Failed"
    Dec 16 13:24:25.850: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:24:25.857
    Dec 16 13:24:25.867: INFO: Waiting for pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 to disappear
    Dec 16 13:24:25.871: INFO: Pod pod-54d36fc7-67e5-47cc-843c-05fc8789a178 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:25.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3275" for this suite. 12/16/22 13:24:25.875
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:25.882
Dec 16 13:24:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:24:25.883
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:25.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:25.898
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-c0d0a5a1-57bc-49df-b7a2-a31f63ecda00 12/16/22 13:24:25.9
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:25.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7064" for this suite. 12/16/22 13:24:25.906
------------------------------
• [0.029 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:25.882
    Dec 16 13:24:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:24:25.883
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:25.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:25.898
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-c0d0a5a1-57bc-49df-b7a2-a31f63ecda00 12/16/22 13:24:25.9
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:25.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7064" for this suite. 12/16/22 13:24:25.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:25.912
Dec 16 13:24:25.912: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename csiinlinevolumes 12/16/22 13:24:25.913
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:25.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:25.98
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 12/16/22 13:24:25.982
STEP: getting 12/16/22 13:24:25.998
STEP: listing 12/16/22 13:24:26.007
STEP: deleting 12/16/22 13:24:26.01
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:26.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9828" for this suite. 12/16/22 13:24:26.031
------------------------------
• [0.125 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:25.912
    Dec 16 13:24:25.912: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename csiinlinevolumes 12/16/22 13:24:25.913
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:25.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:25.98
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 12/16/22 13:24:25.982
    STEP: getting 12/16/22 13:24:25.998
    STEP: listing 12/16/22 13:24:26.007
    STEP: deleting 12/16/22 13:24:26.01
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:26.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9828" for this suite. 12/16/22 13:24:26.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:26.037
Dec 16 13:24:26.037: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:24:26.038
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:26.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:26.053
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7694" for this suite. 12/16/22 13:24:26.093
------------------------------
• [0.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:26.037
    Dec 16 13:24:26.037: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:24:26.038
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:26.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:26.053
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7694" for this suite. 12/16/22 13:24:26.093
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:26.099
Dec 16 13:24:26.099: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:24:26.1
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:26.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:26.114
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 12/16/22 13:24:26.116
STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:24:26.121
STEP: Creating a ResourceQuota with not best effort scope 12/16/22 13:24:28.126
STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:24:28.131
STEP: Creating a best-effort pod 12/16/22 13:24:30.136
STEP: Ensuring resource quota with best effort scope captures the pod usage 12/16/22 13:24:30.149
STEP: Ensuring resource quota with not best effort ignored the pod usage 12/16/22 13:24:32.155
STEP: Deleting the pod 12/16/22 13:24:34.16
STEP: Ensuring resource quota status released the pod usage 12/16/22 13:24:34.169
STEP: Creating a not best-effort pod 12/16/22 13:24:36.175
STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/16/22 13:24:36.185
STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/16/22 13:24:38.19
STEP: Deleting the pod 12/16/22 13:24:40.195
STEP: Ensuring resource quota status released the pod usage 12/16/22 13:24:40.206
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:42.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7270" for this suite. 12/16/22 13:24:42.218
------------------------------
• [SLOW TEST] [16.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:26.099
    Dec 16 13:24:26.099: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:24:26.1
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:26.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:26.114
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 12/16/22 13:24:26.116
    STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:24:26.121
    STEP: Creating a ResourceQuota with not best effort scope 12/16/22 13:24:28.126
    STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:24:28.131
    STEP: Creating a best-effort pod 12/16/22 13:24:30.136
    STEP: Ensuring resource quota with best effort scope captures the pod usage 12/16/22 13:24:30.149
    STEP: Ensuring resource quota with not best effort ignored the pod usage 12/16/22 13:24:32.155
    STEP: Deleting the pod 12/16/22 13:24:34.16
    STEP: Ensuring resource quota status released the pod usage 12/16/22 13:24:34.169
    STEP: Creating a not best-effort pod 12/16/22 13:24:36.175
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/16/22 13:24:36.185
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/16/22 13:24:38.19
    STEP: Deleting the pod 12/16/22 13:24:40.195
    STEP: Ensuring resource quota status released the pod usage 12/16/22 13:24:40.206
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:42.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7270" for this suite. 12/16/22 13:24:42.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:42.225
Dec 16 13:24:42.225: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:24:42.226
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:42.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:42.24
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:24:42.255
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:24:42.497
STEP: Deploying the webhook pod 12/16/22 13:24:42.503
STEP: Wait for the deployment to be ready 12/16/22 13:24:42.513
Dec 16 13:24:42.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:24:44.533
STEP: Verifying the service has paired with the endpoint 12/16/22 13:24:44.547
Dec 16 13:24:45.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/16/22 13:24:45.552
STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:45.553
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/16/22 13:24:45.602
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/16/22 13:24:46.612
STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:46.612
STEP: Having no error when timeout is longer than webhook latency 12/16/22 13:24:47.642
STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:47.642
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/16/22 13:24:52.834
STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:52.835
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9949" for this suite. 12/16/22 13:24:58.066
STEP: Destroying namespace "webhook-9949-markers" for this suite. 12/16/22 13:24:58.073
------------------------------
• [SLOW TEST] [15.862 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:42.225
    Dec 16 13:24:42.225: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:24:42.226
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:42.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:42.24
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:24:42.255
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:24:42.497
    STEP: Deploying the webhook pod 12/16/22 13:24:42.503
    STEP: Wait for the deployment to be ready 12/16/22 13:24:42.513
    Dec 16 13:24:42.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:24:44.533
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:24:44.547
    Dec 16 13:24:45.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/16/22 13:24:45.552
    STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:45.553
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/16/22 13:24:45.602
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/16/22 13:24:46.612
    STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:46.612
    STEP: Having no error when timeout is longer than webhook latency 12/16/22 13:24:47.642
    STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:47.642
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/16/22 13:24:52.834
    STEP: Registering slow webhook via the AdmissionRegistration API 12/16/22 13:24:52.835
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9949" for this suite. 12/16/22 13:24:58.066
    STEP: Destroying namespace "webhook-9949-markers" for this suite. 12/16/22 13:24:58.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:58.088
Dec 16 13:24:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:24:58.089
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:58.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:58.111
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 12/16/22 13:24:58.116
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/16/22 13:24:58.117
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/16/22 13:24:58.117
STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/16/22 13:24:58.117
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/16/22 13:24:58.118
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/16/22 13:24:58.118
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/16/22 13:24:58.12
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:24:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9935" for this suite. 12/16/22 13:24:58.125
------------------------------
• [0.044 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:58.088
    Dec 16 13:24:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:24:58.089
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:58.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:58.111
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 12/16/22 13:24:58.116
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/16/22 13:24:58.117
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/16/22 13:24:58.117
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/16/22 13:24:58.117
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/16/22 13:24:58.118
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/16/22 13:24:58.118
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/16/22 13:24:58.12
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:24:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9935" for this suite. 12/16/22 13:24:58.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:24:58.134
Dec 16 13:24:58.134: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:24:58.134
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:58.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:58.151
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Dec 16 13:24:58.155: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 13:24:59.93
Dec 16 13:24:59.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 create -f -'
Dec 16 13:25:00.620: INFO: stderr: ""
Dec 16 13:25:00.620: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 16 13:25:00.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
Dec 16 13:25:00.692: INFO: stderr: ""
Dec 16 13:25:00.693: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 16 13:25:00.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 apply -f -'
Dec 16 13:25:01.226: INFO: stderr: ""
Dec 16 13:25:01.226: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 16 13:25:01.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
Dec 16 13:25:01.297: INFO: stderr: ""
Dec 16 13:25:01.297: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/16/22 13:25:01.297
Dec 16 13:25:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 explain e2e-test-crd-publish-openapi-4522-crds'
Dec 16 13:25:01.476: INFO: stderr: ""
Dec 16 13:25:01.476: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4522-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2338" for this suite. 12/16/22 13:25:03.308
------------------------------
• [SLOW TEST] [5.181 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:24:58.134
    Dec 16 13:24:58.134: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:24:58.134
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:24:58.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:24:58.151
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Dec 16 13:24:58.155: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 13:24:59.93
    Dec 16 13:24:59.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 create -f -'
    Dec 16 13:25:00.620: INFO: stderr: ""
    Dec 16 13:25:00.620: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 16 13:25:00.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
    Dec 16 13:25:00.692: INFO: stderr: ""
    Dec 16 13:25:00.693: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Dec 16 13:25:00.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 apply -f -'
    Dec 16 13:25:01.226: INFO: stderr: ""
    Dec 16 13:25:01.226: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 16 13:25:01.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 --namespace=crd-publish-openapi-2338 delete e2e-test-crd-publish-openapi-4522-crds test-cr'
    Dec 16 13:25:01.297: INFO: stderr: ""
    Dec 16 13:25:01.297: INFO: stdout: "e2e-test-crd-publish-openapi-4522-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/16/22 13:25:01.297
    Dec 16 13:25:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-2338 explain e2e-test-crd-publish-openapi-4522-crds'
    Dec 16 13:25:01.476: INFO: stderr: ""
    Dec 16 13:25:01.476: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4522-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2338" for this suite. 12/16/22 13:25:03.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:03.315
Dec 16 13:25:03.315: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:25:03.316
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:03.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:03.332
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:03.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8774" for this suite. 12/16/22 13:25:03.378
------------------------------
• [0.068 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:03.315
    Dec 16 13:25:03.315: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:25:03.316
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:03.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:03.332
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:03.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8774" for this suite. 12/16/22 13:25:03.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:03.383
Dec 16 13:25:03.383: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 13:25:03.383
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:03.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:03.401
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 12/16/22 13:25:03.403
STEP: Ensuring job reaches completions 12/16/22 13:25:03.409
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:15.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5349" for this suite. 12/16/22 13:25:15.419
------------------------------
• [SLOW TEST] [12.042 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:03.383
    Dec 16 13:25:03.383: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 13:25:03.383
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:03.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:03.401
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 12/16/22 13:25:03.403
    STEP: Ensuring job reaches completions 12/16/22 13:25:03.409
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:15.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5349" for this suite. 12/16/22 13:25:15.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:15.428
Dec 16 13:25:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:25:15.428
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:15.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:15.446
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-aae4eaab-444a-40c6-a05a-e9ad23ead7a9 12/16/22 13:25:15.448
STEP: Creating a pod to test consume configMaps 12/16/22 13:25:15.452
Dec 16 13:25:15.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f" in namespace "configmap-5776" to be "Succeeded or Failed"
Dec 16 13:25:15.466: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925246ms
Dec 16 13:25:17.472: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152992s
Dec 16 13:25:19.471: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00981878s
STEP: Saw pod success 12/16/22 13:25:19.471
Dec 16 13:25:19.471: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f" satisfied condition "Succeeded or Failed"
Dec 16 13:25:19.474: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:25:19.528
Dec 16 13:25:19.540: INFO: Waiting for pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f to disappear
Dec 16 13:25:19.542: INFO: Pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:19.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5776" for this suite. 12/16/22 13:25:19.546
------------------------------
• [4.123 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:15.428
    Dec 16 13:25:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:25:15.428
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:15.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:15.446
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-aae4eaab-444a-40c6-a05a-e9ad23ead7a9 12/16/22 13:25:15.448
    STEP: Creating a pod to test consume configMaps 12/16/22 13:25:15.452
    Dec 16 13:25:15.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f" in namespace "configmap-5776" to be "Succeeded or Failed"
    Dec 16 13:25:15.466: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925246ms
    Dec 16 13:25:17.472: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152992s
    Dec 16 13:25:19.471: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00981878s
    STEP: Saw pod success 12/16/22 13:25:19.471
    Dec 16 13:25:19.471: INFO: Pod "pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f" satisfied condition "Succeeded or Failed"
    Dec 16 13:25:19.474: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:25:19.528
    Dec 16 13:25:19.540: INFO: Waiting for pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f to disappear
    Dec 16 13:25:19.542: INFO: Pod pod-configmaps-56fe8e8a-48a1-4d90-bafa-6926a140102f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:19.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5776" for this suite. 12/16/22 13:25:19.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:19.552
Dec 16 13:25:19.552: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:25:19.552
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:19.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:19.572
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:25:19.585
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:25:19.873
STEP: Deploying the webhook pod 12/16/22 13:25:19.884
STEP: Wait for the deployment to be ready 12/16/22 13:25:19.894
Dec 16 13:25:19.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:25:21.913
STEP: Verifying the service has paired with the endpoint 12/16/22 13:25:21.931
Dec 16 13:25:22.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/16/22 13:25:22.936
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/16/22 13:25:22.981
STEP: Creating a dummy validating-webhook-configuration object 12/16/22 13:25:23.027
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/16/22 13:25:23.036
STEP: Creating a dummy mutating-webhook-configuration object 12/16/22 13:25:23.042
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/16/22 13:25:23.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:23.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7494" for this suite. 12/16/22 13:25:23.108
STEP: Destroying namespace "webhook-7494-markers" for this suite. 12/16/22 13:25:23.114
------------------------------
• [3.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:19.552
    Dec 16 13:25:19.552: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:25:19.552
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:19.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:19.572
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:25:19.585
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:25:19.873
    STEP: Deploying the webhook pod 12/16/22 13:25:19.884
    STEP: Wait for the deployment to be ready 12/16/22 13:25:19.894
    Dec 16 13:25:19.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:25:21.913
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:25:21.931
    Dec 16 13:25:22.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/16/22 13:25:22.936
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/16/22 13:25:22.981
    STEP: Creating a dummy validating-webhook-configuration object 12/16/22 13:25:23.027
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/16/22 13:25:23.036
    STEP: Creating a dummy mutating-webhook-configuration object 12/16/22 13:25:23.042
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/16/22 13:25:23.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:23.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7494" for this suite. 12/16/22 13:25:23.108
    STEP: Destroying namespace "webhook-7494-markers" for this suite. 12/16/22 13:25:23.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:23.123
Dec 16 13:25:23.123: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:25:23.124
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:23.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:23.143
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:25:23.145
Dec 16 13:25:23.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd" in namespace "projected-466" to be "Succeeded or Failed"
Dec 16 13:25:23.158: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027134ms
Dec 16 13:25:25.333: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17958422s
Dec 16 13:25:27.167: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013174233s
STEP: Saw pod success 12/16/22 13:25:27.167
Dec 16 13:25:27.167: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd" satisfied condition "Succeeded or Failed"
Dec 16 13:25:27.170: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd container client-container: <nil>
STEP: delete the pod 12/16/22 13:25:27.178
Dec 16 13:25:27.187: INFO: Waiting for pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd to disappear
Dec 16 13:25:27.190: INFO: Pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:27.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-466" for this suite. 12/16/22 13:25:27.194
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:23.123
    Dec 16 13:25:23.123: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:25:23.124
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:23.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:23.143
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:25:23.145
    Dec 16 13:25:23.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd" in namespace "projected-466" to be "Succeeded or Failed"
    Dec 16 13:25:23.158: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027134ms
    Dec 16 13:25:25.333: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17958422s
    Dec 16 13:25:27.167: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013174233s
    STEP: Saw pod success 12/16/22 13:25:27.167
    Dec 16 13:25:27.167: INFO: Pod "downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd" satisfied condition "Succeeded or Failed"
    Dec 16 13:25:27.170: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd container client-container: <nil>
    STEP: delete the pod 12/16/22 13:25:27.178
    Dec 16 13:25:27.187: INFO: Waiting for pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd to disappear
    Dec 16 13:25:27.190: INFO: Pod downwardapi-volume-7b88306a-d61b-49c4-938e-8bd4d20232bd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:27.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-466" for this suite. 12/16/22 13:25:27.194
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:27.201
Dec 16 13:25:27.201: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename subpath 12/16/22 13:25:27.202
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:27.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:27.22
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/16/22 13:25:27.222
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-pzvb 12/16/22 13:25:27.229
STEP: Creating a pod to test atomic-volume-subpath 12/16/22 13:25:27.229
Dec 16 13:25:27.237: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pzvb" in namespace "subpath-6330" to be "Succeeded or Failed"
Dec 16 13:25:27.240: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.975629ms
Dec 16 13:25:29.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009086069s
Dec 16 13:25:31.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 4.00848914s
Dec 16 13:25:33.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 6.009294107s
Dec 16 13:25:35.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 8.008905782s
Dec 16 13:25:37.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008338736s
Dec 16 13:25:39.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 12.008407599s
Dec 16 13:25:41.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 14.007898195s
Dec 16 13:25:43.247: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009898548s
Dec 16 13:25:45.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 18.009185643s
Dec 16 13:25:47.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008490602s
Dec 16 13:25:49.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=false. Elapsed: 22.008357015s
Dec 16 13:25:51.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009536675s
STEP: Saw pod success 12/16/22 13:25:51.246
Dec 16 13:25:51.246: INFO: Pod "pod-subpath-test-configmap-pzvb" satisfied condition "Succeeded or Failed"
Dec 16 13:25:51.251: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-configmap-pzvb container test-container-subpath-configmap-pzvb: <nil>
STEP: delete the pod 12/16/22 13:25:51.26
Dec 16 13:25:51.274: INFO: Waiting for pod pod-subpath-test-configmap-pzvb to disappear
Dec 16 13:25:51.277: INFO: Pod pod-subpath-test-configmap-pzvb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pzvb 12/16/22 13:25:51.277
Dec 16 13:25:51.277: INFO: Deleting pod "pod-subpath-test-configmap-pzvb" in namespace "subpath-6330"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6330" for this suite. 12/16/22 13:25:51.284
------------------------------
• [SLOW TEST] [24.090 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:27.201
    Dec 16 13:25:27.201: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename subpath 12/16/22 13:25:27.202
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:27.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:27.22
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/16/22 13:25:27.222
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-pzvb 12/16/22 13:25:27.229
    STEP: Creating a pod to test atomic-volume-subpath 12/16/22 13:25:27.229
    Dec 16 13:25:27.237: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pzvb" in namespace "subpath-6330" to be "Succeeded or Failed"
    Dec 16 13:25:27.240: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.975629ms
    Dec 16 13:25:29.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009086069s
    Dec 16 13:25:31.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 4.00848914s
    Dec 16 13:25:33.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 6.009294107s
    Dec 16 13:25:35.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 8.008905782s
    Dec 16 13:25:37.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008338736s
    Dec 16 13:25:39.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 12.008407599s
    Dec 16 13:25:41.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 14.007898195s
    Dec 16 13:25:43.247: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009898548s
    Dec 16 13:25:45.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 18.009185643s
    Dec 16 13:25:47.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008490602s
    Dec 16 13:25:49.245: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Running", Reason="", readiness=false. Elapsed: 22.008357015s
    Dec 16 13:25:51.246: INFO: Pod "pod-subpath-test-configmap-pzvb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009536675s
    STEP: Saw pod success 12/16/22 13:25:51.246
    Dec 16 13:25:51.246: INFO: Pod "pod-subpath-test-configmap-pzvb" satisfied condition "Succeeded or Failed"
    Dec 16 13:25:51.251: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-configmap-pzvb container test-container-subpath-configmap-pzvb: <nil>
    STEP: delete the pod 12/16/22 13:25:51.26
    Dec 16 13:25:51.274: INFO: Waiting for pod pod-subpath-test-configmap-pzvb to disappear
    Dec 16 13:25:51.277: INFO: Pod pod-subpath-test-configmap-pzvb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pzvb 12/16/22 13:25:51.277
    Dec 16 13:25:51.277: INFO: Deleting pod "pod-subpath-test-configmap-pzvb" in namespace "subpath-6330"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6330" for this suite. 12/16/22 13:25:51.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:51.292
Dec 16 13:25:51.292: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:25:51.292
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:51.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:51.311
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 12/16/22 13:25:51.313
Dec 16 13:25:51.321: INFO: Waiting up to 5m0s for pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be" in namespace "downward-api-9735" to be "Succeeded or Failed"
Dec 16 13:25:51.324: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923906ms
Dec 16 13:25:53.329: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008196163s
Dec 16 13:25:55.332: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011002272s
STEP: Saw pod success 12/16/22 13:25:55.332
Dec 16 13:25:55.332: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be" satisfied condition "Succeeded or Failed"
Dec 16 13:25:55.335: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:25:55.343
Dec 16 13:25:55.356: INFO: Waiting for pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be to disappear
Dec 16 13:25:55.359: INFO: Pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:55.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9735" for this suite. 12/16/22 13:25:55.364
------------------------------
• [4.077 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:51.292
    Dec 16 13:25:51.292: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:25:51.292
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:51.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:51.311
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 12/16/22 13:25:51.313
    Dec 16 13:25:51.321: INFO: Waiting up to 5m0s for pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be" in namespace "downward-api-9735" to be "Succeeded or Failed"
    Dec 16 13:25:51.324: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923906ms
    Dec 16 13:25:53.329: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008196163s
    Dec 16 13:25:55.332: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011002272s
    STEP: Saw pod success 12/16/22 13:25:55.332
    Dec 16 13:25:55.332: INFO: Pod "downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be" satisfied condition "Succeeded or Failed"
    Dec 16 13:25:55.335: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:25:55.343
    Dec 16 13:25:55.356: INFO: Waiting for pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be to disappear
    Dec 16 13:25:55.359: INFO: Pod downward-api-5344d260-8d87-41e0-88c4-aeb1013ee0be no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:55.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9735" for this suite. 12/16/22 13:25:55.364
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:55.369
Dec 16 13:25:55.370: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:25:55.37
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:55.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:55.388
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:25:55.402
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:25:55.566
STEP: Deploying the webhook pod 12/16/22 13:25:55.574
STEP: Wait for the deployment to be ready 12/16/22 13:25:55.584
Dec 16 13:25:55.592: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:25:57.604
STEP: Verifying the service has paired with the endpoint 12/16/22 13:25:57.619
Dec 16 13:25:58.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 12/16/22 13:25:58.679
STEP: Creating a configMap that should be mutated 12/16/22 13:25:58.723
STEP: Deleting the collection of validation webhooks 12/16/22 13:25:58.787
STEP: Creating a configMap that should not be mutated 12/16/22 13:25:58.832
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:25:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4655" for this suite. 12/16/22 13:25:58.88
STEP: Destroying namespace "webhook-4655-markers" for this suite. 12/16/22 13:25:58.886
------------------------------
• [3.522 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:55.369
    Dec 16 13:25:55.370: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:25:55.37
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:55.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:55.388
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:25:55.402
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:25:55.566
    STEP: Deploying the webhook pod 12/16/22 13:25:55.574
    STEP: Wait for the deployment to be ready 12/16/22 13:25:55.584
    Dec 16 13:25:55.592: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:25:57.604
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:25:57.619
    Dec 16 13:25:58.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 12/16/22 13:25:58.679
    STEP: Creating a configMap that should be mutated 12/16/22 13:25:58.723
    STEP: Deleting the collection of validation webhooks 12/16/22 13:25:58.787
    STEP: Creating a configMap that should not be mutated 12/16/22 13:25:58.832
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:25:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4655" for this suite. 12/16/22 13:25:58.88
    STEP: Destroying namespace "webhook-4655-markers" for this suite. 12/16/22 13:25:58.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:25:58.892
Dec 16 13:25:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:25:58.892
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:58.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:58.91
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 12/16/22 13:25:58.913
Dec 16 13:25:58.921: INFO: Waiting up to 5m0s for pod "pod-nzqwp" in namespace "pods-2005" to be "running"
Dec 16 13:25:58.924: INFO: Pod "pod-nzqwp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.053208ms
Dec 16 13:26:00.930: INFO: Pod "pod-nzqwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175146s
Dec 16 13:26:00.930: INFO: Pod "pod-nzqwp" satisfied condition "running"
STEP: patching /status 12/16/22 13:26:00.93
Dec 16 13:26:00.939: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:26:00.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2005" for this suite. 12/16/22 13:26:00.945
------------------------------
• [2.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:25:58.892
    Dec 16 13:25:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:25:58.892
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:25:58.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:25:58.91
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 12/16/22 13:25:58.913
    Dec 16 13:25:58.921: INFO: Waiting up to 5m0s for pod "pod-nzqwp" in namespace "pods-2005" to be "running"
    Dec 16 13:25:58.924: INFO: Pod "pod-nzqwp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.053208ms
    Dec 16 13:26:00.930: INFO: Pod "pod-nzqwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175146s
    Dec 16 13:26:00.930: INFO: Pod "pod-nzqwp" satisfied condition "running"
    STEP: patching /status 12/16/22 13:26:00.93
    Dec 16 13:26:00.939: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:26:00.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2005" for this suite. 12/16/22 13:26:00.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:26:00.952
Dec 16 13:26:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context-test 12/16/22 13:26:00.953
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:00.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:00.972
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Dec 16 13:26:00.984: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a" in namespace "security-context-test-7385" to be "Succeeded or Failed"
Dec 16 13:26:00.987: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133965ms
Dec 16 13:26:02.992: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008082605s
Dec 16 13:26:04.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010103749s
Dec 16 13:26:06.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010060096s
Dec 16 13:26:06.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 13:26:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7385" for this suite. 12/16/22 13:26:07.008
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:26:00.952
    Dec 16 13:26:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context-test 12/16/22 13:26:00.953
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:00.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:00.972
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Dec 16 13:26:00.984: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a" in namespace "security-context-test-7385" to be "Succeeded or Failed"
    Dec 16 13:26:00.987: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133965ms
    Dec 16 13:26:02.992: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008082605s
    Dec 16 13:26:04.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010103749s
    Dec 16 13:26:06.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010060096s
    Dec 16 13:26:06.994: INFO: Pod "alpine-nnp-false-86e09d46-892a-4b0b-a774-ad3e7f18fd1a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:26:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7385" for this suite. 12/16/22 13:26:07.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:26:07.015
Dec 16 13:26:07.015: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:26:07.015
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:07.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:07.032
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6894 12/16/22 13:26:07.035
STEP: creating a selector 12/16/22 13:26:07.035
STEP: Creating the service pods in kubernetes 12/16/22 13:26:07.035
Dec 16 13:26:07.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 13:26:07.066: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6894" to be "running and ready"
Dec 16 13:26:07.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.99484ms
Dec 16 13:26:07.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:26:09.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017164484s
Dec 16 13:26:09.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:26:11.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015857263s
Dec 16 13:26:11.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:26:13.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017613407s
Dec 16 13:26:13.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:26:15.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01741049s
Dec 16 13:26:15.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:26:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01581349s
Dec 16 13:26:17.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:26:19.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015752374s
Dec 16 13:26:19.081: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 16 13:26:19.081: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 16 13:26:19.084: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6894" to be "running and ready"
Dec 16 13:26:19.088: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.207512ms
Dec 16 13:26:19.088: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 16 13:26:19.088: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 16 13:26:19.091: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6894" to be "running and ready"
Dec 16 13:26:19.094: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.271576ms
Dec 16 13:26:19.094: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 16 13:26:19.094: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/16/22 13:26:19.097
Dec 16 13:26:19.103: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6894" to be "running"
Dec 16 13:26:19.106: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16549ms
Dec 16 13:26:21.111: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007956925s
Dec 16 13:26:21.111: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 16 13:26:21.114: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 16 13:26:21.114: INFO: Breadth first check of 192.168.156.153 on host 85.217.161.222...
Dec 16 13:26:21.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.156.153&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:26:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:26:21.117: INFO: ExecWithOptions: Clientset creation
Dec 16 13:26:21.117: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.156.153%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:26:21.237: INFO: Waiting for responses: map[]
Dec 16 13:26:21.237: INFO: reached 192.168.156.153 after 0/1 tries
Dec 16 13:26:21.237: INFO: Breadth first check of 192.168.189.10 on host 85.217.161.242...
Dec 16 13:26:21.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.189.10&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:26:21.241: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:26:21.242: INFO: ExecWithOptions: Clientset creation
Dec 16 13:26:21.242: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.189.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:26:21.369: INFO: Waiting for responses: map[]
Dec 16 13:26:21.369: INFO: reached 192.168.189.10 after 0/1 tries
Dec 16 13:26:21.369: INFO: Breadth first check of 192.168.189.222 on host 85.217.161.213...
Dec 16 13:26:21.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.189.222&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:26:21.374: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:26:21.374: INFO: ExecWithOptions: Clientset creation
Dec 16 13:26:21.374: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.189.222%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 16 13:26:21.503: INFO: Waiting for responses: map[]
Dec 16 13:26:21.503: INFO: reached 192.168.189.222 after 0/1 tries
Dec 16 13:26:21.503: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 16 13:26:21.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6894" for this suite. 12/16/22 13:26:21.509
------------------------------
• [SLOW TEST] [14.502 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:26:07.015
    Dec 16 13:26:07.015: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:26:07.015
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:07.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:07.032
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6894 12/16/22 13:26:07.035
    STEP: creating a selector 12/16/22 13:26:07.035
    STEP: Creating the service pods in kubernetes 12/16/22 13:26:07.035
    Dec 16 13:26:07.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 16 13:26:07.066: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6894" to be "running and ready"
    Dec 16 13:26:07.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.99484ms
    Dec 16 13:26:07.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:26:09.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017164484s
    Dec 16 13:26:09.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:26:11.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015857263s
    Dec 16 13:26:11.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:26:13.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017613407s
    Dec 16 13:26:13.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:26:15.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01741049s
    Dec 16 13:26:15.083: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:26:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01581349s
    Dec 16 13:26:17.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:26:19.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015752374s
    Dec 16 13:26:19.081: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 16 13:26:19.081: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 16 13:26:19.084: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6894" to be "running and ready"
    Dec 16 13:26:19.088: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.207512ms
    Dec 16 13:26:19.088: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 16 13:26:19.088: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 16 13:26:19.091: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6894" to be "running and ready"
    Dec 16 13:26:19.094: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.271576ms
    Dec 16 13:26:19.094: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 16 13:26:19.094: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/16/22 13:26:19.097
    Dec 16 13:26:19.103: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6894" to be "running"
    Dec 16 13:26:19.106: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16549ms
    Dec 16 13:26:21.111: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007956925s
    Dec 16 13:26:21.111: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 16 13:26:21.114: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 16 13:26:21.114: INFO: Breadth first check of 192.168.156.153 on host 85.217.161.222...
    Dec 16 13:26:21.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.156.153&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:26:21.117: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:26:21.117: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:26:21.117: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.156.153%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:26:21.237: INFO: Waiting for responses: map[]
    Dec 16 13:26:21.237: INFO: reached 192.168.156.153 after 0/1 tries
    Dec 16 13:26:21.237: INFO: Breadth first check of 192.168.189.10 on host 85.217.161.242...
    Dec 16 13:26:21.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.189.10&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:26:21.241: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:26:21.242: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:26:21.242: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.189.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:26:21.369: INFO: Waiting for responses: map[]
    Dec 16 13:26:21.369: INFO: reached 192.168.189.10 after 0/1 tries
    Dec 16 13:26:21.369: INFO: Breadth first check of 192.168.189.222 on host 85.217.161.213...
    Dec 16 13:26:21.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.189.11:9080/dial?request=hostname&protocol=udp&host=192.168.189.222&port=8081&tries=1'] Namespace:pod-network-test-6894 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:26:21.374: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:26:21.374: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:26:21.374: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6894/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.189.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.189.222%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 16 13:26:21.503: INFO: Waiting for responses: map[]
    Dec 16 13:26:21.503: INFO: reached 192.168.189.222 after 0/1 tries
    Dec 16 13:26:21.503: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:26:21.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6894" for this suite. 12/16/22 13:26:21.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:26:21.517
Dec 16 13:26:21.517: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:26:21.518
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:21.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:21.536
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:26:21.538
Dec 16 13:26:21.547: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961" in namespace "downward-api-1597" to be "Succeeded or Failed"
Dec 16 13:26:21.551: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242321ms
Dec 16 13:26:23.555: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007796697s
Dec 16 13:26:25.556: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009008514s
STEP: Saw pod success 12/16/22 13:26:25.556
Dec 16 13:26:25.556: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961" satisfied condition "Succeeded or Failed"
Dec 16 13:26:25.559: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 container client-container: <nil>
STEP: delete the pod 12/16/22 13:26:25.568
Dec 16 13:26:25.579: INFO: Waiting for pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 to disappear
Dec 16 13:26:25.583: INFO: Pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:26:25.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1597" for this suite. 12/16/22 13:26:25.586
------------------------------
• [4.075 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:26:21.517
    Dec 16 13:26:21.517: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:26:21.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:21.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:21.536
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:26:21.538
    Dec 16 13:26:21.547: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961" in namespace "downward-api-1597" to be "Succeeded or Failed"
    Dec 16 13:26:21.551: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242321ms
    Dec 16 13:26:23.555: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007796697s
    Dec 16 13:26:25.556: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009008514s
    STEP: Saw pod success 12/16/22 13:26:25.556
    Dec 16 13:26:25.556: INFO: Pod "downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961" satisfied condition "Succeeded or Failed"
    Dec 16 13:26:25.559: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:26:25.568
    Dec 16 13:26:25.579: INFO: Waiting for pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 to disappear
    Dec 16 13:26:25.583: INFO: Pod downwardapi-volume-f1ae0473-d586-4c9f-984a-4f2b6ed95961 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:26:25.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1597" for this suite. 12/16/22 13:26:25.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:26:25.592
Dec 16 13:26:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 13:26:25.593
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:25.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:25.609
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 12/16/22 13:26:25.612
STEP: Ensuring active pods == parallelism 12/16/22 13:26:25.617
STEP: delete a job 12/16/22 13:26:27.623
STEP: deleting Job.batch foo in namespace job-2087, will wait for the garbage collector to delete the pods 12/16/22 13:26:27.623
Dec 16 13:26:27.683: INFO: Deleting Job.batch foo took: 6.597458ms
Dec 16 13:26:27.784: INFO: Terminating Job.batch foo pods took: 100.497061ms
STEP: Ensuring job was deleted 12/16/22 13:27:00.385
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:00.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2087" for this suite. 12/16/22 13:27:00.393
------------------------------
• [SLOW TEST] [34.806 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:26:25.592
    Dec 16 13:26:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 13:26:25.593
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:26:25.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:26:25.609
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 12/16/22 13:26:25.612
    STEP: Ensuring active pods == parallelism 12/16/22 13:26:25.617
    STEP: delete a job 12/16/22 13:26:27.623
    STEP: deleting Job.batch foo in namespace job-2087, will wait for the garbage collector to delete the pods 12/16/22 13:26:27.623
    Dec 16 13:26:27.683: INFO: Deleting Job.batch foo took: 6.597458ms
    Dec 16 13:26:27.784: INFO: Terminating Job.batch foo pods took: 100.497061ms
    STEP: Ensuring job was deleted 12/16/22 13:27:00.385
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:00.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2087" for this suite. 12/16/22 13:27:00.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:00.399
Dec 16 13:27:00.399: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:27:00.4
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:00.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:00.418
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-8bd0c32b-a262-4ecd-a1de-17cc21afee7d 12/16/22 13:27:00.42
STEP: Creating secret with name secret-projected-all-test-volume-85b2a836-a774-47a0-95ce-289570f4f7ff 12/16/22 13:27:00.424
STEP: Creating a pod to test Check all projections for projected volume plugin 12/16/22 13:27:00.427
Dec 16 13:27:00.434: INFO: Waiting up to 5m0s for pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639" in namespace "projected-6286" to be "Succeeded or Failed"
Dec 16 13:27:00.437: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588942ms
Dec 16 13:27:02.442: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008126991s
Dec 16 13:27:04.443: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009030524s
STEP: Saw pod success 12/16/22 13:27:04.443
Dec 16 13:27:04.443: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639" satisfied condition "Succeeded or Failed"
Dec 16 13:27:04.447: INFO: Trying to get logs from node pool-a3802-fsxxd pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 container projected-all-volume-test: <nil>
STEP: delete the pod 12/16/22 13:27:04.456
Dec 16 13:27:04.468: INFO: Waiting for pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 to disappear
Dec 16 13:27:04.471: INFO: Pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:04.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6286" for this suite. 12/16/22 13:27:04.475
------------------------------
• [4.082 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:00.399
    Dec 16 13:27:00.399: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:27:00.4
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:00.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:00.418
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-8bd0c32b-a262-4ecd-a1de-17cc21afee7d 12/16/22 13:27:00.42
    STEP: Creating secret with name secret-projected-all-test-volume-85b2a836-a774-47a0-95ce-289570f4f7ff 12/16/22 13:27:00.424
    STEP: Creating a pod to test Check all projections for projected volume plugin 12/16/22 13:27:00.427
    Dec 16 13:27:00.434: INFO: Waiting up to 5m0s for pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639" in namespace "projected-6286" to be "Succeeded or Failed"
    Dec 16 13:27:00.437: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588942ms
    Dec 16 13:27:02.442: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008126991s
    Dec 16 13:27:04.443: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009030524s
    STEP: Saw pod success 12/16/22 13:27:04.443
    Dec 16 13:27:04.443: INFO: Pod "projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639" satisfied condition "Succeeded or Failed"
    Dec 16 13:27:04.447: INFO: Trying to get logs from node pool-a3802-fsxxd pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 container projected-all-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:27:04.456
    Dec 16 13:27:04.468: INFO: Waiting for pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 to disappear
    Dec 16 13:27:04.471: INFO: Pod projected-volume-09a7ab36-6798-4d88-bfba-d693b4b80639 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:04.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6286" for this suite. 12/16/22 13:27:04.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:04.482
Dec 16 13:27:04.482: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context-test 12/16/22 13:27:04.483
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:04.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:04.498
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Dec 16 13:27:04.508: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e" in namespace "security-context-test-8142" to be "Succeeded or Failed"
Dec 16 13:27:04.510: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707948ms
Dec 16 13:27:06.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Running", Reason="", readiness=false. Elapsed: 2.008572076s
Dec 16 13:27:08.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007985251s
Dec 16 13:27:08.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8142" for this suite. 12/16/22 13:27:08.52
------------------------------
• [4.044 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:04.482
    Dec 16 13:27:04.482: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context-test 12/16/22 13:27:04.483
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:04.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:04.498
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Dec 16 13:27:04.508: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e" in namespace "security-context-test-8142" to be "Succeeded or Failed"
    Dec 16 13:27:04.510: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707948ms
    Dec 16 13:27:06.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Running", Reason="", readiness=false. Elapsed: 2.008572076s
    Dec 16 13:27:08.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007985251s
    Dec 16 13:27:08.516: INFO: Pod "busybox-readonly-false-73ed5081-c319-44b9-9b8f-7e8c6756bd9e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8142" for this suite. 12/16/22 13:27:08.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:08.527
Dec 16 13:27:08.527: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename ingress 12/16/22 13:27:08.528
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:08.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:08.545
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 12/16/22 13:27:08.548
STEP: getting /apis/networking.k8s.io 12/16/22 13:27:08.55
STEP: getting /apis/networking.k8s.iov1 12/16/22 13:27:08.551
STEP: creating 12/16/22 13:27:08.552
STEP: getting 12/16/22 13:27:08.565
STEP: listing 12/16/22 13:27:08.568
STEP: watching 12/16/22 13:27:08.572
Dec 16 13:27:08.572: INFO: starting watch
STEP: cluster-wide listing 12/16/22 13:27:08.573
STEP: cluster-wide watching 12/16/22 13:27:08.576
Dec 16 13:27:08.576: INFO: starting watch
STEP: patching 12/16/22 13:27:08.577
STEP: updating 12/16/22 13:27:08.582
Dec 16 13:27:08.589: INFO: waiting for watch events with expected annotations
Dec 16 13:27:08.590: INFO: saw patched and updated annotations
STEP: patching /status 12/16/22 13:27:08.59
STEP: updating /status 12/16/22 13:27:08.595
STEP: get /status 12/16/22 13:27:08.603
STEP: deleting 12/16/22 13:27:08.606
STEP: deleting a collection 12/16/22 13:27:08.617
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:08.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-9683" for this suite. 12/16/22 13:27:08.636
------------------------------
• [0.114 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:08.527
    Dec 16 13:27:08.527: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename ingress 12/16/22 13:27:08.528
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:08.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:08.545
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 12/16/22 13:27:08.548
    STEP: getting /apis/networking.k8s.io 12/16/22 13:27:08.55
    STEP: getting /apis/networking.k8s.iov1 12/16/22 13:27:08.551
    STEP: creating 12/16/22 13:27:08.552
    STEP: getting 12/16/22 13:27:08.565
    STEP: listing 12/16/22 13:27:08.568
    STEP: watching 12/16/22 13:27:08.572
    Dec 16 13:27:08.572: INFO: starting watch
    STEP: cluster-wide listing 12/16/22 13:27:08.573
    STEP: cluster-wide watching 12/16/22 13:27:08.576
    Dec 16 13:27:08.576: INFO: starting watch
    STEP: patching 12/16/22 13:27:08.577
    STEP: updating 12/16/22 13:27:08.582
    Dec 16 13:27:08.589: INFO: waiting for watch events with expected annotations
    Dec 16 13:27:08.590: INFO: saw patched and updated annotations
    STEP: patching /status 12/16/22 13:27:08.59
    STEP: updating /status 12/16/22 13:27:08.595
    STEP: get /status 12/16/22 13:27:08.603
    STEP: deleting 12/16/22 13:27:08.606
    STEP: deleting a collection 12/16/22 13:27:08.617
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:08.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-9683" for this suite. 12/16/22 13:27:08.636
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:08.642
Dec 16 13:27:08.642: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:27:08.643
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:08.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:08.661
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 12/16/22 13:27:08.663
Dec 16 13:27:08.677: INFO: Waiting up to 5m0s for pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a" in namespace "emptydir-1320" to be "Succeeded or Failed"
Dec 16 13:27:08.680: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033794ms
Dec 16 13:27:10.685: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007879418s
Dec 16 13:27:12.687: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009819616s
STEP: Saw pod success 12/16/22 13:27:12.687
Dec 16 13:27:12.687: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a" satisfied condition "Succeeded or Failed"
Dec 16 13:27:12.690: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a container test-container: <nil>
STEP: delete the pod 12/16/22 13:27:12.699
Dec 16 13:27:12.712: INFO: Waiting for pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a to disappear
Dec 16 13:27:12.715: INFO: Pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:12.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1320" for this suite. 12/16/22 13:27:12.719
------------------------------
• [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:08.642
    Dec 16 13:27:08.642: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:27:08.643
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:08.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:08.661
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 12/16/22 13:27:08.663
    Dec 16 13:27:08.677: INFO: Waiting up to 5m0s for pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a" in namespace "emptydir-1320" to be "Succeeded or Failed"
    Dec 16 13:27:08.680: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033794ms
    Dec 16 13:27:10.685: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007879418s
    Dec 16 13:27:12.687: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009819616s
    STEP: Saw pod success 12/16/22 13:27:12.687
    Dec 16 13:27:12.687: INFO: Pod "pod-d0d16174-d63d-44b5-8d04-191d4acd581a" satisfied condition "Succeeded or Failed"
    Dec 16 13:27:12.690: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a container test-container: <nil>
    STEP: delete the pod 12/16/22 13:27:12.699
    Dec 16 13:27:12.712: INFO: Waiting for pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a to disappear
    Dec 16 13:27:12.715: INFO: Pod pod-d0d16174-d63d-44b5-8d04-191d4acd581a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:12.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1320" for this suite. 12/16/22 13:27:12.719
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:12.724
Dec 16 13:27:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:27:12.725
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:12.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:12.743
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  12/16/22 13:27:12.746
Dec 16 13:27:12.755: INFO: Waiting up to 5m0s for pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024" in namespace "svcaccounts-545" to be "Succeeded or Failed"
Dec 16 13:27:12.758: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994137ms
Dec 16 13:27:14.763: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008168791s
Dec 16 13:27:16.764: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008739001s
STEP: Saw pod success 12/16/22 13:27:16.764
Dec 16 13:27:16.764: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024" satisfied condition "Succeeded or Failed"
Dec 16 13:27:16.768: INFO: Trying to get logs from node pool-a3802-fsxxd pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:27:16.776
Dec 16 13:27:16.787: INFO: Waiting for pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 to disappear
Dec 16 13:27:16.790: INFO: Pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:16.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-545" for this suite. 12/16/22 13:27:16.794
------------------------------
• [4.076 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:12.724
    Dec 16 13:27:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:27:12.725
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:12.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:12.743
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  12/16/22 13:27:12.746
    Dec 16 13:27:12.755: INFO: Waiting up to 5m0s for pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024" in namespace "svcaccounts-545" to be "Succeeded or Failed"
    Dec 16 13:27:12.758: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994137ms
    Dec 16 13:27:14.763: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008168791s
    Dec 16 13:27:16.764: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008739001s
    STEP: Saw pod success 12/16/22 13:27:16.764
    Dec 16 13:27:16.764: INFO: Pod "test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024" satisfied condition "Succeeded or Failed"
    Dec 16 13:27:16.768: INFO: Trying to get logs from node pool-a3802-fsxxd pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:27:16.776
    Dec 16 13:27:16.787: INFO: Waiting for pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 to disappear
    Dec 16 13:27:16.790: INFO: Pod test-pod-5070a09c-ae2d-4b0b-a4f8-b47c1ae00024 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:16.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-545" for this suite. 12/16/22 13:27:16.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:16.801
Dec 16 13:27:16.801: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:27:16.801
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:16.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:16.818
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 12/16/22 13:27:16.82
Dec 16 13:27:16.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 create -f -'
Dec 16 13:27:17.350: INFO: stderr: ""
Dec 16 13:27:17.350: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 13:27:17.35
Dec 16 13:27:17.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 13:27:17.419: INFO: stderr: ""
Dec 16 13:27:17.419: INFO: stdout: "update-demo-nautilus-gfms7 update-demo-nautilus-vj4fl "
Dec 16 13:27:17.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 13:27:17.485: INFO: stderr: ""
Dec 16 13:27:17.485: INFO: stdout: ""
Dec 16 13:27:17.485: INFO: update-demo-nautilus-gfms7 is created but not running
Dec 16 13:27:22.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 13:27:22.553: INFO: stderr: ""
Dec 16 13:27:22.553: INFO: stdout: "update-demo-nautilus-gfms7 update-demo-nautilus-vj4fl "
Dec 16 13:27:22.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 13:27:22.620: INFO: stderr: ""
Dec 16 13:27:22.620: INFO: stdout: "true"
Dec 16 13:27:22.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 13:27:22.685: INFO: stderr: ""
Dec 16 13:27:22.685: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 13:27:22.685: INFO: validating pod update-demo-nautilus-gfms7
Dec 16 13:27:22.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 13:27:22.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 13:27:22.719: INFO: update-demo-nautilus-gfms7 is verified up and running
Dec 16 13:27:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-vj4fl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 13:27:22.785: INFO: stderr: ""
Dec 16 13:27:22.785: INFO: stdout: "true"
Dec 16 13:27:22.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-vj4fl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 13:27:22.850: INFO: stderr: ""
Dec 16 13:27:22.850: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 13:27:22.850: INFO: validating pod update-demo-nautilus-vj4fl
Dec 16 13:27:22.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 13:27:22.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 13:27:22.883: INFO: update-demo-nautilus-vj4fl is verified up and running
STEP: using delete to clean up resources 12/16/22 13:27:22.883
Dec 16 13:27:22.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 delete --grace-period=0 --force -f -'
Dec 16 13:27:22.954: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:27:22.954: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 16 13:27:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get rc,svc -l name=update-demo --no-headers'
Dec 16 13:27:23.029: INFO: stderr: "No resources found in kubectl-4646 namespace.\n"
Dec 16 13:27:23.029: INFO: stdout: ""
Dec 16 13:27:23.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 13:27:23.113: INFO: stderr: ""
Dec 16 13:27:23.113: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:27:23.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4646" for this suite. 12/16/22 13:27:23.118
------------------------------
• [SLOW TEST] [6.324 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:16.801
    Dec 16 13:27:16.801: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:27:16.801
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:16.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:16.818
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 12/16/22 13:27:16.82
    Dec 16 13:27:16.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 create -f -'
    Dec 16 13:27:17.350: INFO: stderr: ""
    Dec 16 13:27:17.350: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 13:27:17.35
    Dec 16 13:27:17.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 13:27:17.419: INFO: stderr: ""
    Dec 16 13:27:17.419: INFO: stdout: "update-demo-nautilus-gfms7 update-demo-nautilus-vj4fl "
    Dec 16 13:27:17.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 13:27:17.485: INFO: stderr: ""
    Dec 16 13:27:17.485: INFO: stdout: ""
    Dec 16 13:27:17.485: INFO: update-demo-nautilus-gfms7 is created but not running
    Dec 16 13:27:22.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 13:27:22.553: INFO: stderr: ""
    Dec 16 13:27:22.553: INFO: stdout: "update-demo-nautilus-gfms7 update-demo-nautilus-vj4fl "
    Dec 16 13:27:22.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 13:27:22.620: INFO: stderr: ""
    Dec 16 13:27:22.620: INFO: stdout: "true"
    Dec 16 13:27:22.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-gfms7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 13:27:22.685: INFO: stderr: ""
    Dec 16 13:27:22.685: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 13:27:22.685: INFO: validating pod update-demo-nautilus-gfms7
    Dec 16 13:27:22.719: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 13:27:22.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 13:27:22.719: INFO: update-demo-nautilus-gfms7 is verified up and running
    Dec 16 13:27:22.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-vj4fl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 13:27:22.785: INFO: stderr: ""
    Dec 16 13:27:22.785: INFO: stdout: "true"
    Dec 16 13:27:22.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods update-demo-nautilus-vj4fl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 13:27:22.850: INFO: stderr: ""
    Dec 16 13:27:22.850: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 13:27:22.850: INFO: validating pod update-demo-nautilus-vj4fl
    Dec 16 13:27:22.883: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 13:27:22.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 13:27:22.883: INFO: update-demo-nautilus-vj4fl is verified up and running
    STEP: using delete to clean up resources 12/16/22 13:27:22.883
    Dec 16 13:27:22.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 delete --grace-period=0 --force -f -'
    Dec 16 13:27:22.954: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:27:22.954: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 16 13:27:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get rc,svc -l name=update-demo --no-headers'
    Dec 16 13:27:23.029: INFO: stderr: "No resources found in kubectl-4646 namespace.\n"
    Dec 16 13:27:23.029: INFO: stdout: ""
    Dec 16 13:27:23.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4646 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 16 13:27:23.113: INFO: stderr: ""
    Dec 16 13:27:23.113: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:27:23.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4646" for this suite. 12/16/22 13:27:23.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:27:23.125
Dec 16 13:27:23.125: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 13:27:23.126
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:23.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:23.145
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 in namespace container-probe-3936 12/16/22 13:27:23.148
Dec 16 13:27:23.160: INFO: Waiting up to 5m0s for pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259" in namespace "container-probe-3936" to be "not pending"
Dec 16 13:27:23.164: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970003ms
Dec 16 13:27:25.168: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259": Phase="Running", Reason="", readiness=true. Elapsed: 2.008422058s
Dec 16 13:27:25.168: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259" satisfied condition "not pending"
Dec 16 13:27:25.168: INFO: Started pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 in namespace container-probe-3936
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:27:25.168
Dec 16 13:27:25.171: INFO: Initial restart count of pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 is 0
Dec 16 13:28:15.325: INFO: Restart count of pod container-probe-3936/busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 is now 1 (50.154209721s elapsed)
STEP: deleting the pod 12/16/22 13:28:15.325
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3936" for this suite. 12/16/22 13:28:15.343
------------------------------
• [SLOW TEST] [52.224 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:27:23.125
    Dec 16 13:27:23.125: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 13:27:23.126
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:27:23.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:27:23.145
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 in namespace container-probe-3936 12/16/22 13:27:23.148
    Dec 16 13:27:23.160: INFO: Waiting up to 5m0s for pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259" in namespace "container-probe-3936" to be "not pending"
    Dec 16 13:27:23.164: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970003ms
    Dec 16 13:27:25.168: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259": Phase="Running", Reason="", readiness=true. Elapsed: 2.008422058s
    Dec 16 13:27:25.168: INFO: Pod "busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259" satisfied condition "not pending"
    Dec 16 13:27:25.168: INFO: Started pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 in namespace container-probe-3936
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:27:25.168
    Dec 16 13:27:25.171: INFO: Initial restart count of pod busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 is 0
    Dec 16 13:28:15.325: INFO: Restart count of pod container-probe-3936/busybox-ec164ab6-37e9-4c17-9fbf-ce389576b259 is now 1 (50.154209721s elapsed)
    STEP: deleting the pod 12/16/22 13:28:15.325
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3936" for this suite. 12/16/22 13:28:15.343
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:15.35
Dec 16 13:28:15.350: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:28:15.35
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:15.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:15.371
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-4601 12/16/22 13:28:15.374
STEP: creating service affinity-clusterip in namespace services-4601 12/16/22 13:28:15.374
STEP: creating replication controller affinity-clusterip in namespace services-4601 12/16/22 13:28:15.388
I1216 13:28:15.396074      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4601, replica count: 3
I1216 13:28:18.446880      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 13:28:18.455: INFO: Creating new exec pod
Dec 16 13:28:18.462: INFO: Waiting up to 5m0s for pod "execpod-affinitydwtg5" in namespace "services-4601" to be "running"
Dec 16 13:28:18.466: INFO: Pod "execpod-affinitydwtg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020216ms
Dec 16 13:28:20.471: INFO: Pod "execpod-affinitydwtg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008563486s
Dec 16 13:28:20.471: INFO: Pod "execpod-affinitydwtg5" satisfied condition "running"
Dec 16 13:28:21.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Dec 16 13:28:21.665: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 16 13:28:21.665: INFO: stdout: ""
Dec 16 13:28:21.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c nc -v -z -w 2 10.99.89.235 80'
Dec 16 13:28:21.863: INFO: stderr: "+ nc -v -z -w 2 10.99.89.235 80\nConnection to 10.99.89.235 80 port [tcp/http] succeeded!\n"
Dec 16 13:28:21.863: INFO: stdout: ""
Dec 16 13:28:21.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.89.235:80/ ; done'
Dec 16 13:28:22.123: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n"
Dec 16 13:28:22.123: INFO: stdout: "\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6"
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
Dec 16 13:28:22.123: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4601, will wait for the garbage collector to delete the pods 12/16/22 13:28:22.137
Dec 16 13:28:22.199: INFO: Deleting ReplicationController affinity-clusterip took: 8.223069ms
Dec 16 13:28:22.299: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.403712ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:24.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4601" for this suite. 12/16/22 13:28:24.527
------------------------------
• [SLOW TEST] [9.183 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:15.35
    Dec 16 13:28:15.350: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:28:15.35
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:15.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:15.371
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-4601 12/16/22 13:28:15.374
    STEP: creating service affinity-clusterip in namespace services-4601 12/16/22 13:28:15.374
    STEP: creating replication controller affinity-clusterip in namespace services-4601 12/16/22 13:28:15.388
    I1216 13:28:15.396074      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4601, replica count: 3
    I1216 13:28:18.446880      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 13:28:18.455: INFO: Creating new exec pod
    Dec 16 13:28:18.462: INFO: Waiting up to 5m0s for pod "execpod-affinitydwtg5" in namespace "services-4601" to be "running"
    Dec 16 13:28:18.466: INFO: Pod "execpod-affinitydwtg5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020216ms
    Dec 16 13:28:20.471: INFO: Pod "execpod-affinitydwtg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008563486s
    Dec 16 13:28:20.471: INFO: Pod "execpod-affinitydwtg5" satisfied condition "running"
    Dec 16 13:28:21.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Dec 16 13:28:21.665: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Dec 16 13:28:21.665: INFO: stdout: ""
    Dec 16 13:28:21.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c nc -v -z -w 2 10.99.89.235 80'
    Dec 16 13:28:21.863: INFO: stderr: "+ nc -v -z -w 2 10.99.89.235 80\nConnection to 10.99.89.235 80 port [tcp/http] succeeded!\n"
    Dec 16 13:28:21.863: INFO: stdout: ""
    Dec 16 13:28:21.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-4601 exec execpod-affinitydwtg5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.89.235:80/ ; done'
    Dec 16 13:28:22.123: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.89.235:80/\n"
    Dec 16 13:28:22.123: INFO: stdout: "\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6\naffinity-clusterip-sxzk6"
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Received response from host: affinity-clusterip-sxzk6
    Dec 16 13:28:22.123: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4601, will wait for the garbage collector to delete the pods 12/16/22 13:28:22.137
    Dec 16 13:28:22.199: INFO: Deleting ReplicationController affinity-clusterip took: 8.223069ms
    Dec 16 13:28:22.299: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.403712ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:24.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4601" for this suite. 12/16/22 13:28:24.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:24.533
Dec 16 13:28:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 13:28:24.534
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:24.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:24.553
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7593 12/16/22 13:28:24.556
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-7593 12/16/22 13:28:24.561
Dec 16 13:28:24.570: INFO: Found 0 stateful pods, waiting for 1
Dec 16 13:28:34.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 12/16/22 13:28:34.587
STEP: updating a scale subresource 12/16/22 13:28:34.591
STEP: verifying the statefulset Spec.Replicas was modified 12/16/22 13:28:34.598
STEP: Patch a scale subresource 12/16/22 13:28:34.601
STEP: verifying the statefulset Spec.Replicas was modified 12/16/22 13:28:34.607
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 13:28:34.611: INFO: Deleting all statefulset in ns statefulset-7593
Dec 16 13:28:34.615: INFO: Scaling statefulset ss to 0
Dec 16 13:28:44.635: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 13:28:44.639: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:44.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7593" for this suite. 12/16/22 13:28:44.658
------------------------------
• [SLOW TEST] [20.133 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:24.533
    Dec 16 13:28:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 13:28:24.534
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:24.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:24.553
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7593 12/16/22 13:28:24.556
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-7593 12/16/22 13:28:24.561
    Dec 16 13:28:24.570: INFO: Found 0 stateful pods, waiting for 1
    Dec 16 13:28:34.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 12/16/22 13:28:34.587
    STEP: updating a scale subresource 12/16/22 13:28:34.591
    STEP: verifying the statefulset Spec.Replicas was modified 12/16/22 13:28:34.598
    STEP: Patch a scale subresource 12/16/22 13:28:34.601
    STEP: verifying the statefulset Spec.Replicas was modified 12/16/22 13:28:34.607
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 13:28:34.611: INFO: Deleting all statefulset in ns statefulset-7593
    Dec 16 13:28:34.615: INFO: Scaling statefulset ss to 0
    Dec 16 13:28:44.635: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 13:28:44.639: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:44.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7593" for this suite. 12/16/22 13:28:44.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:44.667
Dec 16 13:28:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-runtime 12/16/22 13:28:44.667
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:44.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:44.695
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 12/16/22 13:28:44.697
STEP: wait for the container to reach Failed 12/16/22 13:28:44.705
STEP: get the container status 12/16/22 13:28:48.727
STEP: the container should be terminated 12/16/22 13:28:48.73
STEP: the termination message should be set 12/16/22 13:28:48.73
Dec 16 13:28:48.730: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/16/22 13:28:48.73
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:48.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5585" for this suite. 12/16/22 13:28:48.748
------------------------------
• [4.087 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:44.667
    Dec 16 13:28:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-runtime 12/16/22 13:28:44.667
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:44.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:44.695
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 12/16/22 13:28:44.697
    STEP: wait for the container to reach Failed 12/16/22 13:28:44.705
    STEP: get the container status 12/16/22 13:28:48.727
    STEP: the container should be terminated 12/16/22 13:28:48.73
    STEP: the termination message should be set 12/16/22 13:28:48.73
    Dec 16 13:28:48.730: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/16/22 13:28:48.73
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:48.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5585" for this suite. 12/16/22 13:28:48.748
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:48.754
Dec 16 13:28:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:28:48.755
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:48.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:48.773
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 12/16/22 13:28:48.783
STEP: watching for Pod to be ready 12/16/22 13:28:48.794
Dec 16 13:28:48.795: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 16 13:28:48.799: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
Dec 16 13:28:48.812: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
Dec 16 13:28:49.320: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
Dec 16 13:28:50.454: INFO: Found Pod pod-test in namespace pods-7233 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 12/16/22 13:28:50.459
STEP: getting the Pod and ensuring that it's patched 12/16/22 13:28:50.469
STEP: replacing the Pod's status Ready condition to False 12/16/22 13:28:50.472
STEP: check the Pod again to ensure its Ready conditions are False 12/16/22 13:28:50.483
STEP: deleting the Pod via a Collection with a LabelSelector 12/16/22 13:28:50.483
STEP: watching for the Pod to be deleted 12/16/22 13:28:50.491
Dec 16 13:28:50.492: INFO: observed event type MODIFIED
Dec 16 13:28:51.087: INFO: observed event type MODIFIED
Dec 16 13:28:52.762: INFO: observed event type MODIFIED
Dec 16 13:28:53.647: INFO: observed event type MODIFIED
Dec 16 13:28:53.830: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:53.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7233" for this suite. 12/16/22 13:28:53.98
------------------------------
• [SLOW TEST] [5.293 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:48.754
    Dec 16 13:28:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:28:48.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:48.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:48.773
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 12/16/22 13:28:48.783
    STEP: watching for Pod to be ready 12/16/22 13:28:48.794
    Dec 16 13:28:48.795: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Dec 16 13:28:48.799: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
    Dec 16 13:28:48.812: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
    Dec 16 13:28:49.320: INFO: observed Pod pod-test in namespace pods-7233 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
    Dec 16 13:28:50.454: INFO: Found Pod pod-test in namespace pods-7233 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:28:48 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 12/16/22 13:28:50.459
    STEP: getting the Pod and ensuring that it's patched 12/16/22 13:28:50.469
    STEP: replacing the Pod's status Ready condition to False 12/16/22 13:28:50.472
    STEP: check the Pod again to ensure its Ready conditions are False 12/16/22 13:28:50.483
    STEP: deleting the Pod via a Collection with a LabelSelector 12/16/22 13:28:50.483
    STEP: watching for the Pod to be deleted 12/16/22 13:28:50.491
    Dec 16 13:28:50.492: INFO: observed event type MODIFIED
    Dec 16 13:28:51.087: INFO: observed event type MODIFIED
    Dec 16 13:28:52.762: INFO: observed event type MODIFIED
    Dec 16 13:28:53.647: INFO: observed event type MODIFIED
    Dec 16 13:28:53.830: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:53.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7233" for this suite. 12/16/22 13:28:53.98
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:54.047
Dec 16 13:28:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:28:54.048
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:54.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:54.244
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:28:54.247
Dec 16 13:28:54.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 16 13:28:54.319: INFO: stderr: ""
Dec 16 13:28:54.319: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 12/16/22 13:28:54.319
Dec 16 13:28:54.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Dec 16 13:28:54.797: INFO: stderr: ""
Dec 16 13:28:54.797: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:28:54.797
Dec 16 13:28:54.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 delete pods e2e-test-httpd-pod'
Dec 16 13:28:56.484: INFO: stderr: ""
Dec 16 13:28:56.484: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:28:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-989" for this suite. 12/16/22 13:28:56.488
------------------------------
• [2.446 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:54.047
    Dec 16 13:28:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:28:54.048
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:54.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:54.244
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:28:54.247
    Dec 16 13:28:54.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 16 13:28:54.319: INFO: stderr: ""
    Dec 16 13:28:54.319: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 12/16/22 13:28:54.319
    Dec 16 13:28:54.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Dec 16 13:28:54.797: INFO: stderr: ""
    Dec 16 13:28:54.797: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:28:54.797
    Dec 16 13:28:54.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-989 delete pods e2e-test-httpd-pod'
    Dec 16 13:28:56.484: INFO: stderr: ""
    Dec 16 13:28:56.484: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:28:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-989" for this suite. 12/16/22 13:28:56.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:28:56.494
Dec 16 13:28:56.494: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 13:28:56.495
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:56.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:56.524
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 12/16/22 13:28:56.527
Dec 16 13:28:56.539: INFO: Waiting up to 5m0s for pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66" in namespace "var-expansion-6597" to be "Succeeded or Failed"
Dec 16 13:28:56.543: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442113ms
Dec 16 13:28:58.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008580839s
Dec 16 13:29:00.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008855503s
STEP: Saw pod success 12/16/22 13:29:00.548
Dec 16 13:29:00.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66" satisfied condition "Succeeded or Failed"
Dec 16 13:29:00.559: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:29:00.611
Dec 16 13:29:00.623: INFO: Waiting for pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 to disappear
Dec 16 13:29:00.626: INFO: Pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:00.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6597" for this suite. 12/16/22 13:29:00.629
------------------------------
• [4.141 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:28:56.494
    Dec 16 13:28:56.494: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 13:28:56.495
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:28:56.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:28:56.524
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 12/16/22 13:28:56.527
    Dec 16 13:28:56.539: INFO: Waiting up to 5m0s for pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66" in namespace "var-expansion-6597" to be "Succeeded or Failed"
    Dec 16 13:28:56.543: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442113ms
    Dec 16 13:28:58.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008580839s
    Dec 16 13:29:00.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008855503s
    STEP: Saw pod success 12/16/22 13:29:00.548
    Dec 16 13:29:00.548: INFO: Pod "var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66" satisfied condition "Succeeded or Failed"
    Dec 16 13:29:00.559: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:29:00.611
    Dec 16 13:29:00.623: INFO: Waiting for pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 to disappear
    Dec 16 13:29:00.626: INFO: Pod var-expansion-58bd98df-1d93-4adb-8e34-9af6cf82db66 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:00.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6597" for this suite. 12/16/22 13:29:00.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:00.638
Dec 16 13:29:00.638: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 13:29:00.638
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:00.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:00.655
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/16/22 13:29:00.661
Dec 16 13:29:00.673: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2660" to be "running and ready"
Dec 16 13:29:00.678: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607364ms
Dec 16 13:29:00.678: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:29:02.683: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010301007s
Dec 16 13:29:02.683: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 16 13:29:02.683: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 12/16/22 13:29:02.686
Dec 16 13:29:02.692: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2660" to be "running and ready"
Dec 16 13:29:02.695: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129226ms
Dec 16 13:29:02.695: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:29:04.699: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006776643s
Dec 16 13:29:04.699: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Dec 16 13:29:04.699: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/16/22 13:29:04.702
Dec 16 13:29:04.710: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 13:29:04.722: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 13:29:06.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 13:29:06.727: INFO: Pod pod-with-prestop-http-hook still exists
Dec 16 13:29:08.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 16 13:29:08.728: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 12/16/22 13:29:08.728
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:08.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2660" for this suite. 12/16/22 13:29:08.787
------------------------------
• [SLOW TEST] [8.156 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:00.638
    Dec 16 13:29:00.638: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 13:29:00.638
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:00.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:00.655
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/16/22 13:29:00.661
    Dec 16 13:29:00.673: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2660" to be "running and ready"
    Dec 16 13:29:00.678: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607364ms
    Dec 16 13:29:00.678: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:29:02.683: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010301007s
    Dec 16 13:29:02.683: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 16 13:29:02.683: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 12/16/22 13:29:02.686
    Dec 16 13:29:02.692: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2660" to be "running and ready"
    Dec 16 13:29:02.695: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129226ms
    Dec 16 13:29:02.695: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:29:04.699: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006776643s
    Dec 16 13:29:04.699: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Dec 16 13:29:04.699: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/16/22 13:29:04.702
    Dec 16 13:29:04.710: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 16 13:29:04.722: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 16 13:29:06.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 16 13:29:06.727: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 16 13:29:08.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 16 13:29:08.728: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 12/16/22 13:29:08.728
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:08.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2660" for this suite. 12/16/22 13:29:08.787
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:08.793
Dec 16 13:29:08.793: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:29:08.794
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:08.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:08.812
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:29:08.825
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:29:09.038
STEP: Deploying the webhook pod 12/16/22 13:29:09.048
STEP: Wait for the deployment to be ready 12/16/22 13:29:09.059
Dec 16 13:29:09.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:29:11.081
STEP: Verifying the service has paired with the endpoint 12/16/22 13:29:11.094
Dec 16 13:29:12.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Dec 16 13:29:12.099: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9753-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 13:29:12.608
STEP: Creating a custom resource that should be mutated by the webhook 12/16/22 13:29:12.653
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4063" for this suite. 12/16/22 13:29:15.314
STEP: Destroying namespace "webhook-4063-markers" for this suite. 12/16/22 13:29:15.319
------------------------------
• [SLOW TEST] [6.531 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:08.793
    Dec 16 13:29:08.793: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:29:08.794
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:08.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:08.812
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:29:08.825
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:29:09.038
    STEP: Deploying the webhook pod 12/16/22 13:29:09.048
    STEP: Wait for the deployment to be ready 12/16/22 13:29:09.059
    Dec 16 13:29:09.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:29:11.081
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:29:11.094
    Dec 16 13:29:12.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Dec 16 13:29:12.099: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9753-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 13:29:12.608
    STEP: Creating a custom resource that should be mutated by the webhook 12/16/22 13:29:12.653
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4063" for this suite. 12/16/22 13:29:15.314
    STEP: Destroying namespace "webhook-4063-markers" for this suite. 12/16/22 13:29:15.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:15.326
Dec 16 13:29:15.326: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:29:15.327
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:15.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:15.346
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-5871da18-deba-4073-af23-cdc80b57bbe7 12/16/22 13:29:15.348
STEP: Creating a pod to test consume configMaps 12/16/22 13:29:15.355
Dec 16 13:29:15.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd" in namespace "projected-4388" to be "Succeeded or Failed"
Dec 16 13:29:15.368: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184884ms
Dec 16 13:29:17.373: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010178461s
Dec 16 13:29:19.374: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011956675s
STEP: Saw pod success 12/16/22 13:29:19.374
Dec 16 13:29:19.375: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd" satisfied condition "Succeeded or Failed"
Dec 16 13:29:19.378: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:29:19.386
Dec 16 13:29:19.398: INFO: Waiting for pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd to disappear
Dec 16 13:29:19.401: INFO: Pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:19.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4388" for this suite. 12/16/22 13:29:19.405
------------------------------
• [4.085 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:15.326
    Dec 16 13:29:15.326: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:29:15.327
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:15.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:15.346
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-5871da18-deba-4073-af23-cdc80b57bbe7 12/16/22 13:29:15.348
    STEP: Creating a pod to test consume configMaps 12/16/22 13:29:15.355
    Dec 16 13:29:15.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd" in namespace "projected-4388" to be "Succeeded or Failed"
    Dec 16 13:29:15.368: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184884ms
    Dec 16 13:29:17.373: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010178461s
    Dec 16 13:29:19.374: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011956675s
    STEP: Saw pod success 12/16/22 13:29:19.374
    Dec 16 13:29:19.375: INFO: Pod "pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd" satisfied condition "Succeeded or Failed"
    Dec 16 13:29:19.378: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:29:19.386
    Dec 16 13:29:19.398: INFO: Waiting for pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd to disappear
    Dec 16 13:29:19.401: INFO: Pod pod-projected-configmaps-947c7ef0-2759-4379-8f81-e168b73e94bd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:19.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4388" for this suite. 12/16/22 13:29:19.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:19.412
Dec 16 13:29:19.412: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 13:29:19.412
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:19.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:19.431
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Dec 16 13:29:19.434: INFO: Creating deployment "webserver-deployment"
Dec 16 13:29:19.439: INFO: Waiting for observed generation 1
Dec 16 13:29:21.447: INFO: Waiting for all required pods to come up
Dec 16 13:29:21.452: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 12/16/22 13:29:21.452
Dec 16 13:29:21.452: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2kh4v" in namespace "deployment-9645" to be "running"
Dec 16 13:29:21.452: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7jll6" in namespace "deployment-9645" to be "running"
Dec 16 13:29:21.455: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.395212ms
Dec 16 13:29:21.456: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551276ms
Dec 16 13:29:23.460: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008388209s
Dec 16 13:29:23.460: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6" satisfied condition "running"
Dec 16 13:29:23.461: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008916164s
Dec 16 13:29:23.461: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v" satisfied condition "running"
Dec 16 13:29:23.461: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 16 13:29:23.467: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 16 13:29:23.476: INFO: Updating deployment webserver-deployment
Dec 16 13:29:23.476: INFO: Waiting for observed generation 2
Dec 16 13:29:25.484: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 16 13:29:26.407: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 16 13:29:26.413: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 16 13:29:26.453: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 16 13:29:26.453: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 16 13:29:26.457: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 16 13:29:26.467: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 16 13:29:26.468: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 16 13:29:26.530: INFO: Updating deployment webserver-deployment
Dec 16 13:29:26.530: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 16 13:29:26.574: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 16 13:29:26.580: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 13:29:28.592: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9645  827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 680901244 3 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8c6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-16 13:29:26 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-16 13:29:26 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 16 13:29:28.596: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9645  ce8598ea-932a-4977-810f-a6fb340435d8 680901240 3 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 0xc004a8cbc7 0xc004a8cbc8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8cc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 13:29:28.596: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 16 13:29:28.596: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9645  7e5138b5-58f6-47a5-b97b-525a41412cdb 680901227 3 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 0xc004a8cad7 0xc004a8cad8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8cb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-2l6jf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2l6jf webserver-deployment-7f5969cbc7- deployment-9645  b8abfdc4-5649-4b05-b423-34ed10001ef5 680901353 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4d3edae3d3b29317729a7a07faef7927cb1786270e69c5626e63ec6dda9ba83c cni.projectcalico.org/podIP:192.168.156.164/32 cni.projectcalico.org/podIPs:192.168.156.164/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d167 0xc004a8d168}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj7ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj7ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-4zf98" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4zf98 webserver-deployment-7f5969cbc7- deployment-9645  990d2b72-d338-4f0b-92af-5eb0bc32033d 680901386 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ec317b083e92357c61a0e47483bc894a1b5d8de303ac39736d7260cf60121aed cni.projectcalico.org/podIP:192.168.156.166/32 cni.projectcalico.org/podIPs:192.168.156.166/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d357 0xc004a8d358}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klv4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klv4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-5h596" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5h596 webserver-deployment-7f5969cbc7- deployment-9645  6f80533c-72f3-401e-b06a-680efb18e716 680900979 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:635f632fc8ad9f159f5c23e1feb8413bf3c77faf8c5a5f5869b8b13faf12e2a2 cni.projectcalico.org/podIP:192.168.189.28/32 cni.projectcalico.org/podIPs:192.168.189.28/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d547 0xc004a8d548}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbkl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbkl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.28,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff8412501540e5254371778ad0ff76cebd2535d97e6260feab64e03bc28913be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-774wb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-774wb webserver-deployment-7f5969cbc7- deployment-9645  6d585145-eb81-43df-82a0-df2dfef352bb 680900925 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:56c8a4ce62d4d1074964a0a7806d5b61ed895c658f6d34ae947927a75daf8a1c cni.projectcalico.org/podIP:192.168.189.226/32 cni.projectcalico.org/podIPs:192.168.189.226/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d757 0xc004a8d758}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5kp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5kp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.226,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3839456a5a39d55c722daec1c899fc5fb1f0b51fc4ddbc25084914f59a087124,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-7w5jl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7w5jl webserver-deployment-7f5969cbc7- deployment-9645  d2e30211-410d-437c-a8f6-eb0515f1f9a0 680900966 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1739229a00f28564aa56215244b4b1f5de5b004bdb97693c35e2f7d61ecfa4a8 cni.projectcalico.org/podIP:192.168.189.31/32 cni.projectcalico.org/podIPs:192.168.189.31/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d977 0xc004a8d978}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbppf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbppf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.31,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a37e277594532ed6577285a89dcde725c8234576090ce8535b8d43931250436c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-8h9tx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8h9tx webserver-deployment-7f5969cbc7- deployment-9645  94e64749-b1a8-4fd5-99f4-b2b19735a542 680901275 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba6f35ec751565be0b9ab07c87bed2a721a01270624b67cd16612f5088faa3d8 cni.projectcalico.org/podIP:192.168.189.38/32 cni.projectcalico.org/podIPs:192.168.189.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8db87 0xc004a8db88}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz4nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz4nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-8q6dw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8q6dw webserver-deployment-7f5969cbc7- deployment-9645  cf367b5a-5ceb-4bf9-a696-2a6f6956c7fd 680901270 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:38d7aca594cdc096c73e533fecde9cc1b2978408e83d4545602788dde864dce7 cni.projectcalico.org/podIP:192.168.189.36/32 cni.projectcalico.org/podIPs:192.168.189.36/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8dd77 0xc004a8dd78}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfc48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfc48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-cz7x2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cz7x2 webserver-deployment-7f5969cbc7- deployment-9645  16b81e52-521f-4070-8096-c708540ce6d9 680901187 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8df67 0xc004a8df68}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xpph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xpph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-grj7l" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-grj7l webserver-deployment-7f5969cbc7- deployment-9645  0edf8e3b-246f-4bb1-a005-16733a0cf697 680900987 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a1a86d40ba219050de18a3b23915c755d95f064fd96fbe7c2bd1f864699cbfaf cni.projectcalico.org/podIP:192.168.189.228/32 cni.projectcalico.org/podIPs:192.168.189.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc137 0xc003efc138}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5bs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5bs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.228,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1e40fe45a0d1a71630515bf25af9d7c0e0ebbd5a5a4de404fd87144e33edac58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-hzkzd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzkzd webserver-deployment-7f5969cbc7- deployment-9645  f56bc75e-d361-4a0d-a644-f928e9fc2c60 680900991 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8048f956c18d34915bd0151c2a4d87a28aef36255b90acc09a0276a6ada20767 cni.projectcalico.org/podIP:192.168.156.159/32 cni.projectcalico.org/podIPs:192.168.156.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc357 0xc003efc358}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld2jt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld2jt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.159,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f32a46771ab7f9ba2afbbf7f78eeb25453de90a039b0fb2a8da257b0bc5597bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-jqmjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqmjq webserver-deployment-7f5969cbc7- deployment-9645  b2ffd082-a2b5-4f0e-a14d-bf311140a04b 680901396 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1ab6ffe87390ee37f20bd5eb0683263aa10ae0a504b18a2a4b27e8fd5b24ed88 cni.projectcalico.org/podIP:192.168.189.41/32 cni.projectcalico.org/podIPs:192.168.189.41/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc567 0xc003efc568}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpr4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpr4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-jz6gf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jz6gf webserver-deployment-7f5969cbc7- deployment-9645  5fac55fe-84d4-4950-a90a-e1c4dbeacbc1 680900934 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:74eaf34dc53eb77a7cad55d5fca200a80451490a9c126ca0ad88d91cfe63d4e5 cni.projectcalico.org/podIP:192.168.156.157/32 cni.projectcalico.org/podIPs:192.168.156.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc757 0xc003efc758}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kznj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kznj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.157,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://edb1742fd3b73cbda7f1eb7ff46772c30ab00944caf9193a5acde90f789f0b78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-m7h7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m7h7g webserver-deployment-7f5969cbc7- deployment-9645  5590763d-a73d-4e3b-890a-367b9a863a17 680901251 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc967 0xc003efc968}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7n6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7n6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-pz8gx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pz8gx webserver-deployment-7f5969cbc7- deployment-9645  f7c46753-4540-4259-9a93-7e5a066b57de 680901381 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e3b94711365ad067f7af2afa49b10247f40e1c6469d81c316c00df204b59b624 cni.projectcalico.org/podIP:192.168.189.234/32 cni.projectcalico.org/podIPs:192.168.189.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efccc7 0xc003efccc8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvzp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvzp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-rhwgz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rhwgz webserver-deployment-7f5969cbc7- deployment-9645  df482e5e-60a7-4f52-b411-a8010a3dd0ac 680900993 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9c52a0cf38a80103093cdf654d265eb015efd7bb661d616ba2921705b37b5e8f cni.projectcalico.org/podIP:192.168.156.158/32 cni.projectcalico.org/podIPs:192.168.156.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efcec7 0xc003efcec8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr5vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr5vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.158,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e39a8f6ebc45ab2a531b312d281a98fc31a7ad950250e04f203ffc0565e04754,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-s5mdn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s5mdn webserver-deployment-7f5969cbc7- deployment-9645  d9ee9594-f38d-495f-9fb3-139407334bd5 680901295 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c78e48fe3b9b4a5fed4ef777aa9c5490f53c372459c260e7cfc5b2a5462e0be9 cni.projectcalico.org/podIP:192.168.156.162/32 cni.projectcalico.org/podIPs:192.168.156.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd0d7 0xc003efd0d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhsl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhsl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-sxhbq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sxhbq webserver-deployment-7f5969cbc7- deployment-9645  38fd4f12-3027-434d-b193-fa8223a06dda 680901357 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:796b06f95da5951b35577f90ef51a8072b935e09273f8745114f8ca9aa577834 cni.projectcalico.org/podIP:192.168.189.233/32 cni.projectcalico.org/podIPs:192.168.189.233/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd2d7 0xc003efd2d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jfw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jfw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-tsvf9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tsvf9 webserver-deployment-7f5969cbc7- deployment-9645  50ebeaa3-7794-4d79-8fb7-a4806d65e0b2 680900985 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1e4d926a3451615ab148f75a0a208f2d17360d70c3b79881f05f5b376db7bfc9 cni.projectcalico.org/podIP:192.168.189.227/32 cni.projectcalico.org/podIPs:192.168.189.227/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd4e7 0xc003efd4e8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t24gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t24gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.227,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://33ef56208ba9cc7fb0feedc5dfc95bbe306ee0ef2af35fb8a0cb8136518b41be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-7f5969cbc7-vsj6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vsj6l webserver-deployment-7f5969cbc7- deployment-9645  88a704d5-6085-4e18-9318-4a2b08b3a874 680901347 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:28888c19210908f2d92a9a34dfbd463d1d84c817a9cdcbf10aefa4ea42e942fc cni.projectcalico.org/podIP:192.168.189.232/32 cni.projectcalico.org/podIPs:192.168.189.232/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd707 0xc003efd708}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h7dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h7dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-7f5969cbc7-xwrdw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xwrdw webserver-deployment-7f5969cbc7- deployment-9645  e338e5ea-4991-4c93-b955-f4f1d59e82e1 680901382 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8c600e6cc58b1c5497c7ab4c845a3f882d8ace5cf44b690f961a6c5a52598545 cni.projectcalico.org/podIP:192.168.189.42/32 cni.projectcalico.org/podIPs:192.168.189.42/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd917 0xc003efd918}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbjfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbjfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-d9f79cb5-7jvq7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jvq7 webserver-deployment-d9f79cb5- deployment-9645  7fc8cf03-e785-4994-8bc5-d359844b4033 680901114 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3793f5a43ae4867e56ae1bd75576f946e2417b0f7e5349ceb52902b43bd1a88a cni.projectcalico.org/podIP:192.168.156.161/32 cni.projectcalico.org/podIPs:192.168.156.161/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdb17 0xc003efdb18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lqkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lqkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-7zlqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7zlqx webserver-deployment-d9f79cb5- deployment-9645  a831750c-342b-4b2a-bcf2-5902b6bb60df 680901252 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdd27 0xc003efdd28}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55h9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55h9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-8wvmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8wvmq webserver-deployment-d9f79cb5- deployment-9645  6d00f1ed-5a68-40ca-a07a-83aab3df0f74 680901297 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a58e292c1ac979ed932086eb9e09486e415cabe2fab0ac322f1825e0cd1cac58 cni.projectcalico.org/podIP:192.168.189.37/32 cni.projectcalico.org/podIPs:192.168.189.37/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdf17 0xc003efdf18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwjqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwjqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-967vk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-967vk webserver-deployment-d9f79cb5- deployment-9645  44f5d9da-307c-459d-8bbe-9ea3c5b71932 680901330 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:679d65f76cc2bd9e5a36d05c26c4aa2a24cf856491589a0c4f777685ee00d96d cni.projectcalico.org/podIP:192.168.189.39/32 cni.projectcalico.org/podIPs:192.168.189.39/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc0008486c7 0xc0008486c8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdmsn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdmsn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-9vfg9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9vfg9 webserver-deployment-d9f79cb5- deployment-9645  e5a11cde-b93b-4e66-bf79-db9bde290e5a 680901359 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:606ef4b26e8459e1d866615e760c59f448036132c8b702a216f269230ed1b3f4 cni.projectcalico.org/podIP:192.168.156.165/32 cni.projectcalico.org/podIPs:192.168.156.165/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc0008494d7 0xc0008494d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47pnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47pnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-bhc79" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bhc79 webserver-deployment-d9f79cb5- deployment-9645  4ce68d1c-db14-4d37-aad9-cdcdcdbe3b85 680901294 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:54a80ddddaa03ee6c5307169de0e5401b07d6ec3d6a7380cd3765b525fdeae2f cni.projectcalico.org/podIP:192.168.189.230/32 cni.projectcalico.org/podIPs:192.168.189.230/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc000849d17 0xc000849d18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4zqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4zqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-bt2qj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bt2qj webserver-deployment-d9f79cb5- deployment-9645  1b6e7b22-6a5a-4cb4-a6f7-4d837687c606 680901117 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a73cc58fbdb6d449062cb021347267964bc4da6fddb3bcd33ac52124d26db5d8 cni.projectcalico.org/podIP:192.168.189.35/32 cni.projectcalico.org/podIPs:192.168.189.35/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b921b7 0xc003b921b8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlbk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlbk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-cdxwm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cdxwm webserver-deployment-d9f79cb5- deployment-9645  7da0032d-455a-4387-b705-b04ec3f7208b 680901108 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:77a9cd27b547dfebe40d834dd548ed914a31275856df092f3f9d66d3a2466cf1 cni.projectcalico.org/podIP:192.168.156.160/32 cni.projectcalico.org/podIPs:192.168.156.160/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b923d7 0xc003b923d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flpzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flpzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-d75lt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d75lt webserver-deployment-d9f79cb5- deployment-9645  3301927d-d588-47dc-9f58-861f9bb8c853 680901397 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d8fe4181c365e51cb1a576d68a3830e2c25bae65479e7c88b1f5d2d094d35d43 cni.projectcalico.org/podIP:192.168.189.229/32 cni.projectcalico.org/podIPs:192.168.189.229/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92617 0xc003b92618}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwnh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwnh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.229,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:53eb2e46542e1fdece0f25f5a564c8c4fa44640fd815541264efac0315faa8a3: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-glpn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-glpn9 webserver-deployment-d9f79cb5- deployment-9645  f9175eea-4ae9-4f67-a60a-f40aaecc4064 680901350 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1846cee28ce19cdc55b48295e580a4e31fd0ea986e58d0ea8d31ac6c2401bd1d cni.projectcalico.org/podIP:192.168.189.40/32 cni.projectcalico.org/podIPs:192.168.189.40/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92857 0xc003b92858}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lm8tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lm8tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-hjssk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hjssk webserver-deployment-d9f79cb5- deployment-9645  e1f56a95-afae-4e39-ae30-3567b1692161 680901111 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:933b28a93695d0738380867052549903a590985da92b51d5fbc1a9fc3d38d9eb cni.projectcalico.org/podIP:192.168.189.34/32 cni.projectcalico.org/podIPs:192.168.189.34/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b929f7 0xc003b929f8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zptfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zptfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-hpxzz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hpxzz webserver-deployment-d9f79cb5- deployment-9645  6e78979e-265a-448d-a7bd-520c3cea04de 680901325 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0381808d5c77df06c654be02407430b286f6675503f7cae18570c0a377f03f40 cni.projectcalico.org/podIP:192.168.189.231/32 cni.projectcalico.org/podIPs:192.168.189.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92c27 0xc003b92c28}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdkhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdkhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-kjrrx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kjrrx webserver-deployment-d9f79cb5- deployment-9645  f4ed391f-141d-47a9-b736-92643efa3975 680901329 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9186081f80fedad8b8f5edff40e8c7eeacfb4fd2b11027de7b2f8cbfbd6feb51 cni.projectcalico.org/podIP:192.168.156.163/32 cni.projectcalico.org/podIPs:192.168.156.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92e37 0xc003b92e38}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nggth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nggth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9645" for this suite. 12/16/22 13:29:28.615
------------------------------
• [SLOW TEST] [9.209 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:19.412
    Dec 16 13:29:19.412: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 13:29:19.412
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:19.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:19.431
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Dec 16 13:29:19.434: INFO: Creating deployment "webserver-deployment"
    Dec 16 13:29:19.439: INFO: Waiting for observed generation 1
    Dec 16 13:29:21.447: INFO: Waiting for all required pods to come up
    Dec 16 13:29:21.452: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 12/16/22 13:29:21.452
    Dec 16 13:29:21.452: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2kh4v" in namespace "deployment-9645" to be "running"
    Dec 16 13:29:21.452: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7jll6" in namespace "deployment-9645" to be "running"
    Dec 16 13:29:21.455: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.395212ms
    Dec 16 13:29:21.456: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551276ms
    Dec 16 13:29:23.460: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008388209s
    Dec 16 13:29:23.460: INFO: Pod "webserver-deployment-7f5969cbc7-7jll6" satisfied condition "running"
    Dec 16 13:29:23.461: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008916164s
    Dec 16 13:29:23.461: INFO: Pod "webserver-deployment-7f5969cbc7-2kh4v" satisfied condition "running"
    Dec 16 13:29:23.461: INFO: Waiting for deployment "webserver-deployment" to complete
    Dec 16 13:29:23.467: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Dec 16 13:29:23.476: INFO: Updating deployment webserver-deployment
    Dec 16 13:29:23.476: INFO: Waiting for observed generation 2
    Dec 16 13:29:25.484: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Dec 16 13:29:26.407: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Dec 16 13:29:26.413: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 16 13:29:26.453: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Dec 16 13:29:26.453: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Dec 16 13:29:26.457: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 16 13:29:26.467: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Dec 16 13:29:26.468: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Dec 16 13:29:26.530: INFO: Updating deployment webserver-deployment
    Dec 16 13:29:26.530: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Dec 16 13:29:26.574: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Dec 16 13:29:26.580: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 13:29:28.592: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9645  827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 680901244 3 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8c6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-16 13:29:26 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-16 13:29:26 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Dec 16 13:29:28.596: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9645  ce8598ea-932a-4977-810f-a6fb340435d8 680901240 3 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 0xc004a8cbc7 0xc004a8cbc8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8cc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 13:29:28.596: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Dec 16 13:29:28.596: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9645  7e5138b5-58f6-47a5-b97b-525a41412cdb 680901227 3 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e 0xc004a8cad7 0xc004a8cad8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827dfe1a-7a03-48bd-bb88-abe4ea7c8a9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a8cb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-2l6jf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2l6jf webserver-deployment-7f5969cbc7- deployment-9645  b8abfdc4-5649-4b05-b423-34ed10001ef5 680901353 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4d3edae3d3b29317729a7a07faef7927cb1786270e69c5626e63ec6dda9ba83c cni.projectcalico.org/podIP:192.168.156.164/32 cni.projectcalico.org/podIPs:192.168.156.164/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d167 0xc004a8d168}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj7ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj7ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-4zf98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4zf98 webserver-deployment-7f5969cbc7- deployment-9645  990d2b72-d338-4f0b-92af-5eb0bc32033d 680901386 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ec317b083e92357c61a0e47483bc894a1b5d8de303ac39736d7260cf60121aed cni.projectcalico.org/podIP:192.168.156.166/32 cni.projectcalico.org/podIPs:192.168.156.166/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d357 0xc004a8d358}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klv4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klv4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-5h596" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5h596 webserver-deployment-7f5969cbc7- deployment-9645  6f80533c-72f3-401e-b06a-680efb18e716 680900979 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:635f632fc8ad9f159f5c23e1feb8413bf3c77faf8c5a5f5869b8b13faf12e2a2 cni.projectcalico.org/podIP:192.168.189.28/32 cni.projectcalico.org/podIPs:192.168.189.28/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d547 0xc004a8d548}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbkl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbkl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.28,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff8412501540e5254371778ad0ff76cebd2535d97e6260feab64e03bc28913be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-774wb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-774wb webserver-deployment-7f5969cbc7- deployment-9645  6d585145-eb81-43df-82a0-df2dfef352bb 680900925 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:56c8a4ce62d4d1074964a0a7806d5b61ed895c658f6d34ae947927a75daf8a1c cni.projectcalico.org/podIP:192.168.189.226/32 cni.projectcalico.org/podIPs:192.168.189.226/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d757 0xc004a8d758}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5kp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5kp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.226,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3839456a5a39d55c722daec1c899fc5fb1f0b51fc4ddbc25084914f59a087124,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.603: INFO: Pod "webserver-deployment-7f5969cbc7-7w5jl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7w5jl webserver-deployment-7f5969cbc7- deployment-9645  d2e30211-410d-437c-a8f6-eb0515f1f9a0 680900966 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1739229a00f28564aa56215244b4b1f5de5b004bdb97693c35e2f7d61ecfa4a8 cni.projectcalico.org/podIP:192.168.189.31/32 cni.projectcalico.org/podIPs:192.168.189.31/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8d977 0xc004a8d978}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbppf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbppf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.31,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a37e277594532ed6577285a89dcde725c8234576090ce8535b8d43931250436c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-8h9tx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8h9tx webserver-deployment-7f5969cbc7- deployment-9645  94e64749-b1a8-4fd5-99f4-b2b19735a542 680901275 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba6f35ec751565be0b9ab07c87bed2a721a01270624b67cd16612f5088faa3d8 cni.projectcalico.org/podIP:192.168.189.38/32 cni.projectcalico.org/podIPs:192.168.189.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8db87 0xc004a8db88}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz4nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz4nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-8q6dw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8q6dw webserver-deployment-7f5969cbc7- deployment-9645  cf367b5a-5ceb-4bf9-a696-2a6f6956c7fd 680901270 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:38d7aca594cdc096c73e533fecde9cc1b2978408e83d4545602788dde864dce7 cni.projectcalico.org/podIP:192.168.189.36/32 cni.projectcalico.org/podIPs:192.168.189.36/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8dd77 0xc004a8dd78}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfc48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfc48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-cz7x2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cz7x2 webserver-deployment-7f5969cbc7- deployment-9645  16b81e52-521f-4070-8096-c708540ce6d9 680901187 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc004a8df67 0xc004a8df68}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xpph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xpph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-grj7l" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-grj7l webserver-deployment-7f5969cbc7- deployment-9645  0edf8e3b-246f-4bb1-a005-16733a0cf697 680900987 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a1a86d40ba219050de18a3b23915c755d95f064fd96fbe7c2bd1f864699cbfaf cni.projectcalico.org/podIP:192.168.189.228/32 cni.projectcalico.org/podIPs:192.168.189.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc137 0xc003efc138}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5bs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5bs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.228,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1e40fe45a0d1a71630515bf25af9d7c0e0ebbd5a5a4de404fd87144e33edac58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-hzkzd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzkzd webserver-deployment-7f5969cbc7- deployment-9645  f56bc75e-d361-4a0d-a644-f928e9fc2c60 680900991 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8048f956c18d34915bd0151c2a4d87a28aef36255b90acc09a0276a6ada20767 cni.projectcalico.org/podIP:192.168.156.159/32 cni.projectcalico.org/podIPs:192.168.156.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc357 0xc003efc358}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld2jt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld2jt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.159,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f32a46771ab7f9ba2afbbf7f78eeb25453de90a039b0fb2a8da257b0bc5597bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.604: INFO: Pod "webserver-deployment-7f5969cbc7-jqmjq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqmjq webserver-deployment-7f5969cbc7- deployment-9645  b2ffd082-a2b5-4f0e-a14d-bf311140a04b 680901396 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1ab6ffe87390ee37f20bd5eb0683263aa10ae0a504b18a2a4b27e8fd5b24ed88 cni.projectcalico.org/podIP:192.168.189.41/32 cni.projectcalico.org/podIPs:192.168.189.41/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc567 0xc003efc568}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpr4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpr4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-jz6gf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jz6gf webserver-deployment-7f5969cbc7- deployment-9645  5fac55fe-84d4-4950-a90a-e1c4dbeacbc1 680900934 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:74eaf34dc53eb77a7cad55d5fca200a80451490a9c126ca0ad88d91cfe63d4e5 cni.projectcalico.org/podIP:192.168.156.157/32 cni.projectcalico.org/podIPs:192.168.156.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc757 0xc003efc758}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kznj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kznj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.157,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://edb1742fd3b73cbda7f1eb7ff46772c30ab00944caf9193a5acde90f789f0b78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-m7h7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m7h7g webserver-deployment-7f5969cbc7- deployment-9645  5590763d-a73d-4e3b-890a-367b9a863a17 680901251 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efc967 0xc003efc968}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7n6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7n6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-pz8gx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pz8gx webserver-deployment-7f5969cbc7- deployment-9645  f7c46753-4540-4259-9a93-7e5a066b57de 680901381 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e3b94711365ad067f7af2afa49b10247f40e1c6469d81c316c00df204b59b624 cni.projectcalico.org/podIP:192.168.189.234/32 cni.projectcalico.org/podIPs:192.168.189.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efccc7 0xc003efccc8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvzp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvzp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.605: INFO: Pod "webserver-deployment-7f5969cbc7-rhwgz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rhwgz webserver-deployment-7f5969cbc7- deployment-9645  df482e5e-60a7-4f52-b411-a8010a3dd0ac 680900993 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9c52a0cf38a80103093cdf654d265eb015efd7bb661d616ba2921705b37b5e8f cni.projectcalico.org/podIP:192.168.156.158/32 cni.projectcalico.org/podIPs:192.168.156.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efcec7 0xc003efcec8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr5vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr5vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.158,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e39a8f6ebc45ab2a531b312d281a98fc31a7ad950250e04f203ffc0565e04754,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-s5mdn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s5mdn webserver-deployment-7f5969cbc7- deployment-9645  d9ee9594-f38d-495f-9fb3-139407334bd5 680901295 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c78e48fe3b9b4a5fed4ef777aa9c5490f53c372459c260e7cfc5b2a5462e0be9 cni.projectcalico.org/podIP:192.168.156.162/32 cni.projectcalico.org/podIPs:192.168.156.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd0d7 0xc003efd0d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhsl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhsl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-sxhbq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sxhbq webserver-deployment-7f5969cbc7- deployment-9645  38fd4f12-3027-434d-b193-fa8223a06dda 680901357 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:796b06f95da5951b35577f90ef51a8072b935e09273f8745114f8ca9aa577834 cni.projectcalico.org/podIP:192.168.189.233/32 cni.projectcalico.org/podIPs:192.168.189.233/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd2d7 0xc003efd2d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jfw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jfw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.606: INFO: Pod "webserver-deployment-7f5969cbc7-tsvf9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tsvf9 webserver-deployment-7f5969cbc7- deployment-9645  50ebeaa3-7794-4d79-8fb7-a4806d65e0b2 680900985 0 2022-12-16 13:29:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1e4d926a3451615ab148f75a0a208f2d17360d70c3b79881f05f5b376db7bfc9 cni.projectcalico.org/podIP:192.168.189.227/32 cni.projectcalico.org/podIPs:192.168.189.227/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd4e7 0xc003efd4e8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t24gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t24gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.227,StartTime:2022-12-16 13:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://33ef56208ba9cc7fb0feedc5dfc95bbe306ee0ef2af35fb8a0cb8136518b41be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-7f5969cbc7-vsj6l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vsj6l webserver-deployment-7f5969cbc7- deployment-9645  88a704d5-6085-4e18-9318-4a2b08b3a874 680901347 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:28888c19210908f2d92a9a34dfbd463d1d84c817a9cdcbf10aefa4ea42e942fc cni.projectcalico.org/podIP:192.168.189.232/32 cni.projectcalico.org/podIPs:192.168.189.232/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd707 0xc003efd708}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h7dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h7dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-7f5969cbc7-xwrdw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xwrdw webserver-deployment-7f5969cbc7- deployment-9645  e338e5ea-4991-4c93-b955-f4f1d59e82e1 680901382 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8c600e6cc58b1c5497c7ab4c845a3f882d8ace5cf44b690f961a6c5a52598545 cni.projectcalico.org/podIP:192.168.189.42/32 cni.projectcalico.org/podIPs:192.168.189.42/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 7e5138b5-58f6-47a5-b97b-525a41412cdb 0xc003efd917 0xc003efd918}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5138b5-58f6-47a5-b97b-525a41412cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbjfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbjfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.607: INFO: Pod "webserver-deployment-d9f79cb5-7jvq7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jvq7 webserver-deployment-d9f79cb5- deployment-9645  7fc8cf03-e785-4994-8bc5-d359844b4033 680901114 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3793f5a43ae4867e56ae1bd75576f946e2417b0f7e5349ceb52902b43bd1a88a cni.projectcalico.org/podIP:192.168.156.161/32 cni.projectcalico.org/podIPs:192.168.156.161/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdb17 0xc003efdb18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lqkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lqkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-7zlqx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7zlqx webserver-deployment-d9f79cb5- deployment-9645  a831750c-342b-4b2a-bcf2-5902b6bb60df 680901252 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdd27 0xc003efdd28}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55h9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55h9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-8wvmq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8wvmq webserver-deployment-d9f79cb5- deployment-9645  6d00f1ed-5a68-40ca-a07a-83aab3df0f74 680901297 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a58e292c1ac979ed932086eb9e09486e415cabe2fab0ac322f1825e0cd1cac58 cni.projectcalico.org/podIP:192.168.189.37/32 cni.projectcalico.org/podIPs:192.168.189.37/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003efdf17 0xc003efdf18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwjqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwjqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-967vk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-967vk webserver-deployment-d9f79cb5- deployment-9645  44f5d9da-307c-459d-8bbe-9ea3c5b71932 680901330 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:679d65f76cc2bd9e5a36d05c26c4aa2a24cf856491589a0c4f777685ee00d96d cni.projectcalico.org/podIP:192.168.189.39/32 cni.projectcalico.org/podIPs:192.168.189.39/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc0008486c7 0xc0008486c8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdmsn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdmsn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.608: INFO: Pod "webserver-deployment-d9f79cb5-9vfg9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9vfg9 webserver-deployment-d9f79cb5- deployment-9645  e5a11cde-b93b-4e66-bf79-db9bde290e5a 680901359 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:606ef4b26e8459e1d866615e760c59f448036132c8b702a216f269230ed1b3f4 cni.projectcalico.org/podIP:192.168.156.165/32 cni.projectcalico.org/podIPs:192.168.156.165/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc0008494d7 0xc0008494d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47pnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47pnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-bhc79" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bhc79 webserver-deployment-d9f79cb5- deployment-9645  4ce68d1c-db14-4d37-aad9-cdcdcdbe3b85 680901294 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:54a80ddddaa03ee6c5307169de0e5401b07d6ec3d6a7380cd3765b525fdeae2f cni.projectcalico.org/podIP:192.168.189.230/32 cni.projectcalico.org/podIPs:192.168.189.230/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc000849d17 0xc000849d18}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4zqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4zqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-bt2qj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bt2qj webserver-deployment-d9f79cb5- deployment-9645  1b6e7b22-6a5a-4cb4-a6f7-4d837687c606 680901117 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a73cc58fbdb6d449062cb021347267964bc4da6fddb3bcd33ac52124d26db5d8 cni.projectcalico.org/podIP:192.168.189.35/32 cni.projectcalico.org/podIPs:192.168.189.35/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b921b7 0xc003b921b8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlbk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlbk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-cdxwm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cdxwm webserver-deployment-d9f79cb5- deployment-9645  7da0032d-455a-4387-b705-b04ec3f7208b 680901108 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:77a9cd27b547dfebe40d834dd548ed914a31275856df092f3f9d66d3a2466cf1 cni.projectcalico.org/podIP:192.168.156.160/32 cni.projectcalico.org/podIPs:192.168.156.160/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b923d7 0xc003b923d8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flpzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flpzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.609: INFO: Pod "webserver-deployment-d9f79cb5-d75lt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d75lt webserver-deployment-d9f79cb5- deployment-9645  3301927d-d588-47dc-9f58-861f9bb8c853 680901397 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d8fe4181c365e51cb1a576d68a3830e2c25bae65479e7c88b1f5d2d094d35d43 cni.projectcalico.org/podIP:192.168.189.229/32 cni.projectcalico.org/podIPs:192.168.189.229/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92617 0xc003b92618}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwnh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwnh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.229,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:53eb2e46542e1fdece0f25f5a564c8c4fa44640fd815541264efac0315faa8a3: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-glpn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-glpn9 webserver-deployment-d9f79cb5- deployment-9645  f9175eea-4ae9-4f67-a60a-f40aaecc4064 680901350 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1846cee28ce19cdc55b48295e580a4e31fd0ea986e58d0ea8d31ac6c2401bd1d cni.projectcalico.org/podIP:192.168.189.40/32 cni.projectcalico.org/podIPs:192.168.189.40/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92857 0xc003b92858}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lm8tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lm8tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-hjssk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hjssk webserver-deployment-d9f79cb5- deployment-9645  e1f56a95-afae-4e39-ae30-3567b1692161 680901111 0 2022-12-16 13:29:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:933b28a93695d0738380867052549903a590985da92b51d5fbc1a9fc3d38d9eb cni.projectcalico.org/podIP:192.168.189.34/32 cni.projectcalico.org/podIPs:192.168.189.34/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b929f7 0xc003b929f8}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zptfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zptfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 13:29:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-hpxzz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hpxzz webserver-deployment-d9f79cb5- deployment-9645  6e78979e-265a-448d-a7bd-520c3cea04de 680901325 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0381808d5c77df06c654be02407430b286f6675503f7cae18570c0a377f03f40 cni.projectcalico.org/podIP:192.168.189.231/32 cni.projectcalico.org/podIPs:192.168.189.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92c27 0xc003b92c28}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdkhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdkhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 13:29:28.610: INFO: Pod "webserver-deployment-d9f79cb5-kjrrx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kjrrx webserver-deployment-d9f79cb5- deployment-9645  f4ed391f-141d-47a9-b736-92643efa3975 680901329 0 2022-12-16 13:29:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9186081f80fedad8b8f5edff40e8c7eeacfb4fd2b11027de7b2f8cbfbd6feb51 cni.projectcalico.org/podIP:192.168.156.163/32 cni.projectcalico.org/podIPs:192.168.156.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ce8598ea-932a-4977-810f-a6fb340435d8 0xc003b92e37 0xc003b92e38}] [] [{kube-controller-manager Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce8598ea-932a-4977-810f-a6fb340435d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 13:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-16 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nggth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nggth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:29:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:,StartTime:2022-12-16 13:29:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9645" for this suite. 12/16/22 13:29:28.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:28.625
Dec 16 13:29:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 13:29:28.625
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:28.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:28.644
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 12/16/22 13:29:28.646
STEP: Wait for the Deployment to create new ReplicaSet 12/16/22 13:29:28.651
STEP: delete the deployment 12/16/22 13:29:29.17
STEP: wait for all rs to be garbage collected 12/16/22 13:29:29.177
STEP: expected 0 rs, got 1 rs 12/16/22 13:29:29.183
STEP: expected 0 pods, got 2 pods 12/16/22 13:29:29.187
STEP: Gathering metrics 12/16/22 13:29:29.698
W1216 13:29:29.705706      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 13:29:29.705: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:29.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1479" for this suite. 12/16/22 13:29:29.709
------------------------------
• [1.091 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:28.625
    Dec 16 13:29:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 13:29:28.625
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:28.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:28.644
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 12/16/22 13:29:28.646
    STEP: Wait for the Deployment to create new ReplicaSet 12/16/22 13:29:28.651
    STEP: delete the deployment 12/16/22 13:29:29.17
    STEP: wait for all rs to be garbage collected 12/16/22 13:29:29.177
    STEP: expected 0 rs, got 1 rs 12/16/22 13:29:29.183
    STEP: expected 0 pods, got 2 pods 12/16/22 13:29:29.187
    STEP: Gathering metrics 12/16/22 13:29:29.698
    W1216 13:29:29.705706      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 13:29:29.705: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:29.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1479" for this suite. 12/16/22 13:29:29.709
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:29.716
Dec 16 13:29:29.716: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 13:29:29.717
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:29.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:29.735
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 12/16/22 13:29:29.737
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +notcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_tcp@PTR;sleep 1; done
 12/16/22 13:29:29.759
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +notcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_tcp@PTR;sleep 1; done
 12/16/22 13:29:29.759
STEP: creating a pod to probe DNS 12/16/22 13:29:29.759
STEP: submitting the pod to kubernetes 12/16/22 13:29:29.759
Dec 16 13:29:29.767: INFO: Waiting up to 15m0s for pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54" in namespace "dns-5107" to be "running"
Dec 16 13:29:29.769: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67919ms
Dec 16 13:29:31.776: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321453s
Dec 16 13:29:33.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0082013s
Dec 16 13:29:35.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007287683s
Dec 16 13:29:37.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006928033s
Dec 16 13:29:39.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007305972s
Dec 16 13:29:41.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Running", Reason="", readiness=true. Elapsed: 12.00818768s
Dec 16 13:29:41.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:29:41.775
STEP: looking for the results for each expected name from probers 12/16/22 13:29:41.78
Dec 16 13:29:41.844: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc from pod dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54: the server could not find the requested resource (get pods dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54)
Dec 16 13:29:41.931: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5107.svc from pod dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54: the server could not find the requested resource (get pods dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54)
Dec 16 13:29:41.961: INFO: Lookups using dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc jessie_udp@_http._tcp.dns-test-service.dns-5107.svc]

Dec 16 13:29:47.085: INFO: DNS probes using dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54 succeeded

STEP: deleting the pod 12/16/22 13:29:47.085
STEP: deleting the test service 12/16/22 13:29:47.1
STEP: deleting the test headless service 12/16/22 13:29:47.121
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:47.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5107" for this suite. 12/16/22 13:29:47.134
------------------------------
• [SLOW TEST] [17.424 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:29.716
    Dec 16 13:29:29.716: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 13:29:29.717
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:29.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:29.735
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 12/16/22 13:29:29.737
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +notcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_tcp@PTR;sleep 1; done
     12/16/22 13:29:29.759
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5107.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5107.svc;check="$$(dig +notcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.167.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.167.158_tcp@PTR;sleep 1; done
     12/16/22 13:29:29.759
    STEP: creating a pod to probe DNS 12/16/22 13:29:29.759
    STEP: submitting the pod to kubernetes 12/16/22 13:29:29.759
    Dec 16 13:29:29.767: INFO: Waiting up to 15m0s for pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54" in namespace "dns-5107" to be "running"
    Dec 16 13:29:29.769: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67919ms
    Dec 16 13:29:31.776: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321453s
    Dec 16 13:29:33.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0082013s
    Dec 16 13:29:35.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007287683s
    Dec 16 13:29:37.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006928033s
    Dec 16 13:29:39.774: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007305972s
    Dec 16 13:29:41.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54": Phase="Running", Reason="", readiness=true. Elapsed: 12.00818768s
    Dec 16 13:29:41.775: INFO: Pod "dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:29:41.775
    STEP: looking for the results for each expected name from probers 12/16/22 13:29:41.78
    Dec 16 13:29:41.844: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc from pod dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54: the server could not find the requested resource (get pods dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54)
    Dec 16 13:29:41.931: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5107.svc from pod dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54: the server could not find the requested resource (get pods dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54)
    Dec 16 13:29:41.961: INFO: Lookups using dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5107.svc jessie_udp@_http._tcp.dns-test-service.dns-5107.svc]

    Dec 16 13:29:47.085: INFO: DNS probes using dns-5107/dns-test-7bf3433e-0d0a-4eae-bcd8-7382d504ad54 succeeded

    STEP: deleting the pod 12/16/22 13:29:47.085
    STEP: deleting the test service 12/16/22 13:29:47.1
    STEP: deleting the test headless service 12/16/22 13:29:47.121
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:47.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5107" for this suite. 12/16/22 13:29:47.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:47.143
Dec 16 13:29:47.143: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename lease-test 12/16/22 13:29:47.144
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:47.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:47.173
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:47.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-1102" for this suite. 12/16/22 13:29:47.231
------------------------------
• [0.094 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:47.143
    Dec 16 13:29:47.143: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename lease-test 12/16/22 13:29:47.144
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:47.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:47.173
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:47.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-1102" for this suite. 12/16/22 13:29:47.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:47.239
Dec 16 13:29:47.239: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 13:29:47.24
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:47.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:47.258
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Dec 16 13:29:47.305: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"204e35f2-1f90-4b38-8e14-dce2f46d05f9", Controller:(*bool)(0xc003a9e4b2), BlockOwnerDeletion:(*bool)(0xc003a9e4b3)}}
Dec 16 13:29:47.315: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"88a90005-01fe-4953-8f39-9b6d232d68b6", Controller:(*bool)(0xc0032041d2), BlockOwnerDeletion:(*bool)(0xc0032041d3)}}
Dec 16 13:29:47.319: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"762251bb-b07c-4390-93dc-b8ca95212dd3", Controller:(*bool)(0xc003b5eb02), BlockOwnerDeletion:(*bool)(0xc003b5eb03)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:52.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3009" for this suite. 12/16/22 13:29:52.346
------------------------------
• [SLOW TEST] [5.113 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:47.239
    Dec 16 13:29:47.239: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 13:29:47.24
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:47.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:47.258
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Dec 16 13:29:47.305: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"204e35f2-1f90-4b38-8e14-dce2f46d05f9", Controller:(*bool)(0xc003a9e4b2), BlockOwnerDeletion:(*bool)(0xc003a9e4b3)}}
    Dec 16 13:29:47.315: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"88a90005-01fe-4953-8f39-9b6d232d68b6", Controller:(*bool)(0xc0032041d2), BlockOwnerDeletion:(*bool)(0xc0032041d3)}}
    Dec 16 13:29:47.319: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"762251bb-b07c-4390-93dc-b8ca95212dd3", Controller:(*bool)(0xc003b5eb02), BlockOwnerDeletion:(*bool)(0xc003b5eb03)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:52.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3009" for this suite. 12/16/22 13:29:52.346
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:52.352
Dec 16 13:29:52.352: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:29:52.353
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:52.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:52.37
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:29:52.373
Dec 16 13:29:52.380: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf" in namespace "downward-api-9767" to be "Succeeded or Failed"
Dec 16 13:29:52.383: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234558ms
Dec 16 13:29:54.388: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008612158s
Dec 16 13:29:56.387: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007409689s
STEP: Saw pod success 12/16/22 13:29:56.387
Dec 16 13:29:56.387: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf" satisfied condition "Succeeded or Failed"
Dec 16 13:29:56.391: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf container client-container: <nil>
STEP: delete the pod 12/16/22 13:29:56.399
Dec 16 13:29:56.410: INFO: Waiting for pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf to disappear
Dec 16 13:29:56.417: INFO: Pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9767" for this suite. 12/16/22 13:29:56.421
------------------------------
• [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:52.352
    Dec 16 13:29:52.352: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:29:52.353
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:52.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:52.37
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:29:52.373
    Dec 16 13:29:52.380: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf" in namespace "downward-api-9767" to be "Succeeded or Failed"
    Dec 16 13:29:52.383: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234558ms
    Dec 16 13:29:54.388: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008612158s
    Dec 16 13:29:56.387: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007409689s
    STEP: Saw pod success 12/16/22 13:29:56.387
    Dec 16 13:29:56.387: INFO: Pod "downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf" satisfied condition "Succeeded or Failed"
    Dec 16 13:29:56.391: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf container client-container: <nil>
    STEP: delete the pod 12/16/22 13:29:56.399
    Dec 16 13:29:56.410: INFO: Waiting for pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf to disappear
    Dec 16 13:29:56.417: INFO: Pod downwardapi-volume-7330cf49-9c25-4cfa-a116-90555f6855cf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9767" for this suite. 12/16/22 13:29:56.421
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:56.431
Dec 16 13:29:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:29:56.432
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:56.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:56.448
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Dec 16 13:29:56.450: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:29:57.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5608" for this suite. 12/16/22 13:29:57.477
------------------------------
• [1.052 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:56.431
    Dec 16 13:29:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 13:29:56.432
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:56.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:56.448
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Dec 16 13:29:56.450: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:29:57.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5608" for this suite. 12/16/22 13:29:57.477
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:29:57.483
Dec 16 13:29:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:29:57.484
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:57.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:57.503
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:29:57.519
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:29:57.882
STEP: Deploying the webhook pod 12/16/22 13:29:57.891
STEP: Wait for the deployment to be ready 12/16/22 13:29:57.903
Dec 16 13:29:57.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:29:59.935
STEP: Verifying the service has paired with the endpoint 12/16/22 13:29:59.949
Dec 16 13:30:00.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/16/22 13:30:00.954
STEP: create a configmap that should be updated by the webhook 12/16/22 13:30:00.998
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:01.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3886" for this suite. 12/16/22 13:30:01.093
STEP: Destroying namespace "webhook-3886-markers" for this suite. 12/16/22 13:30:01.099
------------------------------
• [3.622 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:29:57.483
    Dec 16 13:29:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:29:57.484
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:29:57.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:29:57.503
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:29:57.519
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:29:57.882
    STEP: Deploying the webhook pod 12/16/22 13:29:57.891
    STEP: Wait for the deployment to be ready 12/16/22 13:29:57.903
    Dec 16 13:29:57.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:29:59.935
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:29:59.949
    Dec 16 13:30:00.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/16/22 13:30:00.954
    STEP: create a configmap that should be updated by the webhook 12/16/22 13:30:00.998
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:01.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3886" for this suite. 12/16/22 13:30:01.093
    STEP: Destroying namespace "webhook-3886-markers" for this suite. 12/16/22 13:30:01.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:01.106
Dec 16 13:30:01.106: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:30:01.107
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:01.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:01.137
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Dec 16 13:30:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: creating the pod 12/16/22 13:30:01.14
STEP: submitting the pod to kubernetes 12/16/22 13:30:01.14
Dec 16 13:30:01.151: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8" in namespace "pods-1794" to be "running and ready"
Dec 16 13:30:01.155: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092785ms
Dec 16 13:30:01.155: INFO: The phase of Pod pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:30:03.160: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008798423s
Dec 16 13:30:03.160: INFO: The phase of Pod pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8 is Running (Ready = true)
Dec 16 13:30:03.160: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:03.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1794" for this suite. 12/16/22 13:30:03.195
------------------------------
• [2.098 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:01.106
    Dec 16 13:30:01.106: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:30:01.107
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:01.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:01.137
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Dec 16 13:30:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: creating the pod 12/16/22 13:30:01.14
    STEP: submitting the pod to kubernetes 12/16/22 13:30:01.14
    Dec 16 13:30:01.151: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8" in namespace "pods-1794" to be "running and ready"
    Dec 16 13:30:01.155: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092785ms
    Dec 16 13:30:01.155: INFO: The phase of Pod pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:30:03.160: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008798423s
    Dec 16 13:30:03.160: INFO: The phase of Pod pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8 is Running (Ready = true)
    Dec 16 13:30:03.160: INFO: Pod "pod-logs-websocket-c7b5e6dc-748e-439d-aead-cc4931e693e8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:03.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1794" for this suite. 12/16/22 13:30:03.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:03.204
Dec 16 13:30:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:30:03.205
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:03.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:03.225
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-01a1cdc8-5f6d-4d3b-a2c9-d5062f4bfe1e 12/16/22 13:30:03.228
STEP: Creating a pod to test consume configMaps 12/16/22 13:30:03.231
Dec 16 13:30:03.239: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9" in namespace "projected-8907" to be "Succeeded or Failed"
Dec 16 13:30:03.242: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923774ms
Dec 16 13:30:05.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484588s
Dec 16 13:30:07.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007658007s
STEP: Saw pod success 12/16/22 13:30:07.247
Dec 16 13:30:07.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9" satisfied condition "Succeeded or Failed"
Dec 16 13:30:07.250: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:30:07.258
Dec 16 13:30:07.268: INFO: Waiting for pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 to disappear
Dec 16 13:30:07.271: INFO: Pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:07.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8907" for this suite. 12/16/22 13:30:07.286
------------------------------
• [4.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:03.204
    Dec 16 13:30:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:30:03.205
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:03.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:03.225
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-01a1cdc8-5f6d-4d3b-a2c9-d5062f4bfe1e 12/16/22 13:30:03.228
    STEP: Creating a pod to test consume configMaps 12/16/22 13:30:03.231
    Dec 16 13:30:03.239: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9" in namespace "projected-8907" to be "Succeeded or Failed"
    Dec 16 13:30:03.242: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923774ms
    Dec 16 13:30:05.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484588s
    Dec 16 13:30:07.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007658007s
    STEP: Saw pod success 12/16/22 13:30:07.247
    Dec 16 13:30:07.247: INFO: Pod "pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9" satisfied condition "Succeeded or Failed"
    Dec 16 13:30:07.250: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:30:07.258
    Dec 16 13:30:07.268: INFO: Waiting for pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 to disappear
    Dec 16 13:30:07.271: INFO: Pod pod-projected-configmaps-46173ad0-4b53-4ccc-a4fe-ceb5a1f8c5c9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:07.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8907" for this suite. 12/16/22 13:30:07.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:07.296
Dec 16 13:30:07.296: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:30:07.297
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:07.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:07.314
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/16/22 13:30:07.317
Dec 16 13:30:07.317: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:30:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:16.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6516" for this suite. 12/16/22 13:30:16.026
------------------------------
• [SLOW TEST] [8.737 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:07.296
    Dec 16 13:30:07.296: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:30:07.297
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:07.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:07.314
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/16/22 13:30:07.317
    Dec 16 13:30:07.317: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:30:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:16.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6516" for this suite. 12/16/22 13:30:16.026
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:16.033
Dec 16 13:30:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:30:16.033
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:16.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:16.084
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-7ecdc6a8-aba1-4d68-b0ae-f68e72f993f2 12/16/22 13:30:16.086
STEP: Creating a pod to test consume configMaps 12/16/22 13:30:16.09
Dec 16 13:30:16.098: INFO: Waiting up to 5m0s for pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6" in namespace "configmap-1572" to be "Succeeded or Failed"
Dec 16 13:30:16.101: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.048398ms
Dec 16 13:30:18.106: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008452417s
Dec 16 13:30:20.107: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009650961s
STEP: Saw pod success 12/16/22 13:30:20.107
Dec 16 13:30:20.107: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6" satisfied condition "Succeeded or Failed"
Dec 16 13:30:20.111: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:30:20.121
Dec 16 13:30:20.135: INFO: Waiting for pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 to disappear
Dec 16 13:30:20.137: INFO: Pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:20.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1572" for this suite. 12/16/22 13:30:20.141
------------------------------
• [4.114 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:16.033
    Dec 16 13:30:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:30:16.033
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:16.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:16.084
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-7ecdc6a8-aba1-4d68-b0ae-f68e72f993f2 12/16/22 13:30:16.086
    STEP: Creating a pod to test consume configMaps 12/16/22 13:30:16.09
    Dec 16 13:30:16.098: INFO: Waiting up to 5m0s for pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6" in namespace "configmap-1572" to be "Succeeded or Failed"
    Dec 16 13:30:16.101: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.048398ms
    Dec 16 13:30:18.106: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008452417s
    Dec 16 13:30:20.107: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009650961s
    STEP: Saw pod success 12/16/22 13:30:20.107
    Dec 16 13:30:20.107: INFO: Pod "pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6" satisfied condition "Succeeded or Failed"
    Dec 16 13:30:20.111: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:30:20.121
    Dec 16 13:30:20.135: INFO: Waiting for pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 to disappear
    Dec 16 13:30:20.137: INFO: Pod pod-configmaps-109bdaa7-e45b-46e0-ae74-2eb82be47dc6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:20.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1572" for this suite. 12/16/22 13:30:20.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:20.148
Dec 16 13:30:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename prestop 12/16/22 13:30:20.149
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:20.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:20.167
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4961 12/16/22 13:30:20.169
STEP: Waiting for pods to come up. 12/16/22 13:30:20.177
Dec 16 13:30:20.178: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4961" to be "running"
Dec 16 13:30:20.181: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227205ms
Dec 16 13:30:22.186: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008128639s
Dec 16 13:30:22.186: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4961 12/16/22 13:30:22.189
Dec 16 13:30:22.194: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4961" to be "running"
Dec 16 13:30:22.197: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073393ms
Dec 16 13:30:24.204: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009286015s
Dec 16 13:30:24.204: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 12/16/22 13:30:24.204
Dec 16 13:30:29.248: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 12/16/22 13:30:29.248
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:29.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4961" for this suite. 12/16/22 13:30:29.262
------------------------------
• [SLOW TEST] [9.119 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:20.148
    Dec 16 13:30:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename prestop 12/16/22 13:30:20.149
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:20.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:20.167
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4961 12/16/22 13:30:20.169
    STEP: Waiting for pods to come up. 12/16/22 13:30:20.177
    Dec 16 13:30:20.178: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4961" to be "running"
    Dec 16 13:30:20.181: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227205ms
    Dec 16 13:30:22.186: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008128639s
    Dec 16 13:30:22.186: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4961 12/16/22 13:30:22.189
    Dec 16 13:30:22.194: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4961" to be "running"
    Dec 16 13:30:22.197: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073393ms
    Dec 16 13:30:24.204: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009286015s
    Dec 16 13:30:24.204: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 12/16/22 13:30:24.204
    Dec 16 13:30:29.248: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 12/16/22 13:30:29.248
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:29.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4961" for this suite. 12/16/22 13:30:29.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:29.268
Dec 16 13:30:29.268: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-runtime 12/16/22 13:30:29.269
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:29.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:29.285
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 12/16/22 13:30:29.287
STEP: wait for the container to reach Succeeded 12/16/22 13:30:29.294
STEP: get the container status 12/16/22 13:30:33.316
STEP: the container should be terminated 12/16/22 13:30:33.319
STEP: the termination message should be set 12/16/22 13:30:33.319
Dec 16 13:30:33.319: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 12/16/22 13:30:33.319
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:33.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6715" for this suite. 12/16/22 13:30:33.335
------------------------------
• [4.073 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:29.268
    Dec 16 13:30:29.268: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-runtime 12/16/22 13:30:29.269
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:29.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:29.285
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 12/16/22 13:30:29.287
    STEP: wait for the container to reach Succeeded 12/16/22 13:30:29.294
    STEP: get the container status 12/16/22 13:30:33.316
    STEP: the container should be terminated 12/16/22 13:30:33.319
    STEP: the termination message should be set 12/16/22 13:30:33.319
    Dec 16 13:30:33.319: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 12/16/22 13:30:33.319
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:33.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6715" for this suite. 12/16/22 13:30:33.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:33.342
Dec 16 13:30:33.342: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:30:33.342
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:33.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:33.359
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 12/16/22 13:30:33.362
Dec 16 13:30:33.371: INFO: Waiting up to 5m0s for pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31" in namespace "downward-api-8320" to be "Succeeded or Failed"
Dec 16 13:30:33.374: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225887ms
Dec 16 13:30:35.378: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007298469s
Dec 16 13:30:37.381: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009701645s
STEP: Saw pod success 12/16/22 13:30:37.381
Dec 16 13:30:37.381: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31" satisfied condition "Succeeded or Failed"
Dec 16 13:30:37.384: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:30:37.396
Dec 16 13:30:37.411: INFO: Waiting for pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 to disappear
Dec 16 13:30:37.413: INFO: Pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:37.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8320" for this suite. 12/16/22 13:30:37.417
------------------------------
• [4.080 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:33.342
    Dec 16 13:30:33.342: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:30:33.342
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:33.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:33.359
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 12/16/22 13:30:33.362
    Dec 16 13:30:33.371: INFO: Waiting up to 5m0s for pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31" in namespace "downward-api-8320" to be "Succeeded or Failed"
    Dec 16 13:30:33.374: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225887ms
    Dec 16 13:30:35.378: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007298469s
    Dec 16 13:30:37.381: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009701645s
    STEP: Saw pod success 12/16/22 13:30:37.381
    Dec 16 13:30:37.381: INFO: Pod "downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31" satisfied condition "Succeeded or Failed"
    Dec 16 13:30:37.384: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:30:37.396
    Dec 16 13:30:37.411: INFO: Waiting for pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 to disappear
    Dec 16 13:30:37.413: INFO: Pod downward-api-a81db335-68d8-4e4a-8e42-40b1a4637f31 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:37.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8320" for this suite. 12/16/22 13:30:37.417
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:37.422
Dec 16 13:30:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename podtemplate 12/16/22 13:30:37.423
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:37.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:37.439
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 12/16/22 13:30:37.441
STEP: Replace a pod template 12/16/22 13:30:37.447
Dec 16 13:30:37.455: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:37.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3070" for this suite. 12/16/22 13:30:37.459
------------------------------
• [0.043 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:37.422
    Dec 16 13:30:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename podtemplate 12/16/22 13:30:37.423
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:37.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:37.439
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 12/16/22 13:30:37.441
    STEP: Replace a pod template 12/16/22 13:30:37.447
    Dec 16 13:30:37.455: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:37.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3070" for this suite. 12/16/22 13:30:37.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:37.465
Dec 16 13:30:37.465: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:30:37.465
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:37.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:37.483
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:30:37.496
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:30:37.745
STEP: Deploying the webhook pod 12/16/22 13:30:37.755
STEP: Wait for the deployment to be ready 12/16/22 13:30:37.767
Dec 16 13:30:37.772: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/16/22 13:30:39.795
STEP: Verifying the service has paired with the endpoint 12/16/22 13:30:39.813
Dec 16 13:30:40.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 12/16/22 13:30:40.818
STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.869
STEP: Updating a validating webhook configuration's rules to not include the create operation 12/16/22 13:30:40.906
STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.916
STEP: Patching a validating webhook configuration's rules to include the create operation 12/16/22 13:30:40.926
STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.933
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:40.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2929" for this suite. 12/16/22 13:30:40.991
STEP: Destroying namespace "webhook-2929-markers" for this suite. 12/16/22 13:30:40.997
------------------------------
• [3.538 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:37.465
    Dec 16 13:30:37.465: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:30:37.465
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:37.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:37.483
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:30:37.496
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:30:37.745
    STEP: Deploying the webhook pod 12/16/22 13:30:37.755
    STEP: Wait for the deployment to be ready 12/16/22 13:30:37.767
    Dec 16 13:30:37.772: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/16/22 13:30:39.795
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:30:39.813
    Dec 16 13:30:40.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 12/16/22 13:30:40.818
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.869
    STEP: Updating a validating webhook configuration's rules to not include the create operation 12/16/22 13:30:40.906
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.916
    STEP: Patching a validating webhook configuration's rules to include the create operation 12/16/22 13:30:40.926
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:30:40.933
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:40.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2929" for this suite. 12/16/22 13:30:40.991
    STEP: Destroying namespace "webhook-2929-markers" for this suite. 12/16/22 13:30:40.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:41.004
Dec 16 13:30:41.004: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 13:30:41.005
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:41.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:41.021
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 12/16/22 13:30:41.023
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 12/16/22 13:30:41.023
STEP: creating a pod to probe /etc/hosts 12/16/22 13:30:41.023
STEP: submitting the pod to kubernetes 12/16/22 13:30:41.023
Dec 16 13:30:41.032: INFO: Waiting up to 15m0s for pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf" in namespace "dns-8051" to be "running"
Dec 16 13:30:41.035: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763969ms
Dec 16 13:30:43.041: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560379s
Dec 16 13:30:43.041: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:30:43.041
STEP: looking for the results for each expected name from probers 12/16/22 13:30:43.044
Dec 16 13:30:43.092: INFO: DNS probes using dns-8051/dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf succeeded

STEP: deleting the pod 12/16/22 13:30:43.092
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:43.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8051" for this suite. 12/16/22 13:30:43.106
------------------------------
• [2.108 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:41.004
    Dec 16 13:30:41.004: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 13:30:41.005
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:41.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:41.021
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     12/16/22 13:30:41.023
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8051.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     12/16/22 13:30:41.023
    STEP: creating a pod to probe /etc/hosts 12/16/22 13:30:41.023
    STEP: submitting the pod to kubernetes 12/16/22 13:30:41.023
    Dec 16 13:30:41.032: INFO: Waiting up to 15m0s for pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf" in namespace "dns-8051" to be "running"
    Dec 16 13:30:41.035: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763969ms
    Dec 16 13:30:43.041: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560379s
    Dec 16 13:30:43.041: INFO: Pod "dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:30:43.041
    STEP: looking for the results for each expected name from probers 12/16/22 13:30:43.044
    Dec 16 13:30:43.092: INFO: DNS probes using dns-8051/dns-test-f1e9c32a-844f-4e0d-858a-00942fdf7cdf succeeded

    STEP: deleting the pod 12/16/22 13:30:43.092
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:43.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8051" for this suite. 12/16/22 13:30:43.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:43.117
Dec 16 13:30:43.117: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename proxy 12/16/22 13:30:43.118
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:43.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:43.146
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Dec 16 13:30:43.148: INFO: Creating pod...
Dec 16 13:30:43.154: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6535" to be "running"
Dec 16 13:30:43.157: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165488ms
Dec 16 13:30:45.161: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007470775s
Dec 16 13:30:45.161: INFO: Pod "agnhost" satisfied condition "running"
Dec 16 13:30:45.161: INFO: Creating service...
Dec 16 13:30:45.174: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/DELETE
Dec 16 13:30:45.209: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 16 13:30:45.209: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/GET
Dec 16 13:30:45.213: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 16 13:30:45.213: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/HEAD
Dec 16 13:30:45.217: INFO: http.Client request:HEAD | StatusCode:200
Dec 16 13:30:45.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 16 13:30:45.221: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 16 13:30:45.221: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/PATCH
Dec 16 13:30:45.225: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 16 13:30:45.225: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/POST
Dec 16 13:30:45.229: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 16 13:30:45.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/PUT
Dec 16 13:30:45.233: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 16 13:30:45.233: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/DELETE
Dec 16 13:30:45.240: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 16 13:30:45.240: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/GET
Dec 16 13:30:45.246: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 16 13:30:45.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/HEAD
Dec 16 13:30:45.252: INFO: http.Client request:HEAD | StatusCode:200
Dec 16 13:30:45.252: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/OPTIONS
Dec 16 13:30:45.267: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 16 13:30:45.267: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/PATCH
Dec 16 13:30:45.274: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 16 13:30:45.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/POST
Dec 16 13:30:45.281: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 16 13:30:45.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/PUT
Dec 16 13:30:45.287: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:45.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6535" for this suite. 12/16/22 13:30:45.292
------------------------------
• [2.179 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:43.117
    Dec 16 13:30:43.117: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename proxy 12/16/22 13:30:43.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:43.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:43.146
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Dec 16 13:30:43.148: INFO: Creating pod...
    Dec 16 13:30:43.154: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6535" to be "running"
    Dec 16 13:30:43.157: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165488ms
    Dec 16 13:30:45.161: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007470775s
    Dec 16 13:30:45.161: INFO: Pod "agnhost" satisfied condition "running"
    Dec 16 13:30:45.161: INFO: Creating service...
    Dec 16 13:30:45.174: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/DELETE
    Dec 16 13:30:45.209: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 16 13:30:45.209: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/GET
    Dec 16 13:30:45.213: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 16 13:30:45.213: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/HEAD
    Dec 16 13:30:45.217: INFO: http.Client request:HEAD | StatusCode:200
    Dec 16 13:30:45.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/OPTIONS
    Dec 16 13:30:45.221: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 16 13:30:45.221: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/PATCH
    Dec 16 13:30:45.225: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 16 13:30:45.225: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/POST
    Dec 16 13:30:45.229: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 16 13:30:45.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/pods/agnhost/proxy/some/path/with/PUT
    Dec 16 13:30:45.233: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 16 13:30:45.233: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/DELETE
    Dec 16 13:30:45.240: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 16 13:30:45.240: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/GET
    Dec 16 13:30:45.246: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 16 13:30:45.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/HEAD
    Dec 16 13:30:45.252: INFO: http.Client request:HEAD | StatusCode:200
    Dec 16 13:30:45.252: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/OPTIONS
    Dec 16 13:30:45.267: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 16 13:30:45.267: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/PATCH
    Dec 16 13:30:45.274: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 16 13:30:45.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/POST
    Dec 16 13:30:45.281: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 16 13:30:45.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6535/services/test-service/proxy/some/path/with/PUT
    Dec 16 13:30:45.287: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:45.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6535" for this suite. 12/16/22 13:30:45.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:45.297
Dec 16 13:30:45.297: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:30:45.297
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:45.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:45.314
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:30:45.317
Dec 16 13:30:45.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2" in namespace "downward-api-7675" to be "Succeeded or Failed"
Dec 16 13:30:45.330: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860141ms
Dec 16 13:30:47.335: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150548s
Dec 16 13:30:49.336: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009450934s
STEP: Saw pod success 12/16/22 13:30:49.336
Dec 16 13:30:49.336: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2" satisfied condition "Succeeded or Failed"
Dec 16 13:30:49.339: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 container client-container: <nil>
STEP: delete the pod 12/16/22 13:30:49.346
Dec 16 13:30:49.357: INFO: Waiting for pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 to disappear
Dec 16 13:30:49.360: INFO: Pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:49.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7675" for this suite. 12/16/22 13:30:49.363
------------------------------
• [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:45.297
    Dec 16 13:30:45.297: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:30:45.297
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:45.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:45.314
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:30:45.317
    Dec 16 13:30:45.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2" in namespace "downward-api-7675" to be "Succeeded or Failed"
    Dec 16 13:30:45.330: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860141ms
    Dec 16 13:30:47.335: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150548s
    Dec 16 13:30:49.336: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009450934s
    STEP: Saw pod success 12/16/22 13:30:49.336
    Dec 16 13:30:49.336: INFO: Pod "downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2" satisfied condition "Succeeded or Failed"
    Dec 16 13:30:49.339: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:30:49.346
    Dec 16 13:30:49.357: INFO: Waiting for pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 to disappear
    Dec 16 13:30:49.360: INFO: Pod downwardapi-volume-e4f684d9-cae2-4b69-8691-ccc8812aaaf2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:49.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7675" for this suite. 12/16/22 13:30:49.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:49.372
Dec 16 13:30:49.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir-wrapper 12/16/22 13:30:49.372
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:49.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:49.392
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Dec 16 13:30:49.410: INFO: Waiting up to 5m0s for pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d" in namespace "emptydir-wrapper-7997" to be "running and ready"
Dec 16 13:30:49.413: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.807153ms
Dec 16 13:30:49.413: INFO: The phase of Pod pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:30:51.417: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006838744s
Dec 16 13:30:51.417: INFO: The phase of Pod pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d is Running (Ready = true)
Dec 16 13:30:51.417: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d" satisfied condition "running and ready"
STEP: Cleaning up the secret 12/16/22 13:30:51.42
STEP: Cleaning up the configmap 12/16/22 13:30:51.425
STEP: Cleaning up the pod 12/16/22 13:30:51.431
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:51.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7997" for this suite. 12/16/22 13:30:51.447
------------------------------
• [2.080 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:49.372
    Dec 16 13:30:49.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir-wrapper 12/16/22 13:30:49.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:49.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:49.392
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Dec 16 13:30:49.410: INFO: Waiting up to 5m0s for pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d" in namespace "emptydir-wrapper-7997" to be "running and ready"
    Dec 16 13:30:49.413: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.807153ms
    Dec 16 13:30:49.413: INFO: The phase of Pod pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:30:51.417: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006838744s
    Dec 16 13:30:51.417: INFO: The phase of Pod pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d is Running (Ready = true)
    Dec 16 13:30:51.417: INFO: Pod "pod-secrets-dff6b733-b41b-4d67-b673-90cb27def98d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 12/16/22 13:30:51.42
    STEP: Cleaning up the configmap 12/16/22 13:30:51.425
    STEP: Cleaning up the pod 12/16/22 13:30:51.431
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:51.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7997" for this suite. 12/16/22 13:30:51.447
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:51.452
Dec 16 13:30:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:30:51.452
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:51.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:51.467
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-4d820c2f-6a77-46dc-b504-9d7215a99fe9 12/16/22 13:30:51.47
STEP: Creating a pod to test consume configMaps 12/16/22 13:30:51.473
Dec 16 13:30:51.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0" in namespace "configmap-4554" to be "Succeeded or Failed"
Dec 16 13:30:51.483: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446209ms
Dec 16 13:30:53.489: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008152192s
Dec 16 13:30:55.490: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009094588s
STEP: Saw pod success 12/16/22 13:30:55.49
Dec 16 13:30:55.490: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0" satisfied condition "Succeeded or Failed"
Dec 16 13:30:55.493: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:30:55.501
Dec 16 13:30:55.511: INFO: Waiting for pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 to disappear
Dec 16 13:30:55.514: INFO: Pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:55.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4554" for this suite. 12/16/22 13:30:55.518
------------------------------
• [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:51.452
    Dec 16 13:30:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:30:51.452
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:51.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:51.467
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-4d820c2f-6a77-46dc-b504-9d7215a99fe9 12/16/22 13:30:51.47
    STEP: Creating a pod to test consume configMaps 12/16/22 13:30:51.473
    Dec 16 13:30:51.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0" in namespace "configmap-4554" to be "Succeeded or Failed"
    Dec 16 13:30:51.483: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446209ms
    Dec 16 13:30:53.489: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008152192s
    Dec 16 13:30:55.490: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009094588s
    STEP: Saw pod success 12/16/22 13:30:55.49
    Dec 16 13:30:55.490: INFO: Pod "pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0" satisfied condition "Succeeded or Failed"
    Dec 16 13:30:55.493: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:30:55.501
    Dec 16 13:30:55.511: INFO: Waiting for pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 to disappear
    Dec 16 13:30:55.514: INFO: Pod pod-configmaps-e9aa0bd9-bc6f-4307-afce-a6543e3cf8a0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:55.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4554" for this suite. 12/16/22 13:30:55.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:55.524
Dec 16 13:30:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:30:55.524
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:55.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:55.541
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-0d856aa7-34d6-484b-97b6-d867eb5d7373 12/16/22 13:30:55.547
STEP: Creating configMap with name cm-test-opt-upd-210a3484-e75f-4858-ac86-36e358927203 12/16/22 13:30:55.55
STEP: Creating the pod 12/16/22 13:30:55.554
Dec 16 13:30:55.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555" in namespace "projected-2818" to be "running and ready"
Dec 16 13:30:55.565: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063825ms
Dec 16 13:30:55.565: INFO: The phase of Pod pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:30:57.571: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555": Phase="Running", Reason="", readiness=true. Elapsed: 2.009496616s
Dec 16 13:30:57.571: INFO: The phase of Pod pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555 is Running (Ready = true)
Dec 16 13:30:57.571: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0d856aa7-34d6-484b-97b6-d867eb5d7373 12/16/22 13:30:57.597
STEP: Updating configmap cm-test-opt-upd-210a3484-e75f-4858-ac86-36e358927203 12/16/22 13:30:57.603
STEP: Creating configMap with name cm-test-opt-create-21b5ccfe-363f-4b2a-adc2-4b5e7c1ffd96 12/16/22 13:30:57.607
STEP: waiting to observe update in volume 12/16/22 13:30:57.61
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:59.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2818" for this suite. 12/16/22 13:30:59.659
------------------------------
• [4.141 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:55.524
    Dec 16 13:30:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:30:55.524
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:55.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:55.541
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-0d856aa7-34d6-484b-97b6-d867eb5d7373 12/16/22 13:30:55.547
    STEP: Creating configMap with name cm-test-opt-upd-210a3484-e75f-4858-ac86-36e358927203 12/16/22 13:30:55.55
    STEP: Creating the pod 12/16/22 13:30:55.554
    Dec 16 13:30:55.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555" in namespace "projected-2818" to be "running and ready"
    Dec 16 13:30:55.565: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063825ms
    Dec 16 13:30:55.565: INFO: The phase of Pod pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:30:57.571: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555": Phase="Running", Reason="", readiness=true. Elapsed: 2.009496616s
    Dec 16 13:30:57.571: INFO: The phase of Pod pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555 is Running (Ready = true)
    Dec 16 13:30:57.571: INFO: Pod "pod-projected-configmaps-73cfad6a-c1f4-4648-901d-80c2c2355555" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0d856aa7-34d6-484b-97b6-d867eb5d7373 12/16/22 13:30:57.597
    STEP: Updating configmap cm-test-opt-upd-210a3484-e75f-4858-ac86-36e358927203 12/16/22 13:30:57.603
    STEP: Creating configMap with name cm-test-opt-create-21b5ccfe-363f-4b2a-adc2-4b5e7c1ffd96 12/16/22 13:30:57.607
    STEP: waiting to observe update in volume 12/16/22 13:30:57.61
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:59.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2818" for this suite. 12/16/22 13:30:59.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:59.665
Dec 16 13:30:59.665: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:30:59.666
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:59.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:59.786
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 12/16/22 13:30:59.788
Dec 16 13:30:59.788: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5293 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 12/16/22 13:30:59.831
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:30:59.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5293" for this suite. 12/16/22 13:30:59.852
------------------------------
• [0.193 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:59.665
    Dec 16 13:30:59.665: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:30:59.666
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:30:59.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:30:59.786
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 12/16/22 13:30:59.788
    Dec 16 13:30:59.788: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5293 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 12/16/22 13:30:59.831
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:30:59.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5293" for this suite. 12/16/22 13:30:59.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:30:59.859
Dec 16 13:30:59.859: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:30:59.86
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:00.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:00.096
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Dec 16 13:31:00.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 create -f -'
Dec 16 13:31:00.668: INFO: stderr: ""
Dec 16 13:31:00.668: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 16 13:31:00.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 create -f -'
Dec 16 13:31:01.250: INFO: stderr: ""
Dec 16 13:31:01.250: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/16/22 13:31:01.25
Dec 16 13:31:02.254: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:31:02.254: INFO: Found 1 / 1
Dec 16 13:31:02.254: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 16 13:31:02.257: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:31:02.257: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 13:31:02.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe pod agnhost-primary-n45f6'
Dec 16 13:31:02.330: INFO: stderr: ""
Dec 16 13:31:02.330: INFO: stdout: "Name:             agnhost-primary-n45f6\nNamespace:        kubectl-8009\nPriority:         0\nService Account:  default\nNode:             pool-a3802-fsxxd/85.217.161.242\nStart Time:       Fri, 16 Dec 2022 13:31:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 00588f2de90a762a15a72b7cb1a0d2e788222280897515f9f3abf13eccce0125\n                  cni.projectcalico.org/podIP: 192.168.189.61/32\n                  cni.projectcalico.org/podIPs: 192.168.189.61/32\nStatus:           Running\nIP:               192.168.189.61\nIPs:\n  IP:           192.168.189.61\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://0bc08f87af5722b87afe86204c19dc93fd286c594d992a3e6ee40d4b384952aa\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 16 Dec 2022 13:31:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fp7d5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fp7d5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-8009/agnhost-primary-n45f6 to pool-a3802-fsxxd\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 16 13:31:02.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe rc agnhost-primary'
Dec 16 13:31:02.413: INFO: stderr: ""
Dec 16 13:31:02.413: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8009\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-n45f6\n"
Dec 16 13:31:02.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe service agnhost-primary'
Dec 16 13:31:02.495: INFO: stderr: ""
Dec 16 13:31:02.495: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8009\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.120.56\nIPs:               10.106.120.56\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.189.61:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 16 13:31:02.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe node pool-a3802-ehprg'
Dec 16 13:31:02.636: INFO: stderr: ""
Dec 16 13:31:02.636: INFO: stdout: "Name:               pool-a3802-ehprg\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pool-a3802-ehprg\n                    kubernetes.io/os=linux\n                    node.exoscale.net/nodepool-id=9772e6a0-9aca-42c9-9d28-fab8c5afeb14\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 85.217.161.222/23\n                    projectcalico.org/IPv4VXLANTunnelAddr: 192.168.156.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 16 Dec 2022 13:10:09 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pool-a3802-ehprg\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 16 Dec 2022 13:30:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 16 Dec 2022 13:10:32 +0000   Fri, 16 Dec 2022 13:10:32 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  85.217.161.222\n  Hostname:    pool-a3802-ehprg\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16381772Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3800m\n  ephemeral-storage:      45577849165\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16074572Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 16633c1a749340e9b4ed865070cbe658\n  System UUID:                16633c1a-7493-40e9-b4ed-865070cbe658\n  Boot ID:                    db739d6d-26bd-49a7-88ef-1c22c3567b66\n  Kernel Version:             5.15.0-53-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-g9rxj                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                 coredns-7cd5b7d6b4-zz7zw                                   100m (2%)     100m (2%)   70Mi (0%)        170Mi (1%)     21m\n  kube-system                 konnectivity-agent-5f7cbf88d-fpk9f                         100m (2%)     100m (2%)   200Mi (1%)       200Mi (1%)     21m\n  kube-system                 kube-proxy-csksk                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    450m (11%)  200m (5%)\n  memory                 270Mi (1%)  370Mi (2%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 20m                kube-proxy       \n  Normal  NodeHasSufficientMemory  20m (x8 over 21m)  kubelet          Node pool-a3802-ehprg status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           20m                node-controller  Node pool-a3802-ehprg event: Registered Node pool-a3802-ehprg in Controller\n"
Dec 16 13:31:02.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe namespace kubectl-8009'
Dec 16 13:31:02.722: INFO: stderr: ""
Dec 16 13:31:02.722: INFO: stdout: "Name:         kubectl-8009\nLabels:       e2e-framework=kubectl\n              e2e-run=6ab7a2a6-4f2a-43ab-a7a0-8623e2091669\n              kubernetes.io/metadata.name=kubectl-8009\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:02.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8009" for this suite. 12/16/22 13:31:02.727
------------------------------
• [2.874 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:30:59.859
    Dec 16 13:30:59.859: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:30:59.86
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:00.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:00.096
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Dec 16 13:31:00.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 create -f -'
    Dec 16 13:31:00.668: INFO: stderr: ""
    Dec 16 13:31:00.668: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Dec 16 13:31:00.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 create -f -'
    Dec 16 13:31:01.250: INFO: stderr: ""
    Dec 16 13:31:01.250: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/16/22 13:31:01.25
    Dec 16 13:31:02.254: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:31:02.254: INFO: Found 1 / 1
    Dec 16 13:31:02.254: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 16 13:31:02.257: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:31:02.257: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 16 13:31:02.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe pod agnhost-primary-n45f6'
    Dec 16 13:31:02.330: INFO: stderr: ""
    Dec 16 13:31:02.330: INFO: stdout: "Name:             agnhost-primary-n45f6\nNamespace:        kubectl-8009\nPriority:         0\nService Account:  default\nNode:             pool-a3802-fsxxd/85.217.161.242\nStart Time:       Fri, 16 Dec 2022 13:31:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 00588f2de90a762a15a72b7cb1a0d2e788222280897515f9f3abf13eccce0125\n                  cni.projectcalico.org/podIP: 192.168.189.61/32\n                  cni.projectcalico.org/podIPs: 192.168.189.61/32\nStatus:           Running\nIP:               192.168.189.61\nIPs:\n  IP:           192.168.189.61\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://0bc08f87af5722b87afe86204c19dc93fd286c594d992a3e6ee40d4b384952aa\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 16 Dec 2022 13:31:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fp7d5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fp7d5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-8009/agnhost-primary-n45f6 to pool-a3802-fsxxd\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Dec 16 13:31:02.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe rc agnhost-primary'
    Dec 16 13:31:02.413: INFO: stderr: ""
    Dec 16 13:31:02.413: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8009\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-n45f6\n"
    Dec 16 13:31:02.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe service agnhost-primary'
    Dec 16 13:31:02.495: INFO: stderr: ""
    Dec 16 13:31:02.495: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8009\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.120.56\nIPs:               10.106.120.56\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.189.61:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Dec 16 13:31:02.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe node pool-a3802-ehprg'
    Dec 16 13:31:02.636: INFO: stderr: ""
    Dec 16 13:31:02.636: INFO: stdout: "Name:               pool-a3802-ehprg\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pool-a3802-ehprg\n                    kubernetes.io/os=linux\n                    node.exoscale.net/nodepool-id=9772e6a0-9aca-42c9-9d28-fab8c5afeb14\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 85.217.161.222/23\n                    projectcalico.org/IPv4VXLANTunnelAddr: 192.168.156.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 16 Dec 2022 13:10:09 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pool-a3802-ehprg\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 16 Dec 2022 13:30:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 16 Dec 2022 13:10:32 +0000   Fri, 16 Dec 2022 13:10:32 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 16 Dec 2022 13:27:30 +0000   Fri, 16 Dec 2022 13:10:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  85.217.161.222\n  Hostname:    pool-a3802-ehprg\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16381772Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3800m\n  ephemeral-storage:      45577849165\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16074572Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 16633c1a749340e9b4ed865070cbe658\n  System UUID:                16633c1a-7493-40e9-b4ed-865070cbe658\n  Boot ID:                    db739d6d-26bd-49a7-88ef-1c22c3567b66\n  Kernel Version:             5.15.0-53-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-g9rxj                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                 coredns-7cd5b7d6b4-zz7zw                                   100m (2%)     100m (2%)   70Mi (0%)        170Mi (1%)     21m\n  kube-system                 konnectivity-agent-5f7cbf88d-fpk9f                         100m (2%)     100m (2%)   200Mi (1%)       200Mi (1%)     21m\n  kube-system                 kube-proxy-csksk                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    450m (11%)  200m (5%)\n  memory                 270Mi (1%)  370Mi (2%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 20m                kube-proxy       \n  Normal  NodeHasSufficientMemory  20m (x8 over 21m)  kubelet          Node pool-a3802-ehprg status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           20m                node-controller  Node pool-a3802-ehprg event: Registered Node pool-a3802-ehprg in Controller\n"
    Dec 16 13:31:02.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-8009 describe namespace kubectl-8009'
    Dec 16 13:31:02.722: INFO: stderr: ""
    Dec 16 13:31:02.722: INFO: stdout: "Name:         kubectl-8009\nLabels:       e2e-framework=kubectl\n              e2e-run=6ab7a2a6-4f2a-43ab-a7a0-8623e2091669\n              kubernetes.io/metadata.name=kubectl-8009\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:02.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8009" for this suite. 12/16/22 13:31:02.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:02.733
Dec 16 13:31:02.733: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:31:02.734
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:02.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:02.753
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-e3917e2b-47b3-4f80-9ff1-468c8a08a169 12/16/22 13:31:02.755
STEP: Creating a pod to test consume configMaps 12/16/22 13:31:02.77
Dec 16 13:31:02.780: INFO: Waiting up to 5m0s for pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12" in namespace "configmap-7842" to be "Succeeded or Failed"
Dec 16 13:31:02.783: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800921ms
Dec 16 13:31:04.788: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Running", Reason="", readiness=false. Elapsed: 2.007733398s
Dec 16 13:31:06.789: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009290883s
STEP: Saw pod success 12/16/22 13:31:06.789
Dec 16 13:31:06.789: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12" satisfied condition "Succeeded or Failed"
Dec 16 13:31:06.792: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:31:06.849
Dec 16 13:31:06.862: INFO: Waiting for pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 to disappear
Dec 16 13:31:06.865: INFO: Pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:06.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7842" for this suite. 12/16/22 13:31:06.869
------------------------------
• [4.141 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:02.733
    Dec 16 13:31:02.733: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:31:02.734
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:02.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:02.753
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-e3917e2b-47b3-4f80-9ff1-468c8a08a169 12/16/22 13:31:02.755
    STEP: Creating a pod to test consume configMaps 12/16/22 13:31:02.77
    Dec 16 13:31:02.780: INFO: Waiting up to 5m0s for pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12" in namespace "configmap-7842" to be "Succeeded or Failed"
    Dec 16 13:31:02.783: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800921ms
    Dec 16 13:31:04.788: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Running", Reason="", readiness=false. Elapsed: 2.007733398s
    Dec 16 13:31:06.789: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009290883s
    STEP: Saw pod success 12/16/22 13:31:06.789
    Dec 16 13:31:06.789: INFO: Pod "pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12" satisfied condition "Succeeded or Failed"
    Dec 16 13:31:06.792: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:31:06.849
    Dec 16 13:31:06.862: INFO: Waiting for pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 to disappear
    Dec 16 13:31:06.865: INFO: Pod pod-configmaps-181f1a82-2f53-478a-89ec-b63cbef76e12 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:06.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7842" for this suite. 12/16/22 13:31:06.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:06.874
Dec 16 13:31:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/16/22 13:31:06.875
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:06.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:06.893
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 12/16/22 13:31:06.895
STEP: Creating hostNetwork=false pod 12/16/22 13:31:06.895
Dec 16 13:31:06.903: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1266" to be "running and ready"
Dec 16 13:31:06.906: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304668ms
Dec 16 13:31:06.906: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:31:08.910: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006896125s
Dec 16 13:31:08.910: INFO: The phase of Pod test-pod is Running (Ready = true)
Dec 16 13:31:08.910: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 12/16/22 13:31:08.912
Dec 16 13:31:08.916: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1266" to be "running and ready"
Dec 16 13:31:08.919: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.485863ms
Dec 16 13:31:08.919: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:31:10.925: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063521s
Dec 16 13:31:10.925: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Dec 16 13:31:10.925: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 12/16/22 13:31:10.928
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/16/22 13:31:10.928
Dec 16 13:31:10.928: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:10.929: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:10.929: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 16 13:31:11.056: INFO: Exec stderr: ""
Dec 16 13:31:11.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.056: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.057: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.057: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 16 13:31:11.180: INFO: Exec stderr: ""
Dec 16 13:31:11.180: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.180: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.180: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 16 13:31:11.295: INFO: Exec stderr: ""
Dec 16 13:31:11.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.295: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.296: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 16 13:31:11.407: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/16/22 13:31:11.407
Dec 16 13:31:11.407: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.407: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.407: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.407: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 16 13:31:11.515: INFO: Exec stderr: ""
Dec 16 13:31:11.515: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.515: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.515: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.515: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 16 13:31:11.639: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/16/22 13:31:11.639
Dec 16 13:31:11.639: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.639: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.639: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 16 13:31:11.760: INFO: Exec stderr: ""
Dec 16 13:31:11.760: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.760: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.761: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 16 13:31:11.878: INFO: Exec stderr: ""
Dec 16 13:31:11.878: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.878: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 16 13:31:11.998: INFO: Exec stderr: ""
Dec 16 13:31:11.998: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:31:11.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:31:11.999: INFO: ExecWithOptions: Clientset creation
Dec 16 13:31:11.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 16 13:31:12.115: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:12.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1266" for this suite. 12/16/22 13:31:12.12
------------------------------
• [SLOW TEST] [5.252 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:06.874
    Dec 16 13:31:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/16/22 13:31:06.875
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:06.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:06.893
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 12/16/22 13:31:06.895
    STEP: Creating hostNetwork=false pod 12/16/22 13:31:06.895
    Dec 16 13:31:06.903: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1266" to be "running and ready"
    Dec 16 13:31:06.906: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304668ms
    Dec 16 13:31:06.906: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:31:08.910: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006896125s
    Dec 16 13:31:08.910: INFO: The phase of Pod test-pod is Running (Ready = true)
    Dec 16 13:31:08.910: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 12/16/22 13:31:08.912
    Dec 16 13:31:08.916: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1266" to be "running and ready"
    Dec 16 13:31:08.919: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.485863ms
    Dec 16 13:31:08.919: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:31:10.925: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063521s
    Dec 16 13:31:10.925: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Dec 16 13:31:10.925: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 12/16/22 13:31:10.928
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/16/22 13:31:10.928
    Dec 16 13:31:10.928: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:10.929: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:10.929: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 16 13:31:11.056: INFO: Exec stderr: ""
    Dec 16 13:31:11.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.056: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.057: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.057: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 16 13:31:11.180: INFO: Exec stderr: ""
    Dec 16 13:31:11.180: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.180: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.180: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 16 13:31:11.295: INFO: Exec stderr: ""
    Dec 16 13:31:11.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.295: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.296: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 16 13:31:11.407: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/16/22 13:31:11.407
    Dec 16 13:31:11.407: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.407: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.407: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.407: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 16 13:31:11.515: INFO: Exec stderr: ""
    Dec 16 13:31:11.515: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.515: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.515: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.515: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 16 13:31:11.639: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/16/22 13:31:11.639
    Dec 16 13:31:11.639: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.639: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.639: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 16 13:31:11.760: INFO: Exec stderr: ""
    Dec 16 13:31:11.760: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.760: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.761: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 16 13:31:11.878: INFO: Exec stderr: ""
    Dec 16 13:31:11.878: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.878: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 16 13:31:11.998: INFO: Exec stderr: ""
    Dec 16 13:31:11.998: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:31:11.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:31:11.999: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:31:11.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 16 13:31:12.115: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:12.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1266" for this suite. 12/16/22 13:31:12.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:12.127
Dec 16 13:31:12.127: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename endpointslice 12/16/22 13:31:12.128
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:12.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:12.143
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 12/16/22 13:31:12.146
STEP: getting /apis/discovery.k8s.io 12/16/22 13:31:12.148
STEP: getting /apis/discovery.k8s.iov1 12/16/22 13:31:12.149
STEP: creating 12/16/22 13:31:12.15
STEP: getting 12/16/22 13:31:12.162
STEP: listing 12/16/22 13:31:12.166
STEP: watching 12/16/22 13:31:12.169
Dec 16 13:31:12.169: INFO: starting watch
STEP: cluster-wide listing 12/16/22 13:31:12.17
STEP: cluster-wide watching 12/16/22 13:31:12.173
Dec 16 13:31:12.173: INFO: starting watch
STEP: patching 12/16/22 13:31:12.174
STEP: updating 12/16/22 13:31:12.178
Dec 16 13:31:12.186: INFO: waiting for watch events with expected annotations
Dec 16 13:31:12.186: INFO: saw patched and updated annotations
STEP: deleting 12/16/22 13:31:12.186
STEP: deleting a collection 12/16/22 13:31:12.196
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:12.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2274" for this suite. 12/16/22 13:31:12.213
------------------------------
• [0.091 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:12.127
    Dec 16 13:31:12.127: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename endpointslice 12/16/22 13:31:12.128
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:12.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:12.143
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 12/16/22 13:31:12.146
    STEP: getting /apis/discovery.k8s.io 12/16/22 13:31:12.148
    STEP: getting /apis/discovery.k8s.iov1 12/16/22 13:31:12.149
    STEP: creating 12/16/22 13:31:12.15
    STEP: getting 12/16/22 13:31:12.162
    STEP: listing 12/16/22 13:31:12.166
    STEP: watching 12/16/22 13:31:12.169
    Dec 16 13:31:12.169: INFO: starting watch
    STEP: cluster-wide listing 12/16/22 13:31:12.17
    STEP: cluster-wide watching 12/16/22 13:31:12.173
    Dec 16 13:31:12.173: INFO: starting watch
    STEP: patching 12/16/22 13:31:12.174
    STEP: updating 12/16/22 13:31:12.178
    Dec 16 13:31:12.186: INFO: waiting for watch events with expected annotations
    Dec 16 13:31:12.186: INFO: saw patched and updated annotations
    STEP: deleting 12/16/22 13:31:12.186
    STEP: deleting a collection 12/16/22 13:31:12.196
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:12.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2274" for this suite. 12/16/22 13:31:12.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:12.219
Dec 16 13:31:12.219: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 13:31:12.22
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:12.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:12.236
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Dec 16 13:31:12.238: INFO: Creating ReplicaSet my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19
Dec 16 13:31:12.247: INFO: Pod name my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Found 0 pods out of 1
Dec 16 13:31:17.253: INFO: Pod name my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Found 1 pods out of 1
Dec 16 13:31:17.253: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19" is running
Dec 16 13:31:17.253: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" in namespace "replicaset-5319" to be "running"
Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq": Phase="Running", Reason="", readiness=true. Elapsed: 3.23679ms
Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" satisfied condition "running"
Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:12 +0000 UTC Reason: Message:}])
Dec 16 13:31:17.256: INFO: Trying to dial the pod
Dec 16 13:31:22.300: INFO: Controller my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Got expected result from replica 1 [my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq]: "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:22.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5319" for this suite. 12/16/22 13:31:22.305
------------------------------
• [SLOW TEST] [10.092 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:12.219
    Dec 16 13:31:12.219: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 13:31:12.22
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:12.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:12.236
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Dec 16 13:31:12.238: INFO: Creating ReplicaSet my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19
    Dec 16 13:31:12.247: INFO: Pod name my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Found 0 pods out of 1
    Dec 16 13:31:17.253: INFO: Pod name my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Found 1 pods out of 1
    Dec 16 13:31:17.253: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19" is running
    Dec 16 13:31:17.253: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" in namespace "replicaset-5319" to be "running"
    Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq": Phase="Running", Reason="", readiness=true. Elapsed: 3.23679ms
    Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" satisfied condition "running"
    Dec 16 13:31:17.256: INFO: Pod "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:31:12 +0000 UTC Reason: Message:}])
    Dec 16 13:31:17.256: INFO: Trying to dial the pod
    Dec 16 13:31:22.300: INFO: Controller my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19: Got expected result from replica 1 [my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq]: "my-hostname-basic-0ed9c2f2-0595-409f-903c-916557e2ef19-59xwq", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:22.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5319" for this suite. 12/16/22 13:31:22.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:22.311
Dec 16 13:31:22.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename runtimeclass 12/16/22 13:31:22.312
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:22.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:22.329
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Dec 16 13:31:22.344: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-240 to be scheduled
Dec 16 13:31:22.347: INFO: 1 pods are not scheduled: [runtimeclass-240/test-runtimeclass-runtimeclass-240-preconfigured-handler-chxnd(ade23545-d0c8-4ed7-86e8-cc559c9a042d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:24.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-240" for this suite. 12/16/22 13:31:24.364
------------------------------
• [2.059 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:22.311
    Dec 16 13:31:22.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename runtimeclass 12/16/22 13:31:22.312
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:22.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:22.329
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Dec 16 13:31:22.344: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-240 to be scheduled
    Dec 16 13:31:22.347: INFO: 1 pods are not scheduled: [runtimeclass-240/test-runtimeclass-runtimeclass-240-preconfigured-handler-chxnd(ade23545-d0c8-4ed7-86e8-cc559c9a042d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:24.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-240" for this suite. 12/16/22 13:31:24.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:24.372
Dec 16 13:31:24.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:31:24.373
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:24.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:24.391
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 12/16/22 13:31:24.394
STEP: watching for the ServiceAccount to be added 12/16/22 13:31:24.401
STEP: patching the ServiceAccount 12/16/22 13:31:24.402
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/16/22 13:31:24.408
STEP: deleting the ServiceAccount 12/16/22 13:31:24.412
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6090" for this suite. 12/16/22 13:31:24.427
------------------------------
• [0.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:24.372
    Dec 16 13:31:24.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:31:24.373
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:24.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:24.391
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 12/16/22 13:31:24.394
    STEP: watching for the ServiceAccount to be added 12/16/22 13:31:24.401
    STEP: patching the ServiceAccount 12/16/22 13:31:24.402
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/16/22 13:31:24.408
    STEP: deleting the ServiceAccount 12/16/22 13:31:24.412
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6090" for this suite. 12/16/22 13:31:24.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:24.437
Dec 16 13:31:24.437: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:31:24.438
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:24.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:24.455
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-ecdedf85-5238-431a-83b5-bdba9ac4b188 12/16/22 13:31:24.457
STEP: Creating a pod to test consume configMaps 12/16/22 13:31:24.463
Dec 16 13:31:24.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c" in namespace "configmap-3322" to be "Succeeded or Failed"
Dec 16 13:31:24.474: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111485ms
Dec 16 13:31:26.479: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007616057s
Dec 16 13:31:28.480: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009206589s
STEP: Saw pod success 12/16/22 13:31:28.48
Dec 16 13:31:28.480: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c" satisfied condition "Succeeded or Failed"
Dec 16 13:31:28.484: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c container configmap-volume-test: <nil>
STEP: delete the pod 12/16/22 13:31:28.493
Dec 16 13:31:28.504: INFO: Waiting for pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c to disappear
Dec 16 13:31:28.507: INFO: Pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:28.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3322" for this suite. 12/16/22 13:31:28.511
------------------------------
• [4.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:24.437
    Dec 16 13:31:24.437: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:31:24.438
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:24.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:24.455
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-ecdedf85-5238-431a-83b5-bdba9ac4b188 12/16/22 13:31:24.457
    STEP: Creating a pod to test consume configMaps 12/16/22 13:31:24.463
    Dec 16 13:31:24.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c" in namespace "configmap-3322" to be "Succeeded or Failed"
    Dec 16 13:31:24.474: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111485ms
    Dec 16 13:31:26.479: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007616057s
    Dec 16 13:31:28.480: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009206589s
    STEP: Saw pod success 12/16/22 13:31:28.48
    Dec 16 13:31:28.480: INFO: Pod "pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c" satisfied condition "Succeeded or Failed"
    Dec 16 13:31:28.484: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c container configmap-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:31:28.493
    Dec 16 13:31:28.504: INFO: Waiting for pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c to disappear
    Dec 16 13:31:28.507: INFO: Pod pod-configmaps-f97c9b57-bac7-4792-b403-ac64d655516c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:28.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3322" for this suite. 12/16/22 13:31:28.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:28.517
Dec 16 13:31:28.517: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 13:31:28.518
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:28.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:28.543
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 12/16/22 13:31:28.545
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:28.55
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:28.55
STEP: creating a pod to probe DNS 12/16/22 13:31:28.55
STEP: submitting the pod to kubernetes 12/16/22 13:31:28.551
Dec 16 13:31:28.557: INFO: Waiting up to 15m0s for pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da" in namespace "dns-954" to be "running"
Dec 16 13:31:28.560: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033021ms
Dec 16 13:31:30.567: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.009426405s
Dec 16 13:31:30.567: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:31:30.567
STEP: looking for the results for each expected name from probers 12/16/22 13:31:30.571
Dec 16 13:31:30.610: INFO: DNS probes using dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da succeeded

STEP: deleting the pod 12/16/22 13:31:30.61
STEP: changing the externalName to bar.example.com 12/16/22 13:31:30.625
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:30.631
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:30.631
STEP: creating a second pod to probe DNS 12/16/22 13:31:30.632
STEP: submitting the pod to kubernetes 12/16/22 13:31:30.632
Dec 16 13:31:30.637: INFO: Waiting up to 15m0s for pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d" in namespace "dns-954" to be "running"
Dec 16 13:31:30.640: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.72391ms
Dec 16 13:31:32.644: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007385434s
Dec 16 13:31:32.644: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:31:32.644
STEP: looking for the results for each expected name from probers 12/16/22 13:31:32.648
Dec 16 13:31:32.681: INFO: File wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod  dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 16 13:31:32.687: INFO: File jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod  dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 16 13:31:32.687: INFO: Lookups using dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d failed for: [wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local]

Dec 16 13:31:37.700: INFO: DNS probes using dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d succeeded

STEP: deleting the pod 12/16/22 13:31:37.7
STEP: changing the service to type=ClusterIP 12/16/22 13:31:37.712
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:37.731
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
 12/16/22 13:31:37.732
STEP: creating a third pod to probe DNS 12/16/22 13:31:37.732
STEP: submitting the pod to kubernetes 12/16/22 13:31:37.735
Dec 16 13:31:37.740: INFO: Waiting up to 15m0s for pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438" in namespace "dns-954" to be "running"
Dec 16 13:31:37.743: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.312894ms
Dec 16 13:31:39.748: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438": Phase="Running", Reason="", readiness=true. Elapsed: 2.007270599s
Dec 16 13:31:39.748: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:31:39.748
STEP: looking for the results for each expected name from probers 12/16/22 13:31:39.751
Dec 16 13:31:39.791: INFO: DNS probes using dns-test-924d21ff-7866-4f87-a74c-14fd8733b438 succeeded

STEP: deleting the pod 12/16/22 13:31:39.791
STEP: deleting the test externalName service 12/16/22 13:31:39.802
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-954" for this suite. 12/16/22 13:31:39.82
------------------------------
• [SLOW TEST] [11.308 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:28.517
    Dec 16 13:31:28.517: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 13:31:28.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:28.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:28.543
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 12/16/22 13:31:28.545
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:28.55
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:28.55
    STEP: creating a pod to probe DNS 12/16/22 13:31:28.55
    STEP: submitting the pod to kubernetes 12/16/22 13:31:28.551
    Dec 16 13:31:28.557: INFO: Waiting up to 15m0s for pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da" in namespace "dns-954" to be "running"
    Dec 16 13:31:28.560: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033021ms
    Dec 16 13:31:30.567: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.009426405s
    Dec 16 13:31:30.567: INFO: Pod "dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:31:30.567
    STEP: looking for the results for each expected name from probers 12/16/22 13:31:30.571
    Dec 16 13:31:30.610: INFO: DNS probes using dns-test-a06162b1-6a76-4e9b-87f1-8050fc04e1da succeeded

    STEP: deleting the pod 12/16/22 13:31:30.61
    STEP: changing the externalName to bar.example.com 12/16/22 13:31:30.625
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:30.631
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:30.631
    STEP: creating a second pod to probe DNS 12/16/22 13:31:30.632
    STEP: submitting the pod to kubernetes 12/16/22 13:31:30.632
    Dec 16 13:31:30.637: INFO: Waiting up to 15m0s for pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d" in namespace "dns-954" to be "running"
    Dec 16 13:31:30.640: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.72391ms
    Dec 16 13:31:32.644: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007385434s
    Dec 16 13:31:32.644: INFO: Pod "dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:31:32.644
    STEP: looking for the results for each expected name from probers 12/16/22 13:31:32.648
    Dec 16 13:31:32.681: INFO: File wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod  dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 16 13:31:32.687: INFO: File jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod  dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 16 13:31:32.687: INFO: Lookups using dns-954/dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d failed for: [wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local]

    Dec 16 13:31:37.700: INFO: DNS probes using dns-test-2cf69bef-7f08-4e97-8c2e-e5f1650cae9d succeeded

    STEP: deleting the pod 12/16/22 13:31:37.7
    STEP: changing the service to type=ClusterIP 12/16/22 13:31:37.712
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:37.731
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-954.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local; sleep 1; done
     12/16/22 13:31:37.732
    STEP: creating a third pod to probe DNS 12/16/22 13:31:37.732
    STEP: submitting the pod to kubernetes 12/16/22 13:31:37.735
    Dec 16 13:31:37.740: INFO: Waiting up to 15m0s for pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438" in namespace "dns-954" to be "running"
    Dec 16 13:31:37.743: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.312894ms
    Dec 16 13:31:39.748: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438": Phase="Running", Reason="", readiness=true. Elapsed: 2.007270599s
    Dec 16 13:31:39.748: INFO: Pod "dns-test-924d21ff-7866-4f87-a74c-14fd8733b438" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:31:39.748
    STEP: looking for the results for each expected name from probers 12/16/22 13:31:39.751
    Dec 16 13:31:39.791: INFO: DNS probes using dns-test-924d21ff-7866-4f87-a74c-14fd8733b438 succeeded

    STEP: deleting the pod 12/16/22 13:31:39.791
    STEP: deleting the test externalName service 12/16/22 13:31:39.802
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-954" for this suite. 12/16/22 13:31:39.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:39.827
Dec 16 13:31:39.827: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:31:39.827
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:39.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:39.845
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/16/22 13:31:39.848
Dec 16 13:31:39.855: INFO: Waiting up to 5m0s for pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81" in namespace "emptydir-678" to be "Succeeded or Failed"
Dec 16 13:31:39.858: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955953ms
Dec 16 13:31:41.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008619429s
Dec 16 13:31:43.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009358455s
STEP: Saw pod success 12/16/22 13:31:43.864
Dec 16 13:31:43.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81" satisfied condition "Succeeded or Failed"
Dec 16 13:31:43.868: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 container test-container: <nil>
STEP: delete the pod 12/16/22 13:31:43.877
Dec 16 13:31:43.890: INFO: Waiting for pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 to disappear
Dec 16 13:31:43.894: INFO: Pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:43.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-678" for this suite. 12/16/22 13:31:43.898
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:39.827
    Dec 16 13:31:39.827: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:31:39.827
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:39.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:39.845
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/16/22 13:31:39.848
    Dec 16 13:31:39.855: INFO: Waiting up to 5m0s for pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81" in namespace "emptydir-678" to be "Succeeded or Failed"
    Dec 16 13:31:39.858: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955953ms
    Dec 16 13:31:41.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008619429s
    Dec 16 13:31:43.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009358455s
    STEP: Saw pod success 12/16/22 13:31:43.864
    Dec 16 13:31:43.864: INFO: Pod "pod-f2d6e404-5082-445c-ae16-2916c09e7d81" satisfied condition "Succeeded or Failed"
    Dec 16 13:31:43.868: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:31:43.877
    Dec 16 13:31:43.890: INFO: Waiting for pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 to disappear
    Dec 16 13:31:43.894: INFO: Pod pod-f2d6e404-5082-445c-ae16-2916c09e7d81 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:43.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-678" for this suite. 12/16/22 13:31:43.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:43.906
Dec 16 13:31:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:31:43.906
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:43.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:43.929
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-a3a99fc1-8d44-4197-ae5b-5b132442a84d 12/16/22 13:31:43.935
STEP: Creating secret with name s-test-opt-upd-f0176008-1b9c-43a0-b8b4-fcd68200a4dc 12/16/22 13:31:43.94
STEP: Creating the pod 12/16/22 13:31:43.946
Dec 16 13:31:43.954: INFO: Waiting up to 5m0s for pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867" in namespace "secrets-8428" to be "running and ready"
Dec 16 13:31:43.958: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867": Phase="Pending", Reason="", readiness=false. Elapsed: 3.986858ms
Dec 16 13:31:43.958: INFO: The phase of Pod pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:31:45.963: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867": Phase="Running", Reason="", readiness=true. Elapsed: 2.009638189s
Dec 16 13:31:45.963: INFO: The phase of Pod pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867 is Running (Ready = true)
Dec 16 13:31:45.963: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a3a99fc1-8d44-4197-ae5b-5b132442a84d 12/16/22 13:31:45.992
STEP: Updating secret s-test-opt-upd-f0176008-1b9c-43a0-b8b4-fcd68200a4dc 12/16/22 13:31:45.998
STEP: Creating secret with name s-test-opt-create-e4e410db-6733-4110-851b-273bd6745cbc 12/16/22 13:31:46.003
STEP: waiting to observe update in volume 12/16/22 13:31:46.007
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:48.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8428" for this suite. 12/16/22 13:31:48.045
------------------------------
• [4.144 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:43.906
    Dec 16 13:31:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:31:43.906
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:43.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:43.929
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-a3a99fc1-8d44-4197-ae5b-5b132442a84d 12/16/22 13:31:43.935
    STEP: Creating secret with name s-test-opt-upd-f0176008-1b9c-43a0-b8b4-fcd68200a4dc 12/16/22 13:31:43.94
    STEP: Creating the pod 12/16/22 13:31:43.946
    Dec 16 13:31:43.954: INFO: Waiting up to 5m0s for pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867" in namespace "secrets-8428" to be "running and ready"
    Dec 16 13:31:43.958: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867": Phase="Pending", Reason="", readiness=false. Elapsed: 3.986858ms
    Dec 16 13:31:43.958: INFO: The phase of Pod pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:31:45.963: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867": Phase="Running", Reason="", readiness=true. Elapsed: 2.009638189s
    Dec 16 13:31:45.963: INFO: The phase of Pod pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867 is Running (Ready = true)
    Dec 16 13:31:45.963: INFO: Pod "pod-secrets-776cd397-9e37-4c9a-a91d-cf76c7c32867" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a3a99fc1-8d44-4197-ae5b-5b132442a84d 12/16/22 13:31:45.992
    STEP: Updating secret s-test-opt-upd-f0176008-1b9c-43a0-b8b4-fcd68200a4dc 12/16/22 13:31:45.998
    STEP: Creating secret with name s-test-opt-create-e4e410db-6733-4110-851b-273bd6745cbc 12/16/22 13:31:46.003
    STEP: waiting to observe update in volume 12/16/22 13:31:46.007
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:48.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8428" for this suite. 12/16/22 13:31:48.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:48.05
Dec 16 13:31:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename tables 12/16/22 13:31:48.051
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:48.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:48.07
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2586" for this suite. 12/16/22 13:31:48.077
------------------------------
• [0.032 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:48.05
    Dec 16 13:31:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename tables 12/16/22 13:31:48.051
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:48.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:48.07
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2586" for this suite. 12/16/22 13:31:48.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:48.083
Dec 16 13:31:48.083: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:31:48.084
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:48.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:48.1
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-e474b11f-2c21-4daa-be5c-11da79f5accb 12/16/22 13:31:48.103
STEP: Creating a pod to test consume secrets 12/16/22 13:31:48.107
Dec 16 13:31:48.115: INFO: Waiting up to 5m0s for pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0" in namespace "secrets-3556" to be "Succeeded or Failed"
Dec 16 13:31:48.118: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175997ms
Dec 16 13:31:50.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008966159s
Dec 16 13:31:52.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008974208s
STEP: Saw pod success 12/16/22 13:31:52.124
Dec 16 13:31:52.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0" satisfied condition "Succeeded or Failed"
Dec 16 13:31:52.128: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:31:52.136
Dec 16 13:31:52.146: INFO: Waiting for pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 to disappear
Dec 16 13:31:52.149: INFO: Pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:52.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3556" for this suite. 12/16/22 13:31:52.153
------------------------------
• [4.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:48.083
    Dec 16 13:31:48.083: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:31:48.084
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:48.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:48.1
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-e474b11f-2c21-4daa-be5c-11da79f5accb 12/16/22 13:31:48.103
    STEP: Creating a pod to test consume secrets 12/16/22 13:31:48.107
    Dec 16 13:31:48.115: INFO: Waiting up to 5m0s for pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0" in namespace "secrets-3556" to be "Succeeded or Failed"
    Dec 16 13:31:48.118: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175997ms
    Dec 16 13:31:50.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008966159s
    Dec 16 13:31:52.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008974208s
    STEP: Saw pod success 12/16/22 13:31:52.124
    Dec 16 13:31:52.124: INFO: Pod "pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0" satisfied condition "Succeeded or Failed"
    Dec 16 13:31:52.128: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:31:52.136
    Dec 16 13:31:52.146: INFO: Waiting for pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 to disappear
    Dec 16 13:31:52.149: INFO: Pod pod-secrets-45667f09-f7b6-4362-9310-95a76b61fba0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:52.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3556" for this suite. 12/16/22 13:31:52.153
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:52.161
Dec 16 13:31:52.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 13:31:52.162
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:52.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:52.18
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/16/22 13:31:52.182
Dec 16 13:31:52.190: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 16 13:31:57.196: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 13:31:57.196
STEP: getting scale subresource 12/16/22 13:31:57.196
STEP: updating a scale subresource 12/16/22 13:31:57.199
STEP: verifying the replicaset Spec.Replicas was modified 12/16/22 13:31:57.204
STEP: Patch a scale subresource 12/16/22 13:31:57.207
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:57.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2515" for this suite. 12/16/22 13:31:57.22
------------------------------
• [SLOW TEST] [5.065 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:52.161
    Dec 16 13:31:52.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 13:31:52.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:52.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:52.18
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/16/22 13:31:52.182
    Dec 16 13:31:52.190: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 16 13:31:57.196: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 13:31:57.196
    STEP: getting scale subresource 12/16/22 13:31:57.196
    STEP: updating a scale subresource 12/16/22 13:31:57.199
    STEP: verifying the replicaset Spec.Replicas was modified 12/16/22 13:31:57.204
    STEP: Patch a scale subresource 12/16/22 13:31:57.207
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:57.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2515" for this suite. 12/16/22 13:31:57.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:57.226
Dec 16 13:31:57.226: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename certificates 12/16/22 13:31:57.227
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:57.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:57.242
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 12/16/22 13:31:57.981
STEP: getting /apis/certificates.k8s.io 12/16/22 13:31:57.984
STEP: getting /apis/certificates.k8s.io/v1 12/16/22 13:31:57.985
STEP: creating 12/16/22 13:31:57.986
STEP: getting 12/16/22 13:31:58
STEP: listing 12/16/22 13:31:58.002
STEP: watching 12/16/22 13:31:58.006
Dec 16 13:31:58.006: INFO: starting watch
STEP: patching 12/16/22 13:31:58.007
STEP: updating 12/16/22 13:31:58.013
Dec 16 13:31:58.018: INFO: waiting for watch events with expected annotations
Dec 16 13:31:58.018: INFO: saw patched and updated annotations
STEP: getting /approval 12/16/22 13:31:58.018
STEP: patching /approval 12/16/22 13:31:58.021
STEP: updating /approval 12/16/22 13:31:58.026
STEP: getting /status 12/16/22 13:31:58.032
STEP: patching /status 12/16/22 13:31:58.036
STEP: updating /status 12/16/22 13:31:58.043
STEP: deleting 12/16/22 13:31:58.049
STEP: deleting a collection 12/16/22 13:31:58.062
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:31:58.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-650" for this suite. 12/16/22 13:31:58.079
------------------------------
• [0.859 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:57.226
    Dec 16 13:31:57.226: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename certificates 12/16/22 13:31:57.227
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:57.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:57.242
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 12/16/22 13:31:57.981
    STEP: getting /apis/certificates.k8s.io 12/16/22 13:31:57.984
    STEP: getting /apis/certificates.k8s.io/v1 12/16/22 13:31:57.985
    STEP: creating 12/16/22 13:31:57.986
    STEP: getting 12/16/22 13:31:58
    STEP: listing 12/16/22 13:31:58.002
    STEP: watching 12/16/22 13:31:58.006
    Dec 16 13:31:58.006: INFO: starting watch
    STEP: patching 12/16/22 13:31:58.007
    STEP: updating 12/16/22 13:31:58.013
    Dec 16 13:31:58.018: INFO: waiting for watch events with expected annotations
    Dec 16 13:31:58.018: INFO: saw patched and updated annotations
    STEP: getting /approval 12/16/22 13:31:58.018
    STEP: patching /approval 12/16/22 13:31:58.021
    STEP: updating /approval 12/16/22 13:31:58.026
    STEP: getting /status 12/16/22 13:31:58.032
    STEP: patching /status 12/16/22 13:31:58.036
    STEP: updating /status 12/16/22 13:31:58.043
    STEP: deleting 12/16/22 13:31:58.049
    STEP: deleting a collection 12/16/22 13:31:58.062
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:31:58.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-650" for this suite. 12/16/22 13:31:58.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:31:58.087
Dec 16 13:31:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 13:31:58.087
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:58.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:58.105
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-7bjsc" 12/16/22 13:31:58.108
Dec 16 13:31:58.112: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
Dec 16 13:31:59.115: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
Dec 16 13:31:59.120: INFO: Found 1 replicas for "e2e-rc-7bjsc" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-7bjsc" 12/16/22 13:31:59.12
STEP: Updating a scale subresource 12/16/22 13:31:59.123
STEP: Verifying replicas where modified for replication controller "e2e-rc-7bjsc" 12/16/22 13:31:59.129
Dec 16 13:31:59.129: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
Dec 16 13:32:00.133: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
Dec 16 13:32:00.139: INFO: Found 2 replicas for "e2e-rc-7bjsc" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:00.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1488" for this suite. 12/16/22 13:32:00.143
------------------------------
• [2.062 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:31:58.087
    Dec 16 13:31:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 13:31:58.087
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:31:58.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:31:58.105
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-7bjsc" 12/16/22 13:31:58.108
    Dec 16 13:31:58.112: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
    Dec 16 13:31:59.115: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
    Dec 16 13:31:59.120: INFO: Found 1 replicas for "e2e-rc-7bjsc" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-7bjsc" 12/16/22 13:31:59.12
    STEP: Updating a scale subresource 12/16/22 13:31:59.123
    STEP: Verifying replicas where modified for replication controller "e2e-rc-7bjsc" 12/16/22 13:31:59.129
    Dec 16 13:31:59.129: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
    Dec 16 13:32:00.133: INFO: Get Replication Controller "e2e-rc-7bjsc" to confirm replicas
    Dec 16 13:32:00.139: INFO: Found 2 replicas for "e2e-rc-7bjsc" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:00.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1488" for this suite. 12/16/22 13:32:00.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:00.149
Dec 16 13:32:00.149: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:32:00.15
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:00.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:00.166
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2203/configmap-test-074b94f1-d101-4a0b-aa91-7579a28067fa 12/16/22 13:32:00.168
STEP: Creating a pod to test consume configMaps 12/16/22 13:32:00.176
Dec 16 13:32:00.185: INFO: Waiting up to 5m0s for pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52" in namespace "configmap-2203" to be "Succeeded or Failed"
Dec 16 13:32:00.188: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777423ms
Dec 16 13:32:02.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007605824s
Dec 16 13:32:04.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008206064s
STEP: Saw pod success 12/16/22 13:32:04.193
Dec 16 13:32:04.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52" satisfied condition "Succeeded or Failed"
Dec 16 13:32:04.198: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 container env-test: <nil>
STEP: delete the pod 12/16/22 13:32:04.205
Dec 16 13:32:04.218: INFO: Waiting for pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 to disappear
Dec 16 13:32:04.221: INFO: Pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:04.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2203" for this suite. 12/16/22 13:32:04.225
------------------------------
• [4.081 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:00.149
    Dec 16 13:32:00.149: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:32:00.15
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:00.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:00.166
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2203/configmap-test-074b94f1-d101-4a0b-aa91-7579a28067fa 12/16/22 13:32:00.168
    STEP: Creating a pod to test consume configMaps 12/16/22 13:32:00.176
    Dec 16 13:32:00.185: INFO: Waiting up to 5m0s for pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52" in namespace "configmap-2203" to be "Succeeded or Failed"
    Dec 16 13:32:00.188: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777423ms
    Dec 16 13:32:02.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007605824s
    Dec 16 13:32:04.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008206064s
    STEP: Saw pod success 12/16/22 13:32:04.193
    Dec 16 13:32:04.193: INFO: Pod "pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52" satisfied condition "Succeeded or Failed"
    Dec 16 13:32:04.198: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 container env-test: <nil>
    STEP: delete the pod 12/16/22 13:32:04.205
    Dec 16 13:32:04.218: INFO: Waiting for pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 to disappear
    Dec 16 13:32:04.221: INFO: Pod pod-configmaps-56da88ce-ba50-4a34-8f5c-0df2827dee52 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:04.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2203" for this suite. 12/16/22 13:32:04.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:04.232
Dec 16 13:32:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 13:32:04.233
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:04.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:04.252
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 12/16/22 13:32:04.258
STEP: Verify that the required pods have come up. 12/16/22 13:32:04.263
Dec 16 13:32:04.267: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 16 13:32:09.271: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 13:32:09.271
STEP: Getting /status 12/16/22 13:32:09.271
Dec 16 13:32:09.277: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 12/16/22 13:32:09.277
Dec 16 13:32:09.286: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 12/16/22 13:32:09.286
Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: ADDED
Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.288: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.288: INFO: Found replicaset test-rs in namespace replicaset-1842 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 16 13:32:09.288: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 12/16/22 13:32:09.288
Dec 16 13:32:09.288: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 16 13:32:09.295: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 12/16/22 13:32:09.295
Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: ADDED
Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.297: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.297: INFO: Observed replicaset test-rs in namespace replicaset-1842 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 16 13:32:09.297: INFO: Observed &ReplicaSet event: MODIFIED
Dec 16 13:32:09.297: INFO: Found replicaset test-rs in namespace replicaset-1842 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec 16 13:32:09.297: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:09.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1842" for this suite. 12/16/22 13:32:09.3
------------------------------
• [SLOW TEST] [5.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:04.232
    Dec 16 13:32:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 13:32:04.233
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:04.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:04.252
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 12/16/22 13:32:04.258
    STEP: Verify that the required pods have come up. 12/16/22 13:32:04.263
    Dec 16 13:32:04.267: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 16 13:32:09.271: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 13:32:09.271
    STEP: Getting /status 12/16/22 13:32:09.271
    Dec 16 13:32:09.277: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 12/16/22 13:32:09.277
    Dec 16 13:32:09.286: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 12/16/22 13:32:09.286
    Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: ADDED
    Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.287: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.288: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.288: INFO: Found replicaset test-rs in namespace replicaset-1842 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 16 13:32:09.288: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 12/16/22 13:32:09.288
    Dec 16 13:32:09.288: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 16 13:32:09.295: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 12/16/22 13:32:09.295
    Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: ADDED
    Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.296: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.297: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.297: INFO: Observed replicaset test-rs in namespace replicaset-1842 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 16 13:32:09.297: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 16 13:32:09.297: INFO: Found replicaset test-rs in namespace replicaset-1842 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Dec 16 13:32:09.297: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:09.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1842" for this suite. 12/16/22 13:32:09.3
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:09.307
Dec 16 13:32:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:32:09.308
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:09.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:09.326
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/16/22 13:32:09.328
Dec 16 13:32:09.336: INFO: Waiting up to 5m0s for pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0" in namespace "emptydir-2553" to be "Succeeded or Failed"
Dec 16 13:32:09.339: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760159ms
Dec 16 13:32:11.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008234434s
Dec 16 13:32:13.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008525062s
STEP: Saw pod success 12/16/22 13:32:13.345
Dec 16 13:32:13.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0" satisfied condition "Succeeded or Failed"
Dec 16 13:32:13.348: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 container test-container: <nil>
STEP: delete the pod 12/16/22 13:32:13.356
Dec 16 13:32:13.368: INFO: Waiting for pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 to disappear
Dec 16 13:32:13.371: INFO: Pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:13.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2553" for this suite. 12/16/22 13:32:13.374
------------------------------
• [4.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:09.307
    Dec 16 13:32:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:32:09.308
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:09.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:09.326
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/16/22 13:32:09.328
    Dec 16 13:32:09.336: INFO: Waiting up to 5m0s for pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0" in namespace "emptydir-2553" to be "Succeeded or Failed"
    Dec 16 13:32:09.339: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760159ms
    Dec 16 13:32:11.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008234434s
    Dec 16 13:32:13.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008525062s
    STEP: Saw pod success 12/16/22 13:32:13.345
    Dec 16 13:32:13.345: INFO: Pod "pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0" satisfied condition "Succeeded or Failed"
    Dec 16 13:32:13.348: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:32:13.356
    Dec 16 13:32:13.368: INFO: Waiting for pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 to disappear
    Dec 16 13:32:13.371: INFO: Pod pod-1b0e50fc-893f-4886-9aad-52d2c3a32dd0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:13.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2553" for this suite. 12/16/22 13:32:13.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:13.381
Dec 16 13:32:13.381: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:32:13.382
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:13.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:13.398
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 12/16/22 13:32:13.4
STEP: Creating a ResourceQuota 12/16/22 13:32:18.404
STEP: Ensuring resource quota status is calculated 12/16/22 13:32:18.41
STEP: Creating a Service 12/16/22 13:32:20.416
STEP: Creating a NodePort Service 12/16/22 13:32:20.448
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/16/22 13:32:20.479
STEP: Ensuring resource quota status captures service creation 12/16/22 13:32:20.507
STEP: Deleting Services 12/16/22 13:32:22.513
STEP: Ensuring resource quota status released usage 12/16/22 13:32:22.55
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:24.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2648" for this suite. 12/16/22 13:32:24.56
------------------------------
• [SLOW TEST] [11.186 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:13.381
    Dec 16 13:32:13.381: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:32:13.382
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:13.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:13.398
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 12/16/22 13:32:13.4
    STEP: Creating a ResourceQuota 12/16/22 13:32:18.404
    STEP: Ensuring resource quota status is calculated 12/16/22 13:32:18.41
    STEP: Creating a Service 12/16/22 13:32:20.416
    STEP: Creating a NodePort Service 12/16/22 13:32:20.448
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/16/22 13:32:20.479
    STEP: Ensuring resource quota status captures service creation 12/16/22 13:32:20.507
    STEP: Deleting Services 12/16/22 13:32:22.513
    STEP: Ensuring resource quota status released usage 12/16/22 13:32:22.55
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:24.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2648" for this suite. 12/16/22 13:32:24.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:24.568
Dec 16 13:32:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:32:24.568
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:24.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:24.588
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 12/16/22 13:32:24.59
Dec 16 13:32:24.590: INFO: Creating e2e-svc-a-dmjzv
Dec 16 13:32:24.606: INFO: Creating e2e-svc-b-45csd
Dec 16 13:32:24.694: INFO: Creating e2e-svc-c-t28p4
STEP: deleting service collection 12/16/22 13:32:24.721
Dec 16 13:32:24.761: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:24.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3525" for this suite. 12/16/22 13:32:24.767
------------------------------
• [0.207 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:24.568
    Dec 16 13:32:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:32:24.568
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:24.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:24.588
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 12/16/22 13:32:24.59
    Dec 16 13:32:24.590: INFO: Creating e2e-svc-a-dmjzv
    Dec 16 13:32:24.606: INFO: Creating e2e-svc-b-45csd
    Dec 16 13:32:24.694: INFO: Creating e2e-svc-c-t28p4
    STEP: deleting service collection 12/16/22 13:32:24.721
    Dec 16 13:32:24.761: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:24.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3525" for this suite. 12/16/22 13:32:24.767
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:24.775
Dec 16 13:32:24.775: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:32:24.775
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:24.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:24.796
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:32:24.809
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:32:25.11
STEP: Deploying the webhook pod 12/16/22 13:32:25.118
STEP: Wait for the deployment to be ready 12/16/22 13:32:25.129
Dec 16 13:32:25.137: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/16/22 13:32:27.149
STEP: Verifying the service has paired with the endpoint 12/16/22 13:32:27.163
Dec 16 13:32:28.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 12/16/22 13:32:28.213
Dec 16 13:32:33.284: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:32:33.46
STEP: Deleting the collection of validation webhooks 12/16/22 13:32:33.555
STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:32:33.602
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:32:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6024" for this suite. 12/16/22 13:32:33.656
STEP: Destroying namespace "webhook-6024-markers" for this suite. 12/16/22 13:32:33.661
------------------------------
• [SLOW TEST] [8.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:24.775
    Dec 16 13:32:24.775: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:32:24.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:24.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:24.796
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:32:24.809
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:32:25.11
    STEP: Deploying the webhook pod 12/16/22 13:32:25.118
    STEP: Wait for the deployment to be ready 12/16/22 13:32:25.129
    Dec 16 13:32:25.137: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/16/22 13:32:27.149
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:32:27.163
    Dec 16 13:32:28.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 12/16/22 13:32:28.213
    Dec 16 13:32:33.284: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:32:33.46
    STEP: Deleting the collection of validation webhooks 12/16/22 13:32:33.555
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/16/22 13:32:33.602
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:32:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6024" for this suite. 12/16/22 13:32:33.656
    STEP: Destroying namespace "webhook-6024-markers" for this suite. 12/16/22 13:32:33.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:32:33.667
Dec 16 13:32:33.667: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:32:33.668
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:33.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:33.684
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-kc7g8" 12/16/22 13:32:33.689
Dec 16 13:32:33.696: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard cpu limit of 500m
Dec 16 13:32:33.696: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.696
STEP: Confirm /status for "e2e-rq-status-kc7g8" resourceQuota via watch 12/16/22 13:32:33.703
Dec 16 13:32:33.704: INFO: observed resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList(nil)
Dec 16 13:32:33.704: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec 16 13:32:33.705: INFO: ResourceQuota "e2e-rq-status-kc7g8" /status was updated
STEP: Patching hard spec values for cpu & memory 12/16/22 13:32:33.708
Dec 16 13:32:33.715: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard cpu limit of 1
Dec 16 13:32:33.715: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.715
STEP: Confirm /status for "e2e-rq-status-kc7g8" resourceQuota via watch 12/16/22 13:32:33.722
Dec 16 13:32:33.723: INFO: observed resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec 16 13:32:33.723: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Dec 16 13:32:33.723: INFO: ResourceQuota "e2e-rq-status-kc7g8" /status was patched
STEP: Get "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.723
Dec 16 13:32:33.727: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard cpu of 1
Dec 16 13:32:33.727: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-kc7g8" /status before checking Spec is unchanged 12/16/22 13:32:33.731
Dec 16 13:32:33.736: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard cpu of 2
Dec 16 13:32:33.736: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard memory of 2Gi
Dec 16 13:32:33.738: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Dec 16 13:33:48.746: INFO: ResourceQuota "e2e-rq-status-kc7g8" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:33:48.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6898" for this suite. 12/16/22 13:33:48.75
------------------------------
• [SLOW TEST] [75.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:32:33.667
    Dec 16 13:32:33.667: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:32:33.668
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:32:33.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:32:33.684
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-kc7g8" 12/16/22 13:32:33.689
    Dec 16 13:32:33.696: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard cpu limit of 500m
    Dec 16 13:32:33.696: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.696
    STEP: Confirm /status for "e2e-rq-status-kc7g8" resourceQuota via watch 12/16/22 13:32:33.703
    Dec 16 13:32:33.704: INFO: observed resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList(nil)
    Dec 16 13:32:33.704: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec 16 13:32:33.705: INFO: ResourceQuota "e2e-rq-status-kc7g8" /status was updated
    STEP: Patching hard spec values for cpu & memory 12/16/22 13:32:33.708
    Dec 16 13:32:33.715: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard cpu limit of 1
    Dec 16 13:32:33.715: INFO: Resource quota "e2e-rq-status-kc7g8" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.715
    STEP: Confirm /status for "e2e-rq-status-kc7g8" resourceQuota via watch 12/16/22 13:32:33.722
    Dec 16 13:32:33.723: INFO: observed resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec 16 13:32:33.723: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Dec 16 13:32:33.723: INFO: ResourceQuota "e2e-rq-status-kc7g8" /status was patched
    STEP: Get "e2e-rq-status-kc7g8" /status 12/16/22 13:32:33.723
    Dec 16 13:32:33.727: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard cpu of 1
    Dec 16 13:32:33.727: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-kc7g8" /status before checking Spec is unchanged 12/16/22 13:32:33.731
    Dec 16 13:32:33.736: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard cpu of 2
    Dec 16 13:32:33.736: INFO: Resourcequota "e2e-rq-status-kc7g8" reports status: hard memory of 2Gi
    Dec 16 13:32:33.738: INFO: Found resourceQuota "e2e-rq-status-kc7g8" in namespace "resourcequota-6898" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Dec 16 13:33:48.746: INFO: ResourceQuota "e2e-rq-status-kc7g8" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:33:48.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6898" for this suite. 12/16/22 13:33:48.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:33:48.759
Dec 16 13:33:48.759: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:33:48.759
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:33:48.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:33:48.777
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-df8bce98-4f21-4aa9-b4f7-f0868b232920 12/16/22 13:33:48.779
STEP: Creating a pod to test consume configMaps 12/16/22 13:33:48.784
Dec 16 13:33:48.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2" in namespace "projected-8561" to be "Succeeded or Failed"
Dec 16 13:33:48.795: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283501ms
Dec 16 13:33:50.800: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008441181s
Dec 16 13:33:52.801: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008860594s
STEP: Saw pod success 12/16/22 13:33:52.801
Dec 16 13:33:52.801: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2" satisfied condition "Succeeded or Failed"
Dec 16 13:33:52.804: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:33:52.855
Dec 16 13:33:52.866: INFO: Waiting for pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 to disappear
Dec 16 13:33:52.869: INFO: Pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:33:52.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8561" for this suite. 12/16/22 13:33:52.873
------------------------------
• [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:33:48.759
    Dec 16 13:33:48.759: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:33:48.759
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:33:48.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:33:48.777
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-df8bce98-4f21-4aa9-b4f7-f0868b232920 12/16/22 13:33:48.779
    STEP: Creating a pod to test consume configMaps 12/16/22 13:33:48.784
    Dec 16 13:33:48.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2" in namespace "projected-8561" to be "Succeeded or Failed"
    Dec 16 13:33:48.795: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283501ms
    Dec 16 13:33:50.800: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008441181s
    Dec 16 13:33:52.801: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008860594s
    STEP: Saw pod success 12/16/22 13:33:52.801
    Dec 16 13:33:52.801: INFO: Pod "pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2" satisfied condition "Succeeded or Failed"
    Dec 16 13:33:52.804: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:33:52.855
    Dec 16 13:33:52.866: INFO: Waiting for pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 to disappear
    Dec 16 13:33:52.869: INFO: Pod pod-projected-configmaps-403cfe41-2182-442d-a576-02de52c409e2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:33:52.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8561" for this suite. 12/16/22 13:33:52.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:33:52.879
Dec 16 13:33:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename taint-single-pod 12/16/22 13:33:52.88
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:33:52.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:33:52.896
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Dec 16 13:33:52.898: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:34:52.916: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Dec 16 13:34:52.921: INFO: Starting informer...
STEP: Starting pod... 12/16/22 13:34:52.921
Dec 16 13:34:53.135: INFO: Pod is running on pool-a3802-fsxxd. Tainting Node
STEP: Trying to apply a taint on the Node 12/16/22 13:34:53.135
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:34:53.147
STEP: Waiting short time to make sure Pod is queued for deletion 12/16/22 13:34:53.151
Dec 16 13:34:53.151: INFO: Pod wasn't evicted. Proceeding
Dec 16 13:34:53.151: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:34:53.161
STEP: Waiting some time to make sure that toleration time passed. 12/16/22 13:34:53.165
Dec 16 13:36:08.165: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-7187" for this suite. 12/16/22 13:36:08.17
------------------------------
• [SLOW TEST] [135.298 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:33:52.879
    Dec 16 13:33:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename taint-single-pod 12/16/22 13:33:52.88
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:33:52.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:33:52.896
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Dec 16 13:33:52.898: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:34:52.916: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Dec 16 13:34:52.921: INFO: Starting informer...
    STEP: Starting pod... 12/16/22 13:34:52.921
    Dec 16 13:34:53.135: INFO: Pod is running on pool-a3802-fsxxd. Tainting Node
    STEP: Trying to apply a taint on the Node 12/16/22 13:34:53.135
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:34:53.147
    STEP: Waiting short time to make sure Pod is queued for deletion 12/16/22 13:34:53.151
    Dec 16 13:34:53.151: INFO: Pod wasn't evicted. Proceeding
    Dec 16 13:34:53.151: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:34:53.161
    STEP: Waiting some time to make sure that toleration time passed. 12/16/22 13:34:53.165
    Dec 16 13:36:08.165: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-7187" for this suite. 12/16/22 13:36:08.17
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:08.178
Dec 16 13:36:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:36:08.179
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:08.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:08.195
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-05ea6c99-2f25-473b-b7f1-ce8f4f03a942 12/16/22 13:36:08.197
STEP: Creating a pod to test consume configMaps 12/16/22 13:36:08.202
Dec 16 13:36:08.209: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922" in namespace "configmap-8323" to be "Succeeded or Failed"
Dec 16 13:36:08.212: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604974ms
Dec 16 13:36:10.217: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007873316s
Dec 16 13:36:12.218: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008385091s
STEP: Saw pod success 12/16/22 13:36:12.218
Dec 16 13:36:12.218: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922" satisfied condition "Succeeded or Failed"
Dec 16 13:36:12.221: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:36:12.273
Dec 16 13:36:12.286: INFO: Waiting for pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 to disappear
Dec 16 13:36:12.289: INFO: Pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:12.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8323" for this suite. 12/16/22 13:36:12.293
------------------------------
• [4.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:08.178
    Dec 16 13:36:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:36:08.179
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:08.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:08.195
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-05ea6c99-2f25-473b-b7f1-ce8f4f03a942 12/16/22 13:36:08.197
    STEP: Creating a pod to test consume configMaps 12/16/22 13:36:08.202
    Dec 16 13:36:08.209: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922" in namespace "configmap-8323" to be "Succeeded or Failed"
    Dec 16 13:36:08.212: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604974ms
    Dec 16 13:36:10.217: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007873316s
    Dec 16 13:36:12.218: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008385091s
    STEP: Saw pod success 12/16/22 13:36:12.218
    Dec 16 13:36:12.218: INFO: Pod "pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922" satisfied condition "Succeeded or Failed"
    Dec 16 13:36:12.221: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:36:12.273
    Dec 16 13:36:12.286: INFO: Waiting for pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 to disappear
    Dec 16 13:36:12.289: INFO: Pod pod-configmaps-f3261eb5-bbce-4ba2-b85b-0f51405e3922 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:12.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8323" for this suite. 12/16/22 13:36:12.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:12.298
Dec 16 13:36:12.298: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename init-container 12/16/22 13:36:12.298
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:12.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:12.314
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 12/16/22 13:36:12.316
Dec 16 13:36:12.316: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:17.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8173" for this suite. 12/16/22 13:36:17.528
------------------------------
• [SLOW TEST] [5.237 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:12.298
    Dec 16 13:36:12.298: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename init-container 12/16/22 13:36:12.298
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:12.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:12.314
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 12/16/22 13:36:12.316
    Dec 16 13:36:12.316: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:17.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8173" for this suite. 12/16/22 13:36:17.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:17.535
Dec 16 13:36:17.535: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename containers 12/16/22 13:36:17.536
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:17.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:17.551
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 12/16/22 13:36:17.554
Dec 16 13:36:17.562: INFO: Waiting up to 5m0s for pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5" in namespace "containers-6151" to be "Succeeded or Failed"
Dec 16 13:36:17.565: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476393ms
Dec 16 13:36:19.570: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008131696s
Dec 16 13:36:21.572: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009744559s
STEP: Saw pod success 12/16/22 13:36:21.572
Dec 16 13:36:21.572: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5" satisfied condition "Succeeded or Failed"
Dec 16 13:36:21.576: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:36:21.585
Dec 16 13:36:21.598: INFO: Waiting for pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 to disappear
Dec 16 13:36:21.600: INFO: Pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:21.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6151" for this suite. 12/16/22 13:36:21.605
------------------------------
• [4.076 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:17.535
    Dec 16 13:36:17.535: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename containers 12/16/22 13:36:17.536
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:17.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:17.551
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 12/16/22 13:36:17.554
    Dec 16 13:36:17.562: INFO: Waiting up to 5m0s for pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5" in namespace "containers-6151" to be "Succeeded or Failed"
    Dec 16 13:36:17.565: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476393ms
    Dec 16 13:36:19.570: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008131696s
    Dec 16 13:36:21.572: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009744559s
    STEP: Saw pod success 12/16/22 13:36:21.572
    Dec 16 13:36:21.572: INFO: Pod "client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5" satisfied condition "Succeeded or Failed"
    Dec 16 13:36:21.576: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:36:21.585
    Dec 16 13:36:21.598: INFO: Waiting for pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 to disappear
    Dec 16 13:36:21.600: INFO: Pod client-containers-2b561e15-be20-4b4c-85ce-06db0b74b0d5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:21.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6151" for this suite. 12/16/22 13:36:21.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:21.612
Dec 16 13:36:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 13:36:21.612
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:21.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:21.632
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 12/16/22 13:36:21.635
Dec 16 13:36:21.643: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2849" to be "running and ready"
Dec 16 13:36:21.646: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112431ms
Dec 16 13:36:21.646: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:36:23.652: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254786s
Dec 16 13:36:23.652: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Dec 16 13:36:23.652: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 12/16/22 13:36:23.656
STEP: Then the orphan pod is adopted 12/16/22 13:36:23.662
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2849" for this suite. 12/16/22 13:36:24.675
------------------------------
• [3.070 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:21.612
    Dec 16 13:36:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 13:36:21.612
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:21.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:21.632
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 12/16/22 13:36:21.635
    Dec 16 13:36:21.643: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2849" to be "running and ready"
    Dec 16 13:36:21.646: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112431ms
    Dec 16 13:36:21.646: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:36:23.652: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254786s
    Dec 16 13:36:23.652: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Dec 16 13:36:23.652: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 12/16/22 13:36:23.656
    STEP: Then the orphan pod is adopted 12/16/22 13:36:23.662
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2849" for this suite. 12/16/22 13:36:24.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:24.682
Dec 16 13:36:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:36:24.684
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:24.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:24.703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 12/16/22 13:36:24.706
Dec 16 13:36:24.706: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: rename a version 12/16/22 13:36:28.864
STEP: check the new version name is served 12/16/22 13:36:28.888
STEP: check the old version name is removed 12/16/22 13:36:30.174
STEP: check the other version is not changed 12/16/22 13:36:30.946
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8782" for this suite. 12/16/22 13:36:34.302
------------------------------
• [SLOW TEST] [9.626 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:24.682
    Dec 16 13:36:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:36:24.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:24.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:24.703
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 12/16/22 13:36:24.706
    Dec 16 13:36:24.706: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: rename a version 12/16/22 13:36:28.864
    STEP: check the new version name is served 12/16/22 13:36:28.888
    STEP: check the old version name is removed 12/16/22 13:36:30.174
    STEP: check the other version is not changed 12/16/22 13:36:30.946
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8782" for this suite. 12/16/22 13:36:34.302
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:34.308
Dec 16 13:36:34.308: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename init-container 12/16/22 13:36:34.308
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:34.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:34.326
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 12/16/22 13:36:34.328
Dec 16 13:36:34.328: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:37.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9814" for this suite. 12/16/22 13:36:37.575
------------------------------
• [3.275 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:34.308
    Dec 16 13:36:34.308: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename init-container 12/16/22 13:36:34.308
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:34.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:34.326
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 12/16/22 13:36:34.328
    Dec 16 13:36:34.328: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:37.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9814" for this suite. 12/16/22 13:36:37.575
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:37.583
Dec 16 13:36:37.583: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:36:37.584
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:37.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:37.602
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 12/16/22 13:36:37.604
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:37.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5057" for this suite. 12/16/22 13:36:37.612
------------------------------
• [0.036 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:37.583
    Dec 16 13:36:37.583: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:36:37.584
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:37.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:37.602
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 12/16/22 13:36:37.604
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:37.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5057" for this suite. 12/16/22 13:36:37.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:37.62
Dec 16 13:36:37.620: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 13:36:37.62
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:37.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:37.689
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Dec 16 13:36:37.691: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 16 13:36:37.705: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 16 13:36:42.711: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 13:36:42.711
Dec 16 13:36:42.711: INFO: Creating deployment "test-rolling-update-deployment"
Dec 16 13:36:42.717: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 16 13:36:42.723: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 16 13:36:44.733: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 16 13:36:44.736: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 13:36:44.747: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2566  5e0323f6-1c4b-4287-861c-3a24950c8dbc 680916475 1 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005906478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 13:36:42 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-16 13:36:43 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 16 13:36:44.750: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2566  ab00338d-22a2-4f04-8a2c-9e16a0d9724e 680916462 1 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5e0323f6-1c4b-4287-861c-3a24950c8dbc 0xc005906a67 0xc005906a68}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0323f6-1c4b-4287-861c-3a24950c8dbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005906b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 13:36:44.750: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 16 13:36:44.750: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2566  e4cea2af-c64f-40db-8346-059dac9ab6a4 680916474 2 2022-12-16 13:36:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5e0323f6-1c4b-4287-861c-3a24950c8dbc 0xc0059068f7 0xc0059068f8}] [] [{e2e.test Update apps/v1 2022-12-16 13:36:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0323f6-1c4b-4287-861c-3a24950c8dbc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0059069b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 13:36:44.754: INFO: Pod "test-rolling-update-deployment-7549d9f46d-mbkww" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-mbkww test-rolling-update-deployment-7549d9f46d- deployment-2566  6398f1ba-0980-4f21-b26b-77614752add7 680916461 0 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:e22608b7a883b64721bb36aad0cf418dadedd348c8d31e3781676c8a190c40d5 cni.projectcalico.org/podIP:192.168.189.21/32 cni.projectcalico.org/podIPs:192.168.189.21/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ab00338d-22a2-4f04-8a2c-9e16a0d9724e 0xc005906fe7 0xc005906fe8}] [] [{kube-controller-manager Update v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab00338d-22a2-4f04-8a2c-9e16a0d9724e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzzqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.21,StartTime:2022-12-16 13:36:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:36:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3c4da88e38ea9bf7c93353a75cccc79f769ea6dd82730c4a4fc955067e9fc2e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:44.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2566" for this suite. 12/16/22 13:36:44.758
------------------------------
• [SLOW TEST] [7.144 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:37.62
    Dec 16 13:36:37.620: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 13:36:37.62
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:37.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:37.689
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Dec 16 13:36:37.691: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Dec 16 13:36:37.705: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 16 13:36:42.711: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 13:36:42.711
    Dec 16 13:36:42.711: INFO: Creating deployment "test-rolling-update-deployment"
    Dec 16 13:36:42.717: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Dec 16 13:36:42.723: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Dec 16 13:36:44.733: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Dec 16 13:36:44.736: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 13:36:44.747: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2566  5e0323f6-1c4b-4287-861c-3a24950c8dbc 680916475 1 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005906478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 13:36:42 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-16 13:36:43 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 16 13:36:44.750: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2566  ab00338d-22a2-4f04-8a2c-9e16a0d9724e 680916462 1 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5e0323f6-1c4b-4287-861c-3a24950c8dbc 0xc005906a67 0xc005906a68}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0323f6-1c4b-4287-861c-3a24950c8dbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005906b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 13:36:44.750: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Dec 16 13:36:44.750: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2566  e4cea2af-c64f-40db-8346-059dac9ab6a4 680916474 2 2022-12-16 13:36:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5e0323f6-1c4b-4287-861c-3a24950c8dbc 0xc0059068f7 0xc0059068f8}] [] [{e2e.test Update apps/v1 2022-12-16 13:36:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0323f6-1c4b-4287-861c-3a24950c8dbc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0059069b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 13:36:44.754: INFO: Pod "test-rolling-update-deployment-7549d9f46d-mbkww" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-mbkww test-rolling-update-deployment-7549d9f46d- deployment-2566  6398f1ba-0980-4f21-b26b-77614752add7 680916461 0 2022-12-16 13:36:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:e22608b7a883b64721bb36aad0cf418dadedd348c8d31e3781676c8a190c40d5 cni.projectcalico.org/podIP:192.168.189.21/32 cni.projectcalico.org/podIPs:192.168.189.21/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ab00338d-22a2-4f04-8a2c-9e16a0d9724e 0xc005906fe7 0xc005906fe8}] [] [{kube-controller-manager Update v1 2022-12-16 13:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab00338d-22a2-4f04-8a2c-9e16a0d9724e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:36:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzzqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:36:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.21,StartTime:2022-12-16 13:36:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:36:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3c4da88e38ea9bf7c93353a75cccc79f769ea6dd82730c4a4fc955067e9fc2e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:44.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2566" for this suite. 12/16/22 13:36:44.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:44.765
Dec 16 13:36:44.765: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:36:44.766
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:44.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:44.783
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 12/16/22 13:36:44.785
STEP: submitting the pod to kubernetes 12/16/22 13:36:44.785
Dec 16 13:36:44.792: INFO: Waiting up to 5m0s for pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" in namespace "pods-2717" to be "running and ready"
Dec 16 13:36:44.795: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.77484ms
Dec 16 13:36:44.795: INFO: The phase of Pod pod-update-f802c945-f58b-479c-a230-b231c77f6cb2 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:36:46.800: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007709057s
Dec 16 13:36:46.800: INFO: The phase of Pod pod-update-f802c945-f58b-479c-a230-b231c77f6cb2 is Running (Ready = true)
Dec 16 13:36:46.800: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/16/22 13:36:46.804
STEP: updating the pod 12/16/22 13:36:46.808
Dec 16 13:36:47.321: INFO: Successfully updated pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2"
Dec 16 13:36:47.321: INFO: Waiting up to 5m0s for pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" in namespace "pods-2717" to be "running"
Dec 16 13:36:47.324: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Running", Reason="", readiness=true. Elapsed: 3.423786ms
Dec 16 13:36:47.324: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 12/16/22 13:36:47.324
Dec 16 13:36:47.328: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:47.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2717" for this suite. 12/16/22 13:36:47.332
------------------------------
• [2.574 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:44.765
    Dec 16 13:36:44.765: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:36:44.766
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:44.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:44.783
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 12/16/22 13:36:44.785
    STEP: submitting the pod to kubernetes 12/16/22 13:36:44.785
    Dec 16 13:36:44.792: INFO: Waiting up to 5m0s for pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" in namespace "pods-2717" to be "running and ready"
    Dec 16 13:36:44.795: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.77484ms
    Dec 16 13:36:44.795: INFO: The phase of Pod pod-update-f802c945-f58b-479c-a230-b231c77f6cb2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:36:46.800: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007709057s
    Dec 16 13:36:46.800: INFO: The phase of Pod pod-update-f802c945-f58b-479c-a230-b231c77f6cb2 is Running (Ready = true)
    Dec 16 13:36:46.800: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/16/22 13:36:46.804
    STEP: updating the pod 12/16/22 13:36:46.808
    Dec 16 13:36:47.321: INFO: Successfully updated pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2"
    Dec 16 13:36:47.321: INFO: Waiting up to 5m0s for pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" in namespace "pods-2717" to be "running"
    Dec 16 13:36:47.324: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2": Phase="Running", Reason="", readiness=true. Elapsed: 3.423786ms
    Dec 16 13:36:47.324: INFO: Pod "pod-update-f802c945-f58b-479c-a230-b231c77f6cb2" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 12/16/22 13:36:47.324
    Dec 16 13:36:47.328: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:47.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2717" for this suite. 12/16/22 13:36:47.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:47.34
Dec 16 13:36:47.340: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 13:36:47.341
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:47.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:47.359
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659 12/16/22 13:36:47.361
Dec 16 13:36:47.369: INFO: Pod name my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Found 0 pods out of 1
Dec 16 13:36:52.375: INFO: Pod name my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Found 1 pods out of 1
Dec 16 13:36:52.375: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659" are running
Dec 16 13:36:52.375: INFO: Waiting up to 5m0s for pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" in namespace "replication-controller-7469" to be "running"
Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6": Phase="Running", Reason="", readiness=true. Elapsed: 3.526556ms
Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" satisfied condition "running"
Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:47 +0000 UTC Reason: Message:}])
Dec 16 13:36:52.379: INFO: Trying to dial the pod
Dec 16 13:36:57.424: INFO: Controller my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Got expected result from replica 1 [my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6]: "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:36:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7469" for this suite. 12/16/22 13:36:57.429
------------------------------
• [SLOW TEST] [10.095 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:47.34
    Dec 16 13:36:47.340: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 13:36:47.341
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:47.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:47.359
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659 12/16/22 13:36:47.361
    Dec 16 13:36:47.369: INFO: Pod name my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Found 0 pods out of 1
    Dec 16 13:36:52.375: INFO: Pod name my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Found 1 pods out of 1
    Dec 16 13:36:52.375: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659" are running
    Dec 16 13:36:52.375: INFO: Waiting up to 5m0s for pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" in namespace "replication-controller-7469" to be "running"
    Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6": Phase="Running", Reason="", readiness=true. Elapsed: 3.526556ms
    Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" satisfied condition "running"
    Dec 16 13:36:52.379: INFO: Pod "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-16 13:36:47 +0000 UTC Reason: Message:}])
    Dec 16 13:36:52.379: INFO: Trying to dial the pod
    Dec 16 13:36:57.424: INFO: Controller my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659: Got expected result from replica 1 [my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6]: "my-hostname-basic-22ee6fba-06fe-41d7-8e1a-f4236f1c8659-6tzt6", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:36:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7469" for this suite. 12/16/22 13:36:57.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:36:57.436
Dec 16 13:36:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:36:57.436
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:57.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:57.463
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 12/16/22 13:36:57.466
Dec 16 13:36:57.475: INFO: created test-pod-1
Dec 16 13:36:57.479: INFO: created test-pod-2
Dec 16 13:36:57.485: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 12/16/22 13:36:57.485
Dec 16 13:36:57.485: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6357' to be running and ready
Dec 16 13:36:57.501: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 16 13:36:57.501: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 16 13:36:57.501: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 16 13:36:57.502: INFO: 0 / 3 pods in namespace 'pods-6357' are running and ready (0 seconds elapsed)
Dec 16 13:36:57.502: INFO: expected 0 pod replicas in namespace 'pods-6357', 0 are Running and Ready.
Dec 16 13:36:57.502: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Dec 16 13:36:57.502: INFO: test-pod-1  pool-a3802-fsxxd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:36:57 +0000 UTC  }]
Dec 16 13:36:57.502: INFO: test-pod-2  pool-a3802-fsxxd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:36:57 +0000 UTC  }]
Dec 16 13:36:57.502: INFO: test-pod-3                    Pending         []
Dec 16 13:36:57.502: INFO: 
Dec 16 13:36:59.545: INFO: 3 / 3 pods in namespace 'pods-6357' are running and ready (2 seconds elapsed)
Dec 16 13:36:59.545: INFO: expected 0 pod replicas in namespace 'pods-6357', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 12/16/22 13:36:59.777
Dec 16 13:36:59.781: INFO: Pod quantity 3 is different from expected quantity 0
Dec 16 13:37:00.787: INFO: Pod quantity 3 is different from expected quantity 0
Dec 16 13:37:01.786: INFO: Pod quantity 1 is different from expected quantity 0
Dec 16 13:37:02.785: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:37:03.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6357" for this suite. 12/16/22 13:37:03.812
------------------------------
• [SLOW TEST] [6.383 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:36:57.436
    Dec 16 13:36:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:36:57.436
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:36:57.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:36:57.463
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 12/16/22 13:36:57.466
    Dec 16 13:36:57.475: INFO: created test-pod-1
    Dec 16 13:36:57.479: INFO: created test-pod-2
    Dec 16 13:36:57.485: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 12/16/22 13:36:57.485
    Dec 16 13:36:57.485: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6357' to be running and ready
    Dec 16 13:36:57.501: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 16 13:36:57.501: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 16 13:36:57.501: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 16 13:36:57.502: INFO: 0 / 3 pods in namespace 'pods-6357' are running and ready (0 seconds elapsed)
    Dec 16 13:36:57.502: INFO: expected 0 pod replicas in namespace 'pods-6357', 0 are Running and Ready.
    Dec 16 13:36:57.502: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Dec 16 13:36:57.502: INFO: test-pod-1  pool-a3802-fsxxd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:36:57 +0000 UTC  }]
    Dec 16 13:36:57.502: INFO: test-pod-2  pool-a3802-fsxxd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 13:36:57 +0000 UTC  }]
    Dec 16 13:36:57.502: INFO: test-pod-3                    Pending         []
    Dec 16 13:36:57.502: INFO: 
    Dec 16 13:36:59.545: INFO: 3 / 3 pods in namespace 'pods-6357' are running and ready (2 seconds elapsed)
    Dec 16 13:36:59.545: INFO: expected 0 pod replicas in namespace 'pods-6357', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 12/16/22 13:36:59.777
    Dec 16 13:36:59.781: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 16 13:37:00.787: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 16 13:37:01.786: INFO: Pod quantity 1 is different from expected quantity 0
    Dec 16 13:37:02.785: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:37:03.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6357" for this suite. 12/16/22 13:37:03.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:37:03.82
Dec 16 13:37:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 13:37:03.821
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:37:03.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:37:03.93
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-825 12/16/22 13:37:03.986
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 12/16/22 13:37:03.991
STEP: Creating stateful set ss in namespace statefulset-825 12/16/22 13:37:03.994
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-825 12/16/22 13:37:04.019
Dec 16 13:37:04.023: INFO: Found 0 stateful pods, waiting for 1
Dec 16 13:37:14.030: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/16/22 13:37:14.03
Dec 16 13:37:14.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 13:37:14.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 13:37:14.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 13:37:14.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 13:37:14.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 16 13:37:24.236: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 13:37:24.236: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 13:37:24.253: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999789s
Dec 16 13:37:25.258: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996064955s
Dec 16 13:37:26.263: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991081297s
Dec 16 13:37:27.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985858849s
Dec 16 13:37:28.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981150848s
Dec 16 13:37:29.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976122233s
Dec 16 13:37:30.283: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970415152s
Dec 16 13:37:31.290: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965400706s
Dec 16 13:37:32.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958928935s
Dec 16 13:37:33.301: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.896741ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-825 12/16/22 13:37:34.301
Dec 16 13:37:34.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 13:37:34.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 13:37:34.497: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 13:37:34.497: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 13:37:34.503: INFO: Found 1 stateful pods, waiting for 3
Dec 16 13:37:44.512: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:37:44.512: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 13:37:44.512: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 12/16/22 13:37:44.512
STEP: Scale down will halt with unhealthy stateful pod 12/16/22 13:37:44.512
Dec 16 13:37:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 13:37:44.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 13:37:44.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 13:37:44.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 13:37:44.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 13:37:44.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 13:37:44.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 13:37:44.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 13:37:44.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 13:37:45.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 13:37:45.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 13:37:45.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 13:37:45.101: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 13:37:45.105: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 16 13:37:55.114: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 13:37:55.114: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 13:37:55.114: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 13:37:55.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999839s
Dec 16 13:37:56.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996420399s
Dec 16 13:37:57.136: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992049947s
Dec 16 13:37:58.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987045614s
Dec 16 13:37:59.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981538938s
Dec 16 13:38:00.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976624045s
Dec 16 13:38:01.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91291699s
Dec 16 13:38:02.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.901672521s
Dec 16 13:38:03.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.895588771s
Dec 16 13:38:04.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 890.228462ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-825 12/16/22 13:38:05.238
Dec 16 13:38:05.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 13:38:05.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 13:38:05.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 13:38:05.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 13:38:05.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 13:38:05.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 13:38:05.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 13:38:05.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 13:38:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 13:38:05.808: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 13:38:05.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 13:38:05.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 13:38:05.809: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 12/16/22 13:38:15.83
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 13:38:15.830: INFO: Deleting all statefulset in ns statefulset-825
Dec 16 13:38:15.834: INFO: Scaling statefulset ss to 0
Dec 16 13:38:15.846: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 13:38:15.849: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:15.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-825" for this suite. 12/16/22 13:38:15.865
------------------------------
• [SLOW TEST] [72.052 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:37:03.82
    Dec 16 13:37:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 13:37:03.821
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:37:03.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:37:03.93
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-825 12/16/22 13:37:03.986
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 12/16/22 13:37:03.991
    STEP: Creating stateful set ss in namespace statefulset-825 12/16/22 13:37:03.994
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-825 12/16/22 13:37:04.019
    Dec 16 13:37:04.023: INFO: Found 0 stateful pods, waiting for 1
    Dec 16 13:37:14.030: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/16/22 13:37:14.03
    Dec 16 13:37:14.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 13:37:14.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 13:37:14.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 13:37:14.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 13:37:14.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 16 13:37:24.236: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 13:37:24.236: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 13:37:24.253: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999789s
    Dec 16 13:37:25.258: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996064955s
    Dec 16 13:37:26.263: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991081297s
    Dec 16 13:37:27.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985858849s
    Dec 16 13:37:28.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981150848s
    Dec 16 13:37:29.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976122233s
    Dec 16 13:37:30.283: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970415152s
    Dec 16 13:37:31.290: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965400706s
    Dec 16 13:37:32.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958928935s
    Dec 16 13:37:33.301: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.896741ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-825 12/16/22 13:37:34.301
    Dec 16 13:37:34.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 13:37:34.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 13:37:34.497: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 13:37:34.497: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 13:37:34.503: INFO: Found 1 stateful pods, waiting for 3
    Dec 16 13:37:44.512: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:37:44.512: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 13:37:44.512: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 12/16/22 13:37:44.512
    STEP: Scale down will halt with unhealthy stateful pod 12/16/22 13:37:44.512
    Dec 16 13:37:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 13:37:44.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 13:37:44.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 13:37:44.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 13:37:44.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 13:37:44.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 13:37:44.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 13:37:44.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 13:37:44.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 13:37:45.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 13:37:45.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 13:37:45.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 13:37:45.101: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 13:37:45.105: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Dec 16 13:37:55.114: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 13:37:55.114: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 13:37:55.114: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 13:37:55.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999839s
    Dec 16 13:37:56.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996420399s
    Dec 16 13:37:57.136: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992049947s
    Dec 16 13:37:58.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987045614s
    Dec 16 13:37:59.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981538938s
    Dec 16 13:38:00.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976624045s
    Dec 16 13:38:01.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91291699s
    Dec 16 13:38:02.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.901672521s
    Dec 16 13:38:03.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.895588771s
    Dec 16 13:38:04.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 890.228462ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-825 12/16/22 13:38:05.238
    Dec 16 13:38:05.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 13:38:05.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 13:38:05.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 13:38:05.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 13:38:05.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 13:38:05.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 13:38:05.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 13:38:05.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 13:38:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 13:38:05.808: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 13:38:05.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 13:38:05.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 13:38:05.809: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 12/16/22 13:38:15.83
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 13:38:15.830: INFO: Deleting all statefulset in ns statefulset-825
    Dec 16 13:38:15.834: INFO: Scaling statefulset ss to 0
    Dec 16 13:38:15.846: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 13:38:15.849: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:15.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-825" for this suite. 12/16/22 13:38:15.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:15.872
Dec 16 13:38:15.872: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-pred 12/16/22 13:38:15.873
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:15.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:15.89
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 16 13:38:15.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 13:38:15.900: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 13:38:15.903: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
Dec 16 13:38:15.909: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 13:38:15.909: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container coredns ready: true, restart count 0
Dec 16 13:38:15.909: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 13:38:15.909: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 13:38:15.909: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 13:38:15.909: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 13:38:15.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 13:38:15.909: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 13:38:15.909: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
Dec 16 13:38:15.931: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.931: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 13:38:15.931: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.931: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 13:38:15.931: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
Dec 16 13:38:15.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 13:38:15.931: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 13:38:15.931: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
Dec 16 13:38:15.937: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 16 13:38:15.937: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 13:38:15.937: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container coredns ready: true, restart count 0
Dec 16 13:38:15.937: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 13:38:15.937: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 13:38:15.937: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container e2e ready: true, restart count 0
Dec 16 13:38:15.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 13:38:15.937: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 13:38:15.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 13:38:15.937: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 13:38:15.937
Dec 16 13:38:15.944: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7932" to be "running"
Dec 16 13:38:15.956: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.178621ms
Dec 16 13:38:17.960: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015850749s
Dec 16 13:38:17.960: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 13:38:17.964
STEP: Trying to apply a random label on the found node. 12/16/22 13:38:17.975
STEP: verifying the node has the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 42 12/16/22 13:38:17.984
STEP: Trying to relaunch the pod, now with labels. 12/16/22 13:38:17.988
Dec 16 13:38:17.994: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7932" to be "not pending"
Dec 16 13:38:17.996: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663956ms
Dec 16 13:38:20.001: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007366476s
Dec 16 13:38:20.001: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 off the node pool-a3802-fsxxd 12/16/22 13:38:20.004
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 12/16/22 13:38:20.015
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:20.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7932" for this suite. 12/16/22 13:38:20.022
------------------------------
• [4.156 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:15.872
    Dec 16 13:38:15.872: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-pred 12/16/22 13:38:15.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:15.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:15.89
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 16 13:38:15.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 16 13:38:15.900: INFO: Waiting for terminating namespaces to be deleted...
    Dec 16 13:38:15.903: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
    Dec 16 13:38:15.909: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 13:38:15.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 13:38:15.909: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
    Dec 16 13:38:15.931: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.931: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 13:38:15.931: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.931: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 13:38:15.931: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
    Dec 16 13:38:15.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 13:38:15.931: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 13:38:15.931: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
    Dec 16 13:38:15.937: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container e2e ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 13:38:15.937: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 13:38:15.937: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 13:38:15.937
    Dec 16 13:38:15.944: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7932" to be "running"
    Dec 16 13:38:15.956: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.178621ms
    Dec 16 13:38:17.960: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015850749s
    Dec 16 13:38:17.960: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 13:38:17.964
    STEP: Trying to apply a random label on the found node. 12/16/22 13:38:17.975
    STEP: verifying the node has the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 42 12/16/22 13:38:17.984
    STEP: Trying to relaunch the pod, now with labels. 12/16/22 13:38:17.988
    Dec 16 13:38:17.994: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7932" to be "not pending"
    Dec 16 13:38:17.996: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663956ms
    Dec 16 13:38:20.001: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007366476s
    Dec 16 13:38:20.001: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 off the node pool-a3802-fsxxd 12/16/22 13:38:20.004
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1fbee056-a92f-4c93-b114-10c3bf89d975 12/16/22 13:38:20.015
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:20.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7932" for this suite. 12/16/22 13:38:20.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:20.029
Dec 16 13:38:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename containers 12/16/22 13:38:20.03
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:20.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:20.055
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Dec 16 13:38:20.071: INFO: Waiting up to 5m0s for pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9" in namespace "containers-1224" to be "running"
Dec 16 13:38:20.074: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879679ms
Dec 16 13:38:22.088: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017773943s
Dec 16 13:38:22.089: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:22.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1224" for this suite. 12/16/22 13:38:22.17
------------------------------
• [2.146 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:20.029
    Dec 16 13:38:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename containers 12/16/22 13:38:20.03
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:20.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:20.055
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Dec 16 13:38:20.071: INFO: Waiting up to 5m0s for pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9" in namespace "containers-1224" to be "running"
    Dec 16 13:38:20.074: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879679ms
    Dec 16 13:38:22.088: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017773943s
    Dec 16 13:38:22.089: INFO: Pod "client-containers-2e469619-1b20-4c55-a717-67d0195eaff9" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:22.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1224" for this suite. 12/16/22 13:38:22.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:22.177
Dec 16 13:38:22.177: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:38:22.178
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:22.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:22.195
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 12/16/22 13:38:22.197
Dec 16 13:38:22.205: INFO: Waiting up to 5m0s for pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617" in namespace "downward-api-4488" to be "Succeeded or Failed"
Dec 16 13:38:22.208: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276547ms
Dec 16 13:38:24.213: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007759781s
Dec 16 13:38:26.214: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008539592s
Dec 16 13:38:28.212: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006051801s
STEP: Saw pod success 12/16/22 13:38:28.212
Dec 16 13:38:28.212: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617" satisfied condition "Succeeded or Failed"
Dec 16 13:38:28.215: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:38:28.223
Dec 16 13:38:28.235: INFO: Waiting for pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 to disappear
Dec 16 13:38:28.237: INFO: Pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4488" for this suite. 12/16/22 13:38:28.241
------------------------------
• [SLOW TEST] [6.071 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:22.177
    Dec 16 13:38:22.177: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:38:22.178
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:22.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:22.195
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 12/16/22 13:38:22.197
    Dec 16 13:38:22.205: INFO: Waiting up to 5m0s for pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617" in namespace "downward-api-4488" to be "Succeeded or Failed"
    Dec 16 13:38:22.208: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276547ms
    Dec 16 13:38:24.213: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007759781s
    Dec 16 13:38:26.214: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008539592s
    Dec 16 13:38:28.212: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006051801s
    STEP: Saw pod success 12/16/22 13:38:28.212
    Dec 16 13:38:28.212: INFO: Pod "downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617" satisfied condition "Succeeded or Failed"
    Dec 16 13:38:28.215: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:38:28.223
    Dec 16 13:38:28.235: INFO: Waiting for pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 to disappear
    Dec 16 13:38:28.237: INFO: Pod downward-api-b2bf36df-d736-4699-b36c-9e13cd5e2617 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4488" for this suite. 12/16/22 13:38:28.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:28.248
Dec 16 13:38:28.248: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename subpath 12/16/22 13:38:28.249
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:28.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:28.304
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/16/22 13:38:28.307
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-5qmb 12/16/22 13:38:28.315
STEP: Creating a pod to test atomic-volume-subpath 12/16/22 13:38:28.315
Dec 16 13:38:28.323: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5qmb" in namespace "subpath-6882" to be "Succeeded or Failed"
Dec 16 13:38:28.326: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732196ms
Dec 16 13:38:30.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008573355s
Dec 16 13:38:32.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.008942883s
Dec 16 13:38:34.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.008223086s
Dec 16 13:38:36.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.007853561s
Dec 16 13:38:38.333: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.00978572s
Dec 16 13:38:40.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.008906207s
Dec 16 13:38:42.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008415253s
Dec 16 13:38:44.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009172854s
Dec 16 13:38:46.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008927084s
Dec 16 13:38:48.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008914844s
Dec 16 13:38:50.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=false. Elapsed: 22.008620939s
Dec 16 13:38:52.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008591473s
STEP: Saw pod success 12/16/22 13:38:52.331
Dec 16 13:38:52.332: INFO: Pod "pod-subpath-test-configmap-5qmb" satisfied condition "Succeeded or Failed"
Dec 16 13:38:52.335: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-configmap-5qmb container test-container-subpath-configmap-5qmb: <nil>
STEP: delete the pod 12/16/22 13:38:52.344
Dec 16 13:38:52.357: INFO: Waiting for pod pod-subpath-test-configmap-5qmb to disappear
Dec 16 13:38:52.360: INFO: Pod pod-subpath-test-configmap-5qmb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5qmb 12/16/22 13:38:52.36
Dec 16 13:38:52.360: INFO: Deleting pod "pod-subpath-test-configmap-5qmb" in namespace "subpath-6882"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:52.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6882" for this suite. 12/16/22 13:38:52.368
------------------------------
• [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:28.248
    Dec 16 13:38:28.248: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename subpath 12/16/22 13:38:28.249
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:28.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:28.304
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/16/22 13:38:28.307
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-5qmb 12/16/22 13:38:28.315
    STEP: Creating a pod to test atomic-volume-subpath 12/16/22 13:38:28.315
    Dec 16 13:38:28.323: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5qmb" in namespace "subpath-6882" to be "Succeeded or Failed"
    Dec 16 13:38:28.326: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732196ms
    Dec 16 13:38:30.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008573355s
    Dec 16 13:38:32.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.008942883s
    Dec 16 13:38:34.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.008223086s
    Dec 16 13:38:36.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.007853561s
    Dec 16 13:38:38.333: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.00978572s
    Dec 16 13:38:40.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.008906207s
    Dec 16 13:38:42.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008415253s
    Dec 16 13:38:44.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009172854s
    Dec 16 13:38:46.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008927084s
    Dec 16 13:38:48.332: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008914844s
    Dec 16 13:38:50.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Running", Reason="", readiness=false. Elapsed: 22.008620939s
    Dec 16 13:38:52.331: INFO: Pod "pod-subpath-test-configmap-5qmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008591473s
    STEP: Saw pod success 12/16/22 13:38:52.331
    Dec 16 13:38:52.332: INFO: Pod "pod-subpath-test-configmap-5qmb" satisfied condition "Succeeded or Failed"
    Dec 16 13:38:52.335: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-configmap-5qmb container test-container-subpath-configmap-5qmb: <nil>
    STEP: delete the pod 12/16/22 13:38:52.344
    Dec 16 13:38:52.357: INFO: Waiting for pod pod-subpath-test-configmap-5qmb to disappear
    Dec 16 13:38:52.360: INFO: Pod pod-subpath-test-configmap-5qmb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-5qmb 12/16/22 13:38:52.36
    Dec 16 13:38:52.360: INFO: Deleting pod "pod-subpath-test-configmap-5qmb" in namespace "subpath-6882"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:52.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6882" for this suite. 12/16/22 13:38:52.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:52.375
Dec 16 13:38:52.376: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 13:38:52.376
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:52.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:52.394
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 12/16/22 13:38:52.4
STEP: waiting for Deployment to be created 12/16/22 13:38:52.406
STEP: waiting for all Replicas to be Ready 12/16/22 13:38:52.408
Dec 16 13:38:52.409: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.409: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.418: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.418: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.430: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.430: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.444: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:52.444: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 16 13:38:53.467: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 16 13:38:53.467: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 16 13:38:53.894: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 12/16/22 13:38:53.894
W1216 13:38:53.907227      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 16 13:38:53.908: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 12/16/22 13:38:53.908
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.920: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.920: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.933: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.933: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:53.951: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:53.951: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:53.956: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:53.956: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:54.905: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:54.905: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:54.931: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
STEP: listing Deployments 12/16/22 13:38:54.931
Dec 16 13:38:54.936: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 12/16/22 13:38:54.936
Dec 16 13:38:54.945: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 12/16/22 13:38:54.945
Dec 16 13:38:54.950: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:54.955: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:54.973: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:54.982: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:54.988: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:55.909: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:55.938: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:55.944: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 16 13:38:57.295: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 12/16/22 13:38:57.317
STEP: fetching the DeploymentStatus 12/16/22 13:38:57.325
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 3
STEP: deleting the Deployment 12/16/22 13:38:57.33
Dec 16 13:38:57.339: INFO: observed event type MODIFIED
Dec 16 13:38:57.339: INFO: observed event type MODIFIED
Dec 16 13:38:57.339: INFO: observed event type MODIFIED
Dec 16 13:38:57.339: INFO: observed event type MODIFIED
Dec 16 13:38:57.339: INFO: observed event type MODIFIED
Dec 16 13:38:57.340: INFO: observed event type MODIFIED
Dec 16 13:38:57.340: INFO: observed event type MODIFIED
Dec 16 13:38:57.340: INFO: observed event type MODIFIED
Dec 16 13:38:57.340: INFO: observed event type MODIFIED
Dec 16 13:38:57.340: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 13:38:57.342: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 16 13:38:57.345: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9225  75652314-e44c-4dd3-bb4c-3a7196f518e3 680919564 2 2022-12-16 13:38:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b247 0xc003e7b248}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b2d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec 16 13:38:57.349: INFO: pod: "test-deployment-7b7876f9d6-js92r":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-js92r test-deployment-7b7876f9d6- deployment-9225  7ff77cf0-9851-4ad8-9b48-e86be7a4177b 680919506 0 2022-12-16 13:38:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c7c475db0aa5cd355c7a2974af489545d47e4aefe412856470e0f405ae6f0182 cni.projectcalico.org/podIP:192.168.189.31/32 cni.projectcalico.org/podIPs:192.168.189.31/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75652314-e44c-4dd3-bb4c-3a7196f518e3 0xc003e7b787 0xc003e7b788}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75652314-e44c-4dd3-bb4c-3a7196f518e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jgjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jgjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.31,StartTime:2022-12-16 13:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2fd71b67090b54b9c55f3d2f047ac4441b04bd9bbfc6862929b433aabdc2cf2e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 13:38:57.349: INFO: pod: "test-deployment-7b7876f9d6-z8mql":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-z8mql test-deployment-7b7876f9d6- deployment-9225  739bc460-32db-4f01-9969-5ac976dc2297 680919563 0 2022-12-16 13:38:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:e66e8d017ccc389cce57e8da2b490ee884ea6da2a5a73f5ea25e562c6f0de8ff cni.projectcalico.org/podIP:192.168.189.237/32 cni.projectcalico.org/podIPs:192.168.189.237/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75652314-e44c-4dd3-bb4c-3a7196f518e3 0xc003e7b9b7 0xc003e7b9b8}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75652314-e44c-4dd3-bb4c-3a7196f518e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cqjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cqjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.237,StartTime:2022-12-16 13:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ac23a1612e85b03d46cbde6aa3206d30dd1b5d22f9d76e081e9dd97af99f770,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 13:38:57.350: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9225  4d926525-ad67-483d-aee3-e7933650bd6c 680919572 4 2022-12-16 13:38:53 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b347 0xc003e7b348}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 16 13:38:57.353: INFO: pod: "test-deployment-7df74c55ff-g8rmf":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-g8rmf test-deployment-7df74c55ff- deployment-9225  26715070-d76a-4877-9d63-5dbded15e27a 680919538 0 2022-12-16 13:38:54 +0000 UTC 2022-12-16 13:38:56 +0000 UTC 0xc001137860 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:ae18d21a78c9fbc42d0c451e3bfd64b2e6c4c395764e1b1ad3bf0bd784859083 cni.projectcalico.org/podIP:192.168.156.173/32 cni.projectcalico.org/podIPs:192.168.156.173/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4d926525-ad67-483d-aee3-e7933650bd6c 0xc001137897 0xc001137898}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d926525-ad67-483d-aee3-e7933650bd6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9xgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9xgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.173,StartTime:2022-12-16 13:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c2f3f0dd1e54ef60a6c1fd787003512afe22a94bd192f3ba61e34dffefe00369,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 13:38:57.353: INFO: pod: "test-deployment-7df74c55ff-t7vjp":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-t7vjp test-deployment-7df74c55ff- deployment-9225  a3d75f92-9026-47d8-97e1-6f452acff3ac 680919568 0 2022-12-16 13:38:53 +0000 UTC 2022-12-16 13:38:58 +0000 UTC 0xc001137a90 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:a94b7b89fe1887f04fdb6905c69dc2bd4854ec0c12a299994f5f2f948e5dc13e cni.projectcalico.org/podIP:192.168.189.35/32 cni.projectcalico.org/podIPs:192.168.189.35/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4d926525-ad67-483d-aee3-e7933650bd6c 0xc001137ac7 0xc001137ac8}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d926525-ad67-483d-aee3-e7933650bd6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n77mx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n77mx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.35,StartTime:2022-12-16 13:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://ebba0ba92901cb15d227ecb164d702db41491f90bb72524c1ea4fbf083bde2f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 16 13:38:57.353: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9225  918bb183-d93b-4ace-852b-c8640a1e29cd 680919450 3 2022-12-16 13:38:52 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b447 0xc003e7b448}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 13:38:57.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9225" for this suite. 12/16/22 13:38:57.363
------------------------------
• [4.996 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:52.375
    Dec 16 13:38:52.376: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 13:38:52.376
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:52.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:52.394
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 12/16/22 13:38:52.4
    STEP: waiting for Deployment to be created 12/16/22 13:38:52.406
    STEP: waiting for all Replicas to be Ready 12/16/22 13:38:52.408
    Dec 16 13:38:52.409: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.409: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.418: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.418: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.430: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.430: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.444: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:52.444: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 16 13:38:53.467: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 16 13:38:53.467: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 16 13:38:53.894: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 12/16/22 13:38:53.894
    W1216 13:38:53.907227      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 16 13:38:53.908: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 12/16/22 13:38:53.908
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 0
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.910: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.920: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.920: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.933: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.933: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:53.951: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:53.951: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:53.956: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:53.956: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:54.905: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:54.905: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:54.931: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    STEP: listing Deployments 12/16/22 13:38:54.931
    Dec 16 13:38:54.936: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 12/16/22 13:38:54.936
    Dec 16 13:38:54.945: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 12/16/22 13:38:54.945
    Dec 16 13:38:54.950: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:54.955: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:54.973: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:54.982: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:54.988: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:55.909: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:55.938: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:55.944: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 16 13:38:57.295: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 12/16/22 13:38:57.317
    STEP: fetching the DeploymentStatus 12/16/22 13:38:57.325
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 1
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 2
    Dec 16 13:38:57.330: INFO: observed Deployment test-deployment in namespace deployment-9225 with ReadyReplicas 3
    STEP: deleting the Deployment 12/16/22 13:38:57.33
    Dec 16 13:38:57.339: INFO: observed event type MODIFIED
    Dec 16 13:38:57.339: INFO: observed event type MODIFIED
    Dec 16 13:38:57.339: INFO: observed event type MODIFIED
    Dec 16 13:38:57.339: INFO: observed event type MODIFIED
    Dec 16 13:38:57.339: INFO: observed event type MODIFIED
    Dec 16 13:38:57.340: INFO: observed event type MODIFIED
    Dec 16 13:38:57.340: INFO: observed event type MODIFIED
    Dec 16 13:38:57.340: INFO: observed event type MODIFIED
    Dec 16 13:38:57.340: INFO: observed event type MODIFIED
    Dec 16 13:38:57.340: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 13:38:57.342: INFO: Log out all the ReplicaSets if there is no deployment created
    Dec 16 13:38:57.345: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9225  75652314-e44c-4dd3-bb4c-3a7196f518e3 680919564 2 2022-12-16 13:38:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b247 0xc003e7b248}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b2d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Dec 16 13:38:57.349: INFO: pod: "test-deployment-7b7876f9d6-js92r":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-js92r test-deployment-7b7876f9d6- deployment-9225  7ff77cf0-9851-4ad8-9b48-e86be7a4177b 680919506 0 2022-12-16 13:38:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c7c475db0aa5cd355c7a2974af489545d47e4aefe412856470e0f405ae6f0182 cni.projectcalico.org/podIP:192.168.189.31/32 cni.projectcalico.org/podIPs:192.168.189.31/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75652314-e44c-4dd3-bb4c-3a7196f518e3 0xc003e7b787 0xc003e7b788}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75652314-e44c-4dd3-bb4c-3a7196f518e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jgjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jgjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.31,StartTime:2022-12-16 13:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2fd71b67090b54b9c55f3d2f047ac4441b04bd9bbfc6862929b433aabdc2cf2e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 16 13:38:57.349: INFO: pod: "test-deployment-7b7876f9d6-z8mql":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-z8mql test-deployment-7b7876f9d6- deployment-9225  739bc460-32db-4f01-9969-5ac976dc2297 680919563 0 2022-12-16 13:38:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:e66e8d017ccc389cce57e8da2b490ee884ea6da2a5a73f5ea25e562c6f0de8ff cni.projectcalico.org/podIP:192.168.189.237/32 cni.projectcalico.org/podIPs:192.168.189.237/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75652314-e44c-4dd3-bb4c-3a7196f518e3 0xc003e7b9b7 0xc003e7b9b8}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75652314-e44c-4dd3-bb4c-3a7196f518e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cqjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cqjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-oewtd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.213,PodIP:192.168.189.237,StartTime:2022-12-16 13:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ac23a1612e85b03d46cbde6aa3206d30dd1b5d22f9d76e081e9dd97af99f770,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 16 13:38:57.350: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9225  4d926525-ad67-483d-aee3-e7933650bd6c 680919572 4 2022-12-16 13:38:53 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b347 0xc003e7b348}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Dec 16 13:38:57.353: INFO: pod: "test-deployment-7df74c55ff-g8rmf":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-g8rmf test-deployment-7df74c55ff- deployment-9225  26715070-d76a-4877-9d63-5dbded15e27a 680919538 0 2022-12-16 13:38:54 +0000 UTC 2022-12-16 13:38:56 +0000 UTC 0xc001137860 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:ae18d21a78c9fbc42d0c451e3bfd64b2e6c4c395764e1b1ad3bf0bd784859083 cni.projectcalico.org/podIP:192.168.156.173/32 cni.projectcalico.org/podIPs:192.168.156.173/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4d926525-ad67-483d-aee3-e7933650bd6c 0xc001137897 0xc001137898}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d926525-ad67-483d-aee3-e7933650bd6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9xgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9xgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.222,PodIP:192.168.156.173,StartTime:2022-12-16 13:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c2f3f0dd1e54ef60a6c1fd787003512afe22a94bd192f3ba61e34dffefe00369,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 16 13:38:57.353: INFO: pod: "test-deployment-7df74c55ff-t7vjp":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-t7vjp test-deployment-7df74c55ff- deployment-9225  a3d75f92-9026-47d8-97e1-6f452acff3ac 680919568 0 2022-12-16 13:38:53 +0000 UTC 2022-12-16 13:38:58 +0000 UTC 0xc001137a90 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:a94b7b89fe1887f04fdb6905c69dc2bd4854ec0c12a299994f5f2f948e5dc13e cni.projectcalico.org/podIP:192.168.189.35/32 cni.projectcalico.org/podIPs:192.168.189.35/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4d926525-ad67-483d-aee3-e7933650bd6c 0xc001137ac7 0xc001137ac8}] [] [{kube-controller-manager Update v1 2022-12-16 13:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d926525-ad67-483d-aee3-e7933650bd6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n77mx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n77mx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 13:38:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.35,StartTime:2022-12-16 13:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 13:38:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://ebba0ba92901cb15d227ecb164d702db41491f90bb72524c1ea4fbf083bde2f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 16 13:38:57.353: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9225  918bb183-d93b-4ace-852b-c8640a1e29cd 680919450 3 2022-12-16 13:38:52 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d856ff29-7cb9-485c-9401-3aaf2f1f1291 0xc003e7b447 0xc003e7b448}] [] [{kube-controller-manager Update apps/v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d856ff29-7cb9-485c-9401-3aaf2f1f1291\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 13:38:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7b4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:38:57.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9225" for this suite. 12/16/22 13:38:57.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:38:57.372
Dec 16 13:38:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:38:57.373
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:57.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:57.39
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4802" for this suite. 12/16/22 13:39:01.415
------------------------------
• [4.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:38:57.372
    Dec 16 13:38:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:38:57.373
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:38:57.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:38:57.39
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4802" for this suite. 12/16/22 13:39:01.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:01.427
Dec 16 13:39:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename containers 12/16/22 13:39:01.427
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:01.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:01.454
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 12/16/22 13:39:01.456
Dec 16 13:39:01.464: INFO: Waiting up to 5m0s for pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091" in namespace "containers-4900" to be "Succeeded or Failed"
Dec 16 13:39:01.467: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871565ms
Dec 16 13:39:03.473: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009220262s
Dec 16 13:39:05.472: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007813587s
STEP: Saw pod success 12/16/22 13:39:05.472
Dec 16 13:39:05.472: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091" satisfied condition "Succeeded or Failed"
Dec 16 13:39:05.475: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 13:39:05.483
Dec 16 13:39:05.495: INFO: Waiting for pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 to disappear
Dec 16 13:39:05.498: INFO: Pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:05.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4900" for this suite. 12/16/22 13:39:05.502
------------------------------
• [4.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:01.427
    Dec 16 13:39:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename containers 12/16/22 13:39:01.427
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:01.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:01.454
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 12/16/22 13:39:01.456
    Dec 16 13:39:01.464: INFO: Waiting up to 5m0s for pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091" in namespace "containers-4900" to be "Succeeded or Failed"
    Dec 16 13:39:01.467: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871565ms
    Dec 16 13:39:03.473: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009220262s
    Dec 16 13:39:05.472: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007813587s
    STEP: Saw pod success 12/16/22 13:39:05.472
    Dec 16 13:39:05.472: INFO: Pod "client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091" satisfied condition "Succeeded or Failed"
    Dec 16 13:39:05.475: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 13:39:05.483
    Dec 16 13:39:05.495: INFO: Waiting for pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 to disappear
    Dec 16 13:39:05.498: INFO: Pod client-containers-4c6b803f-3c50-4653-9849-eb9aaae3c091 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:05.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4900" for this suite. 12/16/22 13:39:05.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:05.507
Dec 16 13:39:05.507: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 13:39:05.508
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:05.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:05.524
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 12/16/22 13:39:05.546
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:39:05.551
Dec 16 13:39:05.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:39:05.558: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:39:06.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 13:39:06.566: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:39:07.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 13:39:07.568: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 12/16/22 13:39:07.571
Dec 16 13:39:07.575: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/16/22 13:39:07.575
Dec 16 13:39:07.584: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/16/22 13:39:07.584
Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: ADDED
Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.586: INFO: Found daemon set daemon-set in namespace daemonsets-5874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 16 13:39:07.586: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/16/22 13:39:07.586
STEP: watching for the daemon set status to be patched 12/16/22 13:39:07.592
Dec 16 13:39:07.593: INFO: Observed &DaemonSet event: ADDED
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Observed daemon set daemon-set in namespace daemonsets-5874 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
Dec 16 13:39:07.594: INFO: Found daemon set daemon-set in namespace daemonsets-5874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec 16 13:39:07.594: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:39:07.599
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5874, will wait for the garbage collector to delete the pods 12/16/22 13:39:07.599
Dec 16 13:39:07.659: INFO: Deleting DaemonSet.extensions daemon-set took: 6.626868ms
Dec 16 13:39:07.760: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.973089ms
Dec 16 13:39:09.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:39:09.966: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 13:39:09.969: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680920070"},"items":null}

Dec 16 13:39:09.972: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680920070"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5874" for this suite. 12/16/22 13:39:09.99
------------------------------
• [4.489 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:05.507
    Dec 16 13:39:05.507: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 13:39:05.508
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:05.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:05.524
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 12/16/22 13:39:05.546
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:39:05.551
    Dec 16 13:39:05.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:39:05.558: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:39:06.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 13:39:06.566: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:39:07.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 13:39:07.568: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 12/16/22 13:39:07.571
    Dec 16 13:39:07.575: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/16/22 13:39:07.575
    Dec 16 13:39:07.584: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/16/22 13:39:07.584
    Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: ADDED
    Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.585: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.586: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.586: INFO: Found daemon set daemon-set in namespace daemonsets-5874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 16 13:39:07.586: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/16/22 13:39:07.586
    STEP: watching for the daemon set status to be patched 12/16/22 13:39:07.592
    Dec 16 13:39:07.593: INFO: Observed &DaemonSet event: ADDED
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Observed daemon set daemon-set in namespace daemonsets-5874 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 16 13:39:07.594: INFO: Observed &DaemonSet event: MODIFIED
    Dec 16 13:39:07.594: INFO: Found daemon set daemon-set in namespace daemonsets-5874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec 16 13:39:07.594: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:39:07.599
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5874, will wait for the garbage collector to delete the pods 12/16/22 13:39:07.599
    Dec 16 13:39:07.659: INFO: Deleting DaemonSet.extensions daemon-set took: 6.626868ms
    Dec 16 13:39:07.760: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.973089ms
    Dec 16 13:39:09.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:39:09.966: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 13:39:09.969: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680920070"},"items":null}

    Dec 16 13:39:09.972: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680920070"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5874" for this suite. 12/16/22 13:39:09.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:09.997
Dec 16 13:39:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sysctl 12/16/22 13:39:09.998
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:10.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:10.017
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 12/16/22 13:39:10.02
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8744" for this suite. 12/16/22 13:39:10.029
------------------------------
• [0.038 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:09.997
    Dec 16 13:39:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sysctl 12/16/22 13:39:09.998
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:10.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:10.017
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 12/16/22 13:39:10.02
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8744" for this suite. 12/16/22 13:39:10.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:10.036
Dec 16 13:39:10.036: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:39:10.037
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:10.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:10.056
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 12/16/22 13:39:10.058
Dec 16 13:39:10.067: INFO: Waiting up to 5m0s for pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859" in namespace "emptydir-9692" to be "Succeeded or Failed"
Dec 16 13:39:10.071: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669866ms
Dec 16 13:39:12.075: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007696426s
Dec 16 13:39:14.076: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008693549s
STEP: Saw pod success 12/16/22 13:39:14.076
Dec 16 13:39:14.076: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859" satisfied condition "Succeeded or Failed"
Dec 16 13:39:14.080: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 container test-container: <nil>
STEP: delete the pod 12/16/22 13:39:14.088
Dec 16 13:39:14.101: INFO: Waiting for pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 to disappear
Dec 16 13:39:14.104: INFO: Pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:14.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9692" for this suite. 12/16/22 13:39:14.108
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:10.036
    Dec 16 13:39:10.036: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:39:10.037
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:10.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:10.056
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/16/22 13:39:10.058
    Dec 16 13:39:10.067: INFO: Waiting up to 5m0s for pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859" in namespace "emptydir-9692" to be "Succeeded or Failed"
    Dec 16 13:39:10.071: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669866ms
    Dec 16 13:39:12.075: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007696426s
    Dec 16 13:39:14.076: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008693549s
    STEP: Saw pod success 12/16/22 13:39:14.076
    Dec 16 13:39:14.076: INFO: Pod "pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859" satisfied condition "Succeeded or Failed"
    Dec 16 13:39:14.080: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:39:14.088
    Dec 16 13:39:14.101: INFO: Waiting for pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 to disappear
    Dec 16 13:39:14.104: INFO: Pod pod-ffd8554f-c7ef-4f8d-9edd-85e7d811d859 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:14.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9692" for this suite. 12/16/22 13:39:14.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:14.118
Dec 16 13:39:14.118: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:39:14.119
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:14.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:14.136
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 12/16/22 13:39:14.138
Dec 16 13:39:14.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4794 create -f -'
Dec 16 13:39:14.324: INFO: stderr: ""
Dec 16 13:39:14.324: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/16/22 13:39:14.324
Dec 16 13:39:15.329: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:39:15.329: INFO: Found 0 / 1
Dec 16 13:39:16.330: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:39:16.330: INFO: Found 1 / 1
Dec 16 13:39:16.330: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 12/16/22 13:39:16.33
Dec 16 13:39:16.333: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:39:16.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 16 13:39:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4794 patch pod agnhost-primary-jpvrr -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 16 13:39:16.409: INFO: stderr: ""
Dec 16 13:39:16.409: INFO: stdout: "pod/agnhost-primary-jpvrr patched\n"
STEP: checking annotations 12/16/22 13:39:16.409
Dec 16 13:39:16.413: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 16 13:39:16.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:16.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4794" for this suite. 12/16/22 13:39:16.417
------------------------------
• [2.305 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:14.118
    Dec 16 13:39:14.118: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:39:14.119
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:14.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:14.136
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 12/16/22 13:39:14.138
    Dec 16 13:39:14.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4794 create -f -'
    Dec 16 13:39:14.324: INFO: stderr: ""
    Dec 16 13:39:14.324: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/16/22 13:39:14.324
    Dec 16 13:39:15.329: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:39:15.329: INFO: Found 0 / 1
    Dec 16 13:39:16.330: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:39:16.330: INFO: Found 1 / 1
    Dec 16 13:39:16.330: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 12/16/22 13:39:16.33
    Dec 16 13:39:16.333: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:39:16.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 16 13:39:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-4794 patch pod agnhost-primary-jpvrr -p {"metadata":{"annotations":{"x":"y"}}}'
    Dec 16 13:39:16.409: INFO: stderr: ""
    Dec 16 13:39:16.409: INFO: stdout: "pod/agnhost-primary-jpvrr patched\n"
    STEP: checking annotations 12/16/22 13:39:16.409
    Dec 16 13:39:16.413: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 16 13:39:16.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:16.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4794" for this suite. 12/16/22 13:39:16.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:16.424
Dec 16 13:39:16.424: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 13:39:16.425
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:16.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:16.444
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 12/16/22 13:39:16.45
STEP: delete the rc 12/16/22 13:39:21.46
STEP: wait for the rc to be deleted 12/16/22 13:39:21.466
Dec 16 13:39:22.483: INFO: 80 pods remaining
Dec 16 13:39:22.483: INFO: 80 pods has nil DeletionTimestamp
Dec 16 13:39:22.483: INFO: 
Dec 16 13:39:23.502: INFO: 71 pods remaining
Dec 16 13:39:23.502: INFO: 70 pods has nil DeletionTimestamp
Dec 16 13:39:23.502: INFO: 
Dec 16 13:39:24.480: INFO: 60 pods remaining
Dec 16 13:39:24.480: INFO: 60 pods has nil DeletionTimestamp
Dec 16 13:39:24.480: INFO: 
Dec 16 13:39:25.478: INFO: 40 pods remaining
Dec 16 13:39:25.478: INFO: 40 pods has nil DeletionTimestamp
Dec 16 13:39:25.478: INFO: 
Dec 16 13:39:26.481: INFO: 31 pods remaining
Dec 16 13:39:26.481: INFO: 31 pods has nil DeletionTimestamp
Dec 16 13:39:26.481: INFO: 
Dec 16 13:39:27.484: INFO: 20 pods remaining
Dec 16 13:39:27.484: INFO: 20 pods has nil DeletionTimestamp
Dec 16 13:39:27.484: INFO: 
STEP: Gathering metrics 12/16/22 13:39:28.473
W1216 13:39:28.480952      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 13:39:28.481: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:28.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8938" for this suite. 12/16/22 13:39:28.484
------------------------------
• [SLOW TEST] [12.065 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:16.424
    Dec 16 13:39:16.424: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 13:39:16.425
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:16.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:16.444
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 12/16/22 13:39:16.45
    STEP: delete the rc 12/16/22 13:39:21.46
    STEP: wait for the rc to be deleted 12/16/22 13:39:21.466
    Dec 16 13:39:22.483: INFO: 80 pods remaining
    Dec 16 13:39:22.483: INFO: 80 pods has nil DeletionTimestamp
    Dec 16 13:39:22.483: INFO: 
    Dec 16 13:39:23.502: INFO: 71 pods remaining
    Dec 16 13:39:23.502: INFO: 70 pods has nil DeletionTimestamp
    Dec 16 13:39:23.502: INFO: 
    Dec 16 13:39:24.480: INFO: 60 pods remaining
    Dec 16 13:39:24.480: INFO: 60 pods has nil DeletionTimestamp
    Dec 16 13:39:24.480: INFO: 
    Dec 16 13:39:25.478: INFO: 40 pods remaining
    Dec 16 13:39:25.478: INFO: 40 pods has nil DeletionTimestamp
    Dec 16 13:39:25.478: INFO: 
    Dec 16 13:39:26.481: INFO: 31 pods remaining
    Dec 16 13:39:26.481: INFO: 31 pods has nil DeletionTimestamp
    Dec 16 13:39:26.481: INFO: 
    Dec 16 13:39:27.484: INFO: 20 pods remaining
    Dec 16 13:39:27.484: INFO: 20 pods has nil DeletionTimestamp
    Dec 16 13:39:27.484: INFO: 
    STEP: Gathering metrics 12/16/22 13:39:28.473
    W1216 13:39:28.480952      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 13:39:28.481: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:28.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8938" for this suite. 12/16/22 13:39:28.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:28.492
Dec 16 13:39:28.492: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 13:39:28.493
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:28.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:28.528
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 12/16/22 13:39:28.53
Dec 16 13:39:28.539: INFO: Waiting up to 5m0s for pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192" in namespace "var-expansion-4584" to be "Succeeded or Failed"
Dec 16 13:39:28.541: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.841172ms
Dec 16 13:39:30.548: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009273062s
Dec 16 13:39:32.547: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008589803s
Dec 16 13:39:34.546: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007466321s
Dec 16 13:39:36.546: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007901079s
STEP: Saw pod success 12/16/22 13:39:36.546
Dec 16 13:39:36.547: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192" satisfied condition "Succeeded or Failed"
Dec 16 13:39:36.550: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 container dapi-container: <nil>
STEP: delete the pod 12/16/22 13:39:36.557
Dec 16 13:39:36.571: INFO: Waiting for pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 to disappear
Dec 16 13:39:36.576: INFO: Pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:36.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4584" for this suite. 12/16/22 13:39:36.581
------------------------------
• [SLOW TEST] [8.095 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:28.492
    Dec 16 13:39:28.492: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 13:39:28.493
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:28.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:28.528
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 12/16/22 13:39:28.53
    Dec 16 13:39:28.539: INFO: Waiting up to 5m0s for pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192" in namespace "var-expansion-4584" to be "Succeeded or Failed"
    Dec 16 13:39:28.541: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.841172ms
    Dec 16 13:39:30.548: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009273062s
    Dec 16 13:39:32.547: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008589803s
    Dec 16 13:39:34.546: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007466321s
    Dec 16 13:39:36.546: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007901079s
    STEP: Saw pod success 12/16/22 13:39:36.546
    Dec 16 13:39:36.547: INFO: Pod "var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192" satisfied condition "Succeeded or Failed"
    Dec 16 13:39:36.550: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 13:39:36.557
    Dec 16 13:39:36.571: INFO: Waiting for pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 to disappear
    Dec 16 13:39:36.576: INFO: Pod var-expansion-f3f2a083-c939-4bce-a425-9f91ce9df192 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:36.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4584" for this suite. 12/16/22 13:39:36.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:36.588
Dec 16 13:39:36.588: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:39:36.588
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:36.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:36.605
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:39:36.623
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:39:36.879
STEP: Deploying the webhook pod 12/16/22 13:39:36.887
STEP: Wait for the deployment to be ready 12/16/22 13:39:36.899
Dec 16 13:39:36.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 13:39:38.917
STEP: Verifying the service has paired with the endpoint 12/16/22 13:39:38.949
Dec 16 13:39:39.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Dec 16 13:39:39.954: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8787-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 13:39:40.466
STEP: Creating a custom resource that should be mutated by the webhook 12/16/22 13:39:40.514
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:43.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1168" for this suite. 12/16/22 13:39:43.189
STEP: Destroying namespace "webhook-1168-markers" for this suite. 12/16/22 13:39:43.194
------------------------------
• [SLOW TEST] [6.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:36.588
    Dec 16 13:39:36.588: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:39:36.588
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:36.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:36.605
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:39:36.623
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:39:36.879
    STEP: Deploying the webhook pod 12/16/22 13:39:36.887
    STEP: Wait for the deployment to be ready 12/16/22 13:39:36.899
    Dec 16 13:39:36.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 13:39:38.917
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:39:38.949
    Dec 16 13:39:39.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Dec 16 13:39:39.954: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8787-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 13:39:40.466
    STEP: Creating a custom resource that should be mutated by the webhook 12/16/22 13:39:40.514
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:43.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1168" for this suite. 12/16/22 13:39:43.189
    STEP: Destroying namespace "webhook-1168-markers" for this suite. 12/16/22 13:39:43.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:43.201
Dec 16 13:39:43.201: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:39:43.202
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:43.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:43.221
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:39:43.223
Dec 16 13:39:43.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58" in namespace "downward-api-8207" to be "Succeeded or Failed"
Dec 16 13:39:43.234: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993703ms
Dec 16 13:39:45.239: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008153521s
Dec 16 13:39:47.240: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00938782s
STEP: Saw pod success 12/16/22 13:39:47.24
Dec 16 13:39:47.241: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58" satisfied condition "Succeeded or Failed"
Dec 16 13:39:47.244: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 container client-container: <nil>
STEP: delete the pod 12/16/22 13:39:47.252
Dec 16 13:39:47.266: INFO: Waiting for pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 to disappear
Dec 16 13:39:47.270: INFO: Pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:47.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8207" for this suite. 12/16/22 13:39:47.274
------------------------------
• [4.080 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:43.201
    Dec 16 13:39:43.201: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:39:43.202
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:43.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:43.221
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:39:43.223
    Dec 16 13:39:43.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58" in namespace "downward-api-8207" to be "Succeeded or Failed"
    Dec 16 13:39:43.234: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993703ms
    Dec 16 13:39:45.239: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008153521s
    Dec 16 13:39:47.240: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00938782s
    STEP: Saw pod success 12/16/22 13:39:47.24
    Dec 16 13:39:47.241: INFO: Pod "downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58" satisfied condition "Succeeded or Failed"
    Dec 16 13:39:47.244: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:39:47.252
    Dec 16 13:39:47.266: INFO: Waiting for pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 to disappear
    Dec 16 13:39:47.270: INFO: Pod downwardapi-volume-32a6da93-0f69-4643-ae8f-620562169e58 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:47.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8207" for this suite. 12/16/22 13:39:47.274
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:47.282
Dec 16 13:39:47.282: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:39:47.282
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:47.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:47.299
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3260 12/16/22 13:39:47.302
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/16/22 13:39:47.321
STEP: creating service externalsvc in namespace services-3260 12/16/22 13:39:47.321
STEP: creating replication controller externalsvc in namespace services-3260 12/16/22 13:39:47.336
I1216 13:39:47.341911      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3260, replica count: 2
I1216 13:39:50.392902      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 12/16/22 13:39:50.396
Dec 16 13:39:50.416: INFO: Creating new exec pod
Dec 16 13:39:50.425: INFO: Waiting up to 5m0s for pod "execpodsr8kz" in namespace "services-3260" to be "running"
Dec 16 13:39:50.428: INFO: Pod "execpodsr8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118996ms
Dec 16 13:39:52.431: INFO: Pod "execpodsr8kz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006649571s
Dec 16 13:39:52.431: INFO: Pod "execpodsr8kz" satisfied condition "running"
Dec 16 13:39:52.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3260 exec execpodsr8kz -- /bin/sh -x -c nslookup nodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local'
Dec 16 13:39:52.657: INFO: stderr: "+ nslookup nodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\n"
Dec 16 13:39:52.657: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\tcanonical name = externalsvc.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local.\nName:\texternalsvc.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\nAddress: 10.100.103.187\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3260, will wait for the garbage collector to delete the pods 12/16/22 13:39:52.657
Dec 16 13:39:52.718: INFO: Deleting ReplicationController externalsvc took: 5.98377ms
Dec 16 13:39:52.818: INFO: Terminating ReplicationController externalsvc pods took: 100.578369ms
Dec 16 13:39:54.952: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:39:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3260" for this suite. 12/16/22 13:39:54.964
------------------------------
• [SLOW TEST] [7.688 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:47.282
    Dec 16 13:39:47.282: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:39:47.282
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:47.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:47.299
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3260 12/16/22 13:39:47.302
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/16/22 13:39:47.321
    STEP: creating service externalsvc in namespace services-3260 12/16/22 13:39:47.321
    STEP: creating replication controller externalsvc in namespace services-3260 12/16/22 13:39:47.336
    I1216 13:39:47.341911      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3260, replica count: 2
    I1216 13:39:50.392902      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 12/16/22 13:39:50.396
    Dec 16 13:39:50.416: INFO: Creating new exec pod
    Dec 16 13:39:50.425: INFO: Waiting up to 5m0s for pod "execpodsr8kz" in namespace "services-3260" to be "running"
    Dec 16 13:39:50.428: INFO: Pod "execpodsr8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118996ms
    Dec 16 13:39:52.431: INFO: Pod "execpodsr8kz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006649571s
    Dec 16 13:39:52.431: INFO: Pod "execpodsr8kz" satisfied condition "running"
    Dec 16 13:39:52.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3260 exec execpodsr8kz -- /bin/sh -x -c nslookup nodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local'
    Dec 16 13:39:52.657: INFO: stderr: "+ nslookup nodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\n"
    Dec 16 13:39:52.657: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\tcanonical name = externalsvc.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local.\nName:\texternalsvc.services-3260.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\nAddress: 10.100.103.187\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3260, will wait for the garbage collector to delete the pods 12/16/22 13:39:52.657
    Dec 16 13:39:52.718: INFO: Deleting ReplicationController externalsvc took: 5.98377ms
    Dec 16 13:39:52.818: INFO: Terminating ReplicationController externalsvc pods took: 100.578369ms
    Dec 16 13:39:54.952: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:39:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3260" for this suite. 12/16/22 13:39:54.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:54.971
Dec 16 13:39:54.971: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption 12/16/22 13:39:54.972
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:54.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:54.99
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:39:54.992
Dec 16 13:39:54.992: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption-2 12/16/22 13:39:54.993
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:55.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:55.009
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 12/16/22 13:39:55.016
STEP: Waiting for the pdb to be processed 12/16/22 13:39:57.041
STEP: Waiting for the pdb to be processed 12/16/22 13:39:59.053
STEP: listing a collection of PDBs across all namespaces 12/16/22 13:40:01.063
STEP: listing a collection of PDBs in namespace disruption-961 12/16/22 13:40:01.066
STEP: deleting a collection of PDBs 12/16/22 13:40:01.542
STEP: Waiting for the PDB collection to be deleted 12/16/22 13:40:01.556
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:01.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-8362" for this suite. 12/16/22 13:40:01.567
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-961" for this suite. 12/16/22 13:40:01.573
------------------------------
• [SLOW TEST] [6.609 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:54.971
    Dec 16 13:39:54.971: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption 12/16/22 13:39:54.972
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:54.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:54.99
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:39:54.992
    Dec 16 13:39:54.992: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption-2 12/16/22 13:39:54.993
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:39:55.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:39:55.009
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 12/16/22 13:39:55.016
    STEP: Waiting for the pdb to be processed 12/16/22 13:39:57.041
    STEP: Waiting for the pdb to be processed 12/16/22 13:39:59.053
    STEP: listing a collection of PDBs across all namespaces 12/16/22 13:40:01.063
    STEP: listing a collection of PDBs in namespace disruption-961 12/16/22 13:40:01.066
    STEP: deleting a collection of PDBs 12/16/22 13:40:01.542
    STEP: Waiting for the PDB collection to be deleted 12/16/22 13:40:01.556
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:01.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-8362" for this suite. 12/16/22 13:40:01.567
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-961" for this suite. 12/16/22 13:40:01.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:01.58
Dec 16 13:40:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:40:01.581
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:01.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:01.6
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 12/16/22 13:40:01.603
Dec 16 13:40:01.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 create -f -'
Dec 16 13:40:02.460: INFO: stderr: ""
Dec 16 13:40:02.460: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 12/16/22 13:40:02.46
Dec 16 13:40:02.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 diff -f -'
Dec 16 13:40:02.666: INFO: rc: 1
Dec 16 13:40:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 delete -f -'
Dec 16 13:40:02.743: INFO: stderr: ""
Dec 16 13:40:02.743: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:02.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1928" for this suite. 12/16/22 13:40:02.748
------------------------------
• [1.174 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:01.58
    Dec 16 13:40:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:40:01.581
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:01.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:01.6
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 12/16/22 13:40:01.603
    Dec 16 13:40:01.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 create -f -'
    Dec 16 13:40:02.460: INFO: stderr: ""
    Dec 16 13:40:02.460: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 12/16/22 13:40:02.46
    Dec 16 13:40:02.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 diff -f -'
    Dec 16 13:40:02.666: INFO: rc: 1
    Dec 16 13:40:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-1928 delete -f -'
    Dec 16 13:40:02.743: INFO: stderr: ""
    Dec 16 13:40:02.743: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:02.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1928" for this suite. 12/16/22 13:40:02.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:02.755
Dec 16 13:40:02.755: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:40:02.756
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:02.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:02.789
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-cb9821ca-3ef7-4d79-93c9-96bc37733dcb 12/16/22 13:40:02.796
STEP: Creating configMap with name cm-test-opt-upd-39181f9f-eadc-4608-b924-7ac16bf32e44 12/16/22 13:40:02.8
STEP: Creating the pod 12/16/22 13:40:02.804
Dec 16 13:40:02.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1" in namespace "configmap-8955" to be "running and ready"
Dec 16 13:40:02.820: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030733ms
Dec 16 13:40:02.820: INFO: The phase of Pod pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:40:04.826: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010423082s
Dec 16 13:40:04.826: INFO: The phase of Pod pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1 is Running (Ready = true)
Dec 16 13:40:04.826: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-cb9821ca-3ef7-4d79-93c9-96bc37733dcb 12/16/22 13:40:04.854
STEP: Updating configmap cm-test-opt-upd-39181f9f-eadc-4608-b924-7ac16bf32e44 12/16/22 13:40:04.86
STEP: Creating configMap with name cm-test-opt-create-0414fdd4-1a9c-47d8-a8c7-4313f17cdafb 12/16/22 13:40:04.865
STEP: waiting to observe update in volume 12/16/22 13:40:04.869
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:06.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8955" for this suite. 12/16/22 13:40:06.903
------------------------------
• [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:02.755
    Dec 16 13:40:02.755: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:40:02.756
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:02.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:02.789
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-cb9821ca-3ef7-4d79-93c9-96bc37733dcb 12/16/22 13:40:02.796
    STEP: Creating configMap with name cm-test-opt-upd-39181f9f-eadc-4608-b924-7ac16bf32e44 12/16/22 13:40:02.8
    STEP: Creating the pod 12/16/22 13:40:02.804
    Dec 16 13:40:02.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1" in namespace "configmap-8955" to be "running and ready"
    Dec 16 13:40:02.820: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030733ms
    Dec 16 13:40:02.820: INFO: The phase of Pod pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:40:04.826: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010423082s
    Dec 16 13:40:04.826: INFO: The phase of Pod pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1 is Running (Ready = true)
    Dec 16 13:40:04.826: INFO: Pod "pod-configmaps-d90f8e8d-e582-40e0-b013-62e775f689b1" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-cb9821ca-3ef7-4d79-93c9-96bc37733dcb 12/16/22 13:40:04.854
    STEP: Updating configmap cm-test-opt-upd-39181f9f-eadc-4608-b924-7ac16bf32e44 12/16/22 13:40:04.86
    STEP: Creating configMap with name cm-test-opt-create-0414fdd4-1a9c-47d8-a8c7-4313f17cdafb 12/16/22 13:40:04.865
    STEP: waiting to observe update in volume 12/16/22 13:40:04.869
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:06.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8955" for this suite. 12/16/22 13:40:06.903
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:06.909
Dec 16 13:40:06.909: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:40:06.91
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:06.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:06.94
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Dec 16 13:40:06.951: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987" in namespace "kubelet-test-7184" to be "running and ready"
Dec 16 13:40:06.954: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789969ms
Dec 16 13:40:06.954: INFO: The phase of Pod busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:40:08.960: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987": Phase="Running", Reason="", readiness=true. Elapsed: 2.008657248s
Dec 16 13:40:08.960: INFO: The phase of Pod busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987 is Running (Ready = true)
Dec 16 13:40:08.960: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:08.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7184" for this suite. 12/16/22 13:40:08.976
------------------------------
• [2.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:06.909
    Dec 16 13:40:06.909: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubelet-test 12/16/22 13:40:06.91
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:06.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:06.94
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Dec 16 13:40:06.951: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987" in namespace "kubelet-test-7184" to be "running and ready"
    Dec 16 13:40:06.954: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789969ms
    Dec 16 13:40:06.954: INFO: The phase of Pod busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:40:08.960: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987": Phase="Running", Reason="", readiness=true. Elapsed: 2.008657248s
    Dec 16 13:40:08.960: INFO: The phase of Pod busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987 is Running (Ready = true)
    Dec 16 13:40:08.960: INFO: Pod "busybox-readonly-fs1976e96f-8465-4553-9490-d4338bdb5987" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:08.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7184" for this suite. 12/16/22 13:40:08.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:08.985
Dec 16 13:40:08.985: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename podtemplate 12/16/22 13:40:08.985
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.005
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2652" for this suite. 12/16/22 13:40:09.055
------------------------------
• [0.077 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:08.985
    Dec 16 13:40:08.985: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename podtemplate 12/16/22 13:40:08.985
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.005
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2652" for this suite. 12/16/22 13:40:09.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:09.062
Dec 16 13:40:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:40:09.063
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.082
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-68c36205-67ce-4628-b87a-40f23992faf7 12/16/22 13:40:09.085
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:09.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6387" for this suite. 12/16/22 13:40:09.09
------------------------------
• [0.034 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:09.062
    Dec 16 13:40:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:40:09.063
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.082
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-68c36205-67ce-4628-b87a-40f23992faf7 12/16/22 13:40:09.085
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:09.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6387" for this suite. 12/16/22 13:40:09.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:09.097
Dec 16 13:40:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename proxy 12/16/22 13:40:09.098
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.116
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 12/16/22 13:40:09.133
STEP: creating replication controller proxy-service-b5548 in namespace proxy-5375 12/16/22 13:40:09.133
I1216 13:40:09.138739      21 runners.go:193] Created replication controller with name: proxy-service-b5548, namespace: proxy-5375, replica count: 1
I1216 13:40:10.190964      21 runners.go:193] proxy-service-b5548 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1216 13:40:11.191781      21 runners.go:193] proxy-service-b5548 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 13:40:11.196: INFO: setup took 2.078358407s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/16/22 13:40:11.196
Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 44.391716ms)
Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 44.713996ms)
Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 44.556319ms)
Dec 16 13:40:11.242: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 45.944133ms)
Dec 16 13:40:11.242: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 45.915788ms)
Dec 16 13:40:11.243: INFO: (0) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 46.668521ms)
Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 49.579429ms)
Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 49.859892ms)
Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 49.625191ms)
Dec 16 13:40:11.247: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 50.698114ms)
Dec 16 13:40:11.247: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 50.830271ms)
Dec 16 13:40:11.250: INFO: (0) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 53.207987ms)
Dec 16 13:40:11.250: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 53.792678ms)
Dec 16 13:40:11.251: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 54.468935ms)
Dec 16 13:40:11.251: INFO: (0) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 54.660153ms)
Dec 16 13:40:11.254: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 57.137728ms)
Dec 16 13:40:11.258: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.522952ms)
Dec 16 13:40:11.258: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.701423ms)
Dec 16 13:40:11.259: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.24273ms)
Dec 16 13:40:11.261: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.468428ms)
Dec 16 13:40:11.261: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.597352ms)
Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 31.548998ms)
Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 31.468076ms)
Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 31.936071ms)
Dec 16 13:40:11.287: INFO: (1) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 33.070108ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 74.611912ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 74.584725ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 74.208444ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 74.5516ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 74.589679ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 74.545665ms)
Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 74.509502ms)
Dec 16 13:40:11.334: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.276825ms)
Dec 16 13:40:11.334: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.460664ms)
Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.774845ms)
Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.795869ms)
Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.519268ms)
Dec 16 13:40:11.336: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.618866ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 8.362383ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 8.324067ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.362923ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.247978ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.392096ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.32987ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.336363ms)
Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.491544ms)
Dec 16 13:40:11.338: INFO: (2) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 9.030456ms)
Dec 16 13:40:11.338: INFO: (2) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 9.469077ms)
Dec 16 13:40:11.343: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.228402ms)
Dec 16 13:40:11.343: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.811316ms)
Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.580262ms)
Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.651322ms)
Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.716022ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 6.260537ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.807212ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.753889ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.681533ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.049408ms)
Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.868948ms)
Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.218518ms)
Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.499491ms)
Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.839108ms)
Dec 16 13:40:11.347: INFO: (3) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.583549ms)
Dec 16 13:40:11.347: INFO: (3) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.78629ms)
Dec 16 13:40:11.352: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.317619ms)
Dec 16 13:40:11.352: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.58593ms)
Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.876493ms)
Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.733703ms)
Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.778157ms)
Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.933241ms)
Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.923168ms)
Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.609734ms)
Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.665449ms)
Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.875657ms)
Dec 16 13:40:11.425: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 77.990854ms)
Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 78.355918ms)
Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 78.386389ms)
Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 78.455368ms)
Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 78.458724ms)
Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 78.462025ms)
Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.87968ms)
Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.214879ms)
Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.182229ms)
Dec 16 13:40:11.433: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.567348ms)
Dec 16 13:40:11.433: INFO: (5) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.314096ms)
Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.480632ms)
Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.592437ms)
Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 7.663005ms)
Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.75961ms)
Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 7.749667ms)
Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 11.115062ms)
Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 11.022405ms)
Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 11.233433ms)
Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 11.436043ms)
Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 11.22618ms)
Dec 16 13:40:11.438: INFO: (5) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 11.378284ms)
Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 3.940329ms)
Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.071305ms)
Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.343529ms)
Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.434189ms)
Dec 16 13:40:11.443: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.941481ms)
Dec 16 13:40:11.443: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.02734ms)
Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.776203ms)
Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.939853ms)
Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.165153ms)
Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.405287ms)
Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.570687ms)
Dec 16 13:40:11.445: INFO: (6) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.657416ms)
Dec 16 13:40:11.445: INFO: (6) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.453731ms)
Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 9.77241ms)
Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 9.831831ms)
Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 9.994629ms)
Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 3.806509ms)
Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 4.183319ms)
Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.589518ms)
Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.387886ms)
Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.283341ms)
Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.161604ms)
Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 5.534139ms)
Dec 16 13:40:11.454: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.725361ms)
Dec 16 13:40:11.454: INFO: (7) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 6.313669ms)
Dec 16 13:40:11.455: INFO: (7) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 6.774744ms)
Dec 16 13:40:11.455: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.895712ms)
Dec 16 13:40:11.479: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 31.551889ms)
Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 77.759275ms)
Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 77.829336ms)
Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 78.143721ms)
Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 77.926837ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.506803ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.619ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.796862ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.934676ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.253227ms)
Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.896452ms)
Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.572ms)
Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.632864ms)
Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.966751ms)
Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.922622ms)
Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.106983ms)
Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 7.606486ms)
Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.701371ms)
Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.676857ms)
Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.897799ms)
Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.253713ms)
Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.376317ms)
Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.574862ms)
Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.752326ms)
Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.73987ms)
Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.637085ms)
Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.411441ms)
Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.181131ms)
Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.323604ms)
Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.620348ms)
Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.390122ms)
Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 6.463425ms)
Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 6.579813ms)
Dec 16 13:40:11.542: INFO: (9) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 6.857304ms)
Dec 16 13:40:11.542: INFO: (9) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.142537ms)
Dec 16 13:40:11.543: INFO: (9) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.618499ms)
Dec 16 13:40:11.543: INFO: (9) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.644331ms)
Dec 16 13:40:11.548: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.175684ms)
Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.109426ms)
Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.332004ms)
Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.360619ms)
Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.276316ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.210889ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.265449ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.366222ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.486019ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.540009ms)
Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.766185ms)
Dec 16 13:40:11.551: INFO: (10) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.621774ms)
Dec 16 13:40:11.551: INFO: (10) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.608554ms)
Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.075604ms)
Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.296949ms)
Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.495307ms)
Dec 16 13:40:11.563: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 10.263971ms)
Dec 16 13:40:11.563: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 10.494722ms)
Dec 16 13:40:11.564: INFO: (11) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 11.995679ms)
Dec 16 13:40:11.564: INFO: (11) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 12.097205ms)
Dec 16 13:40:11.565: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 12.123668ms)
Dec 16 13:40:11.566: INFO: (11) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 13.106269ms)
Dec 16 13:40:11.566: INFO: (11) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 13.326307ms)
Dec 16 13:40:11.625: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 73.341064ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 73.461557ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 73.57543ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 73.529684ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 73.799974ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 73.675823ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 73.581656ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 73.829731ms)
Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 73.67148ms)
Dec 16 13:40:11.631: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.341633ms)
Dec 16 13:40:11.631: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.516187ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.540354ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.373229ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.319991ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.389534ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.475004ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.437354ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.643001ms)
Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.479514ms)
Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.06452ms)
Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.030623ms)
Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.142229ms)
Dec 16 13:40:11.644: INFO: (12) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 17.797358ms)
Dec 16 13:40:11.644: INFO: (12) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 17.782822ms)
Dec 16 13:40:11.646: INFO: (12) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 19.377221ms)
Dec 16 13:40:11.650: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.091802ms)
Dec 16 13:40:11.651: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.7234ms)
Dec 16 13:40:11.651: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.705601ms)
Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.702543ms)
Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.09473ms)
Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.175717ms)
Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.108678ms)
Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.140034ms)
Dec 16 13:40:11.653: INFO: (13) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.817013ms)
Dec 16 13:40:11.653: INFO: (13) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.916643ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.233621ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.260755ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 7.26843ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 7.253196ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 7.357384ms)
Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.186555ms)
Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.066129ms)
Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.177854ms)
Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.493204ms)
Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.344013ms)
Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.655404ms)
Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.333254ms)
Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 5.667014ms)
Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.414365ms)
Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.765276ms)
Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.143592ms)
Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.191917ms)
Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.711933ms)
Dec 16 13:40:11.662: INFO: (14) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.850307ms)
Dec 16 13:40:11.662: INFO: (14) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 7.904282ms)
Dec 16 13:40:11.663: INFO: (14) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.000608ms)
Dec 16 13:40:11.725: INFO: (14) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 70.750227ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.204331ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.364115ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.439519ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.126887ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.46876ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.823966ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.695194ms)
Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.760944ms)
Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 6.9793ms)
Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.503542ms)
Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.883439ms)
Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.571392ms)
Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.581062ms)
Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 31.082514ms)
Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 30.987343ms)
Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 31.068628ms)
Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.143234ms)
Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 5.465889ms)
Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.385304ms)
Dec 16 13:40:11.764: INFO: (16) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.258662ms)
Dec 16 13:40:11.764: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 7.262301ms)
Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.632916ms)
Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.787248ms)
Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.961071ms)
Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.346769ms)
Dec 16 13:40:11.766: INFO: (16) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 9.261979ms)
Dec 16 13:40:11.769: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 11.63046ms)
Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 13.147594ms)
Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 13.208502ms)
Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 13.278405ms)
Dec 16 13:40:11.771: INFO: (16) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 14.046454ms)
Dec 16 13:40:11.776: INFO: (16) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 19.130205ms)
Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.277817ms)
Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.367428ms)
Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.464308ms)
Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.568965ms)
Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.153986ms)
Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.405368ms)
Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.522444ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.085149ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.82882ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.904933ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 7.941987ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.048955ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.973094ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 7.952619ms)
Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 8.090748ms)
Dec 16 13:40:11.827: INFO: (17) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 50.3648ms)
Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.113575ms)
Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.146397ms)
Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.293863ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.455258ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.728149ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.237302ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.360207ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 6.368653ms)
Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.502583ms)
Dec 16 13:40:11.834: INFO: (18) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.086368ms)
Dec 16 13:40:11.834: INFO: (18) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.535874ms)
Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.273404ms)
Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.451395ms)
Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 8.21565ms)
Dec 16 13:40:11.836: INFO: (18) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 9.154216ms)
Dec 16 13:40:11.837: INFO: (18) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 10.008213ms)
Dec 16 13:40:11.842: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.024029ms)
Dec 16 13:40:11.842: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.154672ms)
Dec 16 13:40:11.843: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.615576ms)
Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.382743ms)
Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.743916ms)
Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.759118ms)
Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.720483ms)
Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.820448ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.603658ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 7.510911ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.859291ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.071722ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.031782ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.855306ms)
Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.232355ms)
Dec 16 13:40:11.846: INFO: (19) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.534776ms)
STEP: deleting ReplicationController proxy-service-b5548 in namespace proxy-5375, will wait for the garbage collector to delete the pods 12/16/22 13:40:11.846
Dec 16 13:40:11.906: INFO: Deleting ReplicationController proxy-service-b5548 took: 7.071002ms
Dec 16 13:40:12.007: INFO: Terminating ReplicationController proxy-service-b5548 pods took: 100.65959ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5375" for this suite. 12/16/22 13:40:14.013
------------------------------
• [4.921 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:09.097
    Dec 16 13:40:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename proxy 12/16/22 13:40:09.098
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:09.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:09.116
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 12/16/22 13:40:09.133
    STEP: creating replication controller proxy-service-b5548 in namespace proxy-5375 12/16/22 13:40:09.133
    I1216 13:40:09.138739      21 runners.go:193] Created replication controller with name: proxy-service-b5548, namespace: proxy-5375, replica count: 1
    I1216 13:40:10.190964      21 runners.go:193] proxy-service-b5548 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I1216 13:40:11.191781      21 runners.go:193] proxy-service-b5548 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 13:40:11.196: INFO: setup took 2.078358407s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/16/22 13:40:11.196
    Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 44.391716ms)
    Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 44.713996ms)
    Dec 16 13:40:11.241: INFO: (0) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 44.556319ms)
    Dec 16 13:40:11.242: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 45.944133ms)
    Dec 16 13:40:11.242: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 45.915788ms)
    Dec 16 13:40:11.243: INFO: (0) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 46.668521ms)
    Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 49.579429ms)
    Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 49.859892ms)
    Dec 16 13:40:11.246: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 49.625191ms)
    Dec 16 13:40:11.247: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 50.698114ms)
    Dec 16 13:40:11.247: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 50.830271ms)
    Dec 16 13:40:11.250: INFO: (0) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 53.207987ms)
    Dec 16 13:40:11.250: INFO: (0) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 53.792678ms)
    Dec 16 13:40:11.251: INFO: (0) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 54.468935ms)
    Dec 16 13:40:11.251: INFO: (0) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 54.660153ms)
    Dec 16 13:40:11.254: INFO: (0) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 57.137728ms)
    Dec 16 13:40:11.258: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.522952ms)
    Dec 16 13:40:11.258: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.701423ms)
    Dec 16 13:40:11.259: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.24273ms)
    Dec 16 13:40:11.261: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.468428ms)
    Dec 16 13:40:11.261: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.597352ms)
    Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 31.548998ms)
    Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 31.468076ms)
    Dec 16 13:40:11.286: INFO: (1) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 31.936071ms)
    Dec 16 13:40:11.287: INFO: (1) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 33.070108ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 74.611912ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 74.584725ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 74.208444ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 74.5516ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 74.589679ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 74.545665ms)
    Dec 16 13:40:11.329: INFO: (1) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 74.509502ms)
    Dec 16 13:40:11.334: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.276825ms)
    Dec 16 13:40:11.334: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.460664ms)
    Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.774845ms)
    Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.795869ms)
    Dec 16 13:40:11.335: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.519268ms)
    Dec 16 13:40:11.336: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.618866ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 8.362383ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 8.324067ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.362923ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.247978ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.392096ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.32987ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.336363ms)
    Dec 16 13:40:11.337: INFO: (2) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.491544ms)
    Dec 16 13:40:11.338: INFO: (2) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 9.030456ms)
    Dec 16 13:40:11.338: INFO: (2) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 9.469077ms)
    Dec 16 13:40:11.343: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.228402ms)
    Dec 16 13:40:11.343: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.811316ms)
    Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.580262ms)
    Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.651322ms)
    Dec 16 13:40:11.344: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.716022ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 6.260537ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.807212ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.753889ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.681533ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.049408ms)
    Dec 16 13:40:11.345: INFO: (3) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.868948ms)
    Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.218518ms)
    Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.499491ms)
    Dec 16 13:40:11.346: INFO: (3) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.839108ms)
    Dec 16 13:40:11.347: INFO: (3) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.583549ms)
    Dec 16 13:40:11.347: INFO: (3) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.78629ms)
    Dec 16 13:40:11.352: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.317619ms)
    Dec 16 13:40:11.352: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.58593ms)
    Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.876493ms)
    Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.733703ms)
    Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.778157ms)
    Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.933241ms)
    Dec 16 13:40:11.353: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.923168ms)
    Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.609734ms)
    Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.665449ms)
    Dec 16 13:40:11.356: INFO: (4) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.875657ms)
    Dec 16 13:40:11.425: INFO: (4) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 77.990854ms)
    Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 78.355918ms)
    Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 78.386389ms)
    Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 78.455368ms)
    Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 78.458724ms)
    Dec 16 13:40:11.426: INFO: (4) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 78.462025ms)
    Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.87968ms)
    Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.214879ms)
    Dec 16 13:40:11.432: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.182229ms)
    Dec 16 13:40:11.433: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.567348ms)
    Dec 16 13:40:11.433: INFO: (5) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.314096ms)
    Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.480632ms)
    Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.592437ms)
    Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 7.663005ms)
    Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.75961ms)
    Dec 16 13:40:11.434: INFO: (5) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 7.749667ms)
    Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 11.115062ms)
    Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 11.022405ms)
    Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 11.233433ms)
    Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 11.436043ms)
    Dec 16 13:40:11.437: INFO: (5) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 11.22618ms)
    Dec 16 13:40:11.438: INFO: (5) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 11.378284ms)
    Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 3.940329ms)
    Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.071305ms)
    Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.343529ms)
    Dec 16 13:40:11.442: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.434189ms)
    Dec 16 13:40:11.443: INFO: (6) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 4.941481ms)
    Dec 16 13:40:11.443: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.02734ms)
    Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.776203ms)
    Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.939853ms)
    Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.165153ms)
    Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.405287ms)
    Dec 16 13:40:11.444: INFO: (6) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.570687ms)
    Dec 16 13:40:11.445: INFO: (6) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.657416ms)
    Dec 16 13:40:11.445: INFO: (6) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.453731ms)
    Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 9.77241ms)
    Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 9.831831ms)
    Dec 16 13:40:11.448: INFO: (6) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 9.994629ms)
    Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 3.806509ms)
    Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 4.183319ms)
    Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.589518ms)
    Dec 16 13:40:11.452: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.387886ms)
    Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.283341ms)
    Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.161604ms)
    Dec 16 13:40:11.453: INFO: (7) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 5.534139ms)
    Dec 16 13:40:11.454: INFO: (7) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.725361ms)
    Dec 16 13:40:11.454: INFO: (7) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 6.313669ms)
    Dec 16 13:40:11.455: INFO: (7) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 6.774744ms)
    Dec 16 13:40:11.455: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.895712ms)
    Dec 16 13:40:11.479: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 31.551889ms)
    Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 77.759275ms)
    Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 77.829336ms)
    Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 78.143721ms)
    Dec 16 13:40:11.526: INFO: (7) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 77.926837ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.506803ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.619ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.796862ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.934676ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.253227ms)
    Dec 16 13:40:11.532: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.896452ms)
    Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.572ms)
    Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.632864ms)
    Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.966751ms)
    Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.922622ms)
    Dec 16 13:40:11.533: INFO: (8) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.106983ms)
    Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 7.606486ms)
    Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.701371ms)
    Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.676857ms)
    Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.897799ms)
    Dec 16 13:40:11.534: INFO: (8) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.253713ms)
    Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.376317ms)
    Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 4.574862ms)
    Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.752326ms)
    Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.73987ms)
    Dec 16 13:40:11.539: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.637085ms)
    Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.411441ms)
    Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.181131ms)
    Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.323604ms)
    Dec 16 13:40:11.540: INFO: (9) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.620348ms)
    Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.390122ms)
    Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 6.463425ms)
    Dec 16 13:40:11.541: INFO: (9) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 6.579813ms)
    Dec 16 13:40:11.542: INFO: (9) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 6.857304ms)
    Dec 16 13:40:11.542: INFO: (9) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.142537ms)
    Dec 16 13:40:11.543: INFO: (9) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.618499ms)
    Dec 16 13:40:11.543: INFO: (9) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.644331ms)
    Dec 16 13:40:11.548: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.175684ms)
    Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.109426ms)
    Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.332004ms)
    Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.360619ms)
    Dec 16 13:40:11.549: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.276316ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.210889ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.265449ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.366222ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.486019ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.540009ms)
    Dec 16 13:40:11.550: INFO: (10) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.766185ms)
    Dec 16 13:40:11.551: INFO: (10) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.621774ms)
    Dec 16 13:40:11.551: INFO: (10) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.608554ms)
    Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.075604ms)
    Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.296949ms)
    Dec 16 13:40:11.552: INFO: (10) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.495307ms)
    Dec 16 13:40:11.563: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 10.263971ms)
    Dec 16 13:40:11.563: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 10.494722ms)
    Dec 16 13:40:11.564: INFO: (11) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 11.995679ms)
    Dec 16 13:40:11.564: INFO: (11) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 12.097205ms)
    Dec 16 13:40:11.565: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 12.123668ms)
    Dec 16 13:40:11.566: INFO: (11) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 13.106269ms)
    Dec 16 13:40:11.566: INFO: (11) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 13.326307ms)
    Dec 16 13:40:11.625: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 73.341064ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 73.461557ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 73.57543ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 73.529684ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 73.799974ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 73.675823ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 73.581656ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 73.829731ms)
    Dec 16 13:40:11.626: INFO: (11) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 73.67148ms)
    Dec 16 13:40:11.631: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.341633ms)
    Dec 16 13:40:11.631: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.516187ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.540354ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.373229ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.319991ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.389534ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.475004ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.437354ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.643001ms)
    Dec 16 13:40:11.633: INFO: (12) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.479514ms)
    Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.06452ms)
    Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.030623ms)
    Dec 16 13:40:11.634: INFO: (12) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.142229ms)
    Dec 16 13:40:11.644: INFO: (12) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 17.797358ms)
    Dec 16 13:40:11.644: INFO: (12) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 17.782822ms)
    Dec 16 13:40:11.646: INFO: (12) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 19.377221ms)
    Dec 16 13:40:11.650: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.091802ms)
    Dec 16 13:40:11.651: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.7234ms)
    Dec 16 13:40:11.651: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.705601ms)
    Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.702543ms)
    Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.09473ms)
    Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.175717ms)
    Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.108678ms)
    Dec 16 13:40:11.652: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.140034ms)
    Dec 16 13:40:11.653: INFO: (13) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 6.817013ms)
    Dec 16 13:40:11.653: INFO: (13) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.916643ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.233621ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.260755ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 7.26843ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 7.253196ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 7.357384ms)
    Dec 16 13:40:11.654: INFO: (13) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.186555ms)
    Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.066129ms)
    Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 4.177854ms)
    Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.493204ms)
    Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.344013ms)
    Dec 16 13:40:11.659: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.655404ms)
    Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.333254ms)
    Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 5.667014ms)
    Dec 16 13:40:11.660: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.414365ms)
    Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.765276ms)
    Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.143592ms)
    Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.191917ms)
    Dec 16 13:40:11.661: INFO: (14) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.711933ms)
    Dec 16 13:40:11.662: INFO: (14) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.850307ms)
    Dec 16 13:40:11.662: INFO: (14) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 7.904282ms)
    Dec 16 13:40:11.663: INFO: (14) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.000608ms)
    Dec 16 13:40:11.725: INFO: (14) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 70.750227ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.204331ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.364115ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 6.439519ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.126887ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.46876ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.823966ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 6.695194ms)
    Dec 16 13:40:11.732: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.760944ms)
    Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 6.9793ms)
    Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.503542ms)
    Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.883439ms)
    Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.571392ms)
    Dec 16 13:40:11.733: INFO: (15) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.581062ms)
    Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 31.082514ms)
    Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 30.987343ms)
    Dec 16 13:40:11.757: INFO: (15) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 31.068628ms)
    Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.143234ms)
    Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 5.465889ms)
    Dec 16 13:40:11.762: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.385304ms)
    Dec 16 13:40:11.764: INFO: (16) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 7.258662ms)
    Dec 16 13:40:11.764: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 7.262301ms)
    Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.632916ms)
    Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.787248ms)
    Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.961071ms)
    Dec 16 13:40:11.765: INFO: (16) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.346769ms)
    Dec 16 13:40:11.766: INFO: (16) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 9.261979ms)
    Dec 16 13:40:11.769: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 11.63046ms)
    Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 13.147594ms)
    Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 13.208502ms)
    Dec 16 13:40:11.770: INFO: (16) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 13.278405ms)
    Dec 16 13:40:11.771: INFO: (16) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 14.046454ms)
    Dec 16 13:40:11.776: INFO: (16) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 19.130205ms)
    Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 4.277817ms)
    Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 4.367428ms)
    Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 4.464308ms)
    Dec 16 13:40:11.781: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 4.568965ms)
    Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.153986ms)
    Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 5.405368ms)
    Dec 16 13:40:11.782: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.522444ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 8.085149ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 7.82882ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 7.904933ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 7.941987ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.048955ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.973094ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 7.952619ms)
    Dec 16 13:40:11.784: INFO: (17) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 8.090748ms)
    Dec 16 13:40:11.827: INFO: (17) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 50.3648ms)
    Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 5.113575ms)
    Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 5.146397ms)
    Dec 16 13:40:11.832: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 5.293863ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 5.455258ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.728149ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 6.237302ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.360207ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 6.368653ms)
    Dec 16 13:40:11.833: INFO: (18) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 6.502583ms)
    Dec 16 13:40:11.834: INFO: (18) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.086368ms)
    Dec 16 13:40:11.834: INFO: (18) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.535874ms)
    Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 8.273404ms)
    Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.451395ms)
    Dec 16 13:40:11.835: INFO: (18) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 8.21565ms)
    Dec 16 13:40:11.836: INFO: (18) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 9.154216ms)
    Dec 16 13:40:11.837: INFO: (18) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 10.008213ms)
    Dec 16 13:40:11.842: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:160/proxy/: foo (200; 5.024029ms)
    Dec 16 13:40:11.842: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg/proxy/rewriteme">test</a> (200; 5.154672ms)
    Dec 16 13:40:11.843: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:162/proxy/: bar (200; 5.615576ms)
    Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:460/proxy/: tls baz (200; 6.382743ms)
    Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:1080/proxy/rewriteme">test<... (200; 6.743916ms)
    Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:1080/proxy/rewriteme">... (200; 6.759118ms)
    Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:462/proxy/: tls qux (200; 6.720483ms)
    Dec 16 13:40:11.844: INFO: (19) /api/v1/namespaces/proxy-5375/pods/http:proxy-service-b5548-9pbjg:160/proxy/: foo (200; 6.820448ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname2/proxy/: tls qux (200; 7.603658ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/: <a href="/api/v1/namespaces/proxy-5375/pods/https:proxy-service-b5548-9pbjg:443/proxy/tlsrewritem... (200; 7.510911ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/pods/proxy-service-b5548-9pbjg:162/proxy/: bar (200; 7.859291ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname1/proxy/: foo (200; 8.071722ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/https:proxy-service-b5548:tlsportname1/proxy/: tls baz (200; 8.031782ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname1/proxy/: foo (200; 7.855306ms)
    Dec 16 13:40:11.845: INFO: (19) /api/v1/namespaces/proxy-5375/services/proxy-service-b5548:portname2/proxy/: bar (200; 8.232355ms)
    Dec 16 13:40:11.846: INFO: (19) /api/v1/namespaces/proxy-5375/services/http:proxy-service-b5548:portname2/proxy/: bar (200; 8.534776ms)
    STEP: deleting ReplicationController proxy-service-b5548 in namespace proxy-5375, will wait for the garbage collector to delete the pods 12/16/22 13:40:11.846
    Dec 16 13:40:11.906: INFO: Deleting ReplicationController proxy-service-b5548 took: 7.071002ms
    Dec 16 13:40:12.007: INFO: Terminating ReplicationController proxy-service-b5548 pods took: 100.65959ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5375" for this suite. 12/16/22 13:40:14.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:14.02
Dec 16 13:40:14.020: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:40:14.021
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:14.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:14.046
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6019 12/16/22 13:40:14.048
STEP: creating a selector 12/16/22 13:40:14.049
STEP: Creating the service pods in kubernetes 12/16/22 13:40:14.049
Dec 16 13:40:14.049: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 13:40:14.075: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6019" to be "running and ready"
Dec 16 13:40:14.078: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584325ms
Dec 16 13:40:14.078: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:40:16.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006920503s
Dec 16 13:40:16.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:40:18.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009073251s
Dec 16 13:40:18.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:40:20.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009086408s
Dec 16 13:40:20.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:40:22.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009366423s
Dec 16 13:40:22.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:40:24.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00848149s
Dec 16 13:40:24.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 13:40:26.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007121939s
Dec 16 13:40:26.083: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 16 13:40:26.083: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 16 13:40:26.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6019" to be "running and ready"
Dec 16 13:40:26.090: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.335521ms
Dec 16 13:40:26.090: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 16 13:40:26.090: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 16 13:40:26.093: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6019" to be "running and ready"
Dec 16 13:40:26.095: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.452019ms
Dec 16 13:40:26.095: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 16 13:40:26.095: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/16/22 13:40:26.098
Dec 16 13:40:26.108: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6019" to be "running"
Dec 16 13:40:26.110: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632951ms
Dec 16 13:40:28.116: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690698s
Dec 16 13:40:28.116: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 16 13:40:28.120: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6019" to be "running"
Dec 16 13:40:28.124: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.960338ms
Dec 16 13:40:28.124: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 16 13:40:28.128: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 16 13:40:28.128: INFO: Going to poll 192.168.156.150 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 16 13:40:28.131: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.156.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:40:28.131: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:40:28.131: INFO: ExecWithOptions: Clientset creation
Dec 16 13:40:28.131: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.156.150+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 13:40:29.267: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 16 13:40:29.267: INFO: Going to poll 192.168.189.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 16 13:40:29.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.189.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:40:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:40:29.272: INFO: ExecWithOptions: Clientset creation
Dec 16 13:40:29.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.189.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 13:40:30.385: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 16 13:40:30.385: INFO: Going to poll 192.168.189.210 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 16 13:40:30.389: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.189.210 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:40:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:40:30.389: INFO: ExecWithOptions: Clientset creation
Dec 16 13:40:30.389: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.189.210+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 13:40:31.515: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:31.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6019" for this suite. 12/16/22 13:40:31.52
------------------------------
• [SLOW TEST] [17.506 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:14.02
    Dec 16 13:40:14.020: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pod-network-test 12/16/22 13:40:14.021
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:14.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:14.046
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6019 12/16/22 13:40:14.048
    STEP: creating a selector 12/16/22 13:40:14.049
    STEP: Creating the service pods in kubernetes 12/16/22 13:40:14.049
    Dec 16 13:40:14.049: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 16 13:40:14.075: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6019" to be "running and ready"
    Dec 16 13:40:14.078: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584325ms
    Dec 16 13:40:14.078: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:40:16.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006920503s
    Dec 16 13:40:16.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:40:18.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009073251s
    Dec 16 13:40:18.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:40:20.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009086408s
    Dec 16 13:40:20.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:40:22.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009366423s
    Dec 16 13:40:22.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:40:24.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00848149s
    Dec 16 13:40:24.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 13:40:26.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007121939s
    Dec 16 13:40:26.083: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 16 13:40:26.083: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 16 13:40:26.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6019" to be "running and ready"
    Dec 16 13:40:26.090: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.335521ms
    Dec 16 13:40:26.090: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 16 13:40:26.090: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 16 13:40:26.093: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6019" to be "running and ready"
    Dec 16 13:40:26.095: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.452019ms
    Dec 16 13:40:26.095: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 16 13:40:26.095: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/16/22 13:40:26.098
    Dec 16 13:40:26.108: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6019" to be "running"
    Dec 16 13:40:26.110: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632951ms
    Dec 16 13:40:28.116: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690698s
    Dec 16 13:40:28.116: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 16 13:40:28.120: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6019" to be "running"
    Dec 16 13:40:28.124: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.960338ms
    Dec 16 13:40:28.124: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 16 13:40:28.128: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 16 13:40:28.128: INFO: Going to poll 192.168.156.150 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 13:40:28.131: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.156.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:40:28.131: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:40:28.131: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:40:28.131: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.156.150+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 13:40:29.267: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 16 13:40:29.267: INFO: Going to poll 192.168.189.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 13:40:29.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.189.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:40:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:40:29.272: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:40:29.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.189.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 13:40:30.385: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 16 13:40:30.385: INFO: Going to poll 192.168.189.210 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 13:40:30.389: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.189.210 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6019 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:40:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:40:30.389: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:40:30.389: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6019/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.189.210+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 13:40:31.515: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:31.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6019" for this suite. 12/16/22 13:40:31.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:31.527
Dec 16 13:40:31.527: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename init-container 12/16/22 13:40:31.528
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:31.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:31.547
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 12/16/22 13:40:31.549
Dec 16 13:40:31.549: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:40:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6115" for this suite. 12/16/22 13:40:35.468
------------------------------
• [3.947 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:31.527
    Dec 16 13:40:31.527: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename init-container 12/16/22 13:40:31.528
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:31.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:31.547
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 12/16/22 13:40:31.549
    Dec 16 13:40:31.549: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:40:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6115" for this suite. 12/16/22 13:40:35.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:40:35.474
Dec 16 13:40:35.474: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename taint-multiple-pods 12/16/22 13:40:35.475
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:35.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:35.492
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Dec 16 13:40:35.494: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:41:35.514: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Dec 16 13:41:35.518: INFO: Starting informer...
STEP: Starting pods... 12/16/22 13:41:35.518
Dec 16 13:41:35.737: INFO: Pod1 is running on pool-a3802-fsxxd. Tainting Node
Dec 16 13:41:35.947: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6236" to be "running"
Dec 16 13:41:35.950: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320322ms
Dec 16 13:41:37.956: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009123834s
Dec 16 13:41:37.956: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec 16 13:41:37.956: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6236" to be "running"
Dec 16 13:41:37.959: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.846685ms
Dec 16 13:41:37.959: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec 16 13:41:37.959: INFO: Pod2 is running on pool-a3802-fsxxd. Tainting Node
STEP: Trying to apply a taint on the Node 12/16/22 13:41:37.959
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:41:37.968
STEP: Waiting for Pod1 and Pod2 to be deleted 12/16/22 13:41:37.972
Dec 16 13:41:43.604: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 16 13:42:03.639: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:42:03.649
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:42:03.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-6236" for this suite. 12/16/22 13:42:03.656
------------------------------
• [SLOW TEST] [88.186 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:40:35.474
    Dec 16 13:40:35.474: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename taint-multiple-pods 12/16/22 13:40:35.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:40:35.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:40:35.492
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Dec 16 13:40:35.494: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:41:35.514: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Dec 16 13:41:35.518: INFO: Starting informer...
    STEP: Starting pods... 12/16/22 13:41:35.518
    Dec 16 13:41:35.737: INFO: Pod1 is running on pool-a3802-fsxxd. Tainting Node
    Dec 16 13:41:35.947: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6236" to be "running"
    Dec 16 13:41:35.950: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320322ms
    Dec 16 13:41:37.956: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009123834s
    Dec 16 13:41:37.956: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec 16 13:41:37.956: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6236" to be "running"
    Dec 16 13:41:37.959: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.846685ms
    Dec 16 13:41:37.959: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec 16 13:41:37.959: INFO: Pod2 is running on pool-a3802-fsxxd. Tainting Node
    STEP: Trying to apply a taint on the Node 12/16/22 13:41:37.959
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:41:37.968
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/16/22 13:41:37.972
    Dec 16 13:41:43.604: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec 16 13:42:03.639: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/16/22 13:42:03.649
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:42:03.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-6236" for this suite. 12/16/22 13:42:03.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:42:03.661
Dec 16 13:42:03.661: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:42:03.662
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.68
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 12/16/22 13:42:03.682
Dec 16 13:42:03.682: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2968 proxy --unix-socket=/tmp/kubectl-proxy-unix3868307715/test'
STEP: retrieving proxy /api/ output 12/16/22 13:42:03.724
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:42:03.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2968" for this suite. 12/16/22 13:42:03.729
------------------------------
• [0.074 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:42:03.661
    Dec 16 13:42:03.661: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:42:03.662
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.68
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 12/16/22 13:42:03.682
    Dec 16 13:42:03.682: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2968 proxy --unix-socket=/tmp/kubectl-proxy-unix3868307715/test'
    STEP: retrieving proxy /api/ output 12/16/22 13:42:03.724
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:42:03.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2968" for this suite. 12/16/22 13:42:03.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:42:03.736
Dec 16 13:42:03.736: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename endpointslice 12/16/22 13:42:03.736
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.753
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Dec 16 13:42:03.765: INFO: Endpoints addresses: [194.182.160.75] , ports: [32438]
Dec 16 13:42:03.765: INFO: EndpointSlices addresses: [194.182.160.75] , ports: [32438]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 16 13:42:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6126" for this suite. 12/16/22 13:42:03.768
------------------------------
• [0.039 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:42:03.736
    Dec 16 13:42:03.736: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename endpointslice 12/16/22 13:42:03.736
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.753
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Dec 16 13:42:03.765: INFO: Endpoints addresses: [194.182.160.75] , ports: [32438]
    Dec 16 13:42:03.765: INFO: EndpointSlices addresses: [194.182.160.75] , ports: [32438]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:42:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6126" for this suite. 12/16/22 13:42:03.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:42:03.774
Dec 16 13:42:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename cronjob 12/16/22 13:42:03.775
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.793
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 12/16/22 13:42:03.795
STEP: Ensuring no jobs are scheduled 12/16/22 13:42:03.799
STEP: Ensuring no job exists by listing jobs explicitly 12/16/22 13:47:03.809
STEP: Removing cronjob 12/16/22 13:47:03.812
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:03.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3460" for this suite. 12/16/22 13:47:03.823
------------------------------
• [SLOW TEST] [300.054 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:42:03.774
    Dec 16 13:42:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename cronjob 12/16/22 13:42:03.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:42:03.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:42:03.793
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 12/16/22 13:42:03.795
    STEP: Ensuring no jobs are scheduled 12/16/22 13:42:03.799
    STEP: Ensuring no job exists by listing jobs explicitly 12/16/22 13:47:03.809
    STEP: Removing cronjob 12/16/22 13:47:03.812
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:03.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3460" for this suite. 12/16/22 13:47:03.823
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:03.829
Dec 16 13:47:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:47:03.829
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:03.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:03.848
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:47:03.851
Dec 16 13:47:03.860: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1" in namespace "downward-api-1827" to be "Succeeded or Failed"
Dec 16 13:47:03.863: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211756ms
Dec 16 13:47:05.868: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008544338s
Dec 16 13:47:07.869: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008938306s
STEP: Saw pod success 12/16/22 13:47:07.869
Dec 16 13:47:07.869: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1" satisfied condition "Succeeded or Failed"
Dec 16 13:47:07.872: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 container client-container: <nil>
STEP: delete the pod 12/16/22 13:47:07.927
Dec 16 13:47:07.941: INFO: Waiting for pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 to disappear
Dec 16 13:47:07.944: INFO: Pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:07.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1827" for this suite. 12/16/22 13:47:07.947
------------------------------
• [4.125 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:03.829
    Dec 16 13:47:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:47:03.829
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:03.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:03.848
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:47:03.851
    Dec 16 13:47:03.860: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1" in namespace "downward-api-1827" to be "Succeeded or Failed"
    Dec 16 13:47:03.863: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211756ms
    Dec 16 13:47:05.868: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008544338s
    Dec 16 13:47:07.869: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008938306s
    STEP: Saw pod success 12/16/22 13:47:07.869
    Dec 16 13:47:07.869: INFO: Pod "downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1" satisfied condition "Succeeded or Failed"
    Dec 16 13:47:07.872: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:47:07.927
    Dec 16 13:47:07.941: INFO: Waiting for pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 to disappear
    Dec 16 13:47:07.944: INFO: Pod downwardapi-volume-6e395bd4-d1be-4e8b-8696-d653982a8bc1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:07.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1827" for this suite. 12/16/22 13:47:07.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:07.954
Dec 16 13:47:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 13:47:07.954
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:07.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:07.973
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 12/16/22 13:47:07.976
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
 12/16/22 13:47:07.982
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
 12/16/22 13:47:07.983
STEP: creating a pod to probe DNS 12/16/22 13:47:07.983
STEP: submitting the pod to kubernetes 12/16/22 13:47:07.983
Dec 16 13:47:07.993: INFO: Waiting up to 15m0s for pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201" in namespace "dns-8791" to be "running"
Dec 16 13:47:07.996: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214733ms
Dec 16 13:47:10.001: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201": Phase="Running", Reason="", readiness=true. Elapsed: 2.008344818s
Dec 16 13:47:10.001: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:47:10.001
STEP: looking for the results for each expected name from probers 12/16/22 13:47:10.005
Dec 16 13:47:15.078: INFO: DNS probes using dns-8791/dns-test-b21fe19b-c818-4349-aa2c-e0776550d201 succeeded

STEP: deleting the pod 12/16/22 13:47:15.078
STEP: deleting the test headless service 12/16/22 13:47:15.089
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:15.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8791" for this suite. 12/16/22 13:47:15.105
------------------------------
• [SLOW TEST] [7.156 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:07.954
    Dec 16 13:47:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 13:47:07.954
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:07.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:07.973
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 12/16/22 13:47:07.976
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
     12/16/22 13:47:07.982
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8791.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
     12/16/22 13:47:07.983
    STEP: creating a pod to probe DNS 12/16/22 13:47:07.983
    STEP: submitting the pod to kubernetes 12/16/22 13:47:07.983
    Dec 16 13:47:07.993: INFO: Waiting up to 15m0s for pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201" in namespace "dns-8791" to be "running"
    Dec 16 13:47:07.996: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214733ms
    Dec 16 13:47:10.001: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201": Phase="Running", Reason="", readiness=true. Elapsed: 2.008344818s
    Dec 16 13:47:10.001: INFO: Pod "dns-test-b21fe19b-c818-4349-aa2c-e0776550d201" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:47:10.001
    STEP: looking for the results for each expected name from probers 12/16/22 13:47:10.005
    Dec 16 13:47:15.078: INFO: DNS probes using dns-8791/dns-test-b21fe19b-c818-4349-aa2c-e0776550d201 succeeded

    STEP: deleting the pod 12/16/22 13:47:15.078
    STEP: deleting the test headless service 12/16/22 13:47:15.089
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:15.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8791" for this suite. 12/16/22 13:47:15.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:15.111
Dec 16 13:47:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:47:15.112
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:15.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:15.13
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 12/16/22 13:47:15.133
Dec 16 13:47:15.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277" in namespace "downward-api-9712" to be "Succeeded or Failed"
Dec 16 13:47:15.145: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Pending", Reason="", readiness=false. Elapsed: 3.125821ms
Dec 16 13:47:17.151: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197427s
Dec 16 13:47:19.152: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010327612s
STEP: Saw pod success 12/16/22 13:47:19.152
Dec 16 13:47:19.152: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277" satisfied condition "Succeeded or Failed"
Dec 16 13:47:19.156: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 container client-container: <nil>
STEP: delete the pod 12/16/22 13:47:19.165
Dec 16 13:47:19.178: INFO: Waiting for pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 to disappear
Dec 16 13:47:19.181: INFO: Pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9712" for this suite. 12/16/22 13:47:19.185
------------------------------
• [4.080 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:15.111
    Dec 16 13:47:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:47:15.112
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:15.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:15.13
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 12/16/22 13:47:15.133
    Dec 16 13:47:15.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277" in namespace "downward-api-9712" to be "Succeeded or Failed"
    Dec 16 13:47:15.145: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Pending", Reason="", readiness=false. Elapsed: 3.125821ms
    Dec 16 13:47:17.151: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197427s
    Dec 16 13:47:19.152: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010327612s
    STEP: Saw pod success 12/16/22 13:47:19.152
    Dec 16 13:47:19.152: INFO: Pod "downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277" satisfied condition "Succeeded or Failed"
    Dec 16 13:47:19.156: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 container client-container: <nil>
    STEP: delete the pod 12/16/22 13:47:19.165
    Dec 16 13:47:19.178: INFO: Waiting for pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 to disappear
    Dec 16 13:47:19.181: INFO: Pod downwardapi-volume-de02716b-e8cd-4696-b3f9-4b3f118a1277 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9712" for this suite. 12/16/22 13:47:19.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:19.192
Dec 16 13:47:19.192: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 13:47:19.192
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:19.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:19.21
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Dec 16 13:47:19.231: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/16/22 13:47:19.236
Dec 16 13:47:19.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:19.239: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/16/22 13:47:19.239
Dec 16 13:47:19.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:19.256: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:47:20.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 13:47:20.267: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/16/22 13:47:20.272
Dec 16 13:47:20.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 13:47:20.287: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec 16 13:47:21.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:21.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/16/22 13:47:21.292
Dec 16 13:47:21.303: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:21.303: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:47:22.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:22.307: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:47:23.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:23.307: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:47:24.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 13:47:24.307: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:47:24.313
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1616, will wait for the garbage collector to delete the pods 12/16/22 13:47:24.314
Dec 16 13:47:24.373: INFO: Deleting DaemonSet.extensions daemon-set took: 6.25644ms
Dec 16 13:47:24.474: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.018137ms
Dec 16 13:47:27.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:47:27.278: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 13:47:27.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680938665"},"items":null}

Dec 16 13:47:27.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680938666"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:27.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1616" for this suite. 12/16/22 13:47:27.308
------------------------------
• [SLOW TEST] [8.122 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:19.192
    Dec 16 13:47:19.192: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 13:47:19.192
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:19.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:19.21
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Dec 16 13:47:19.231: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/16/22 13:47:19.236
    Dec 16 13:47:19.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:19.239: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/16/22 13:47:19.239
    Dec 16 13:47:19.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:19.256: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:47:20.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 13:47:20.267: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/16/22 13:47:20.272
    Dec 16 13:47:20.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 13:47:20.287: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Dec 16 13:47:21.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:21.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/16/22 13:47:21.292
    Dec 16 13:47:21.303: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:21.303: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:47:22.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:22.307: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:47:23.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:23.307: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:47:24.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 13:47:24.307: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 13:47:24.313
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1616, will wait for the garbage collector to delete the pods 12/16/22 13:47:24.314
    Dec 16 13:47:24.373: INFO: Deleting DaemonSet.extensions daemon-set took: 6.25644ms
    Dec 16 13:47:24.474: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.018137ms
    Dec 16 13:47:27.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:47:27.278: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 13:47:27.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680938665"},"items":null}

    Dec 16 13:47:27.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680938666"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:27.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1616" for this suite. 12/16/22 13:47:27.308
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:27.314
Dec 16 13:47:27.314: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename watch 12/16/22 13:47:27.314
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:27.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:27.331
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 12/16/22 13:47:27.333
STEP: creating a new configmap 12/16/22 13:47:27.334
STEP: modifying the configmap once 12/16/22 13:47:27.338
STEP: closing the watch once it receives two notifications 12/16/22 13:47:27.357
Dec 16 13:47:27.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938684 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 13:47:27.358: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938686 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 12/16/22 13:47:27.358
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/16/22 13:47:27.364
STEP: deleting the configmap 12/16/22 13:47:27.365
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/16/22 13:47:27.377
Dec 16 13:47:27.377: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938690 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 13:47:27.377: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938692 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:27.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1429" for this suite. 12/16/22 13:47:27.381
------------------------------
• [0.074 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:27.314
    Dec 16 13:47:27.314: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename watch 12/16/22 13:47:27.314
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:27.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:27.331
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 12/16/22 13:47:27.333
    STEP: creating a new configmap 12/16/22 13:47:27.334
    STEP: modifying the configmap once 12/16/22 13:47:27.338
    STEP: closing the watch once it receives two notifications 12/16/22 13:47:27.357
    Dec 16 13:47:27.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938684 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 13:47:27.358: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938686 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 12/16/22 13:47:27.358
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/16/22 13:47:27.364
    STEP: deleting the configmap 12/16/22 13:47:27.365
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/16/22 13:47:27.377
    Dec 16 13:47:27.377: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938690 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 13:47:27.377: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1429  a33209e9-bebd-46c5-b3aa-7837c6731212 680938692 0 2022-12-16 13:47:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-16 13:47:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:27.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1429" for this suite. 12/16/22 13:47:27.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:27.388
Dec 16 13:47:27.388: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:47:27.389
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:27.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:27.407
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 12/16/22 13:47:27.409
STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:47:27.414
STEP: Creating a ResourceQuota with not terminating scope 12/16/22 13:47:29.419
STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:47:29.424
STEP: Creating a long running pod 12/16/22 13:47:31.429
STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/16/22 13:47:31.443
STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/16/22 13:47:33.448
STEP: Deleting the pod 12/16/22 13:47:35.453
STEP: Ensuring resource quota status released the pod usage 12/16/22 13:47:35.461
STEP: Creating a terminating pod 12/16/22 13:47:37.468
STEP: Ensuring resource quota with terminating scope captures the pod usage 12/16/22 13:47:37.477
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/16/22 13:47:39.538
STEP: Deleting the pod 12/16/22 13:47:41.544
STEP: Ensuring resource quota status released the pod usage 12/16/22 13:47:41.768
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:43.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9365" for this suite. 12/16/22 13:47:43.778
------------------------------
• [SLOW TEST] [16.396 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:27.388
    Dec 16 13:47:27.388: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:47:27.389
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:27.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:27.407
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 12/16/22 13:47:27.409
    STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:47:27.414
    STEP: Creating a ResourceQuota with not terminating scope 12/16/22 13:47:29.419
    STEP: Ensuring ResourceQuota status is calculated 12/16/22 13:47:29.424
    STEP: Creating a long running pod 12/16/22 13:47:31.429
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/16/22 13:47:31.443
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/16/22 13:47:33.448
    STEP: Deleting the pod 12/16/22 13:47:35.453
    STEP: Ensuring resource quota status released the pod usage 12/16/22 13:47:35.461
    STEP: Creating a terminating pod 12/16/22 13:47:37.468
    STEP: Ensuring resource quota with terminating scope captures the pod usage 12/16/22 13:47:37.477
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/16/22 13:47:39.538
    STEP: Deleting the pod 12/16/22 13:47:41.544
    STEP: Ensuring resource quota status released the pod usage 12/16/22 13:47:41.768
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:43.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9365" for this suite. 12/16/22 13:47:43.778
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:43.784
Dec 16 13:47:43.784: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 13:47:43.785
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:43.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:43.804
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 12/16/22 13:47:43.807
STEP: Ensuring job reaches completions 12/16/22 13:47:43.813
STEP: Ensuring pods with index for job exist 12/16/22 13:47:51.818
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 13:47:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2438" for this suite. 12/16/22 13:47:51.827
------------------------------
• [SLOW TEST] [8.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:43.784
    Dec 16 13:47:43.784: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 13:47:43.785
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:43.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:43.804
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 12/16/22 13:47:43.807
    STEP: Ensuring job reaches completions 12/16/22 13:47:43.813
    STEP: Ensuring pods with index for job exist 12/16/22 13:47:51.818
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:47:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2438" for this suite. 12/16/22 13:47:51.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:47:51.834
Dec 16 13:47:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 13:47:51.835
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:51.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:51.853
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 12/16/22 13:48:08.86
STEP: Creating a ResourceQuota 12/16/22 13:48:13.865
STEP: Ensuring resource quota status is calculated 12/16/22 13:48:13.871
STEP: Creating a ConfigMap 12/16/22 13:48:15.876
STEP: Ensuring resource quota status captures configMap creation 12/16/22 13:48:15.888
STEP: Deleting a ConfigMap 12/16/22 13:48:17.892
STEP: Ensuring resource quota status released usage 12/16/22 13:48:17.899
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 13:48:19.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6510" for this suite. 12/16/22 13:48:19.909
------------------------------
• [SLOW TEST] [28.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:47:51.834
    Dec 16 13:47:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 13:47:51.835
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:47:51.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:47:51.853
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 12/16/22 13:48:08.86
    STEP: Creating a ResourceQuota 12/16/22 13:48:13.865
    STEP: Ensuring resource quota status is calculated 12/16/22 13:48:13.871
    STEP: Creating a ConfigMap 12/16/22 13:48:15.876
    STEP: Ensuring resource quota status captures configMap creation 12/16/22 13:48:15.888
    STEP: Deleting a ConfigMap 12/16/22 13:48:17.892
    STEP: Ensuring resource quota status released usage 12/16/22 13:48:17.899
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:48:19.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6510" for this suite. 12/16/22 13:48:19.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:48:19.915
Dec 16 13:48:19.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename cronjob 12/16/22 13:48:19.916
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:48:19.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:48:19.934
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 12/16/22 13:48:19.937
STEP: creating 12/16/22 13:48:19.937
STEP: getting 12/16/22 13:48:19.942
STEP: listing 12/16/22 13:48:19.944
STEP: watching 12/16/22 13:48:19.947
Dec 16 13:48:19.947: INFO: starting watch
STEP: cluster-wide listing 12/16/22 13:48:19.948
STEP: cluster-wide watching 12/16/22 13:48:19.951
Dec 16 13:48:19.951: INFO: starting watch
STEP: patching 12/16/22 13:48:19.952
STEP: updating 12/16/22 13:48:19.958
Dec 16 13:48:19.964: INFO: waiting for watch events with expected annotations
Dec 16 13:48:19.964: INFO: saw patched and updated annotations
STEP: patching /status 12/16/22 13:48:19.964
STEP: updating /status 12/16/22 13:48:19.971
STEP: get /status 12/16/22 13:48:19.985
STEP: deleting 12/16/22 13:48:19.99
STEP: deleting a collection 12/16/22 13:48:20.002
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 16 13:48:20.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1039" for this suite. 12/16/22 13:48:20.014
------------------------------
• [0.106 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:48:19.915
    Dec 16 13:48:19.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename cronjob 12/16/22 13:48:19.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:48:19.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:48:19.934
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 12/16/22 13:48:19.937
    STEP: creating 12/16/22 13:48:19.937
    STEP: getting 12/16/22 13:48:19.942
    STEP: listing 12/16/22 13:48:19.944
    STEP: watching 12/16/22 13:48:19.947
    Dec 16 13:48:19.947: INFO: starting watch
    STEP: cluster-wide listing 12/16/22 13:48:19.948
    STEP: cluster-wide watching 12/16/22 13:48:19.951
    Dec 16 13:48:19.951: INFO: starting watch
    STEP: patching 12/16/22 13:48:19.952
    STEP: updating 12/16/22 13:48:19.958
    Dec 16 13:48:19.964: INFO: waiting for watch events with expected annotations
    Dec 16 13:48:19.964: INFO: saw patched and updated annotations
    STEP: patching /status 12/16/22 13:48:19.964
    STEP: updating /status 12/16/22 13:48:19.971
    STEP: get /status 12/16/22 13:48:19.985
    STEP: deleting 12/16/22 13:48:19.99
    STEP: deleting a collection 12/16/22 13:48:20.002
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:48:20.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1039" for this suite. 12/16/22 13:48:20.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:48:20.023
Dec 16 13:48:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 13:48:20.023
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:48:20.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:48:20.044
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 in namespace container-probe-3264 12/16/22 13:48:20.047
Dec 16 13:48:20.056: INFO: Waiting up to 5m0s for pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419" in namespace "container-probe-3264" to be "not pending"
Dec 16 13:48:20.060: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419": Phase="Pending", Reason="", readiness=false. Elapsed: 3.944265ms
Dec 16 13:48:22.065: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419": Phase="Running", Reason="", readiness=true. Elapsed: 2.008752702s
Dec 16 13:48:22.065: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419" satisfied condition "not pending"
Dec 16 13:48:22.065: INFO: Started pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 in namespace container-probe-3264
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:48:22.065
Dec 16 13:48:22.068: INFO: Initial restart count of pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 is 0
STEP: deleting the pod 12/16/22 13:52:23.315
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 13:52:23.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3264" for this suite. 12/16/22 13:52:23.333
------------------------------
• [SLOW TEST] [243.317 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:48:20.023
    Dec 16 13:48:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 13:48:20.023
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:48:20.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:48:20.044
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 in namespace container-probe-3264 12/16/22 13:48:20.047
    Dec 16 13:48:20.056: INFO: Waiting up to 5m0s for pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419" in namespace "container-probe-3264" to be "not pending"
    Dec 16 13:48:20.060: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419": Phase="Pending", Reason="", readiness=false. Elapsed: 3.944265ms
    Dec 16 13:48:22.065: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419": Phase="Running", Reason="", readiness=true. Elapsed: 2.008752702s
    Dec 16 13:48:22.065: INFO: Pod "test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419" satisfied condition "not pending"
    Dec 16 13:48:22.065: INFO: Started pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 in namespace container-probe-3264
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:48:22.065
    Dec 16 13:48:22.068: INFO: Initial restart count of pod test-webserver-dd813ac9-6646-4497-9578-48f7eefbd419 is 0
    STEP: deleting the pod 12/16/22 13:52:23.315
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:52:23.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3264" for this suite. 12/16/22 13:52:23.333
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:52:23.34
Dec 16 13:52:23.340: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:52:23.341
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:23.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:23.359
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-cb44dd96-ad68-47eb-a7ff-2fb1f2f29b7e 12/16/22 13:52:23.364
STEP: Creating the pod 12/16/22 13:52:23.368
Dec 16 13:52:23.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b" in namespace "configmap-296" to be "running and ready"
Dec 16 13:52:23.378: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.860984ms
Dec 16 13:52:23.378: INFO: The phase of Pod pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:52:25.384: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008361469s
Dec 16 13:52:25.384: INFO: The phase of Pod pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b is Running (Ready = true)
Dec 16 13:52:25.384: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-cb44dd96-ad68-47eb-a7ff-2fb1f2f29b7e 12/16/22 13:52:25.442
STEP: waiting to observe update in volume 12/16/22 13:52:25.447
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:52:27.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-296" for this suite. 12/16/22 13:52:27.469
------------------------------
• [4.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:52:23.34
    Dec 16 13:52:23.340: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:52:23.341
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:23.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:23.359
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-cb44dd96-ad68-47eb-a7ff-2fb1f2f29b7e 12/16/22 13:52:23.364
    STEP: Creating the pod 12/16/22 13:52:23.368
    Dec 16 13:52:23.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b" in namespace "configmap-296" to be "running and ready"
    Dec 16 13:52:23.378: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.860984ms
    Dec 16 13:52:23.378: INFO: The phase of Pod pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:52:25.384: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008361469s
    Dec 16 13:52:25.384: INFO: The phase of Pod pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b is Running (Ready = true)
    Dec 16 13:52:25.384: INFO: Pod "pod-configmaps-9b362769-e85c-4a15-a323-cc3ff375b83b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-cb44dd96-ad68-47eb-a7ff-2fb1f2f29b7e 12/16/22 13:52:25.442
    STEP: waiting to observe update in volume 12/16/22 13:52:25.447
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:52:27.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-296" for this suite. 12/16/22 13:52:27.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:52:27.475
Dec 16 13:52:27.475: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename proxy 12/16/22 13:52:27.476
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:27.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:27.492
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Dec 16 13:52:27.494: INFO: Creating pod...
Dec 16 13:52:27.502: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9338" to be "running"
Dec 16 13:52:27.504: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.529835ms
Dec 16 13:52:29.510: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008336506s
Dec 16 13:52:29.510: INFO: Pod "agnhost" satisfied condition "running"
Dec 16 13:52:29.510: INFO: Creating service...
Dec 16 13:52:29.525: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=DELETE
Dec 16 13:52:29.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 16 13:52:29.559: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=OPTIONS
Dec 16 13:52:29.564: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 16 13:52:29.564: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=PATCH
Dec 16 13:52:29.569: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 16 13:52:29.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=POST
Dec 16 13:52:29.575: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 16 13:52:29.575: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=PUT
Dec 16 13:52:29.579: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 16 13:52:29.579: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=DELETE
Dec 16 13:52:29.586: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 16 13:52:29.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec 16 13:52:29.594: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 16 13:52:29.594: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=PATCH
Dec 16 13:52:29.600: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 16 13:52:29.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=POST
Dec 16 13:52:29.607: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 16 13:52:29.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=PUT
Dec 16 13:52:29.613: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 16 13:52:29.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=GET
Dec 16 13:52:29.615: INFO: http.Client request:GET StatusCode:301
Dec 16 13:52:29.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=GET
Dec 16 13:52:29.619: INFO: http.Client request:GET StatusCode:301
Dec 16 13:52:29.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=HEAD
Dec 16 13:52:29.622: INFO: http.Client request:HEAD StatusCode:301
Dec 16 13:52:29.622: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=HEAD
Dec 16 13:52:29.627: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 16 13:52:29.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9338" for this suite. 12/16/22 13:52:29.631
------------------------------
• [2.162 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:52:27.475
    Dec 16 13:52:27.475: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename proxy 12/16/22 13:52:27.476
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:27.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:27.492
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Dec 16 13:52:27.494: INFO: Creating pod...
    Dec 16 13:52:27.502: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9338" to be "running"
    Dec 16 13:52:27.504: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.529835ms
    Dec 16 13:52:29.510: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008336506s
    Dec 16 13:52:29.510: INFO: Pod "agnhost" satisfied condition "running"
    Dec 16 13:52:29.510: INFO: Creating service...
    Dec 16 13:52:29.525: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=DELETE
    Dec 16 13:52:29.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 16 13:52:29.559: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=OPTIONS
    Dec 16 13:52:29.564: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 16 13:52:29.564: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=PATCH
    Dec 16 13:52:29.569: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 16 13:52:29.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=POST
    Dec 16 13:52:29.575: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 16 13:52:29.575: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=PUT
    Dec 16 13:52:29.579: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 16 13:52:29.579: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=DELETE
    Dec 16 13:52:29.586: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 16 13:52:29.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Dec 16 13:52:29.594: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 16 13:52:29.594: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=PATCH
    Dec 16 13:52:29.600: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 16 13:52:29.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=POST
    Dec 16 13:52:29.607: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 16 13:52:29.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=PUT
    Dec 16 13:52:29.613: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 16 13:52:29.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=GET
    Dec 16 13:52:29.615: INFO: http.Client request:GET StatusCode:301
    Dec 16 13:52:29.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=GET
    Dec 16 13:52:29.619: INFO: http.Client request:GET StatusCode:301
    Dec 16 13:52:29.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/pods/agnhost/proxy?method=HEAD
    Dec 16 13:52:29.622: INFO: http.Client request:HEAD StatusCode:301
    Dec 16 13:52:29.622: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9338/services/e2e-proxy-test-service/proxy?method=HEAD
    Dec 16 13:52:29.627: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:52:29.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9338" for this suite. 12/16/22 13:52:29.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:52:29.638
Dec 16 13:52:29.638: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 13:52:29.639
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:29.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:29.656
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 12/16/22 13:52:29.658
STEP: waiting for pod running 12/16/22 13:52:29.667
Dec 16 13:52:29.667: INFO: Waiting up to 2m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140" to be "running"
Dec 16 13:52:29.670: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008479ms
Dec 16 13:52:31.674: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454598s
Dec 16 13:52:31.674: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" satisfied condition "running"
STEP: creating a file in subpath 12/16/22 13:52:31.675
Dec 16 13:52:31.678: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1140 PodName:var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:52:31.678: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:52:31.678: INFO: ExecWithOptions: Clientset creation
Dec 16 13:52:31.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1140/pods/var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 12/16/22 13:52:31.805
Dec 16 13:52:31.809: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1140 PodName:var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 13:52:31.809: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:52:31.810: INFO: ExecWithOptions: Clientset creation
Dec 16 13:52:31.810: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1140/pods/var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 12/16/22 13:52:31.929
Dec 16 13:52:32.442: INFO: Successfully updated pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104"
STEP: waiting for annotated pod running 12/16/22 13:52:32.442
Dec 16 13:52:32.442: INFO: Waiting up to 2m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140" to be "running"
Dec 16 13:52:32.445: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Running", Reason="", readiness=true. Elapsed: 2.979338ms
Dec 16 13:52:32.445: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" satisfied condition "running"
STEP: deleting the pod gracefully 12/16/22 13:52:32.445
Dec 16 13:52:32.445: INFO: Deleting pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140"
Dec 16 13:52:32.451: INFO: Wait up to 5m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:06.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1140" for this suite. 12/16/22 13:53:06.463
------------------------------
• [SLOW TEST] [36.831 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:52:29.638
    Dec 16 13:52:29.638: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 13:52:29.639
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:52:29.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:52:29.656
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 12/16/22 13:52:29.658
    STEP: waiting for pod running 12/16/22 13:52:29.667
    Dec 16 13:52:29.667: INFO: Waiting up to 2m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140" to be "running"
    Dec 16 13:52:29.670: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008479ms
    Dec 16 13:52:31.674: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454598s
    Dec 16 13:52:31.674: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" satisfied condition "running"
    STEP: creating a file in subpath 12/16/22 13:52:31.675
    Dec 16 13:52:31.678: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1140 PodName:var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:52:31.678: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:52:31.678: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:52:31.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1140/pods/var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 12/16/22 13:52:31.805
    Dec 16 13:52:31.809: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1140 PodName:var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 13:52:31.809: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:52:31.810: INFO: ExecWithOptions: Clientset creation
    Dec 16 13:52:31.810: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1140/pods/var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 12/16/22 13:52:31.929
    Dec 16 13:52:32.442: INFO: Successfully updated pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104"
    STEP: waiting for annotated pod running 12/16/22 13:52:32.442
    Dec 16 13:52:32.442: INFO: Waiting up to 2m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140" to be "running"
    Dec 16 13:52:32.445: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104": Phase="Running", Reason="", readiness=true. Elapsed: 2.979338ms
    Dec 16 13:52:32.445: INFO: Pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" satisfied condition "running"
    STEP: deleting the pod gracefully 12/16/22 13:52:32.445
    Dec 16 13:52:32.445: INFO: Deleting pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" in namespace "var-expansion-1140"
    Dec 16 13:52:32.451: INFO: Wait up to 5m0s for pod "var-expansion-138270aa-19f1-4f37-9a69-883c2aa48104" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:06.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1140" for this suite. 12/16/22 13:53:06.463
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:06.469
Dec 16 13:53:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 13:53:06.47
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:06.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:06.487
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 12/16/22 13:53:06.507
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:53:06.512
Dec 16 13:53:06.518: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 13:53:06.518: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 13:53:07.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 13:53:07.530: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 13:53:08.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 13:53:08.527: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 12/16/22 13:53:08.533
STEP: DeleteCollection of the DaemonSets 12/16/22 13:53:08.537
STEP: Verify that ReplicaSets have been deleted 12/16/22 13:53:08.545
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Dec 16 13:53:08.554: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680945306"},"items":null}

Dec 16 13:53:08.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680945306"},"items":[{"metadata":{"name":"daemon-set-8l542","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"c8f80be0-fa25-4a21-a314-1a3dad0ff76c","resourceVersion":"680945301","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"54b508f19e391a1d5009b2e41a79cc13d342c11ca9d941aaffe1443e51f4f908","cni.projectcalico.org/podIP":"192.168.189.212/32","cni.projectcalico.org/podIPs":"192.168.189.212/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qvkns","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qvkns","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-oewtd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-oewtd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.213","podIP":"192.168.189.212","podIPs":[{"ip":"192.168.189.212"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f88bce629c2acc37697a9299a0e7335d2cb297659879c4808530397cdd0d2c9e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dvlqz","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"ee6b483f-d202-40b7-a0c8-e33be23fd1df","resourceVersion":"680945290","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a3a81ab69fcf1cbd897fb2b0a89f725363988cd0ea8194c10b6ff3de18e828e0","cni.projectcalico.org/podIP":"192.168.156.151/32","cni.projectcalico.org/podIPs":"192.168.156.151/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-csmrv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-csmrv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-ehprg","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-ehprg"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.222","podIP":"192.168.156.151","podIPs":[{"ip":"192.168.156.151"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://05a99dc5e36f26619a9e98663b7135dc017b23c6e9889d5c608295637a1a8120","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v7tdw","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"ba1aa2d5-1186-4cd5-8a5c-9ed153bb6da6","resourceVersion":"680945296","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c12bd9a246ef7a431e35baa84f5d40df9dcd787d7928e663fe21ee992cd284a6","cni.projectcalico.org/podIP":"192.168.189.31/32","cni.projectcalico.org/podIPs":"192.168.189.31/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pssmd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pssmd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-fsxxd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-fsxxd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.242","podIP":"192.168.189.31","podIPs":[{"ip":"192.168.189.31"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7692690aaad7f08ded67534307987653e3270e10d33a71062aee8e4ee21895dc","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:08.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8361" for this suite. 12/16/22 13:53:08.573
------------------------------
• [2.110 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:06.469
    Dec 16 13:53:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 13:53:06.47
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:06.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:06.487
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 12/16/22 13:53:06.507
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 13:53:06.512
    Dec 16 13:53:06.518: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 13:53:06.518: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 13:53:07.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 13:53:07.530: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 13:53:08.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 13:53:08.527: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 12/16/22 13:53:08.533
    STEP: DeleteCollection of the DaemonSets 12/16/22 13:53:08.537
    STEP: Verify that ReplicaSets have been deleted 12/16/22 13:53:08.545
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Dec 16 13:53:08.554: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680945306"},"items":null}

    Dec 16 13:53:08.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680945306"},"items":[{"metadata":{"name":"daemon-set-8l542","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"c8f80be0-fa25-4a21-a314-1a3dad0ff76c","resourceVersion":"680945301","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"54b508f19e391a1d5009b2e41a79cc13d342c11ca9d941aaffe1443e51f4f908","cni.projectcalico.org/podIP":"192.168.189.212/32","cni.projectcalico.org/podIPs":"192.168.189.212/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qvkns","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qvkns","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-oewtd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-oewtd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.213","podIP":"192.168.189.212","podIPs":[{"ip":"192.168.189.212"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f88bce629c2acc37697a9299a0e7335d2cb297659879c4808530397cdd0d2c9e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dvlqz","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"ee6b483f-d202-40b7-a0c8-e33be23fd1df","resourceVersion":"680945290","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a3a81ab69fcf1cbd897fb2b0a89f725363988cd0ea8194c10b6ff3de18e828e0","cni.projectcalico.org/podIP":"192.168.156.151/32","cni.projectcalico.org/podIPs":"192.168.156.151/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-csmrv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-csmrv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-ehprg","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-ehprg"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.222","podIP":"192.168.156.151","podIPs":[{"ip":"192.168.156.151"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://05a99dc5e36f26619a9e98663b7135dc017b23c6e9889d5c608295637a1a8120","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v7tdw","generateName":"daemon-set-","namespace":"daemonsets-8361","uid":"ba1aa2d5-1186-4cd5-8a5c-9ed153bb6da6","resourceVersion":"680945296","creationTimestamp":"2022-12-16T13:53:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c12bd9a246ef7a431e35baa84f5d40df9dcd787d7928e663fe21ee992cd284a6","cni.projectcalico.org/podIP":"192.168.189.31/32","cni.projectcalico.org/podIPs":"192.168.189.31/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"128b0db8-b43f-463d-b183-d7a6e046e19b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"128b0db8-b43f-463d-b183-d7a6e046e19b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-16T13:53:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pssmd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pssmd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-a3802-fsxxd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-a3802-fsxxd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-16T13:53:06Z"}],"hostIP":"85.217.161.242","podIP":"192.168.189.31","podIPs":[{"ip":"192.168.189.31"}],"startTime":"2022-12-16T13:53:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-16T13:53:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7692690aaad7f08ded67534307987653e3270e10d33a71062aee8e4ee21895dc","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:08.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8361" for this suite. 12/16/22 13:53:08.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:08.579
Dec 16 13:53:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:53:08.58
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:08.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:08.596
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 12/16/22 13:53:08.598
STEP: submitting the pod to kubernetes 12/16/22 13:53:08.598
STEP: verifying QOS class is set on the pod 12/16/22 13:53:08.605
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:08.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9091" for this suite. 12/16/22 13:53:08.611
------------------------------
• [0.039 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:08.579
    Dec 16 13:53:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:53:08.58
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:08.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:08.596
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 12/16/22 13:53:08.598
    STEP: submitting the pod to kubernetes 12/16/22 13:53:08.598
    STEP: verifying QOS class is set on the pod 12/16/22 13:53:08.605
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:08.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9091" for this suite. 12/16/22 13:53:08.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:08.618
Dec 16 13:53:08.618: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:53:08.619
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:08.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:08.638
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 12/16/22 13:53:08.64
Dec 16 13:53:08.648: INFO: Waiting up to 5m0s for pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15" in namespace "projected-5272" to be "running and ready"
Dec 16 13:53:08.651: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058638ms
Dec 16 13:53:08.651: INFO: The phase of Pod labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:53:10.656: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15": Phase="Running", Reason="", readiness=true. Elapsed: 2.007756928s
Dec 16 13:53:10.656: INFO: The phase of Pod labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15 is Running (Ready = true)
Dec 16 13:53:10.656: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15" satisfied condition "running and ready"
Dec 16 13:53:11.179: INFO: Successfully updated pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:15.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5272" for this suite. 12/16/22 13:53:15.212
------------------------------
• [SLOW TEST] [6.600 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:08.618
    Dec 16 13:53:08.618: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:53:08.619
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:08.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:08.638
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 12/16/22 13:53:08.64
    Dec 16 13:53:08.648: INFO: Waiting up to 5m0s for pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15" in namespace "projected-5272" to be "running and ready"
    Dec 16 13:53:08.651: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058638ms
    Dec 16 13:53:08.651: INFO: The phase of Pod labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:53:10.656: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15": Phase="Running", Reason="", readiness=true. Elapsed: 2.007756928s
    Dec 16 13:53:10.656: INFO: The phase of Pod labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15 is Running (Ready = true)
    Dec 16 13:53:10.656: INFO: Pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15" satisfied condition "running and ready"
    Dec 16 13:53:11.179: INFO: Successfully updated pod "labelsupdatecd0abc85-1fca-4b85-9d47-6c5bf9280b15"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:15.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5272" for this suite. 12/16/22 13:53:15.212
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:15.219
Dec 16 13:53:15.219: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:15.219
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:15.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:15.239
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/16/22 13:53:15.242
Dec 16 13:53:15.251: INFO: Waiting up to 5m0s for pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967" in namespace "emptydir-9564" to be "Succeeded or Failed"
Dec 16 13:53:15.254: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.222988ms
Dec 16 13:53:17.262: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010578017s
Dec 16 13:53:19.260: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615126s
STEP: Saw pod success 12/16/22 13:53:19.26
Dec 16 13:53:19.260: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967" satisfied condition "Succeeded or Failed"
Dec 16 13:53:19.263: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 container test-container: <nil>
STEP: delete the pod 12/16/22 13:53:19.271
Dec 16 13:53:19.292: INFO: Waiting for pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 to disappear
Dec 16 13:53:19.295: INFO: Pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:19.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9564" for this suite. 12/16/22 13:53:19.299
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:15.219
    Dec 16 13:53:15.219: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:15.219
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:15.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:15.239
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/16/22 13:53:15.242
    Dec 16 13:53:15.251: INFO: Waiting up to 5m0s for pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967" in namespace "emptydir-9564" to be "Succeeded or Failed"
    Dec 16 13:53:15.254: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.222988ms
    Dec 16 13:53:17.262: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010578017s
    Dec 16 13:53:19.260: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615126s
    STEP: Saw pod success 12/16/22 13:53:19.26
    Dec 16 13:53:19.260: INFO: Pod "pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967" satisfied condition "Succeeded or Failed"
    Dec 16 13:53:19.263: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:53:19.271
    Dec 16 13:53:19.292: INFO: Waiting for pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 to disappear
    Dec 16 13:53:19.295: INFO: Pod pod-9ebf2423-0d51-4901-bf47-4c67a3cb5967 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:19.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9564" for this suite. 12/16/22 13:53:19.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:19.304
Dec 16 13:53:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 13:53:19.305
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:19.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:19.322
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 12/16/22 13:53:19.324
Dec 16 13:53:19.331: INFO: Waiting up to 5m0s for pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608" in namespace "downward-api-8830" to be "running and ready"
Dec 16 13:53:19.334: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.964837ms
Dec 16 13:53:19.334: INFO: The phase of Pod annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:53:21.339: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608": Phase="Running", Reason="", readiness=true. Elapsed: 2.0082881s
Dec 16 13:53:21.339: INFO: The phase of Pod annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608 is Running (Ready = true)
Dec 16 13:53:21.339: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608" satisfied condition "running and ready"
Dec 16 13:53:21.863: INFO: Successfully updated pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8830" for this suite. 12/16/22 13:53:23.886
------------------------------
• [4.587 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:19.304
    Dec 16 13:53:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 13:53:19.305
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:19.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:19.322
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 12/16/22 13:53:19.324
    Dec 16 13:53:19.331: INFO: Waiting up to 5m0s for pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608" in namespace "downward-api-8830" to be "running and ready"
    Dec 16 13:53:19.334: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.964837ms
    Dec 16 13:53:19.334: INFO: The phase of Pod annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:53:21.339: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608": Phase="Running", Reason="", readiness=true. Elapsed: 2.0082881s
    Dec 16 13:53:21.339: INFO: The phase of Pod annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608 is Running (Ready = true)
    Dec 16 13:53:21.339: INFO: Pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608" satisfied condition "running and ready"
    Dec 16 13:53:21.863: INFO: Successfully updated pod "annotationupdate5b3129ef-7250-4d47-b9e4-ff098e5cd608"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8830" for this suite. 12/16/22 13:53:23.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:23.893
Dec 16 13:53:23.893: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:53:23.893
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:23.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:23.91
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7948 12/16/22 13:53:23.913
STEP: changing the ExternalName service to type=NodePort 12/16/22 13:53:23.917
STEP: creating replication controller externalname-service in namespace services-7948 12/16/22 13:53:23.938
I1216 13:53:23.943017      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7948, replica count: 2
I1216 13:53:26.995231      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 13:53:26.995: INFO: Creating new exec pod
Dec 16 13:53:27.004: INFO: Waiting up to 5m0s for pod "execpod65z9r" in namespace "services-7948" to be "running"
Dec 16 13:53:27.007: INFO: Pod "execpod65z9r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402285ms
Dec 16 13:53:29.012: INFO: Pod "execpod65z9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.008692333s
Dec 16 13:53:29.013: INFO: Pod "execpod65z9r" satisfied condition "running"
Dec 16 13:53:30.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec 16 13:53:30.210: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 16 13:53:30.210: INFO: stdout: ""
Dec 16 13:53:30.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 10.104.41.59 80'
Dec 16 13:53:30.406: INFO: stderr: "+ nc -v -z -w 2 10.104.41.59 80\nConnection to 10.104.41.59 80 port [tcp/http] succeeded!\n"
Dec 16 13:53:30.406: INFO: stdout: ""
Dec 16 13:53:30.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 32507'
Dec 16 13:53:30.588: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 32507\nConnection to 85.217.161.242 32507 port [tcp/*] succeeded!\n"
Dec 16 13:53:30.588: INFO: stdout: ""
Dec 16 13:53:30.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 85.217.161.222 32507'
Dec 16 13:53:30.773: INFO: stderr: "+ nc -v -z -w 2 85.217.161.222 32507\nConnection to 85.217.161.222 32507 port [tcp/*] succeeded!\n"
Dec 16 13:53:30.773: INFO: stdout: ""
Dec 16 13:53:30.773: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:30.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7948" for this suite. 12/16/22 13:53:30.8
------------------------------
• [SLOW TEST] [6.914 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:23.893
    Dec 16 13:53:23.893: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:53:23.893
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:23.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:23.91
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7948 12/16/22 13:53:23.913
    STEP: changing the ExternalName service to type=NodePort 12/16/22 13:53:23.917
    STEP: creating replication controller externalname-service in namespace services-7948 12/16/22 13:53:23.938
    I1216 13:53:23.943017      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7948, replica count: 2
    I1216 13:53:26.995231      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 13:53:26.995: INFO: Creating new exec pod
    Dec 16 13:53:27.004: INFO: Waiting up to 5m0s for pod "execpod65z9r" in namespace "services-7948" to be "running"
    Dec 16 13:53:27.007: INFO: Pod "execpod65z9r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402285ms
    Dec 16 13:53:29.012: INFO: Pod "execpod65z9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.008692333s
    Dec 16 13:53:29.013: INFO: Pod "execpod65z9r" satisfied condition "running"
    Dec 16 13:53:30.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec 16 13:53:30.210: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 16 13:53:30.210: INFO: stdout: ""
    Dec 16 13:53:30.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 10.104.41.59 80'
    Dec 16 13:53:30.406: INFO: stderr: "+ nc -v -z -w 2 10.104.41.59 80\nConnection to 10.104.41.59 80 port [tcp/http] succeeded!\n"
    Dec 16 13:53:30.406: INFO: stdout: ""
    Dec 16 13:53:30.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 32507'
    Dec 16 13:53:30.588: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 32507\nConnection to 85.217.161.242 32507 port [tcp/*] succeeded!\n"
    Dec 16 13:53:30.588: INFO: stdout: ""
    Dec 16 13:53:30.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-7948 exec execpod65z9r -- /bin/sh -x -c nc -v -z -w 2 85.217.161.222 32507'
    Dec 16 13:53:30.773: INFO: stderr: "+ nc -v -z -w 2 85.217.161.222 32507\nConnection to 85.217.161.222 32507 port [tcp/*] succeeded!\n"
    Dec 16 13:53:30.773: INFO: stdout: ""
    Dec 16 13:53:30.773: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:30.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7948" for this suite. 12/16/22 13:53:30.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:30.807
Dec 16 13:53:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:53:30.808
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:30.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:30.826
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 12/16/22 13:53:30.828
Dec 16 13:53:30.836: INFO: Waiting up to 5m0s for pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532" in namespace "projected-6289" to be "running and ready"
Dec 16 13:53:30.839: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066613ms
Dec 16 13:53:30.839: INFO: The phase of Pod annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:53:32.846: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532": Phase="Running", Reason="", readiness=true. Elapsed: 2.009699685s
Dec 16 13:53:32.846: INFO: The phase of Pod annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532 is Running (Ready = true)
Dec 16 13:53:32.846: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532" satisfied condition "running and ready"
Dec 16 13:53:33.370: INFO: Successfully updated pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:37.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6289" for this suite. 12/16/22 13:53:37.406
------------------------------
• [SLOW TEST] [6.606 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:30.807
    Dec 16 13:53:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:53:30.808
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:30.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:30.826
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 12/16/22 13:53:30.828
    Dec 16 13:53:30.836: INFO: Waiting up to 5m0s for pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532" in namespace "projected-6289" to be "running and ready"
    Dec 16 13:53:30.839: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066613ms
    Dec 16 13:53:30.839: INFO: The phase of Pod annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:53:32.846: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532": Phase="Running", Reason="", readiness=true. Elapsed: 2.009699685s
    Dec 16 13:53:32.846: INFO: The phase of Pod annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532 is Running (Ready = true)
    Dec 16 13:53:32.846: INFO: Pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532" satisfied condition "running and ready"
    Dec 16 13:53:33.370: INFO: Successfully updated pod "annotationupdatebee73fad-9eeb-43b5-9d77-bdd166f47532"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:37.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6289" for this suite. 12/16/22 13:53:37.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:37.414
Dec 16 13:53:37.414: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:53:37.414
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:37.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:37.431
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-075bd89d-6607-4936-b3ad-c9739cc8e59d 12/16/22 13:53:37.436
STEP: Creating the pod 12/16/22 13:53:37.44
Dec 16 13:53:37.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80" in namespace "configmap-8951" to be "running"
Dec 16 13:53:37.449: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900314ms
Dec 16 13:53:39.455: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80": Phase="Running", Reason="", readiness=false. Elapsed: 2.007985654s
Dec 16 13:53:39.455: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80" satisfied condition "running"
STEP: Waiting for pod with text data 12/16/22 13:53:39.455
STEP: Waiting for pod with binary data 12/16/22 13:53:39.464
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:39.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8951" for this suite. 12/16/22 13:53:39.477
------------------------------
• [2.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:37.414
    Dec 16 13:53:37.414: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:53:37.414
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:37.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:37.431
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-075bd89d-6607-4936-b3ad-c9739cc8e59d 12/16/22 13:53:37.436
    STEP: Creating the pod 12/16/22 13:53:37.44
    Dec 16 13:53:37.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80" in namespace "configmap-8951" to be "running"
    Dec 16 13:53:37.449: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900314ms
    Dec 16 13:53:39.455: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80": Phase="Running", Reason="", readiness=false. Elapsed: 2.007985654s
    Dec 16 13:53:39.455: INFO: Pod "pod-configmaps-5e97af45-edc6-4d35-b3fe-cfd3e73bfa80" satisfied condition "running"
    STEP: Waiting for pod with text data 12/16/22 13:53:39.455
    STEP: Waiting for pod with binary data 12/16/22 13:53:39.464
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:39.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8951" for this suite. 12/16/22 13:53:39.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:39.484
Dec 16 13:53:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:39.484
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:39.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:39.518
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 12/16/22 13:53:39.52
Dec 16 13:53:39.527: INFO: Waiting up to 5m0s for pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8" in namespace "emptydir-5264" to be "Succeeded or Failed"
Dec 16 13:53:39.531: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892035ms
Dec 16 13:53:41.603: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075936509s
Dec 16 13:53:43.538: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534015s
STEP: Saw pod success 12/16/22 13:53:43.538
Dec 16 13:53:43.538: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8" satisfied condition "Succeeded or Failed"
Dec 16 13:53:43.541: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 container test-container: <nil>
STEP: delete the pod 12/16/22 13:53:43.592
Dec 16 13:53:43.636: INFO: Waiting for pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 to disappear
Dec 16 13:53:43.639: INFO: Pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:43.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5264" for this suite. 12/16/22 13:53:43.643
------------------------------
• [4.171 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:39.484
    Dec 16 13:53:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:39.484
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:39.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:39.518
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/16/22 13:53:39.52
    Dec 16 13:53:39.527: INFO: Waiting up to 5m0s for pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8" in namespace "emptydir-5264" to be "Succeeded or Failed"
    Dec 16 13:53:39.531: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892035ms
    Dec 16 13:53:41.603: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075936509s
    Dec 16 13:53:43.538: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534015s
    STEP: Saw pod success 12/16/22 13:53:43.538
    Dec 16 13:53:43.538: INFO: Pod "pod-a47b515a-6a76-4a9e-809d-f614c56b36f8" satisfied condition "Succeeded or Failed"
    Dec 16 13:53:43.541: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:53:43.592
    Dec 16 13:53:43.636: INFO: Waiting for pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 to disappear
    Dec 16 13:53:43.639: INFO: Pod pod-a47b515a-6a76-4a9e-809d-f614c56b36f8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:43.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5264" for this suite. 12/16/22 13:53:43.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:43.656
Dec 16 13:53:43.656: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:43.656
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:43.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:43.755
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 12/16/22 13:53:43.757
Dec 16 13:53:43.765: INFO: Waiting up to 5m0s for pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165" in namespace "emptydir-1861" to be "Succeeded or Failed"
Dec 16 13:53:43.849: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Pending", Reason="", readiness=false. Elapsed: 83.578435ms
Dec 16 13:53:45.853: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087953265s
Dec 16 13:53:47.854: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089161662s
STEP: Saw pod success 12/16/22 13:53:47.854
Dec 16 13:53:47.854: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165" satisfied condition "Succeeded or Failed"
Dec 16 13:53:47.860: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 container test-container: <nil>
STEP: delete the pod 12/16/22 13:53:47.868
Dec 16 13:53:47.877: INFO: Waiting for pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 to disappear
Dec 16 13:53:47.879: INFO: Pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:47.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1861" for this suite. 12/16/22 13:53:47.883
------------------------------
• [4.233 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:43.656
    Dec 16 13:53:43.656: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 13:53:43.656
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:43.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:43.755
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/16/22 13:53:43.757
    Dec 16 13:53:43.765: INFO: Waiting up to 5m0s for pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165" in namespace "emptydir-1861" to be "Succeeded or Failed"
    Dec 16 13:53:43.849: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Pending", Reason="", readiness=false. Elapsed: 83.578435ms
    Dec 16 13:53:45.853: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087953265s
    Dec 16 13:53:47.854: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089161662s
    STEP: Saw pod success 12/16/22 13:53:47.854
    Dec 16 13:53:47.854: INFO: Pod "pod-2262bae2-7a2c-4d03-a762-94f997d42165" satisfied condition "Succeeded or Failed"
    Dec 16 13:53:47.860: INFO: Trying to get logs from node pool-a3802-ehprg pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:53:47.868
    Dec 16 13:53:47.877: INFO: Waiting for pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 to disappear
    Dec 16 13:53:47.879: INFO: Pod pod-2262bae2-7a2c-4d03-a762-94f997d42165 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:47.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1861" for this suite. 12/16/22 13:53:47.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:47.89
Dec 16 13:53:47.890: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 13:53:47.891
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:47.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:47.909
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5494" 12/16/22 13:53:47.911
Dec 16 13:53:47.920: INFO: Namespace "namespaces-5494" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"6ab7a2a6-4f2a-43ab-a7a0-8623e2091669", "kubernetes.io/metadata.name":"namespaces-5494", "namespaces-5494":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:47.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5494" for this suite. 12/16/22 13:53:47.924
------------------------------
• [0.040 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:47.89
    Dec 16 13:53:47.890: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 13:53:47.891
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:47.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:47.909
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5494" 12/16/22 13:53:47.911
    Dec 16 13:53:47.920: INFO: Namespace "namespaces-5494" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"6ab7a2a6-4f2a-43ab-a7a0-8623e2091669", "kubernetes.io/metadata.name":"namespaces-5494", "namespaces-5494":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:47.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5494" for this suite. 12/16/22 13:53:47.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:47.937
Dec 16 13:53:47.937: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:53:47.938
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:47.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:47.956
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-f2ad4467-9664-42b1-b7e8-e3c0947e25fe 12/16/22 13:53:47.958
STEP: Creating a pod to test consume secrets 12/16/22 13:53:47.962
Dec 16 13:53:47.970: INFO: Waiting up to 5m0s for pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d" in namespace "secrets-4350" to be "Succeeded or Failed"
Dec 16 13:53:47.973: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492519ms
Dec 16 13:53:49.978: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007342489s
Dec 16 13:53:51.977: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006461856s
STEP: Saw pod success 12/16/22 13:53:51.977
Dec 16 13:53:51.977: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d" satisfied condition "Succeeded or Failed"
Dec 16 13:53:51.980: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:53:51.989
Dec 16 13:53:51.999: INFO: Waiting for pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d to disappear
Dec 16 13:53:52.001: INFO: Pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:53:52.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4350" for this suite. 12/16/22 13:53:52.005
------------------------------
• [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:47.937
    Dec 16 13:53:47.937: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:53:47.938
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:47.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:47.956
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-f2ad4467-9664-42b1-b7e8-e3c0947e25fe 12/16/22 13:53:47.958
    STEP: Creating a pod to test consume secrets 12/16/22 13:53:47.962
    Dec 16 13:53:47.970: INFO: Waiting up to 5m0s for pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d" in namespace "secrets-4350" to be "Succeeded or Failed"
    Dec 16 13:53:47.973: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492519ms
    Dec 16 13:53:49.978: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007342489s
    Dec 16 13:53:51.977: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006461856s
    STEP: Saw pod success 12/16/22 13:53:51.977
    Dec 16 13:53:51.977: INFO: Pod "pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d" satisfied condition "Succeeded or Failed"
    Dec 16 13:53:51.980: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:53:51.989
    Dec 16 13:53:51.999: INFO: Waiting for pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d to disappear
    Dec 16 13:53:52.001: INFO: Pod pod-secrets-281a6e43-ec53-4e1d-b36c-851a8272470d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:53:52.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4350" for this suite. 12/16/22 13:53:52.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:53:52.01
Dec 16 13:53:52.010: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 13:53:52.011
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:52.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:52.028
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 in namespace container-probe-3859 12/16/22 13:53:52.031
Dec 16 13:53:52.038: INFO: Waiting up to 5m0s for pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966" in namespace "container-probe-3859" to be "not pending"
Dec 16 13:53:52.041: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646112ms
Dec 16 13:53:54.046: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966": Phase="Running", Reason="", readiness=true. Elapsed: 2.007812757s
Dec 16 13:53:54.046: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966" satisfied condition "not pending"
Dec 16 13:53:54.046: INFO: Started pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 in namespace container-probe-3859
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:53:54.046
Dec 16 13:53:54.050: INFO: Initial restart count of pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is 0
Dec 16 13:54:14.110: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 1 (20.060123631s elapsed)
Dec 16 13:54:34.160: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 2 (40.110436395s elapsed)
Dec 16 13:54:54.216: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 3 (1m0.166258981s elapsed)
Dec 16 13:55:14.268: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 4 (1m20.218189921s elapsed)
Dec 16 13:56:22.624: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 5 (2m28.574923467s elapsed)
STEP: deleting the pod 12/16/22 13:56:22.624
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:22.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3859" for this suite. 12/16/22 13:56:22.663
------------------------------
• [SLOW TEST] [150.662 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:53:52.01
    Dec 16 13:53:52.010: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 13:53:52.011
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:53:52.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:53:52.028
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 in namespace container-probe-3859 12/16/22 13:53:52.031
    Dec 16 13:53:52.038: INFO: Waiting up to 5m0s for pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966" in namespace "container-probe-3859" to be "not pending"
    Dec 16 13:53:52.041: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646112ms
    Dec 16 13:53:54.046: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966": Phase="Running", Reason="", readiness=true. Elapsed: 2.007812757s
    Dec 16 13:53:54.046: INFO: Pod "liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966" satisfied condition "not pending"
    Dec 16 13:53:54.046: INFO: Started pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 in namespace container-probe-3859
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 13:53:54.046
    Dec 16 13:53:54.050: INFO: Initial restart count of pod liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is 0
    Dec 16 13:54:14.110: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 1 (20.060123631s elapsed)
    Dec 16 13:54:34.160: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 2 (40.110436395s elapsed)
    Dec 16 13:54:54.216: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 3 (1m0.166258981s elapsed)
    Dec 16 13:55:14.268: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 4 (1m20.218189921s elapsed)
    Dec 16 13:56:22.624: INFO: Restart count of pod container-probe-3859/liveness-054874be-0bfb-4ab1-b7ba-1fc94812a966 is now 5 (2m28.574923467s elapsed)
    STEP: deleting the pod 12/16/22 13:56:22.624
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:22.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3859" for this suite. 12/16/22 13:56:22.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:22.673
Dec 16 13:56:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption 12/16/22 13:56:22.674
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:22.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:22.693
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 12/16/22 13:56:22.696
STEP: Waiting for the pdb to be processed 12/16/22 13:56:22.702
STEP: updating the pdb 12/16/22 13:56:24.71
STEP: Waiting for the pdb to be processed 12/16/22 13:56:24.719
STEP: patching the pdb 12/16/22 13:56:26.727
STEP: Waiting for the pdb to be processed 12/16/22 13:56:26.737
STEP: Waiting for the pdb to be deleted 12/16/22 13:56:28.762
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6669" for this suite. 12/16/22 13:56:28.769
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:22.673
    Dec 16 13:56:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption 12/16/22 13:56:22.674
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:22.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:22.693
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 12/16/22 13:56:22.696
    STEP: Waiting for the pdb to be processed 12/16/22 13:56:22.702
    STEP: updating the pdb 12/16/22 13:56:24.71
    STEP: Waiting for the pdb to be processed 12/16/22 13:56:24.719
    STEP: patching the pdb 12/16/22 13:56:26.727
    STEP: Waiting for the pdb to be processed 12/16/22 13:56:26.737
    STEP: Waiting for the pdb to be deleted 12/16/22 13:56:28.762
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6669" for this suite. 12/16/22 13:56:28.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:28.776
Dec 16 13:56:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 13:56:28.776
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:28.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:28.795
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-8317/configmap-test-b1692c0f-4a98-4889-99d1-837c1b88c82f 12/16/22 13:56:28.798
STEP: Creating a pod to test consume configMaps 12/16/22 13:56:28.803
Dec 16 13:56:28.811: INFO: Waiting up to 5m0s for pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282" in namespace "configmap-8317" to be "Succeeded or Failed"
Dec 16 13:56:28.815: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961097ms
Dec 16 13:56:30.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009606811s
Dec 16 13:56:32.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009570574s
STEP: Saw pod success 12/16/22 13:56:32.82
Dec 16 13:56:32.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282" satisfied condition "Succeeded or Failed"
Dec 16 13:56:32.824: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 container env-test: <nil>
STEP: delete the pod 12/16/22 13:56:32.876
Dec 16 13:56:32.891: INFO: Waiting for pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 to disappear
Dec 16 13:56:32.894: INFO: Pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:32.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8317" for this suite. 12/16/22 13:56:32.898
------------------------------
• [4.128 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:28.776
    Dec 16 13:56:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 13:56:28.776
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:28.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:28.795
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-8317/configmap-test-b1692c0f-4a98-4889-99d1-837c1b88c82f 12/16/22 13:56:28.798
    STEP: Creating a pod to test consume configMaps 12/16/22 13:56:28.803
    Dec 16 13:56:28.811: INFO: Waiting up to 5m0s for pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282" in namespace "configmap-8317" to be "Succeeded or Failed"
    Dec 16 13:56:28.815: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961097ms
    Dec 16 13:56:30.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009606811s
    Dec 16 13:56:32.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009570574s
    STEP: Saw pod success 12/16/22 13:56:32.82
    Dec 16 13:56:32.820: INFO: Pod "pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282" satisfied condition "Succeeded or Failed"
    Dec 16 13:56:32.824: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 container env-test: <nil>
    STEP: delete the pod 12/16/22 13:56:32.876
    Dec 16 13:56:32.891: INFO: Waiting for pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 to disappear
    Dec 16 13:56:32.894: INFO: Pod pod-configmaps-db5562a2-c0ee-4575-8a92-d7e637d54282 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:32.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8317" for this suite. 12/16/22 13:56:32.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:32.906
Dec 16 13:56:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:56:32.906
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:32.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:32.924
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 12/16/22 13:56:32.927
Dec 16 13:56:32.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 create -f -'
Dec 16 13:56:33.128: INFO: stderr: ""
Dec 16 13:56:33.128: INFO: stdout: "pod/pause created\n"
Dec 16 13:56:33.128: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 16 13:56:33.128: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5821" to be "running and ready"
Dec 16 13:56:33.132: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91066ms
Dec 16 13:56:33.132: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pool-a3802-fsxxd' to be 'Running' but was 'Pending'
Dec 16 13:56:35.136: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008244247s
Dec 16 13:56:35.136: INFO: Pod "pause" satisfied condition "running and ready"
Dec 16 13:56:35.136: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 12/16/22 13:56:35.136
Dec 16 13:56:35.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 label pods pause testing-label=testing-label-value'
Dec 16 13:56:35.212: INFO: stderr: ""
Dec 16 13:56:35.212: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 12/16/22 13:56:35.212
Dec 16 13:56:35.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pod pause -L testing-label'
Dec 16 13:56:35.279: INFO: stderr: ""
Dec 16 13:56:35.279: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 12/16/22 13:56:35.279
Dec 16 13:56:35.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 label pods pause testing-label-'
Dec 16 13:56:35.352: INFO: stderr: ""
Dec 16 13:56:35.352: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 12/16/22 13:56:35.352
Dec 16 13:56:35.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pod pause -L testing-label'
Dec 16 13:56:35.419: INFO: stderr: ""
Dec 16 13:56:35.419: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 12/16/22 13:56:35.419
Dec 16 13:56:35.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 delete --grace-period=0 --force -f -'
Dec 16 13:56:35.490: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 13:56:35.490: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 16 13:56:35.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get rc,svc -l name=pause --no-headers'
Dec 16 13:56:35.559: INFO: stderr: "No resources found in kubectl-5821 namespace.\n"
Dec 16 13:56:35.559: INFO: stdout: ""
Dec 16 13:56:35.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 13:56:35.622: INFO: stderr: ""
Dec 16 13:56:35.622: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:35.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5821" for this suite. 12/16/22 13:56:35.628
------------------------------
• [2.729 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:32.906
    Dec 16 13:56:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:56:32.906
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:32.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:32.924
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 12/16/22 13:56:32.927
    Dec 16 13:56:32.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 create -f -'
    Dec 16 13:56:33.128: INFO: stderr: ""
    Dec 16 13:56:33.128: INFO: stdout: "pod/pause created\n"
    Dec 16 13:56:33.128: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Dec 16 13:56:33.128: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5821" to be "running and ready"
    Dec 16 13:56:33.132: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91066ms
    Dec 16 13:56:33.132: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pool-a3802-fsxxd' to be 'Running' but was 'Pending'
    Dec 16 13:56:35.136: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008244247s
    Dec 16 13:56:35.136: INFO: Pod "pause" satisfied condition "running and ready"
    Dec 16 13:56:35.136: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 12/16/22 13:56:35.136
    Dec 16 13:56:35.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 label pods pause testing-label=testing-label-value'
    Dec 16 13:56:35.212: INFO: stderr: ""
    Dec 16 13:56:35.212: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 12/16/22 13:56:35.212
    Dec 16 13:56:35.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pod pause -L testing-label'
    Dec 16 13:56:35.279: INFO: stderr: ""
    Dec 16 13:56:35.279: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 12/16/22 13:56:35.279
    Dec 16 13:56:35.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 label pods pause testing-label-'
    Dec 16 13:56:35.352: INFO: stderr: ""
    Dec 16 13:56:35.352: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 12/16/22 13:56:35.352
    Dec 16 13:56:35.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pod pause -L testing-label'
    Dec 16 13:56:35.419: INFO: stderr: ""
    Dec 16 13:56:35.419: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 12/16/22 13:56:35.419
    Dec 16 13:56:35.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 delete --grace-period=0 --force -f -'
    Dec 16 13:56:35.490: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 13:56:35.490: INFO: stdout: "pod \"pause\" force deleted\n"
    Dec 16 13:56:35.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get rc,svc -l name=pause --no-headers'
    Dec 16 13:56:35.559: INFO: stderr: "No resources found in kubectl-5821 namespace.\n"
    Dec 16 13:56:35.559: INFO: stdout: ""
    Dec 16 13:56:35.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-5821 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 16 13:56:35.622: INFO: stderr: ""
    Dec 16 13:56:35.622: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:35.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5821" for this suite. 12/16/22 13:56:35.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:35.634
Dec 16 13:56:35.634: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 13:56:35.635
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:35.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:35.653
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Dec 16 13:56:35.662: INFO: Waiting up to 5m0s for pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b" in namespace "pods-8359" to be "running and ready"
Dec 16 13:56:35.666: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214131ms
Dec 16 13:56:35.666: INFO: The phase of Pod server-envvars-19525881-0394-4a34-9b6a-692f3345620b is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:56:37.672: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009287183s
Dec 16 13:56:37.672: INFO: The phase of Pod server-envvars-19525881-0394-4a34-9b6a-692f3345620b is Running (Ready = true)
Dec 16 13:56:37.672: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b" satisfied condition "running and ready"
Dec 16 13:56:37.695: INFO: Waiting up to 5m0s for pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019" in namespace "pods-8359" to be "Succeeded or Failed"
Dec 16 13:56:37.698: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.557994ms
Dec 16 13:56:39.703: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007504183s
Dec 16 13:56:41.704: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008390802s
STEP: Saw pod success 12/16/22 13:56:41.704
Dec 16 13:56:41.704: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019" satisfied condition "Succeeded or Failed"
Dec 16 13:56:41.708: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 container env3cont: <nil>
STEP: delete the pod 12/16/22 13:56:41.716
Dec 16 13:56:41.728: INFO: Waiting for pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 to disappear
Dec 16 13:56:41.731: INFO: Pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:41.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8359" for this suite. 12/16/22 13:56:41.736
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:35.634
    Dec 16 13:56:35.634: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 13:56:35.635
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:35.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:35.653
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Dec 16 13:56:35.662: INFO: Waiting up to 5m0s for pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b" in namespace "pods-8359" to be "running and ready"
    Dec 16 13:56:35.666: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214131ms
    Dec 16 13:56:35.666: INFO: The phase of Pod server-envvars-19525881-0394-4a34-9b6a-692f3345620b is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:56:37.672: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009287183s
    Dec 16 13:56:37.672: INFO: The phase of Pod server-envvars-19525881-0394-4a34-9b6a-692f3345620b is Running (Ready = true)
    Dec 16 13:56:37.672: INFO: Pod "server-envvars-19525881-0394-4a34-9b6a-692f3345620b" satisfied condition "running and ready"
    Dec 16 13:56:37.695: INFO: Waiting up to 5m0s for pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019" in namespace "pods-8359" to be "Succeeded or Failed"
    Dec 16 13:56:37.698: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.557994ms
    Dec 16 13:56:39.703: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007504183s
    Dec 16 13:56:41.704: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008390802s
    STEP: Saw pod success 12/16/22 13:56:41.704
    Dec 16 13:56:41.704: INFO: Pod "client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019" satisfied condition "Succeeded or Failed"
    Dec 16 13:56:41.708: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 container env3cont: <nil>
    STEP: delete the pod 12/16/22 13:56:41.716
    Dec 16 13:56:41.728: INFO: Waiting for pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 to disappear
    Dec 16 13:56:41.731: INFO: Pod client-envvars-92277b99-a00b-411c-80a8-6e8d59cd6019 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:41.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8359" for this suite. 12/16/22 13:56:41.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:41.742
Dec 16 13:56:41.742: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 13:56:41.742
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:41.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:41.759
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 12/16/22 13:56:41.762
STEP: Verify that the required pods have come up 12/16/22 13:56:41.768
Dec 16 13:56:41.771: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec 16 13:56:46.776: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 12/16/22 13:56:46.776
Dec 16 13:56:46.779: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 12/16/22 13:56:46.779
STEP: DeleteCollection of the ReplicaSets 12/16/22 13:56:46.782
STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/16/22 13:56:46.79
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:46.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1769" for this suite. 12/16/22 13:56:46.799
------------------------------
• [SLOW TEST] [5.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:41.742
    Dec 16 13:56:41.742: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 13:56:41.742
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:41.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:41.759
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 12/16/22 13:56:41.762
    STEP: Verify that the required pods have come up 12/16/22 13:56:41.768
    Dec 16 13:56:41.771: INFO: Pod name sample-pod: Found 0 pods out of 3
    Dec 16 13:56:46.776: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 12/16/22 13:56:46.776
    Dec 16 13:56:46.779: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 12/16/22 13:56:46.779
    STEP: DeleteCollection of the ReplicaSets 12/16/22 13:56:46.782
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/16/22 13:56:46.79
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:46.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1769" for this suite. 12/16/22 13:56:46.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:46.808
Dec 16 13:56:46.808: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 13:56:46.809
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:46.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:46.845
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:56:46.85
Dec 16 13:56:46.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6536 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Dec 16 13:56:46.932: INFO: stderr: ""
Dec 16 13:56:46.932: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 12/16/22 13:56:46.932
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Dec 16 13:56:46.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6536 delete pods e2e-test-httpd-pod'
Dec 16 13:56:49.284: INFO: stderr: ""
Dec 16 13:56:49.284: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:49.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6536" for this suite. 12/16/22 13:56:49.289
------------------------------
• [2.489 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:46.808
    Dec 16 13:56:46.808: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 13:56:46.809
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:46.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:46.845
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 13:56:46.85
    Dec 16 13:56:46.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6536 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Dec 16 13:56:46.932: INFO: stderr: ""
    Dec 16 13:56:46.932: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 12/16/22 13:56:46.932
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Dec 16 13:56:46.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6536 delete pods e2e-test-httpd-pod'
    Dec 16 13:56:49.284: INFO: stderr: ""
    Dec 16 13:56:49.284: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:49.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6536" for this suite. 12/16/22 13:56:49.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:49.298
Dec 16 13:56:49.298: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 13:56:49.298
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:49.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:49.314
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
 12/16/22 13:56:49.317
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
 12/16/22 13:56:49.317
STEP: creating a pod to probe DNS 12/16/22 13:56:49.317
STEP: submitting the pod to kubernetes 12/16/22 13:56:49.317
Dec 16 13:56:49.324: INFO: Waiting up to 15m0s for pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296" in namespace "dns-8574" to be "running"
Dec 16 13:56:49.327: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395733ms
Dec 16 13:56:51.332: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296": Phase="Running", Reason="", readiness=true. Elapsed: 2.00816846s
Dec 16 13:56:51.332: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296" satisfied condition "running"
STEP: retrieving the pod 12/16/22 13:56:51.332
STEP: looking for the results for each expected name from probers 12/16/22 13:56:51.337
Dec 16 13:56:51.389: INFO: DNS probes using dns-8574/dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296 succeeded

STEP: deleting the pod 12/16/22 13:56:51.389
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:51.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8574" for this suite. 12/16/22 13:56:51.406
------------------------------
• [2.116 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:49.298
    Dec 16 13:56:49.298: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 13:56:49.298
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:49.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:49.314
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
     12/16/22 13:56:49.317
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;sleep 1; done
     12/16/22 13:56:49.317
    STEP: creating a pod to probe DNS 12/16/22 13:56:49.317
    STEP: submitting the pod to kubernetes 12/16/22 13:56:49.317
    Dec 16 13:56:49.324: INFO: Waiting up to 15m0s for pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296" in namespace "dns-8574" to be "running"
    Dec 16 13:56:49.327: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395733ms
    Dec 16 13:56:51.332: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296": Phase="Running", Reason="", readiness=true. Elapsed: 2.00816846s
    Dec 16 13:56:51.332: INFO: Pod "dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 13:56:51.332
    STEP: looking for the results for each expected name from probers 12/16/22 13:56:51.337
    Dec 16 13:56:51.389: INFO: DNS probes using dns-8574/dns-test-c9aaa2a5-ab1d-4581-aff9-9309440e2296 succeeded

    STEP: deleting the pod 12/16/22 13:56:51.389
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:51.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8574" for this suite. 12/16/22 13:56:51.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:51.414
Dec 16 13:56:51.414: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:56:51.415
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:51.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:51.43
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Dec 16 13:56:51.444: INFO: Waiting up to 5m0s for pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31" in namespace "svcaccounts-7073" to be "running"
Dec 16 13:56:51.446: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.803869ms
Dec 16 13:56:53.452: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31": Phase="Running", Reason="", readiness=true. Elapsed: 2.008568273s
Dec 16 13:56:53.452: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31" satisfied condition "running"
STEP: reading a file in the container 12/16/22 13:56:53.452
Dec 16 13:56:53.452: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 12/16/22 13:56:53.632
Dec 16 13:56:53.632: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 12/16/22 13:56:53.813
Dec 16 13:56:53.813: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec 16 13:56:54.012: INFO: Got root ca configmap in namespace "svcaccounts-7073"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 13:56:54.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7073" for this suite. 12/16/22 13:56:54.018
------------------------------
• [2.609 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:51.414
    Dec 16 13:56:51.414: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 13:56:51.415
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:51.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:51.43
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Dec 16 13:56:51.444: INFO: Waiting up to 5m0s for pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31" in namespace "svcaccounts-7073" to be "running"
    Dec 16 13:56:51.446: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.803869ms
    Dec 16 13:56:53.452: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31": Phase="Running", Reason="", readiness=true. Elapsed: 2.008568273s
    Dec 16 13:56:53.452: INFO: Pod "pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31" satisfied condition "running"
    STEP: reading a file in the container 12/16/22 13:56:53.452
    Dec 16 13:56:53.452: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 12/16/22 13:56:53.632
    Dec 16 13:56:53.632: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 12/16/22 13:56:53.813
    Dec 16 13:56:53.813: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7073 pod-service-account-b09f8c53-8c69-4386-be2e-0b59b557dc31 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Dec 16 13:56:54.012: INFO: Got root ca configmap in namespace "svcaccounts-7073"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:56:54.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7073" for this suite. 12/16/22 13:56:54.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:56:54.023
Dec 16 13:56:54.024: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:56:54.024
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:54.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:54.042
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 16 13:56:54.055: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 16 13:57:54.077: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:57:54.081
Dec 16 13:57:54.081: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-preemption-path 12/16/22 13:57:54.082
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:57:54.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:57:54.1
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Dec 16 13:57:54.115: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec 16 13:57:54.118: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Dec 16 13:57:54.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:57:54.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4545" for this suite. 12/16/22 13:57:54.194
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3862" for this suite. 12/16/22 13:57:54.2
------------------------------
• [SLOW TEST] [60.181 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:56:54.023
    Dec 16 13:56:54.024: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption 12/16/22 13:56:54.024
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:56:54.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:56:54.042
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 16 13:56:54.055: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 16 13:57:54.077: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:57:54.081
    Dec 16 13:57:54.081: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-preemption-path 12/16/22 13:57:54.082
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:57:54.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:57:54.1
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Dec 16 13:57:54.115: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec 16 13:57:54.118: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:57:54.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:57:54.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4545" for this suite. 12/16/22 13:57:54.194
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3862" for this suite. 12/16/22 13:57:54.2
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:57:54.205
Dec 16 13:57:54.205: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 13:57:54.205
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:57:54.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:57:54.222
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 12/16/22 13:57:54.225
STEP: When the matched label of one of its pods change 12/16/22 13:57:54.229
Dec 16 13:57:54.232: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 16 13:57:59.236: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 12/16/22 13:57:59.247
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:00.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-74" for this suite. 12/16/22 13:58:00.259
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:57:54.205
    Dec 16 13:57:54.205: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 13:57:54.205
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:57:54.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:57:54.222
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 12/16/22 13:57:54.225
    STEP: When the matched label of one of its pods change 12/16/22 13:57:54.229
    Dec 16 13:57:54.232: INFO: Pod name pod-release: Found 0 pods out of 1
    Dec 16 13:57:59.236: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/16/22 13:57:59.247
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:00.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-74" for this suite. 12/16/22 13:58:00.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:00.271
Dec 16 13:58:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:58:00.272
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:00.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:00.289
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/16/22 13:58:00.291
Dec 16 13:58:00.292: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/16/22 13:58:07.004
Dec 16 13:58:07.004: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 13:58:08.758: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:15.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5704" for this suite. 12/16/22 13:58:15.658
------------------------------
• [SLOW TEST] [15.393 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:00.271
    Dec 16 13:58:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 13:58:00.272
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:00.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:00.289
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/16/22 13:58:00.291
    Dec 16 13:58:00.292: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/16/22 13:58:07.004
    Dec 16 13:58:07.004: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 13:58:08.758: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:15.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5704" for this suite. 12/16/22 13:58:15.658
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:15.664
Dec 16 13:58:15.664: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 13:58:15.665
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:15.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:15.68
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 13:58:15.693
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:58:15.966
STEP: Deploying the webhook pod 12/16/22 13:58:15.978
STEP: Wait for the deployment to be ready 12/16/22 13:58:15.988
Dec 16 13:58:15.995: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/16/22 13:58:18.006
STEP: Verifying the service has paired with the endpoint 12/16/22 13:58:18.021
Dec 16 13:58:19.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 12/16/22 13:58:19.026
Dec 16 13:58:19.043: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/16/22 13:58:19.183
STEP: Creating a configMap that should not be mutated 12/16/22 13:58:19.191
STEP: Patching a mutating webhook configuration's rules to include the create operation 12/16/22 13:58:19.201
STEP: Creating a configMap that should be mutated 12/16/22 13:58:19.207
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-527" for this suite. 12/16/22 13:58:19.298
STEP: Destroying namespace "webhook-527-markers" for this suite. 12/16/22 13:58:19.303
------------------------------
• [3.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:15.664
    Dec 16 13:58:15.664: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 13:58:15.665
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:15.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:15.68
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 13:58:15.693
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 13:58:15.966
    STEP: Deploying the webhook pod 12/16/22 13:58:15.978
    STEP: Wait for the deployment to be ready 12/16/22 13:58:15.988
    Dec 16 13:58:15.995: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/16/22 13:58:18.006
    STEP: Verifying the service has paired with the endpoint 12/16/22 13:58:18.021
    Dec 16 13:58:19.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 12/16/22 13:58:19.026
    Dec 16 13:58:19.043: INFO: Waiting for webhook configuration to be ready...
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/16/22 13:58:19.183
    STEP: Creating a configMap that should not be mutated 12/16/22 13:58:19.191
    STEP: Patching a mutating webhook configuration's rules to include the create operation 12/16/22 13:58:19.201
    STEP: Creating a configMap that should be mutated 12/16/22 13:58:19.207
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-527" for this suite. 12/16/22 13:58:19.298
    STEP: Destroying namespace "webhook-527-markers" for this suite. 12/16/22 13:58:19.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:19.311
Dec 16 13:58:19.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context 12/16/22 13:58:19.311
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:19.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:19.325
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/16/22 13:58:19.328
Dec 16 13:58:19.336: INFO: Waiting up to 5m0s for pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348" in namespace "security-context-2537" to be "Succeeded or Failed"
Dec 16 13:58:19.339: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271351ms
Dec 16 13:58:21.347: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011120295s
Dec 16 13:58:23.345: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009710649s
STEP: Saw pod success 12/16/22 13:58:23.345
Dec 16 13:58:23.345: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348" satisfied condition "Succeeded or Failed"
Dec 16 13:58:23.349: INFO: Trying to get logs from node pool-a3802-fsxxd pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 container test-container: <nil>
STEP: delete the pod 12/16/22 13:58:23.4
Dec 16 13:58:23.412: INFO: Waiting for pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 to disappear
Dec 16 13:58:23.415: INFO: Pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:23.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2537" for this suite. 12/16/22 13:58:23.419
------------------------------
• [4.113 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:19.311
    Dec 16 13:58:19.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context 12/16/22 13:58:19.311
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:19.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:19.325
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/16/22 13:58:19.328
    Dec 16 13:58:19.336: INFO: Waiting up to 5m0s for pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348" in namespace "security-context-2537" to be "Succeeded or Failed"
    Dec 16 13:58:19.339: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271351ms
    Dec 16 13:58:21.347: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011120295s
    Dec 16 13:58:23.345: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009710649s
    STEP: Saw pod success 12/16/22 13:58:23.345
    Dec 16 13:58:23.345: INFO: Pod "security-context-d6444997-f974-4270-bf6b-2d67f555a348" satisfied condition "Succeeded or Failed"
    Dec 16 13:58:23.349: INFO: Trying to get logs from node pool-a3802-fsxxd pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 container test-container: <nil>
    STEP: delete the pod 12/16/22 13:58:23.4
    Dec 16 13:58:23.412: INFO: Waiting for pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 to disappear
    Dec 16 13:58:23.415: INFO: Pod security-context-d6444997-f974-4270-bf6b-2d67f555a348 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:23.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2537" for this suite. 12/16/22 13:58:23.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:23.425
Dec 16 13:58:23.425: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 13:58:23.426
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:23.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:23.44
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-4bc02aae-048b-4766-bd98-62b9c262154f 12/16/22 13:58:23.442
STEP: Creating a pod to test consume secrets 12/16/22 13:58:23.447
Dec 16 13:58:23.454: INFO: Waiting up to 5m0s for pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5" in namespace "secrets-4715" to be "Succeeded or Failed"
Dec 16 13:58:23.457: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069436ms
Dec 16 13:58:25.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008809013s
Dec 16 13:58:27.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008594131s
STEP: Saw pod success 12/16/22 13:58:27.462
Dec 16 13:58:27.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5" satisfied condition "Succeeded or Failed"
Dec 16 13:58:27.466: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 13:58:27.474
Dec 16 13:58:27.484: INFO: Waiting for pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 to disappear
Dec 16 13:58:27.486: INFO: Pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:27.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4715" for this suite. 12/16/22 13:58:27.49
------------------------------
• [4.077 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:23.425
    Dec 16 13:58:23.425: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 13:58:23.426
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:23.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:23.44
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-4bc02aae-048b-4766-bd98-62b9c262154f 12/16/22 13:58:23.442
    STEP: Creating a pod to test consume secrets 12/16/22 13:58:23.447
    Dec 16 13:58:23.454: INFO: Waiting up to 5m0s for pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5" in namespace "secrets-4715" to be "Succeeded or Failed"
    Dec 16 13:58:23.457: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069436ms
    Dec 16 13:58:25.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008809013s
    Dec 16 13:58:27.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008594131s
    STEP: Saw pod success 12/16/22 13:58:27.462
    Dec 16 13:58:27.462: INFO: Pod "pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5" satisfied condition "Succeeded or Failed"
    Dec 16 13:58:27.466: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 13:58:27.474
    Dec 16 13:58:27.484: INFO: Waiting for pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 to disappear
    Dec 16 13:58:27.486: INFO: Pod pod-secrets-25ffbb03-a5a5-4408-9b41-dc6a47b1b0a5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:27.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4715" for this suite. 12/16/22 13:58:27.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:27.503
Dec 16 13:58:27.503: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:58:27.504
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:27.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:27.52
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 12/16/22 13:58:27.525
STEP: watching for the Service to be added 12/16/22 13:58:27.54
Dec 16 13:58:27.541: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 16 13:58:27.541: INFO: Service test-service-9vx78 created
STEP: Getting /status 12/16/22 13:58:27.541
Dec 16 13:58:27.545: INFO: Service test-service-9vx78 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 12/16/22 13:58:27.545
STEP: watching for the Service to be patched 12/16/22 13:58:27.55
Dec 16 13:58:27.552: INFO: observed Service test-service-9vx78 in namespace services-795 with annotations: map[] & LoadBalancer: {[]}
Dec 16 13:58:27.552: INFO: Found Service test-service-9vx78 in namespace services-795 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 16 13:58:27.552: INFO: Service test-service-9vx78 has service status patched
STEP: updating the ServiceStatus 12/16/22 13:58:27.552
Dec 16 13:58:27.563: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 12/16/22 13:58:27.563
Dec 16 13:58:27.564: INFO: Observed Service test-service-9vx78 in namespace services-795 with annotations: map[] & Conditions: {[]}
Dec 16 13:58:27.564: INFO: Observed event: &Service{ObjectMeta:{test-service-9vx78  services-795  d7aa973e-51f6-4622-a406-80fe49f2ab8b 680956019 0 2022-12-16 13:58:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-16 13:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-16 13:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.101.19.209,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.101.19.209],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 16 13:58:27.565: INFO: Found Service test-service-9vx78 in namespace services-795 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 16 13:58:27.565: INFO: Service test-service-9vx78 has service status updated
STEP: patching the service 12/16/22 13:58:27.565
STEP: watching for the Service to be patched 12/16/22 13:58:27.574
Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
Dec 16 13:58:27.575: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service:patched test-service-static:true]
Dec 16 13:58:27.575: INFO: Service test-service-9vx78 patched
STEP: deleting the service 12/16/22 13:58:27.575
STEP: watching for the Service to be deleted 12/16/22 13:58:27.589
Dec 16 13:58:27.591: INFO: Observed event: ADDED
Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
Dec 16 13:58:27.591: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 16 13:58:27.591: INFO: Service test-service-9vx78 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:27.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-795" for this suite. 12/16/22 13:58:27.594
------------------------------
• [0.097 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:27.503
    Dec 16 13:58:27.503: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:58:27.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:27.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:27.52
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 12/16/22 13:58:27.525
    STEP: watching for the Service to be added 12/16/22 13:58:27.54
    Dec 16 13:58:27.541: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Dec 16 13:58:27.541: INFO: Service test-service-9vx78 created
    STEP: Getting /status 12/16/22 13:58:27.541
    Dec 16 13:58:27.545: INFO: Service test-service-9vx78 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 12/16/22 13:58:27.545
    STEP: watching for the Service to be patched 12/16/22 13:58:27.55
    Dec 16 13:58:27.552: INFO: observed Service test-service-9vx78 in namespace services-795 with annotations: map[] & LoadBalancer: {[]}
    Dec 16 13:58:27.552: INFO: Found Service test-service-9vx78 in namespace services-795 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Dec 16 13:58:27.552: INFO: Service test-service-9vx78 has service status patched
    STEP: updating the ServiceStatus 12/16/22 13:58:27.552
    Dec 16 13:58:27.563: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 12/16/22 13:58:27.563
    Dec 16 13:58:27.564: INFO: Observed Service test-service-9vx78 in namespace services-795 with annotations: map[] & Conditions: {[]}
    Dec 16 13:58:27.564: INFO: Observed event: &Service{ObjectMeta:{test-service-9vx78  services-795  d7aa973e-51f6-4622-a406-80fe49f2ab8b 680956019 0 2022-12-16 13:58:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-16 13:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-16 13:58:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.101.19.209,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.101.19.209],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Dec 16 13:58:27.565: INFO: Found Service test-service-9vx78 in namespace services-795 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 16 13:58:27.565: INFO: Service test-service-9vx78 has service status updated
    STEP: patching the service 12/16/22 13:58:27.565
    STEP: watching for the Service to be patched 12/16/22 13:58:27.574
    Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
    Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
    Dec 16 13:58:27.575: INFO: observed Service test-service-9vx78 in namespace services-795 with labels: map[test-service-static:true]
    Dec 16 13:58:27.575: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service:patched test-service-static:true]
    Dec 16 13:58:27.575: INFO: Service test-service-9vx78 patched
    STEP: deleting the service 12/16/22 13:58:27.575
    STEP: watching for the Service to be deleted 12/16/22 13:58:27.589
    Dec 16 13:58:27.591: INFO: Observed event: ADDED
    Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
    Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
    Dec 16 13:58:27.591: INFO: Observed event: MODIFIED
    Dec 16 13:58:27.591: INFO: Found Service test-service-9vx78 in namespace services-795 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Dec 16 13:58:27.591: INFO: Service test-service-9vx78 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:27.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-795" for this suite. 12/16/22 13:58:27.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:27.601
Dec 16 13:58:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 13:58:27.601
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:27.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:27.618
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2135 12/16/22 13:58:27.649
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/16/22 13:58:27.662
STEP: creating service externalsvc in namespace services-2135 12/16/22 13:58:27.662
STEP: creating replication controller externalsvc in namespace services-2135 12/16/22 13:58:27.676
I1216 13:58:27.682818      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2135, replica count: 2
I1216 13:58:30.734651      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 12/16/22 13:58:30.738
Dec 16 13:58:30.757: INFO: Creating new exec pod
Dec 16 13:58:30.762: INFO: Waiting up to 5m0s for pod "execpod6bdns" in namespace "services-2135" to be "running"
Dec 16 13:58:30.765: INFO: Pod "execpod6bdns": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463514ms
Dec 16 13:58:32.769: INFO: Pod "execpod6bdns": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607703s
Dec 16 13:58:32.769: INFO: Pod "execpod6bdns" satisfied condition "running"
Dec 16 13:58:32.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-2135 exec execpod6bdns -- /bin/sh -x -c nslookup clusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local'
Dec 16 13:58:32.991: INFO: stderr: "+ nslookup clusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\n"
Dec 16 13:58:32.991: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\tcanonical name = externalsvc.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local.\nName:\texternalsvc.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\nAddress: 10.96.133.6\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2135, will wait for the garbage collector to delete the pods 12/16/22 13:58:32.991
Dec 16 13:58:33.055: INFO: Deleting ReplicationController externalsvc took: 7.555973ms
Dec 16 13:58:33.155: INFO: Terminating ReplicationController externalsvc pods took: 100.514093ms
Dec 16 13:58:35.576: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 13:58:35.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2135" for this suite. 12/16/22 13:58:35.591
------------------------------
• [SLOW TEST] [7.996 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:27.601
    Dec 16 13:58:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 13:58:27.601
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:27.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:27.618
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2135 12/16/22 13:58:27.649
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/16/22 13:58:27.662
    STEP: creating service externalsvc in namespace services-2135 12/16/22 13:58:27.662
    STEP: creating replication controller externalsvc in namespace services-2135 12/16/22 13:58:27.676
    I1216 13:58:27.682818      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2135, replica count: 2
    I1216 13:58:30.734651      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 12/16/22 13:58:30.738
    Dec 16 13:58:30.757: INFO: Creating new exec pod
    Dec 16 13:58:30.762: INFO: Waiting up to 5m0s for pod "execpod6bdns" in namespace "services-2135" to be "running"
    Dec 16 13:58:30.765: INFO: Pod "execpod6bdns": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463514ms
    Dec 16 13:58:32.769: INFO: Pod "execpod6bdns": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607703s
    Dec 16 13:58:32.769: INFO: Pod "execpod6bdns" satisfied condition "running"
    Dec 16 13:58:32.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-2135 exec execpod6bdns -- /bin/sh -x -c nslookup clusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local'
    Dec 16 13:58:32.991: INFO: stderr: "+ nslookup clusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\n"
    Dec 16 13:58:32.991: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\tcanonical name = externalsvc.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local.\nName:\texternalsvc.services-2135.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local\nAddress: 10.96.133.6\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2135, will wait for the garbage collector to delete the pods 12/16/22 13:58:32.991
    Dec 16 13:58:33.055: INFO: Deleting ReplicationController externalsvc took: 7.555973ms
    Dec 16 13:58:33.155: INFO: Terminating ReplicationController externalsvc pods took: 100.514093ms
    Dec 16 13:58:35.576: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 13:58:35.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2135" for this suite. 12/16/22 13:58:35.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 13:58:35.597
Dec 16 13:58:35.597: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 13:58:35.597
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:35.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:35.611
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bff95bc5-9ae0-429d-a3bb-3caac28510e9 12/16/22 13:58:35.617
STEP: Creating the pod 12/16/22 13:58:35.622
Dec 16 13:58:35.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39" in namespace "projected-6278" to be "running and ready"
Dec 16 13:58:35.631: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731932ms
Dec 16 13:58:35.631: INFO: The phase of Pod pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 13:58:37.636: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39": Phase="Running", Reason="", readiness=true. Elapsed: 2.008135237s
Dec 16 13:58:37.636: INFO: The phase of Pod pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39 is Running (Ready = true)
Dec 16 13:58:37.636: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-bff95bc5-9ae0-429d-a3bb-3caac28510e9 12/16/22 13:58:37.649
STEP: waiting to observe update in volume 12/16/22 13:58:37.654
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:08.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6278" for this suite. 12/16/22 14:00:08.148
------------------------------
• [SLOW TEST] [92.560 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 13:58:35.597
    Dec 16 13:58:35.597: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 13:58:35.597
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 13:58:35.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 13:58:35.611
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-bff95bc5-9ae0-429d-a3bb-3caac28510e9 12/16/22 13:58:35.617
    STEP: Creating the pod 12/16/22 13:58:35.622
    Dec 16 13:58:35.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39" in namespace "projected-6278" to be "running and ready"
    Dec 16 13:58:35.631: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731932ms
    Dec 16 13:58:35.631: INFO: The phase of Pod pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 13:58:37.636: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39": Phase="Running", Reason="", readiness=true. Elapsed: 2.008135237s
    Dec 16 13:58:37.636: INFO: The phase of Pod pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39 is Running (Ready = true)
    Dec 16 13:58:37.636: INFO: Pod "pod-projected-configmaps-db923982-933c-49c2-9230-cfd99040df39" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-bff95bc5-9ae0-429d-a3bb-3caac28510e9 12/16/22 13:58:37.649
    STEP: waiting to observe update in volume 12/16/22 13:58:37.654
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:08.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6278" for this suite. 12/16/22 14:00:08.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:08.158
Dec 16 14:00:08.158: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:00:08.158
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:08.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:08.175
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:00:08.178
Dec 16 14:00:08.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931" in namespace "projected-3740" to be "Succeeded or Failed"
Dec 16 14:00:08.190: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467759ms
Dec 16 14:00:10.196: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653404s
Dec 16 14:00:12.196: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509906s
STEP: Saw pod success 12/16/22 14:00:12.196
Dec 16 14:00:12.197: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931" satisfied condition "Succeeded or Failed"
Dec 16 14:00:12.201: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 container client-container: <nil>
STEP: delete the pod 12/16/22 14:00:12.209
Dec 16 14:00:12.219: INFO: Waiting for pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 to disappear
Dec 16 14:00:12.223: INFO: Pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:12.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3740" for this suite. 12/16/22 14:00:12.227
------------------------------
• [4.076 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:08.158
    Dec 16 14:00:08.158: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:00:08.158
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:08.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:08.175
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:00:08.178
    Dec 16 14:00:08.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931" in namespace "projected-3740" to be "Succeeded or Failed"
    Dec 16 14:00:08.190: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467759ms
    Dec 16 14:00:10.196: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653404s
    Dec 16 14:00:12.196: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509906s
    STEP: Saw pod success 12/16/22 14:00:12.196
    Dec 16 14:00:12.197: INFO: Pod "downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931" satisfied condition "Succeeded or Failed"
    Dec 16 14:00:12.201: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 container client-container: <nil>
    STEP: delete the pod 12/16/22 14:00:12.209
    Dec 16 14:00:12.219: INFO: Waiting for pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 to disappear
    Dec 16 14:00:12.223: INFO: Pod downwardapi-volume-17c773b3-f806-42b8-9b17-821580d42931 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:12.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3740" for this suite. 12/16/22 14:00:12.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:12.234
Dec 16 14:00:12.234: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 14:00:12.235
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:12.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:12.251
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/16/22 14:00:12.254
Dec 16 14:00:12.262: INFO: Waiting up to 5m0s for pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3" in namespace "emptydir-5987" to be "Succeeded or Failed"
Dec 16 14:00:12.266: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442721ms
Dec 16 14:00:14.271: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008756363s
Dec 16 14:00:16.272: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009074002s
STEP: Saw pod success 12/16/22 14:00:16.272
Dec 16 14:00:16.272: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3" satisfied condition "Succeeded or Failed"
Dec 16 14:00:16.276: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 container test-container: <nil>
STEP: delete the pod 12/16/22 14:00:16.285
Dec 16 14:00:16.296: INFO: Waiting for pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 to disappear
Dec 16 14:00:16.299: INFO: Pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:16.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5987" for this suite. 12/16/22 14:00:16.303
------------------------------
• [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:12.234
    Dec 16 14:00:12.234: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 14:00:12.235
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:12.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:12.251
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/16/22 14:00:12.254
    Dec 16 14:00:12.262: INFO: Waiting up to 5m0s for pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3" in namespace "emptydir-5987" to be "Succeeded or Failed"
    Dec 16 14:00:12.266: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442721ms
    Dec 16 14:00:14.271: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008756363s
    Dec 16 14:00:16.272: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009074002s
    STEP: Saw pod success 12/16/22 14:00:16.272
    Dec 16 14:00:16.272: INFO: Pod "pod-c0284932-0e42-4f83-bb9f-557a86f49ff3" satisfied condition "Succeeded or Failed"
    Dec 16 14:00:16.276: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 container test-container: <nil>
    STEP: delete the pod 12/16/22 14:00:16.285
    Dec 16 14:00:16.296: INFO: Waiting for pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 to disappear
    Dec 16 14:00:16.299: INFO: Pod pod-c0284932-0e42-4f83-bb9f-557a86f49ff3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:16.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5987" for this suite. 12/16/22 14:00:16.303
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:16.31
Dec 16 14:00:16.310: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubelet-test 12/16/22 14:00:16.31
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:16.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:16.326
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Dec 16 14:00:16.335: INFO: Waiting up to 5m0s for pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834" in namespace "kubelet-test-9406" to be "running and ready"
Dec 16 14:00:16.340: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452437ms
Dec 16 14:00:16.340: INFO: The phase of Pod busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:00:18.346: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834": Phase="Running", Reason="", readiness=true. Elapsed: 2.011147142s
Dec 16 14:00:18.346: INFO: The phase of Pod busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834 is Running (Ready = true)
Dec 16 14:00:18.346: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9406" for this suite. 12/16/22 14:00:18.363
------------------------------
• [2.061 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:16.31
    Dec 16 14:00:16.310: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubelet-test 12/16/22 14:00:16.31
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:16.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:16.326
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Dec 16 14:00:16.335: INFO: Waiting up to 5m0s for pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834" in namespace "kubelet-test-9406" to be "running and ready"
    Dec 16 14:00:16.340: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452437ms
    Dec 16 14:00:16.340: INFO: The phase of Pod busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:00:18.346: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834": Phase="Running", Reason="", readiness=true. Elapsed: 2.011147142s
    Dec 16 14:00:18.346: INFO: The phase of Pod busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834 is Running (Ready = true)
    Dec 16 14:00:18.346: INFO: Pod "busybox-scheduling-821c4c05-41ac-44b8-9910-1104b665e834" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9406" for this suite. 12/16/22 14:00:18.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:18.371
Dec 16 14:00:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 14:00:18.372
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:18.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:18.389
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Dec 16 14:00:18.391: INFO: Creating deployment "test-recreate-deployment"
Dec 16 14:00:18.396: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 16 14:00:18.403: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 16 14:00:20.413: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 16 14:00:20.570: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 16 14:00:20.580: INFO: Updating deployment test-recreate-deployment
Dec 16 14:00:20.580: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 14:00:20.687: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1348  0ae4877e-8a20-46a6-a742-ea5182d61cea 680958589 2 2022-12-16 14:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4aa88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-16 14:00:20 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-16 14:00:20 +0000 UTC,LastTransitionTime:2022-12-16 14:00:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 16 14:00:20.690: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1348  6b2eda81-f76d-4474-89fc-73601f989d73 680958587 1 2022-12-16 14:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0ae4877e-8a20-46a6-a742-ea5182d61cea 0xc003d4af50 0xc003d4af51}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae4877e-8a20-46a6-a742-ea5182d61cea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4afe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:00:20.690: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 16 14:00:20.690: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1348  cfe33971-d78e-4bf7-882c-2708c3052b48 680958575 2 2022-12-16 14:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0ae4877e-8a20-46a6-a742-ea5182d61cea 0xc003d4ae37 0xc003d4ae38}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae4877e-8a20-46a6-a742-ea5182d61cea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4aee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:00:20.693: INFO: Pod "test-recreate-deployment-cff6dc657-4vhxw" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4vhxw test-recreate-deployment-cff6dc657- deployment-1348  f075522b-9951-4106-94e4-cdcfa199a932 680958588 0 2022-12-16 14:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6b2eda81-f76d-4474-89fc-73601f989d73 0xc003d4b460 0xc003d4b461}] [] [{kube-controller-manager Update v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b2eda81-f76d-4474-89fc-73601f989d73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdp9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdp9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 14:00:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:20.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1348" for this suite. 12/16/22 14:00:20.697
------------------------------
• [2.357 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:18.371
    Dec 16 14:00:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 14:00:18.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:18.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:18.389
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Dec 16 14:00:18.391: INFO: Creating deployment "test-recreate-deployment"
    Dec 16 14:00:18.396: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Dec 16 14:00:18.403: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Dec 16 14:00:20.413: INFO: Waiting deployment "test-recreate-deployment" to complete
    Dec 16 14:00:20.570: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Dec 16 14:00:20.580: INFO: Updating deployment test-recreate-deployment
    Dec 16 14:00:20.580: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 14:00:20.687: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1348  0ae4877e-8a20-46a6-a742-ea5182d61cea 680958589 2 2022-12-16 14:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4aa88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-16 14:00:20 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-16 14:00:20 +0000 UTC,LastTransitionTime:2022-12-16 14:00:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 16 14:00:20.690: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1348  6b2eda81-f76d-4474-89fc-73601f989d73 680958587 1 2022-12-16 14:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0ae4877e-8a20-46a6-a742-ea5182d61cea 0xc003d4af50 0xc003d4af51}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae4877e-8a20-46a6-a742-ea5182d61cea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4afe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:00:20.690: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Dec 16 14:00:20.690: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1348  cfe33971-d78e-4bf7-882c-2708c3052b48 680958575 2 2022-12-16 14:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0ae4877e-8a20-46a6-a742-ea5182d61cea 0xc003d4ae37 0xc003d4ae38}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae4877e-8a20-46a6-a742-ea5182d61cea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4aee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:00:20.693: INFO: Pod "test-recreate-deployment-cff6dc657-4vhxw" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4vhxw test-recreate-deployment-cff6dc657- deployment-1348  f075522b-9951-4106-94e4-cdcfa199a932 680958588 0 2022-12-16 14:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6b2eda81-f76d-4474-89fc-73601f989d73 0xc003d4b460 0xc003d4b461}] [] [{kube-controller-manager Update v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b2eda81-f76d-4474-89fc-73601f989d73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 14:00:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdp9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdp9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:00:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:,StartTime:2022-12-16 14:00:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:20.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1348" for this suite. 12/16/22 14:00:20.697
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:20.728
Dec 16 14:00:20.728: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 14:00:20.729
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:20.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:20.747
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a in namespace container-probe-6673 12/16/22 14:00:20.756
Dec 16 14:00:20.766: INFO: Waiting up to 5m0s for pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a" in namespace "container-probe-6673" to be "not pending"
Dec 16 14:00:20.774: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.002468ms
Dec 16 14:00:22.779: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013372286s
Dec 16 14:00:22.779: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a" satisfied condition "not pending"
Dec 16 14:00:22.779: INFO: Started pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a in namespace container-probe-6673
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:00:22.779
Dec 16 14:00:22.782: INFO: Initial restart count of pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a is 0
Dec 16 14:00:42.847: INFO: Restart count of pod container-probe-6673/liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a is now 1 (20.064265053s elapsed)
STEP: deleting the pod 12/16/22 14:00:42.847
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 14:00:42.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6673" for this suite. 12/16/22 14:00:42.865
------------------------------
• [SLOW TEST] [22.143 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:20.728
    Dec 16 14:00:20.728: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 14:00:20.729
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:20.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:20.747
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a in namespace container-probe-6673 12/16/22 14:00:20.756
    Dec 16 14:00:20.766: INFO: Waiting up to 5m0s for pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a" in namespace "container-probe-6673" to be "not pending"
    Dec 16 14:00:20.774: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.002468ms
    Dec 16 14:00:22.779: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013372286s
    Dec 16 14:00:22.779: INFO: Pod "liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a" satisfied condition "not pending"
    Dec 16 14:00:22.779: INFO: Started pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a in namespace container-probe-6673
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:00:22.779
    Dec 16 14:00:22.782: INFO: Initial restart count of pod liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a is 0
    Dec 16 14:00:42.847: INFO: Restart count of pod container-probe-6673/liveness-36be81e1-419a-4c36-88a5-2cfd5f68209a is now 1 (20.064265053s elapsed)
    STEP: deleting the pod 12/16/22 14:00:42.847
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:00:42.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6673" for this suite. 12/16/22 14:00:42.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:00:42.872
Dec 16 14:00:42.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 14:00:42.873
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:42.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:42.891
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9609 12/16/22 14:00:42.893
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 12/16/22 14:00:42.898
Dec 16 14:00:42.906: INFO: Found 0 stateful pods, waiting for 3
Dec 16 14:00:52.914: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 14:00:52.914: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 14:00:52.914: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 14:00:52.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:00:53.128: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:00:53.128: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:00:53.128: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/16/22 14:01:03.146
Dec 16 14:01:03.171: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/16/22 14:01:03.171
STEP: Updating Pods in reverse ordinal order 12/16/22 14:01:13.188
Dec 16 14:01:13.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 14:01:13.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 14:01:13.392: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 14:01:13.392: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 12/16/22 14:01:23.417
Dec 16 14:01:23.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:01:23.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:01:23.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:01:23.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 14:01:33.655: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 12/16/22 14:01:43.676
Dec 16 14:01:43.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 14:01:43.873: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 14:01:43.873: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 14:01:43.873: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 14:01:53.901: INFO: Deleting all statefulset in ns statefulset-9609
Dec 16 14:01:53.905: INFO: Scaling statefulset ss2 to 0
Dec 16 14:02:03.923: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:02:03.927: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:02:03.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9609" for this suite. 12/16/22 14:02:03.945
------------------------------
• [SLOW TEST] [81.082 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:00:42.872
    Dec 16 14:00:42.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 14:00:42.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:00:42.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:00:42.891
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9609 12/16/22 14:00:42.893
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 12/16/22 14:00:42.898
    Dec 16 14:00:42.906: INFO: Found 0 stateful pods, waiting for 3
    Dec 16 14:00:52.914: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 14:00:52.914: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 14:00:52.914: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 14:00:52.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:00:53.128: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:00:53.128: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:00:53.128: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/16/22 14:01:03.146
    Dec 16 14:01:03.171: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/16/22 14:01:03.171
    STEP: Updating Pods in reverse ordinal order 12/16/22 14:01:13.188
    Dec 16 14:01:13.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 14:01:13.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 14:01:13.392: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 14:01:13.392: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 12/16/22 14:01:23.417
    Dec 16 14:01:23.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:01:23.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:01:23.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:01:23.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 14:01:33.655: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 12/16/22 14:01:43.676
    Dec 16 14:01:43.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-9609 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 14:01:43.873: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 14:01:43.873: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 14:01:43.873: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 14:01:53.901: INFO: Deleting all statefulset in ns statefulset-9609
    Dec 16 14:01:53.905: INFO: Scaling statefulset ss2 to 0
    Dec 16 14:02:03.923: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:02:03.927: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:02:03.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9609" for this suite. 12/16/22 14:02:03.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:02:03.955
Dec 16 14:02:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 14:02:03.956
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:03.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:03.972
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 12/16/22 14:02:03.975
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 12/16/22 14:02:03.986
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 12/16/22 14:02:03.986
STEP: creating a pod to probe DNS 12/16/22 14:02:03.986
STEP: submitting the pod to kubernetes 12/16/22 14:02:03.986
Dec 16 14:02:03.996: INFO: Waiting up to 15m0s for pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b" in namespace "dns-9355" to be "running"
Dec 16 14:02:03.999: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498314ms
Dec 16 14:02:06.004: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00847647s
Dec 16 14:02:06.004: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b" satisfied condition "running"
STEP: retrieving the pod 12/16/22 14:02:06.004
STEP: looking for the results for each expected name from probers 12/16/22 14:02:06.008
Dec 16 14:02:06.063: INFO: DNS probes using dns-9355/dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b succeeded

STEP: deleting the pod 12/16/22 14:02:06.063
STEP: deleting the test headless service 12/16/22 14:02:06.075
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 14:02:06.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9355" for this suite. 12/16/22 14:02:06.089
------------------------------
• [2.140 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:02:03.955
    Dec 16 14:02:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 14:02:03.956
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:03.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:03.972
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 12/16/22 14:02:03.975
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     12/16/22 14:02:03.986
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9355.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     12/16/22 14:02:03.986
    STEP: creating a pod to probe DNS 12/16/22 14:02:03.986
    STEP: submitting the pod to kubernetes 12/16/22 14:02:03.986
    Dec 16 14:02:03.996: INFO: Waiting up to 15m0s for pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b" in namespace "dns-9355" to be "running"
    Dec 16 14:02:03.999: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498314ms
    Dec 16 14:02:06.004: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00847647s
    Dec 16 14:02:06.004: INFO: Pod "dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 14:02:06.004
    STEP: looking for the results for each expected name from probers 12/16/22 14:02:06.008
    Dec 16 14:02:06.063: INFO: DNS probes using dns-9355/dns-test-175d6dad-09a1-44b0-980f-3d7d3b44778b succeeded

    STEP: deleting the pod 12/16/22 14:02:06.063
    STEP: deleting the test headless service 12/16/22 14:02:06.075
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:02:06.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9355" for this suite. 12/16/22 14:02:06.089
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:02:06.096
Dec 16 14:02:06.096: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 14:02:06.096
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:06.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:06.114
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Dec 16 14:02:06.142: INFO: Create a RollingUpdate DaemonSet
Dec 16 14:02:06.147: INFO: Check that daemon pods launch on every node of the cluster
Dec 16 14:02:06.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:02:06.158: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:02:07.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 14:02:07.168: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:02:08.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 14:02:08.168: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Dec 16 14:02:08.168: INFO: Update the DaemonSet to trigger a rollout
Dec 16 14:02:08.178: INFO: Updating DaemonSet daemon-set
Dec 16 14:02:10.198: INFO: Roll back the DaemonSet before rollout is complete
Dec 16 14:02:10.207: INFO: Updating DaemonSet daemon-set
Dec 16 14:02:10.207: INFO: Make sure DaemonSet rollback is complete
Dec 16 14:02:10.212: INFO: Wrong image for pod: daemon-set-994jg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Dec 16 14:02:10.212: INFO: Pod daemon-set-994jg is not available
Dec 16 14:02:15.221: INFO: Pod daemon-set-mgppk is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:02:15.233
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1652, will wait for the garbage collector to delete the pods 12/16/22 14:02:15.233
Dec 16 14:02:15.296: INFO: Deleting DaemonSet.extensions daemon-set took: 8.241599ms
Dec 16 14:02:15.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.896542ms
Dec 16 14:02:19.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:02:19.101: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 14:02:19.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680961544"},"items":null}

Dec 16 14:02:19.109: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680961544"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:02:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1652" for this suite. 12/16/22 14:02:19.128
------------------------------
• [SLOW TEST] [13.038 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:02:06.096
    Dec 16 14:02:06.096: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 14:02:06.096
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:06.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:06.114
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Dec 16 14:02:06.142: INFO: Create a RollingUpdate DaemonSet
    Dec 16 14:02:06.147: INFO: Check that daemon pods launch on every node of the cluster
    Dec 16 14:02:06.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:02:06.158: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:02:07.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 14:02:07.168: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:02:08.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 14:02:08.168: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Dec 16 14:02:08.168: INFO: Update the DaemonSet to trigger a rollout
    Dec 16 14:02:08.178: INFO: Updating DaemonSet daemon-set
    Dec 16 14:02:10.198: INFO: Roll back the DaemonSet before rollout is complete
    Dec 16 14:02:10.207: INFO: Updating DaemonSet daemon-set
    Dec 16 14:02:10.207: INFO: Make sure DaemonSet rollback is complete
    Dec 16 14:02:10.212: INFO: Wrong image for pod: daemon-set-994jg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Dec 16 14:02:10.212: INFO: Pod daemon-set-994jg is not available
    Dec 16 14:02:15.221: INFO: Pod daemon-set-mgppk is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:02:15.233
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1652, will wait for the garbage collector to delete the pods 12/16/22 14:02:15.233
    Dec 16 14:02:15.296: INFO: Deleting DaemonSet.extensions daemon-set took: 8.241599ms
    Dec 16 14:02:15.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.896542ms
    Dec 16 14:02:19.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:02:19.101: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 14:02:19.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680961544"},"items":null}

    Dec 16 14:02:19.109: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680961544"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:02:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1652" for this suite. 12/16/22 14:02:19.128
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:02:19.134
Dec 16 14:02:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 14:02:19.135
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:19.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:19.149
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 in namespace container-probe-3882 12/16/22 14:02:19.152
Dec 16 14:02:19.159: INFO: Waiting up to 5m0s for pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4" in namespace "container-probe-3882" to be "not pending"
Dec 16 14:02:19.163: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16627ms
Dec 16 14:02:21.168: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008174693s
Dec 16 14:02:21.168: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4" satisfied condition "not pending"
Dec 16 14:02:21.168: INFO: Started pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 in namespace container-probe-3882
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:02:21.168
Dec 16 14:02:21.172: INFO: Initial restart count of pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 is 0
STEP: deleting the pod 12/16/22 14:06:21.893
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:21.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3882" for this suite. 12/16/22 14:06:21.91
------------------------------
• [SLOW TEST] [242.781 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:02:19.134
    Dec 16 14:02:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 14:02:19.135
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:02:19.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:02:19.149
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 in namespace container-probe-3882 12/16/22 14:02:19.152
    Dec 16 14:02:19.159: INFO: Waiting up to 5m0s for pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4" in namespace "container-probe-3882" to be "not pending"
    Dec 16 14:02:19.163: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16627ms
    Dec 16 14:02:21.168: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008174693s
    Dec 16 14:02:21.168: INFO: Pod "liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4" satisfied condition "not pending"
    Dec 16 14:02:21.168: INFO: Started pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 in namespace container-probe-3882
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:02:21.168
    Dec 16 14:02:21.172: INFO: Initial restart count of pod liveness-4046028f-4a77-4fe5-9ce6-ed43607238a4 is 0
    STEP: deleting the pod 12/16/22 14:06:21.893
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:21.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3882" for this suite. 12/16/22 14:06:21.91
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:21.915
Dec 16 14:06:21.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:06:21.916
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:21.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:21.939
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:06:21.941
Dec 16 14:06:21.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8" in namespace "projected-2144" to be "Succeeded or Failed"
Dec 16 14:06:21.952: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746381ms
Dec 16 14:06:23.957: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00769277s
Dec 16 14:06:25.958: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008275651s
STEP: Saw pod success 12/16/22 14:06:25.958
Dec 16 14:06:25.958: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8" satisfied condition "Succeeded or Failed"
Dec 16 14:06:25.961: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 container client-container: <nil>
STEP: delete the pod 12/16/22 14:06:26.011
Dec 16 14:06:26.022: INFO: Waiting for pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 to disappear
Dec 16 14:06:26.025: INFO: Pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:26.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2144" for this suite. 12/16/22 14:06:26.028
------------------------------
• [4.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:21.915
    Dec 16 14:06:21.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:06:21.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:21.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:21.939
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:06:21.941
    Dec 16 14:06:21.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8" in namespace "projected-2144" to be "Succeeded or Failed"
    Dec 16 14:06:21.952: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746381ms
    Dec 16 14:06:23.957: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00769277s
    Dec 16 14:06:25.958: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008275651s
    STEP: Saw pod success 12/16/22 14:06:25.958
    Dec 16 14:06:25.958: INFO: Pod "downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8" satisfied condition "Succeeded or Failed"
    Dec 16 14:06:25.961: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 container client-container: <nil>
    STEP: delete the pod 12/16/22 14:06:26.011
    Dec 16 14:06:26.022: INFO: Waiting for pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 to disappear
    Dec 16 14:06:26.025: INFO: Pod downwardapi-volume-0600c4f2-ae76-4d2a-9e75-70af23c7f3c8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:26.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2144" for this suite. 12/16/22 14:06:26.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:26.034
Dec 16 14:06:26.034: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 14:06:26.035
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:26.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:26.05
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 12/16/22 14:06:26.053
STEP: setting up watch 12/16/22 14:06:26.053
STEP: submitting the pod to kubernetes 12/16/22 14:06:26.157
STEP: verifying the pod is in kubernetes 12/16/22 14:06:26.166
STEP: verifying pod creation was observed 12/16/22 14:06:26.17
Dec 16 14:06:26.170: INFO: Waiting up to 5m0s for pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea" in namespace "pods-6126" to be "running"
Dec 16 14:06:26.174: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373967ms
Dec 16 14:06:28.179: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.008227791s
Dec 16 14:06:28.179: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea" satisfied condition "running"
STEP: deleting the pod gracefully 12/16/22 14:06:28.182
STEP: verifying pod deletion was observed 12/16/22 14:06:28.19
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6126" for this suite. 12/16/22 14:06:30.419
------------------------------
• [4.391 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:26.034
    Dec 16 14:06:26.034: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 14:06:26.035
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:26.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:26.05
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 12/16/22 14:06:26.053
    STEP: setting up watch 12/16/22 14:06:26.053
    STEP: submitting the pod to kubernetes 12/16/22 14:06:26.157
    STEP: verifying the pod is in kubernetes 12/16/22 14:06:26.166
    STEP: verifying pod creation was observed 12/16/22 14:06:26.17
    Dec 16 14:06:26.170: INFO: Waiting up to 5m0s for pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea" in namespace "pods-6126" to be "running"
    Dec 16 14:06:26.174: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373967ms
    Dec 16 14:06:28.179: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.008227791s
    Dec 16 14:06:28.179: INFO: Pod "pod-submit-remove-510dc477-474a-4a11-88d6-8cd9501128ea" satisfied condition "running"
    STEP: deleting the pod gracefully 12/16/22 14:06:28.182
    STEP: verifying pod deletion was observed 12/16/22 14:06:28.19
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6126" for this suite. 12/16/22 14:06:30.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:30.425
Dec 16 14:06:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:06:30.426
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:30.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:30.441
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-9c3bea11-1c90-4656-a749-617fdcee14b4 12/16/22 14:06:30.443
STEP: Creating a pod to test consume secrets 12/16/22 14:06:30.448
Dec 16 14:06:30.457: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2" in namespace "projected-3169" to be "Succeeded or Failed"
Dec 16 14:06:30.460: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078111ms
Dec 16 14:06:32.466: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00848872s
Dec 16 14:06:34.467: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009544545s
STEP: Saw pod success 12/16/22 14:06:34.467
Dec 16 14:06:34.467: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2" satisfied condition "Succeeded or Failed"
Dec 16 14:06:34.472: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:06:34.483
Dec 16 14:06:34.495: INFO: Waiting for pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 to disappear
Dec 16 14:06:34.499: INFO: Pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:34.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3169" for this suite. 12/16/22 14:06:34.503
------------------------------
• [4.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:30.425
    Dec 16 14:06:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:06:30.426
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:30.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:30.441
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-9c3bea11-1c90-4656-a749-617fdcee14b4 12/16/22 14:06:30.443
    STEP: Creating a pod to test consume secrets 12/16/22 14:06:30.448
    Dec 16 14:06:30.457: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2" in namespace "projected-3169" to be "Succeeded or Failed"
    Dec 16 14:06:30.460: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078111ms
    Dec 16 14:06:32.466: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00848872s
    Dec 16 14:06:34.467: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009544545s
    STEP: Saw pod success 12/16/22 14:06:34.467
    Dec 16 14:06:34.467: INFO: Pod "pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2" satisfied condition "Succeeded or Failed"
    Dec 16 14:06:34.472: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:06:34.483
    Dec 16 14:06:34.495: INFO: Waiting for pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 to disappear
    Dec 16 14:06:34.499: INFO: Pod pod-projected-secrets-bc50d2f1-8d49-40fa-8baa-cdcad6426ee2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:34.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3169" for this suite. 12/16/22 14:06:34.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:34.509
Dec 16 14:06:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 14:06:34.51
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:34.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:34.526
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 12/16/22 14:06:34.529
Dec 16 14:06:34.537: INFO: Waiting up to 5m0s for pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf" in namespace "var-expansion-488" to be "Succeeded or Failed"
Dec 16 14:06:34.541: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.507598ms
Dec 16 14:06:36.547: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009539599s
Dec 16 14:06:38.548: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01007544s
STEP: Saw pod success 12/16/22 14:06:38.548
Dec 16 14:06:38.548: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf" satisfied condition "Succeeded or Failed"
Dec 16 14:06:38.551: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf container dapi-container: <nil>
STEP: delete the pod 12/16/22 14:06:38.56
Dec 16 14:06:38.570: INFO: Waiting for pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf to disappear
Dec 16 14:06:38.572: INFO: Pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-488" for this suite. 12/16/22 14:06:38.576
------------------------------
• [4.073 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:34.509
    Dec 16 14:06:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 14:06:34.51
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:34.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:34.526
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 12/16/22 14:06:34.529
    Dec 16 14:06:34.537: INFO: Waiting up to 5m0s for pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf" in namespace "var-expansion-488" to be "Succeeded or Failed"
    Dec 16 14:06:34.541: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.507598ms
    Dec 16 14:06:36.547: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009539599s
    Dec 16 14:06:38.548: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01007544s
    STEP: Saw pod success 12/16/22 14:06:38.548
    Dec 16 14:06:38.548: INFO: Pod "var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf" satisfied condition "Succeeded or Failed"
    Dec 16 14:06:38.551: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf container dapi-container: <nil>
    STEP: delete the pod 12/16/22 14:06:38.56
    Dec 16 14:06:38.570: INFO: Waiting for pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf to disappear
    Dec 16 14:06:38.572: INFO: Pod var-expansion-7f9085c1-1b67-45bc-93c5-86cde3b83eaf no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-488" for this suite. 12/16/22 14:06:38.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:38.583
Dec 16 14:06:38.583: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 14:06:38.584
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:38.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:38.597
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-48ce9595-635e-45b5-a314-bf3cba927abc 12/16/22 14:06:38.6
STEP: Creating a pod to test consume secrets 12/16/22 14:06:38.604
Dec 16 14:06:38.610: INFO: Waiting up to 5m0s for pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2" in namespace "secrets-9152" to be "Succeeded or Failed"
Dec 16 14:06:38.614: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267237ms
Dec 16 14:06:40.619: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332756s
Dec 16 14:06:42.620: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009617728s
STEP: Saw pod success 12/16/22 14:06:42.62
Dec 16 14:06:42.620: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2" satisfied condition "Succeeded or Failed"
Dec 16 14:06:42.624: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 container secret-env-test: <nil>
STEP: delete the pod 12/16/22 14:06:42.632
Dec 16 14:06:42.643: INFO: Waiting for pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 to disappear
Dec 16 14:06:42.646: INFO: Pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:42.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9152" for this suite. 12/16/22 14:06:42.651
------------------------------
• [4.073 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:38.583
    Dec 16 14:06:38.583: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 14:06:38.584
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:38.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:38.597
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-48ce9595-635e-45b5-a314-bf3cba927abc 12/16/22 14:06:38.6
    STEP: Creating a pod to test consume secrets 12/16/22 14:06:38.604
    Dec 16 14:06:38.610: INFO: Waiting up to 5m0s for pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2" in namespace "secrets-9152" to be "Succeeded or Failed"
    Dec 16 14:06:38.614: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267237ms
    Dec 16 14:06:40.619: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332756s
    Dec 16 14:06:42.620: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009617728s
    STEP: Saw pod success 12/16/22 14:06:42.62
    Dec 16 14:06:42.620: INFO: Pod "pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2" satisfied condition "Succeeded or Failed"
    Dec 16 14:06:42.624: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 container secret-env-test: <nil>
    STEP: delete the pod 12/16/22 14:06:42.632
    Dec 16 14:06:42.643: INFO: Waiting for pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 to disappear
    Dec 16 14:06:42.646: INFO: Pod pod-secrets-07239307-41d5-4d5d-90c9-e9bc3b150af2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:42.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9152" for this suite. 12/16/22 14:06:42.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:42.656
Dec 16 14:06:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-webhook 12/16/22 14:06:42.657
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:42.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:42.673
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/16/22 14:06:42.677
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/16/22 14:06:42.998
STEP: Deploying the custom resource conversion webhook pod 12/16/22 14:06:43.008
STEP: Wait for the deployment to be ready 12/16/22 14:06:43.019
Dec 16 14:06:43.026: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:06:45.038
STEP: Verifying the service has paired with the endpoint 12/16/22 14:06:45.054
Dec 16 14:06:46.055: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Dec 16 14:06:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Creating a v1 custom resource 12/16/22 14:06:48.711
STEP: Create a v2 custom resource 12/16/22 14:06:48.728
STEP: List CRs in v1 12/16/22 14:06:48.799
STEP: List CRs in v2 12/16/22 14:06:48.805
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:49.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1095" for this suite. 12/16/22 14:06:49.369
------------------------------
• [SLOW TEST] [6.718 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:42.656
    Dec 16 14:06:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-webhook 12/16/22 14:06:42.657
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:42.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:42.673
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/16/22 14:06:42.677
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/16/22 14:06:42.998
    STEP: Deploying the custom resource conversion webhook pod 12/16/22 14:06:43.008
    STEP: Wait for the deployment to be ready 12/16/22 14:06:43.019
    Dec 16 14:06:43.026: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:06:45.038
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:06:45.054
    Dec 16 14:06:46.055: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Dec 16 14:06:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Creating a v1 custom resource 12/16/22 14:06:48.711
    STEP: Create a v2 custom resource 12/16/22 14:06:48.728
    STEP: List CRs in v1 12/16/22 14:06:48.799
    STEP: List CRs in v2 12/16/22 14:06:48.805
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:49.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1095" for this suite. 12/16/22 14:06:49.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:49.377
Dec 16 14:06:49.377: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 14:06:49.378
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:49.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:49.391
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:06:49.394
Dec 16 14:06:49.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d" in namespace "downward-api-3331" to be "Succeeded or Failed"
Dec 16 14:06:49.406: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820068ms
Dec 16 14:06:51.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009222852s
Dec 16 14:06:53.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009449248s
STEP: Saw pod success 12/16/22 14:06:53.412
Dec 16 14:06:53.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d" satisfied condition "Succeeded or Failed"
Dec 16 14:06:53.417: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d container client-container: <nil>
STEP: delete the pod 12/16/22 14:06:53.424
Dec 16 14:06:53.434: INFO: Waiting for pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d to disappear
Dec 16 14:06:53.436: INFO: Pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 14:06:53.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3331" for this suite. 12/16/22 14:06:53.44
------------------------------
• [4.069 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:49.377
    Dec 16 14:06:49.377: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 14:06:49.378
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:49.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:49.391
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:06:49.394
    Dec 16 14:06:49.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d" in namespace "downward-api-3331" to be "Succeeded or Failed"
    Dec 16 14:06:49.406: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820068ms
    Dec 16 14:06:51.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009222852s
    Dec 16 14:06:53.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009449248s
    STEP: Saw pod success 12/16/22 14:06:53.412
    Dec 16 14:06:53.412: INFO: Pod "downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d" satisfied condition "Succeeded or Failed"
    Dec 16 14:06:53.417: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d container client-container: <nil>
    STEP: delete the pod 12/16/22 14:06:53.424
    Dec 16 14:06:53.434: INFO: Waiting for pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d to disappear
    Dec 16 14:06:53.436: INFO: Pod downwardapi-volume-5ac9c3e7-886c-49c6-be5c-ac25e175786d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:06:53.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3331" for this suite. 12/16/22 14:06:53.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:06:53.447
Dec 16 14:06:53.447: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename subpath 12/16/22 14:06:53.447
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:53.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:53.461
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/16/22 14:06:53.464
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-8mmb 12/16/22 14:06:53.472
STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:06:53.472
Dec 16 14:06:53.480: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8mmb" in namespace "subpath-3742" to be "Succeeded or Failed"
Dec 16 14:06:53.484: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192036ms
Dec 16 14:06:55.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009170944s
Dec 16 14:06:57.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.010192628s
Dec 16 14:06:59.488: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007656145s
Dec 16 14:07:01.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.009731383s
Dec 16 14:07:03.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.010396561s
Dec 16 14:07:05.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.01004139s
Dec 16 14:07:07.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009633308s
Dec 16 14:07:09.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009561293s
Dec 16 14:07:11.489: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008711866s
Dec 16 14:07:13.489: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008882163s
Dec 16 14:07:15.492: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011245817s
Dec 16 14:07:17.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010347725s
STEP: Saw pod success 12/16/22 14:07:17.491
Dec 16 14:07:17.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb" satisfied condition "Succeeded or Failed"
Dec 16 14:07:17.495: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-downwardapi-8mmb container test-container-subpath-downwardapi-8mmb: <nil>
STEP: delete the pod 12/16/22 14:07:17.503
Dec 16 14:07:17.514: INFO: Waiting for pod pod-subpath-test-downwardapi-8mmb to disappear
Dec 16 14:07:17.518: INFO: Pod pod-subpath-test-downwardapi-8mmb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8mmb 12/16/22 14:07:17.518
Dec 16 14:07:17.518: INFO: Deleting pod "pod-subpath-test-downwardapi-8mmb" in namespace "subpath-3742"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 16 14:07:17.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3742" for this suite. 12/16/22 14:07:17.526
------------------------------
• [SLOW TEST] [24.085 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:06:53.447
    Dec 16 14:06:53.447: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename subpath 12/16/22 14:06:53.447
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:06:53.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:06:53.461
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/16/22 14:06:53.464
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-8mmb 12/16/22 14:06:53.472
    STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:06:53.472
    Dec 16 14:06:53.480: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8mmb" in namespace "subpath-3742" to be "Succeeded or Failed"
    Dec 16 14:06:53.484: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192036ms
    Dec 16 14:06:55.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009170944s
    Dec 16 14:06:57.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.010192628s
    Dec 16 14:06:59.488: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007656145s
    Dec 16 14:07:01.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.009731383s
    Dec 16 14:07:03.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.010396561s
    Dec 16 14:07:05.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.01004139s
    Dec 16 14:07:07.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009633308s
    Dec 16 14:07:09.490: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009561293s
    Dec 16 14:07:11.489: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008711866s
    Dec 16 14:07:13.489: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008882163s
    Dec 16 14:07:15.492: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011245817s
    Dec 16 14:07:17.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010347725s
    STEP: Saw pod success 12/16/22 14:07:17.491
    Dec 16 14:07:17.491: INFO: Pod "pod-subpath-test-downwardapi-8mmb" satisfied condition "Succeeded or Failed"
    Dec 16 14:07:17.495: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-downwardapi-8mmb container test-container-subpath-downwardapi-8mmb: <nil>
    STEP: delete the pod 12/16/22 14:07:17.503
    Dec 16 14:07:17.514: INFO: Waiting for pod pod-subpath-test-downwardapi-8mmb to disappear
    Dec 16 14:07:17.518: INFO: Pod pod-subpath-test-downwardapi-8mmb no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-8mmb 12/16/22 14:07:17.518
    Dec 16 14:07:17.518: INFO: Deleting pod "pod-subpath-test-downwardapi-8mmb" in namespace "subpath-3742"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:07:17.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3742" for this suite. 12/16/22 14:07:17.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:07:17.533
Dec 16 14:07:17.533: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 14:07:17.533
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:17.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:17.55
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Dec 16 14:07:17.553: INFO: Creating simple deployment test-new-deployment
Dec 16 14:07:17.565: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 12/16/22 14:07:19.583
STEP: updating a scale subresource 12/16/22 14:07:19.586
STEP: verifying the deployment Spec.Replicas was modified 12/16/22 14:07:19.593
STEP: Patch a scale subresource 12/16/22 14:07:19.596
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 14:07:19.612: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9725  25fe0e4e-a44d-4459-aa46-6b89a17d3f79 680969337 3 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-16 14:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001137768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 14:07:18 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-16 14:07:18 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 16 14:07:19.616: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9725  5a0efad1-ff4e-41dc-9744-488a3884c2be 680969341 2 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 25fe0e4e-a44d-4459-aa46-6b89a17d3f79 0xc001137b97 0xc001137b98}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25fe0e4e-a44d-4459-aa46-6b89a17d3f79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001137c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:07:19.620: INFO: Pod "test-new-deployment-7f5969cbc7-rs7bk" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rs7bk test-new-deployment-7f5969cbc7- deployment-9725  8f40e131-b56c-4b47-a4a3-4b582848d9f5 680969342 0 2022-12-16 14:07:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5a0efad1-ff4e-41dc-9744-488a3884c2be 0xc003384b37 0xc003384b38}] [] [{kube-controller-manager Update v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a0efad1-ff4e-41dc-9744-488a3884c2be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf4ff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf4ff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 14:07:19.620: INFO: Pod "test-new-deployment-7f5969cbc7-tqmdx" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tqmdx test-new-deployment-7f5969cbc7- deployment-9725  16e5b6b4-58b7-450b-be5d-c8b28942a1b9 680969304 0 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4886696888ef8e3de2815d4183a38fb1bf12f0e224f1025cdd8a0782dceeee14 cni.projectcalico.org/podIP:192.168.189.17/32 cni.projectcalico.org/podIPs:192.168.189.17/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5a0efad1-ff4e-41dc-9744-488a3884c2be 0xc003384ca7 0xc003384ca8}] [] [{kube-controller-manager Update v1 2022-12-16 14:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a0efad1-ff4e-41dc-9744-488a3884c2be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m76hm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m76hm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.17,StartTime:2022-12-16 14:07:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4b64e3bb398023dbead7276da4e4eb0e8cd1ba0afa9313cbc3dd3ae85a27d954,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 14:07:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9725" for this suite. 12/16/22 14:07:19.626
------------------------------
• [2.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:07:17.533
    Dec 16 14:07:17.533: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 14:07:17.533
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:17.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:17.55
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Dec 16 14:07:17.553: INFO: Creating simple deployment test-new-deployment
    Dec 16 14:07:17.565: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 12/16/22 14:07:19.583
    STEP: updating a scale subresource 12/16/22 14:07:19.586
    STEP: verifying the deployment Spec.Replicas was modified 12/16/22 14:07:19.593
    STEP: Patch a scale subresource 12/16/22 14:07:19.596
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 14:07:19.612: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9725  25fe0e4e-a44d-4459-aa46-6b89a17d3f79 680969337 3 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-16 14:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001137768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 14:07:18 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-16 14:07:18 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 16 14:07:19.616: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9725  5a0efad1-ff4e-41dc-9744-488a3884c2be 680969341 2 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 25fe0e4e-a44d-4459-aa46-6b89a17d3f79 0xc001137b97 0xc001137b98}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25fe0e4e-a44d-4459-aa46-6b89a17d3f79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001137c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:07:19.620: INFO: Pod "test-new-deployment-7f5969cbc7-rs7bk" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rs7bk test-new-deployment-7f5969cbc7- deployment-9725  8f40e131-b56c-4b47-a4a3-4b582848d9f5 680969342 0 2022-12-16 14:07:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5a0efad1-ff4e-41dc-9744-488a3884c2be 0xc003384b37 0xc003384b38}] [] [{kube-controller-manager Update v1 2022-12-16 14:07:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a0efad1-ff4e-41dc-9744-488a3884c2be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf4ff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf4ff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-ehprg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 14:07:19.620: INFO: Pod "test-new-deployment-7f5969cbc7-tqmdx" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tqmdx test-new-deployment-7f5969cbc7- deployment-9725  16e5b6b4-58b7-450b-be5d-c8b28942a1b9 680969304 0 2022-12-16 14:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4886696888ef8e3de2815d4183a38fb1bf12f0e224f1025cdd8a0782dceeee14 cni.projectcalico.org/podIP:192.168.189.17/32 cni.projectcalico.org/podIPs:192.168.189.17/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5a0efad1-ff4e-41dc-9744-488a3884c2be 0xc003384ca7 0xc003384ca8}] [] [{kube-controller-manager Update v1 2022-12-16 14:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a0efad1-ff4e-41dc-9744-488a3884c2be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m76hm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m76hm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.17,StartTime:2022-12-16 14:07:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4b64e3bb398023dbead7276da4e4eb0e8cd1ba0afa9313cbc3dd3ae85a27d954,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:07:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9725" for this suite. 12/16/22 14:07:19.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:07:19.636
Dec 16 14:07:19.636: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 14:07:19.637
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:19.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:19.652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 12/16/22 14:07:19.66
Dec 16 14:07:19.669: INFO: Waiting up to 5m0s for pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2" in namespace "var-expansion-4525" to be "Succeeded or Failed"
Dec 16 14:07:19.672: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991502ms
Dec 16 14:07:21.678: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008869291s
Dec 16 14:07:23.677: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644832s
STEP: Saw pod success 12/16/22 14:07:23.677
Dec 16 14:07:23.678: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2" satisfied condition "Succeeded or Failed"
Dec 16 14:07:23.681: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 container dapi-container: <nil>
STEP: delete the pod 12/16/22 14:07:23.689
Dec 16 14:07:23.698: INFO: Waiting for pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 to disappear
Dec 16 14:07:23.701: INFO: Pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 14:07:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4525" for this suite. 12/16/22 14:07:23.705
------------------------------
• [4.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:07:19.636
    Dec 16 14:07:19.636: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 14:07:19.637
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:19.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:19.652
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 12/16/22 14:07:19.66
    Dec 16 14:07:19.669: INFO: Waiting up to 5m0s for pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2" in namespace "var-expansion-4525" to be "Succeeded or Failed"
    Dec 16 14:07:19.672: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991502ms
    Dec 16 14:07:21.678: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008869291s
    Dec 16 14:07:23.677: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644832s
    STEP: Saw pod success 12/16/22 14:07:23.677
    Dec 16 14:07:23.678: INFO: Pod "var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2" satisfied condition "Succeeded or Failed"
    Dec 16 14:07:23.681: INFO: Trying to get logs from node pool-a3802-fsxxd pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 14:07:23.689
    Dec 16 14:07:23.698: INFO: Waiting for pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 to disappear
    Dec 16 14:07:23.701: INFO: Pod var-expansion-763ae30c-6d33-4f32-a23b-f4e3eeaef7a2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:07:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4525" for this suite. 12/16/22 14:07:23.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:07:23.711
Dec 16 14:07:23.712: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 14:07:23.712
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:23.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:23.728
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 12/16/22 14:07:23.73
Dec 16 14:07:23.738: INFO: Waiting up to 2m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678" to be "running"
Dec 16 14:07:23.741: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183173ms
Dec 16 14:07:25.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008659714s
Dec 16 14:07:27.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009923811s
Dec 16 14:07:29.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009138376s
Dec 16 14:07:31.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00918578s
Dec 16 14:07:33.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009010794s
Dec 16 14:07:35.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008500341s
Dec 16 14:07:37.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008722216s
Dec 16 14:07:39.745: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007468788s
Dec 16 14:07:41.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007805405s
Dec 16 14:07:43.879: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 20.140928903s
Dec 16 14:07:45.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00836723s
Dec 16 14:07:47.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00939734s
Dec 16 14:07:49.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009796681s
Dec 16 14:07:51.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008461545s
Dec 16 14:07:53.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011166953s
Dec 16 14:07:55.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010155453s
Dec 16 14:07:57.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00980218s
Dec 16 14:07:59.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008480576s
Dec 16 14:08:01.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009295224s
Dec 16 14:08:03.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010531994s
Dec 16 14:08:05.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008684627s
Dec 16 14:08:07.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008548812s
Dec 16 14:08:09.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009313518s
Dec 16 14:08:11.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008866465s
Dec 16 14:08:13.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009913685s
Dec 16 14:08:15.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008957764s
Dec 16 14:08:17.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010015219s
Dec 16 14:08:19.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011466611s
Dec 16 14:08:21.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008595479s
Dec 16 14:08:23.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008660872s
Dec 16 14:08:25.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009366949s
Dec 16 14:08:27.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011106927s
Dec 16 14:08:29.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009279229s
Dec 16 14:08:31.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008538151s
Dec 16 14:08:33.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009041543s
Dec 16 14:08:35.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007984894s
Dec 16 14:08:37.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009750803s
Dec 16 14:08:39.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008971818s
Dec 16 14:08:41.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009016879s
Dec 16 14:08:43.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007922685s
Dec 16 14:08:45.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007873445s
Dec 16 14:08:47.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010154983s
Dec 16 14:08:49.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01005981s
Dec 16 14:08:51.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009090551s
Dec 16 14:08:53.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010078687s
Dec 16 14:08:55.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008479852s
Dec 16 14:08:57.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010181654s
Dec 16 14:08:59.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009591584s
Dec 16 14:09:01.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009379901s
Dec 16 14:09:03.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009542349s
Dec 16 14:09:05.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01015103s
Dec 16 14:09:07.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011461748s
Dec 16 14:09:09.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010263259s
Dec 16 14:09:11.750: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011921511s
Dec 16 14:09:13.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008737701s
Dec 16 14:09:15.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008026362s
Dec 16 14:09:17.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008233074s
Dec 16 14:09:19.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008009702s
Dec 16 14:09:21.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007638881s
Dec 16 14:09:23.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010645807s
Dec 16 14:09:23.753: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014741376s
STEP: updating the pod 12/16/22 14:09:23.753
Dec 16 14:09:24.266: INFO: Successfully updated pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837"
STEP: waiting for pod running 12/16/22 14:09:24.266
Dec 16 14:09:24.266: INFO: Waiting up to 2m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678" to be "running"
Dec 16 14:09:24.270: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967888ms
Dec 16 14:09:26.275: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Running", Reason="", readiness=true. Elapsed: 2.008532407s
Dec 16 14:09:26.275: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" satisfied condition "running"
STEP: deleting the pod gracefully 12/16/22 14:09:26.275
Dec 16 14:09:26.275: INFO: Deleting pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678"
Dec 16 14:09:26.282: INFO: Wait up to 5m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 14:09:58.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7678" for this suite. 12/16/22 14:09:58.297
------------------------------
• [SLOW TEST] [154.591 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:07:23.711
    Dec 16 14:07:23.712: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 14:07:23.712
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:07:23.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:07:23.728
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 12/16/22 14:07:23.73
    Dec 16 14:07:23.738: INFO: Waiting up to 2m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678" to be "running"
    Dec 16 14:07:23.741: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183173ms
    Dec 16 14:07:25.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008659714s
    Dec 16 14:07:27.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009923811s
    Dec 16 14:07:29.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009138376s
    Dec 16 14:07:31.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00918578s
    Dec 16 14:07:33.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009010794s
    Dec 16 14:07:35.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008500341s
    Dec 16 14:07:37.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008722216s
    Dec 16 14:07:39.745: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007468788s
    Dec 16 14:07:41.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007805405s
    Dec 16 14:07:43.879: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 20.140928903s
    Dec 16 14:07:45.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00836723s
    Dec 16 14:07:47.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00939734s
    Dec 16 14:07:49.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009796681s
    Dec 16 14:07:51.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008461545s
    Dec 16 14:07:53.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011166953s
    Dec 16 14:07:55.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010155453s
    Dec 16 14:07:57.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00980218s
    Dec 16 14:07:59.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008480576s
    Dec 16 14:08:01.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009295224s
    Dec 16 14:08:03.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010531994s
    Dec 16 14:08:05.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008684627s
    Dec 16 14:08:07.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008548812s
    Dec 16 14:08:09.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009313518s
    Dec 16 14:08:11.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008866465s
    Dec 16 14:08:13.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009913685s
    Dec 16 14:08:15.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008957764s
    Dec 16 14:08:17.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010015219s
    Dec 16 14:08:19.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011466611s
    Dec 16 14:08:21.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008595479s
    Dec 16 14:08:23.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008660872s
    Dec 16 14:08:25.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009366949s
    Dec 16 14:08:27.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011106927s
    Dec 16 14:08:29.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009279229s
    Dec 16 14:08:31.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008538151s
    Dec 16 14:08:33.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009041543s
    Dec 16 14:08:35.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007984894s
    Dec 16 14:08:37.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009750803s
    Dec 16 14:08:39.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008971818s
    Dec 16 14:08:41.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009016879s
    Dec 16 14:08:43.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007922685s
    Dec 16 14:08:45.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007873445s
    Dec 16 14:08:47.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010154983s
    Dec 16 14:08:49.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01005981s
    Dec 16 14:08:51.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009090551s
    Dec 16 14:08:53.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010078687s
    Dec 16 14:08:55.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008479852s
    Dec 16 14:08:57.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010181654s
    Dec 16 14:08:59.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009591584s
    Dec 16 14:09:01.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009379901s
    Dec 16 14:09:03.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009542349s
    Dec 16 14:09:05.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01015103s
    Dec 16 14:09:07.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011461748s
    Dec 16 14:09:09.748: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010263259s
    Dec 16 14:09:11.750: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011921511s
    Dec 16 14:09:13.747: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008737701s
    Dec 16 14:09:15.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008026362s
    Dec 16 14:09:17.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008233074s
    Dec 16 14:09:19.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008009702s
    Dec 16 14:09:21.746: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007638881s
    Dec 16 14:09:23.749: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010645807s
    Dec 16 14:09:23.753: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014741376s
    STEP: updating the pod 12/16/22 14:09:23.753
    Dec 16 14:09:24.266: INFO: Successfully updated pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837"
    STEP: waiting for pod running 12/16/22 14:09:24.266
    Dec 16 14:09:24.266: INFO: Waiting up to 2m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678" to be "running"
    Dec 16 14:09:24.270: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967888ms
    Dec 16 14:09:26.275: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837": Phase="Running", Reason="", readiness=true. Elapsed: 2.008532407s
    Dec 16 14:09:26.275: INFO: Pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" satisfied condition "running"
    STEP: deleting the pod gracefully 12/16/22 14:09:26.275
    Dec 16 14:09:26.275: INFO: Deleting pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" in namespace "var-expansion-7678"
    Dec 16 14:09:26.282: INFO: Wait up to 5m0s for pod "var-expansion-b31494a0-39c4-4ba2-8d16-9c5eee4ab837" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:09:58.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7678" for this suite. 12/16/22 14:09:58.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:09:58.303
Dec 16 14:09:58.303: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 14:09:58.304
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:09:58.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:09:58.319
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 12/16/22 14:09:58.321
STEP: Creating a ResourceQuota 12/16/22 14:10:03.326
STEP: Ensuring resource quota status is calculated 12/16/22 14:10:03.331
STEP: Creating a Pod that fits quota 12/16/22 14:10:05.337
STEP: Ensuring ResourceQuota status captures the pod usage 12/16/22 14:10:05.351
STEP: Not allowing a pod to be created that exceeds remaining quota 12/16/22 14:10:07.357
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/16/22 14:10:07.36
STEP: Ensuring a pod cannot update its resource requirements 12/16/22 14:10:07.362
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/16/22 14:10:07.367
STEP: Deleting the pod 12/16/22 14:10:09.373
STEP: Ensuring resource quota status released the pod usage 12/16/22 14:10:09.386
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3171" for this suite. 12/16/22 14:10:11.398
------------------------------
• [SLOW TEST] [13.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:09:58.303
    Dec 16 14:09:58.303: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 14:09:58.304
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:09:58.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:09:58.319
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 12/16/22 14:09:58.321
    STEP: Creating a ResourceQuota 12/16/22 14:10:03.326
    STEP: Ensuring resource quota status is calculated 12/16/22 14:10:03.331
    STEP: Creating a Pod that fits quota 12/16/22 14:10:05.337
    STEP: Ensuring ResourceQuota status captures the pod usage 12/16/22 14:10:05.351
    STEP: Not allowing a pod to be created that exceeds remaining quota 12/16/22 14:10:07.357
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/16/22 14:10:07.36
    STEP: Ensuring a pod cannot update its resource requirements 12/16/22 14:10:07.362
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/16/22 14:10:07.367
    STEP: Deleting the pod 12/16/22 14:10:09.373
    STEP: Ensuring resource quota status released the pod usage 12/16/22 14:10:09.386
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3171" for this suite. 12/16/22 14:10:11.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:11.405
Dec 16 14:10:11.405: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 14:10:11.406
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:11.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:11.423
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 12/16/22 14:10:11.429
Dec 16 14:10:11.429: INFO: Creating simple deployment test-deployment-7qhw9
Dec 16 14:10:11.443: INFO: deployment "test-deployment-7qhw9" doesn't have the required revision set
STEP: Getting /status 12/16/22 14:10:13.459
Dec 16 14:10:13.463: INFO: Deployment test-deployment-7qhw9 has Conditions: [{Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 12/16/22 14:10:13.464
Dec 16 14:10:13.473: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 10, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7qhw9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 12/16/22 14:10:13.473
Dec 16 14:10:13.475: INFO: Observed &Deployment event: ADDED
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qhw9-54bc444df" is progressing.}
Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
Dec 16 14:10:13.475: INFO: Found Deployment test-deployment-7qhw9 in namespace deployment-2557 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 16 14:10:13.475: INFO: Deployment test-deployment-7qhw9 has an updated status
STEP: patching the Statefulset Status 12/16/22 14:10:13.475
Dec 16 14:10:13.476: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 16 14:10:13.483: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 12/16/22 14:10:13.483
Dec 16 14:10:13.485: INFO: Observed &Deployment event: ADDED
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qhw9-54bc444df" is progressing.}
Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
Dec 16 14:10:13.486: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 16 14:10:13.486: INFO: Observed &Deployment event: MODIFIED
Dec 16 14:10:13.486: INFO: Found deployment test-deployment-7qhw9 in namespace deployment-2557 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec 16 14:10:13.486: INFO: Deployment test-deployment-7qhw9 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 14:10:13.489: INFO: Deployment "test-deployment-7qhw9":
&Deployment{ObjectMeta:{test-deployment-7qhw9  deployment-2557  7af94ec3-7de3-4fea-9b1c-8764fcfe18ab 680973954 1 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-16 14:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038d4ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 16 14:10:13.493: INFO: New ReplicaSet "test-deployment-7qhw9-54bc444df" of Deployment "test-deployment-7qhw9":
&ReplicaSet{ObjectMeta:{test-deployment-7qhw9-54bc444df  deployment-2557  c7646945-c38a-4cd0-a851-1cecf5e17884 680973929 1 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7qhw9 7af94ec3-7de3-4fea-9b1c-8764fcfe18ab 0xc0038d52a7 0xc0038d52a8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af94ec3-7de3-4fea-9b1c-8764fcfe18ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038d5358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:10:13.497: INFO: Pod "test-deployment-7qhw9-54bc444df-cs6tl" is available:
&Pod{ObjectMeta:{test-deployment-7qhw9-54bc444df-cs6tl test-deployment-7qhw9-54bc444df- deployment-2557  f7c693e1-a5db-419c-b665-a883bb68e596 680973928 0 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:734e3be611a59c6813bd5d1cc245330f6601bafd72927695b0a32b8461e4b100 cni.projectcalico.org/podIP:192.168.189.22/32 cni.projectcalico.org/podIPs:192.168.189.22/32] [{apps/v1 ReplicaSet test-deployment-7qhw9-54bc444df c7646945-c38a-4cd0-a851-1cecf5e17884 0xc0038d5727 0xc0038d5728}] [] [{calico Update v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7646945-c38a-4cd0-a851-1cecf5e17884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4zv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4zv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.22,StartTime:2022-12-16 14:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:10:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://27ff4b0a8c86cd601e47b3469eaaa06fd59420b516f545b6609886eb98562b68,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:13.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2557" for this suite. 12/16/22 14:10:13.501
------------------------------
• [2.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:11.405
    Dec 16 14:10:11.405: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 14:10:11.406
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:11.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:11.423
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 12/16/22 14:10:11.429
    Dec 16 14:10:11.429: INFO: Creating simple deployment test-deployment-7qhw9
    Dec 16 14:10:11.443: INFO: deployment "test-deployment-7qhw9" doesn't have the required revision set
    STEP: Getting /status 12/16/22 14:10:13.459
    Dec 16 14:10:13.463: INFO: Deployment test-deployment-7qhw9 has Conditions: [{Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 12/16/22 14:10:13.464
    Dec 16 14:10:13.473: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 10, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7qhw9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 12/16/22 14:10:13.473
    Dec 16 14:10:13.475: INFO: Observed &Deployment event: ADDED
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
    Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qhw9-54bc444df" is progressing.}
    Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
    Dec 16 14:10:13.475: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 16 14:10:13.475: INFO: Observed Deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
    Dec 16 14:10:13.475: INFO: Found Deployment test-deployment-7qhw9 in namespace deployment-2557 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 16 14:10:13.475: INFO: Deployment test-deployment-7qhw9 has an updated status
    STEP: patching the Statefulset Status 12/16/22 14:10:13.475
    Dec 16 14:10:13.476: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 16 14:10:13.483: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 12/16/22 14:10:13.483
    Dec 16 14:10:13.485: INFO: Observed &Deployment event: ADDED
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
    Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qhw9-54bc444df"}
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:11 +0000 UTC 2022-12-16 14:10:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qhw9-54bc444df" is progressing.}
    Dec 16 14:10:13.485: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 16 14:10:13.485: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
    Dec 16 14:10:13.486: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-16 14:10:12 +0000 UTC 2022-12-16 14:10:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qhw9-54bc444df" has successfully progressed.}
    Dec 16 14:10:13.486: INFO: Observed deployment test-deployment-7qhw9 in namespace deployment-2557 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 16 14:10:13.486: INFO: Observed &Deployment event: MODIFIED
    Dec 16 14:10:13.486: INFO: Found deployment test-deployment-7qhw9 in namespace deployment-2557 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Dec 16 14:10:13.486: INFO: Deployment test-deployment-7qhw9 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 14:10:13.489: INFO: Deployment "test-deployment-7qhw9":
    &Deployment{ObjectMeta:{test-deployment-7qhw9  deployment-2557  7af94ec3-7de3-4fea-9b1c-8764fcfe18ab 680973954 1 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-16 14:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038d4ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 16 14:10:13.493: INFO: New ReplicaSet "test-deployment-7qhw9-54bc444df" of Deployment "test-deployment-7qhw9":
    &ReplicaSet{ObjectMeta:{test-deployment-7qhw9-54bc444df  deployment-2557  c7646945-c38a-4cd0-a851-1cecf5e17884 680973929 1 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7qhw9 7af94ec3-7de3-4fea-9b1c-8764fcfe18ab 0xc0038d52a7 0xc0038d52a8}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af94ec3-7de3-4fea-9b1c-8764fcfe18ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038d5358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:10:13.497: INFO: Pod "test-deployment-7qhw9-54bc444df-cs6tl" is available:
    &Pod{ObjectMeta:{test-deployment-7qhw9-54bc444df-cs6tl test-deployment-7qhw9-54bc444df- deployment-2557  f7c693e1-a5db-419c-b665-a883bb68e596 680973928 0 2022-12-16 14:10:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:734e3be611a59c6813bd5d1cc245330f6601bafd72927695b0a32b8461e4b100 cni.projectcalico.org/podIP:192.168.189.22/32 cni.projectcalico.org/podIPs:192.168.189.22/32] [{apps/v1 ReplicaSet test-deployment-7qhw9-54bc444df c7646945-c38a-4cd0-a851-1cecf5e17884 0xc0038d5727 0xc0038d5728}] [] [{calico Update v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-16 14:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7646945-c38a-4cd0-a851-1cecf5e17884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-16 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4zv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4zv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.22,StartTime:2022-12-16 14:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:10:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://27ff4b0a8c86cd601e47b3469eaaa06fd59420b516f545b6609886eb98562b68,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:13.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2557" for this suite. 12/16/22 14:10:13.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:13.508
Dec 16 14:10:13.508: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 14:10:13.508
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:13.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:13.522
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 14:10:13.525
Dec 16 14:10:13.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 16 14:10:13.605: INFO: stderr: ""
Dec 16 14:10:13.605: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 12/16/22 14:10:13.605
STEP: verifying the pod e2e-test-httpd-pod was created 12/16/22 14:10:18.658
Dec 16 14:10:18.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 get pod e2e-test-httpd-pod -o json'
Dec 16 14:10:18.729: INFO: stderr: ""
Dec 16 14:10:18.729: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"88a9c0bbb331c34d0847434cf988bd3ded0e23e7d007166ee5f9e5d4b672a69f\",\n            \"cni.projectcalico.org/podIP\": \"192.168.189.21/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.189.21/32\"\n        },\n        \"creationTimestamp\": \"2022-12-16T14:10:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2143\",\n        \"resourceVersion\": \"680973996\",\n        \"uid\": \"b0c14b44-8488-4f36-90a1-9f8cf73baa19\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6ftnb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pool-a3802-fsxxd\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6ftnb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://fae3919dcb89a84ddaa0fd8de4579c3bf9668c9f9865b90a295cff3a3e9b37b8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-16T14:10:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"85.217.161.242\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.189.21\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.189.21\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-16T14:10:13Z\"\n    }\n}\n"
STEP: replace the image in the pod 12/16/22 14:10:18.729
Dec 16 14:10:18.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 replace -f -'
Dec 16 14:10:18.913: INFO: stderr: ""
Dec 16 14:10:18.913: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/16/22 14:10:18.913
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Dec 16 14:10:18.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 delete pods e2e-test-httpd-pod'
Dec 16 14:10:20.883: INFO: stderr: ""
Dec 16 14:10:20.883: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:20.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2143" for this suite. 12/16/22 14:10:20.889
------------------------------
• [SLOW TEST] [7.388 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:13.508
    Dec 16 14:10:13.508: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 14:10:13.508
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:13.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:13.522
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/16/22 14:10:13.525
    Dec 16 14:10:13.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 16 14:10:13.605: INFO: stderr: ""
    Dec 16 14:10:13.605: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 12/16/22 14:10:13.605
    STEP: verifying the pod e2e-test-httpd-pod was created 12/16/22 14:10:18.658
    Dec 16 14:10:18.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 get pod e2e-test-httpd-pod -o json'
    Dec 16 14:10:18.729: INFO: stderr: ""
    Dec 16 14:10:18.729: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"88a9c0bbb331c34d0847434cf988bd3ded0e23e7d007166ee5f9e5d4b672a69f\",\n            \"cni.projectcalico.org/podIP\": \"192.168.189.21/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.189.21/32\"\n        },\n        \"creationTimestamp\": \"2022-12-16T14:10:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2143\",\n        \"resourceVersion\": \"680973996\",\n        \"uid\": \"b0c14b44-8488-4f36-90a1-9f8cf73baa19\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6ftnb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pool-a3802-fsxxd\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6ftnb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-16T14:10:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://fae3919dcb89a84ddaa0fd8de4579c3bf9668c9f9865b90a295cff3a3e9b37b8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-16T14:10:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"85.217.161.242\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.189.21\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.189.21\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-16T14:10:13Z\"\n    }\n}\n"
    STEP: replace the image in the pod 12/16/22 14:10:18.729
    Dec 16 14:10:18.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 replace -f -'
    Dec 16 14:10:18.913: INFO: stderr: ""
    Dec 16 14:10:18.913: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/16/22 14:10:18.913
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Dec 16 14:10:18.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-2143 delete pods e2e-test-httpd-pod'
    Dec 16 14:10:20.883: INFO: stderr: ""
    Dec 16 14:10:20.883: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:20.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2143" for this suite. 12/16/22 14:10:20.889
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:20.896
Dec 16 14:10:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 14:10:20.896
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:20.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:20.911
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-w7mgl" 12/16/22 14:10:20.914
Dec 16 14:10:20.927: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-w7mgl-6381" 12/16/22 14:10:20.927
Dec 16 14:10:20.936: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-w7mgl-6381" 12/16/22 14:10:20.936
Dec 16 14:10:20.943: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:20.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7459" for this suite. 12/16/22 14:10:20.948
STEP: Destroying namespace "e2e-ns-w7mgl-6381" for this suite. 12/16/22 14:10:20.953
------------------------------
• [0.063 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:20.896
    Dec 16 14:10:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 14:10:20.896
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:20.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:20.911
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-w7mgl" 12/16/22 14:10:20.914
    Dec 16 14:10:20.927: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-w7mgl-6381" 12/16/22 14:10:20.927
    Dec 16 14:10:20.936: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-w7mgl-6381" 12/16/22 14:10:20.936
    Dec 16 14:10:20.943: INFO: Namespace "e2e-ns-w7mgl-6381" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:20.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7459" for this suite. 12/16/22 14:10:20.948
    STEP: Destroying namespace "e2e-ns-w7mgl-6381" for this suite. 12/16/22 14:10:20.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:20.959
Dec 16 14:10:20.959: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 14:10:20.96
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:20.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:20.976
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-97e00b44-5a1b-4039-9abd-ccda93aa7249 12/16/22 14:10:20.979
STEP: Creating a pod to test consume secrets 12/16/22 14:10:20.985
Dec 16 14:10:20.994: INFO: Waiting up to 5m0s for pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47" in namespace "secrets-1497" to be "Succeeded or Failed"
Dec 16 14:10:20.997: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355491ms
Dec 16 14:10:23.004: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009613118s
Dec 16 14:10:25.001: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00732945s
STEP: Saw pod success 12/16/22 14:10:25.001
Dec 16 14:10:25.002: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47" satisfied condition "Succeeded or Failed"
Dec 16 14:10:25.005: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:10:25.055
Dec 16 14:10:25.070: INFO: Waiting for pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 to disappear
Dec 16 14:10:25.073: INFO: Pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:25.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1497" for this suite. 12/16/22 14:10:25.077
------------------------------
• [4.124 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:20.959
    Dec 16 14:10:20.959: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 14:10:20.96
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:20.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:20.976
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-97e00b44-5a1b-4039-9abd-ccda93aa7249 12/16/22 14:10:20.979
    STEP: Creating a pod to test consume secrets 12/16/22 14:10:20.985
    Dec 16 14:10:20.994: INFO: Waiting up to 5m0s for pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47" in namespace "secrets-1497" to be "Succeeded or Failed"
    Dec 16 14:10:20.997: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355491ms
    Dec 16 14:10:23.004: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009613118s
    Dec 16 14:10:25.001: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00732945s
    STEP: Saw pod success 12/16/22 14:10:25.001
    Dec 16 14:10:25.002: INFO: Pod "pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47" satisfied condition "Succeeded or Failed"
    Dec 16 14:10:25.005: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:10:25.055
    Dec 16 14:10:25.070: INFO: Waiting for pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 to disappear
    Dec 16 14:10:25.073: INFO: Pod pod-secrets-36416dd9-2987-47aa-a9cf-5a8971c36f47 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:25.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1497" for this suite. 12/16/22 14:10:25.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:25.084
Dec 16 14:10:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 14:10:25.085
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:25.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:25.1
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Dec 16 14:10:25.110: INFO: Waiting up to 2m0s for pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" in namespace "var-expansion-8228" to be "container 0 failed with reason CreateContainerConfigError"
Dec 16 14:10:25.113: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918288ms
Dec 16 14:10:27.118: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008170547s
Dec 16 14:10:27.118: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 16 14:10:27.118: INFO: Deleting pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" in namespace "var-expansion-8228"
Dec 16 14:10:27.125: INFO: Wait up to 5m0s for pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8228" for this suite. 12/16/22 14:10:29.139
------------------------------
• [4.061 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:25.084
    Dec 16 14:10:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 14:10:25.085
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:25.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:25.1
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Dec 16 14:10:25.110: INFO: Waiting up to 2m0s for pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" in namespace "var-expansion-8228" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 16 14:10:25.113: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918288ms
    Dec 16 14:10:27.118: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008170547s
    Dec 16 14:10:27.118: INFO: Pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 16 14:10:27.118: INFO: Deleting pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" in namespace "var-expansion-8228"
    Dec 16 14:10:27.125: INFO: Wait up to 5m0s for pod "var-expansion-92cfe8fa-4a66-4307-9273-f26c7b6d0e4c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8228" for this suite. 12/16/22 14:10:29.139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:29.145
Dec 16 14:10:29.145: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename kubectl 12/16/22 14:10:29.146
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:29.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:29.161
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 12/16/22 14:10:29.164
Dec 16 14:10:29.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 create -f -'
Dec 16 14:10:29.350: INFO: stderr: ""
Dec 16 14:10:29.350: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:29.35
Dec 16 14:10:29.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 14:10:29.418: INFO: stderr: ""
Dec 16 14:10:29.418: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-xbql9 "
Dec 16 14:10:29.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:29.484: INFO: stderr: ""
Dec 16 14:10:29.484: INFO: stdout: ""
Dec 16 14:10:29.484: INFO: update-demo-nautilus-nhpgd is created but not running
Dec 16 14:10:34.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 14:10:34.558: INFO: stderr: ""
Dec 16 14:10:34.558: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-xbql9 "
Dec 16 14:10:34.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:34.624: INFO: stderr: ""
Dec 16 14:10:34.624: INFO: stdout: "true"
Dec 16 14:10:34.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:34.690: INFO: stderr: ""
Dec 16 14:10:34.690: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:34.690: INFO: validating pod update-demo-nautilus-nhpgd
Dec 16 14:10:34.721: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:34.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:34.721: INFO: update-demo-nautilus-nhpgd is verified up and running
Dec 16 14:10:34.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-xbql9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:34.786: INFO: stderr: ""
Dec 16 14:10:34.786: INFO: stdout: "true"
Dec 16 14:10:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-xbql9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:34.853: INFO: stderr: ""
Dec 16 14:10:34.853: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:34.853: INFO: validating pod update-demo-nautilus-xbql9
Dec 16 14:10:34.885: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:34.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:34.886: INFO: update-demo-nautilus-xbql9 is verified up and running
STEP: scaling down the replication controller 12/16/22 14:10:34.886
Dec 16 14:10:34.887: INFO: scanned /root for discovery docs: <nil>
Dec 16 14:10:34.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 16 14:10:35.971: INFO: stderr: ""
Dec 16 14:10:35.971: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:35.971
Dec 16 14:10:35.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 14:10:36.037: INFO: stderr: ""
Dec 16 14:10:36.037: INFO: stdout: "update-demo-nautilus-nhpgd "
Dec 16 14:10:36.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:36.104: INFO: stderr: ""
Dec 16 14:10:36.104: INFO: stdout: "true"
Dec 16 14:10:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:36.173: INFO: stderr: ""
Dec 16 14:10:36.173: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:36.173: INFO: validating pod update-demo-nautilus-nhpgd
Dec 16 14:10:36.179: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:36.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:36.179: INFO: update-demo-nautilus-nhpgd is verified up and running
STEP: scaling up the replication controller 12/16/22 14:10:36.179
Dec 16 14:10:36.180: INFO: scanned /root for discovery docs: <nil>
Dec 16 14:10:36.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 16 14:10:37.262: INFO: stderr: ""
Dec 16 14:10:37.262: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:37.262
Dec 16 14:10:37.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 14:10:37.331: INFO: stderr: ""
Dec 16 14:10:37.331: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-vtkv9 "
Dec 16 14:10:37.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:37.398: INFO: stderr: ""
Dec 16 14:10:37.398: INFO: stdout: "true"
Dec 16 14:10:37.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:37.465: INFO: stderr: ""
Dec 16 14:10:37.465: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:37.465: INFO: validating pod update-demo-nautilus-nhpgd
Dec 16 14:10:37.472: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:37.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:37.472: INFO: update-demo-nautilus-nhpgd is verified up and running
Dec 16 14:10:37.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:37.536: INFO: stderr: ""
Dec 16 14:10:37.536: INFO: stdout: ""
Dec 16 14:10:37.536: INFO: update-demo-nautilus-vtkv9 is created but not running
Dec 16 14:10:42.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 16 14:10:42.615: INFO: stderr: ""
Dec 16 14:10:42.615: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-vtkv9 "
Dec 16 14:10:42.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:42.684: INFO: stderr: ""
Dec 16 14:10:42.684: INFO: stdout: "true"
Dec 16 14:10:42.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:42.754: INFO: stderr: ""
Dec 16 14:10:42.754: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:42.754: INFO: validating pod update-demo-nautilus-nhpgd
Dec 16 14:10:42.768: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:42.768: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:42.768: INFO: update-demo-nautilus-nhpgd is verified up and running
Dec 16 14:10:42.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 16 14:10:42.836: INFO: stderr: ""
Dec 16 14:10:42.836: INFO: stdout: "true"
Dec 16 14:10:42.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 16 14:10:42.905: INFO: stderr: ""
Dec 16 14:10:42.905: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 16 14:10:42.905: INFO: validating pod update-demo-nautilus-vtkv9
Dec 16 14:10:42.939: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 16 14:10:42.939: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 16 14:10:42.939: INFO: update-demo-nautilus-vtkv9 is verified up and running
STEP: using delete to clean up resources 12/16/22 14:10:42.939
Dec 16 14:10:42.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 delete --grace-period=0 --force -f -'
Dec 16 14:10:43.018: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 16 14:10:43.018: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 16 14:10:43.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get rc,svc -l name=update-demo --no-headers'
Dec 16 14:10:43.109: INFO: stderr: "No resources found in kubectl-6631 namespace.\n"
Dec 16 14:10:43.109: INFO: stdout: ""
Dec 16 14:10:43.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 16 14:10:43.212: INFO: stderr: ""
Dec 16 14:10:43.212: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:43.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6631" for this suite. 12/16/22 14:10:43.218
------------------------------
• [SLOW TEST] [14.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:29.145
    Dec 16 14:10:29.145: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename kubectl 12/16/22 14:10:29.146
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:29.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:29.161
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 12/16/22 14:10:29.164
    Dec 16 14:10:29.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 create -f -'
    Dec 16 14:10:29.350: INFO: stderr: ""
    Dec 16 14:10:29.350: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:29.35
    Dec 16 14:10:29.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 14:10:29.418: INFO: stderr: ""
    Dec 16 14:10:29.418: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-xbql9 "
    Dec 16 14:10:29.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:29.484: INFO: stderr: ""
    Dec 16 14:10:29.484: INFO: stdout: ""
    Dec 16 14:10:29.484: INFO: update-demo-nautilus-nhpgd is created but not running
    Dec 16 14:10:34.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 14:10:34.558: INFO: stderr: ""
    Dec 16 14:10:34.558: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-xbql9 "
    Dec 16 14:10:34.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:34.624: INFO: stderr: ""
    Dec 16 14:10:34.624: INFO: stdout: "true"
    Dec 16 14:10:34.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:34.690: INFO: stderr: ""
    Dec 16 14:10:34.690: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:34.690: INFO: validating pod update-demo-nautilus-nhpgd
    Dec 16 14:10:34.721: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:34.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:34.721: INFO: update-demo-nautilus-nhpgd is verified up and running
    Dec 16 14:10:34.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-xbql9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:34.786: INFO: stderr: ""
    Dec 16 14:10:34.786: INFO: stdout: "true"
    Dec 16 14:10:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-xbql9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:34.853: INFO: stderr: ""
    Dec 16 14:10:34.853: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:34.853: INFO: validating pod update-demo-nautilus-xbql9
    Dec 16 14:10:34.885: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:34.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:34.886: INFO: update-demo-nautilus-xbql9 is verified up and running
    STEP: scaling down the replication controller 12/16/22 14:10:34.886
    Dec 16 14:10:34.887: INFO: scanned /root for discovery docs: <nil>
    Dec 16 14:10:34.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Dec 16 14:10:35.971: INFO: stderr: ""
    Dec 16 14:10:35.971: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:35.971
    Dec 16 14:10:35.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 14:10:36.037: INFO: stderr: ""
    Dec 16 14:10:36.037: INFO: stdout: "update-demo-nautilus-nhpgd "
    Dec 16 14:10:36.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:36.104: INFO: stderr: ""
    Dec 16 14:10:36.104: INFO: stdout: "true"
    Dec 16 14:10:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:36.173: INFO: stderr: ""
    Dec 16 14:10:36.173: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:36.173: INFO: validating pod update-demo-nautilus-nhpgd
    Dec 16 14:10:36.179: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:36.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:36.179: INFO: update-demo-nautilus-nhpgd is verified up and running
    STEP: scaling up the replication controller 12/16/22 14:10:36.179
    Dec 16 14:10:36.180: INFO: scanned /root for discovery docs: <nil>
    Dec 16 14:10:36.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Dec 16 14:10:37.262: INFO: stderr: ""
    Dec 16 14:10:37.262: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/16/22 14:10:37.262
    Dec 16 14:10:37.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 14:10:37.331: INFO: stderr: ""
    Dec 16 14:10:37.331: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-vtkv9 "
    Dec 16 14:10:37.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:37.398: INFO: stderr: ""
    Dec 16 14:10:37.398: INFO: stdout: "true"
    Dec 16 14:10:37.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:37.465: INFO: stderr: ""
    Dec 16 14:10:37.465: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:37.465: INFO: validating pod update-demo-nautilus-nhpgd
    Dec 16 14:10:37.472: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:37.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:37.472: INFO: update-demo-nautilus-nhpgd is verified up and running
    Dec 16 14:10:37.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:37.536: INFO: stderr: ""
    Dec 16 14:10:37.536: INFO: stdout: ""
    Dec 16 14:10:37.536: INFO: update-demo-nautilus-vtkv9 is created but not running
    Dec 16 14:10:42.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 16 14:10:42.615: INFO: stderr: ""
    Dec 16 14:10:42.615: INFO: stdout: "update-demo-nautilus-nhpgd update-demo-nautilus-vtkv9 "
    Dec 16 14:10:42.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:42.684: INFO: stderr: ""
    Dec 16 14:10:42.684: INFO: stdout: "true"
    Dec 16 14:10:42.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-nhpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:42.754: INFO: stderr: ""
    Dec 16 14:10:42.754: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:42.754: INFO: validating pod update-demo-nautilus-nhpgd
    Dec 16 14:10:42.768: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:42.768: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:42.768: INFO: update-demo-nautilus-nhpgd is verified up and running
    Dec 16 14:10:42.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 16 14:10:42.836: INFO: stderr: ""
    Dec 16 14:10:42.836: INFO: stdout: "true"
    Dec 16 14:10:42.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods update-demo-nautilus-vtkv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 16 14:10:42.905: INFO: stderr: ""
    Dec 16 14:10:42.905: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 16 14:10:42.905: INFO: validating pod update-demo-nautilus-vtkv9
    Dec 16 14:10:42.939: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 16 14:10:42.939: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 16 14:10:42.939: INFO: update-demo-nautilus-vtkv9 is verified up and running
    STEP: using delete to clean up resources 12/16/22 14:10:42.939
    Dec 16 14:10:42.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 delete --grace-period=0 --force -f -'
    Dec 16 14:10:43.018: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 16 14:10:43.018: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 16 14:10:43.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get rc,svc -l name=update-demo --no-headers'
    Dec 16 14:10:43.109: INFO: stderr: "No resources found in kubectl-6631 namespace.\n"
    Dec 16 14:10:43.109: INFO: stdout: ""
    Dec 16 14:10:43.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=kubectl-6631 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 16 14:10:43.212: INFO: stderr: ""
    Dec 16 14:10:43.212: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:43.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6631" for this suite. 12/16/22 14:10:43.218
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:43.225
Dec 16 14:10:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:10:43.226
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.246
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-k24km"  12/16/22 14:10:43.248
Dec 16 14:10:43.253: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-k24km"  12/16/22 14:10:43.253
Dec 16 14:10:43.262: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:43.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4568" for this suite. 12/16/22 14:10:43.266
------------------------------
• [0.047 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:43.225
    Dec 16 14:10:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:10:43.226
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.246
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-k24km"  12/16/22 14:10:43.248
    Dec 16 14:10:43.253: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-k24km"  12/16/22 14:10:43.253
    Dec 16 14:10:43.262: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:43.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4568" for this suite. 12/16/22 14:10:43.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:43.273
Dec 16 14:10:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:10:43.273
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.289
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 12/16/22 14:10:43.304
STEP: waiting for available Endpoint 12/16/22 14:10:43.309
STEP: listing all Endpoints 12/16/22 14:10:43.31
STEP: updating the Endpoint 12/16/22 14:10:43.314
STEP: fetching the Endpoint 12/16/22 14:10:43.32
STEP: patching the Endpoint 12/16/22 14:10:43.323
STEP: fetching the Endpoint 12/16/22 14:10:43.341
STEP: deleting the Endpoint by Collection 12/16/22 14:10:43.344
STEP: waiting for Endpoint deletion 12/16/22 14:10:43.357
STEP: fetching the Endpoint 12/16/22 14:10:43.358
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8415" for this suite. 12/16/22 14:10:43.365
------------------------------
• [0.105 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:43.273
    Dec 16 14:10:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:10:43.273
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.289
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 12/16/22 14:10:43.304
    STEP: waiting for available Endpoint 12/16/22 14:10:43.309
    STEP: listing all Endpoints 12/16/22 14:10:43.31
    STEP: updating the Endpoint 12/16/22 14:10:43.314
    STEP: fetching the Endpoint 12/16/22 14:10:43.32
    STEP: patching the Endpoint 12/16/22 14:10:43.323
    STEP: fetching the Endpoint 12/16/22 14:10:43.341
    STEP: deleting the Endpoint by Collection 12/16/22 14:10:43.344
    STEP: waiting for Endpoint deletion 12/16/22 14:10:43.357
    STEP: fetching the Endpoint 12/16/22 14:10:43.358
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8415" for this suite. 12/16/22 14:10:43.365
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:43.378
Dec 16 14:10:43.378: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:10:43.378
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.392
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:10:43.395
Dec 16 14:10:43.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732" in namespace "projected-7656" to be "Succeeded or Failed"
Dec 16 14:10:43.407: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58786ms
Dec 16 14:10:45.414: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010794111s
Dec 16 14:10:47.415: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01255556s
STEP: Saw pod success 12/16/22 14:10:47.415
Dec 16 14:10:47.415: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732" satisfied condition "Succeeded or Failed"
Dec 16 14:10:47.420: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 container client-container: <nil>
STEP: delete the pod 12/16/22 14:10:47.432
Dec 16 14:10:47.442: INFO: Waiting for pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 to disappear
Dec 16 14:10:47.446: INFO: Pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:47.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7656" for this suite. 12/16/22 14:10:47.45
------------------------------
• [4.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:43.378
    Dec 16 14:10:43.378: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:10:43.378
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:43.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:43.392
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:10:43.395
    Dec 16 14:10:43.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732" in namespace "projected-7656" to be "Succeeded or Failed"
    Dec 16 14:10:43.407: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58786ms
    Dec 16 14:10:45.414: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010794111s
    Dec 16 14:10:47.415: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01255556s
    STEP: Saw pod success 12/16/22 14:10:47.415
    Dec 16 14:10:47.415: INFO: Pod "downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732" satisfied condition "Succeeded or Failed"
    Dec 16 14:10:47.420: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 container client-container: <nil>
    STEP: delete the pod 12/16/22 14:10:47.432
    Dec 16 14:10:47.442: INFO: Waiting for pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 to disappear
    Dec 16 14:10:47.446: INFO: Pod downwardapi-volume-dfb8c4c7-23f3-4fdc-88ad-4fc4d0b53732 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:47.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7656" for this suite. 12/16/22 14:10:47.45
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:47.458
Dec 16 14:10:47.458: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 14:10:47.458
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.473
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 12/16/22 14:10:47.476
STEP: fetching the ConfigMap 12/16/22 14:10:47.481
STEP: patching the ConfigMap 12/16/22 14:10:47.484
STEP: listing all ConfigMaps in all namespaces with a label selector 12/16/22 14:10:47.489
STEP: deleting the ConfigMap by collection with a label selector 12/16/22 14:10:47.494
STEP: listing all ConfigMaps in test namespace 12/16/22 14:10:47.502
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:47.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1985" for this suite. 12/16/22 14:10:47.509
------------------------------
• [0.058 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:47.458
    Dec 16 14:10:47.458: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 14:10:47.458
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.473
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 12/16/22 14:10:47.476
    STEP: fetching the ConfigMap 12/16/22 14:10:47.481
    STEP: patching the ConfigMap 12/16/22 14:10:47.484
    STEP: listing all ConfigMaps in all namespaces with a label selector 12/16/22 14:10:47.489
    STEP: deleting the ConfigMap by collection with a label selector 12/16/22 14:10:47.494
    STEP: listing all ConfigMaps in test namespace 12/16/22 14:10:47.502
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:47.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1985" for this suite. 12/16/22 14:10:47.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:47.516
Dec 16 14:10:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename events 12/16/22 14:10:47.516
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.532
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 12/16/22 14:10:47.535
STEP: get a list of Events with a label in the current namespace 12/16/22 14:10:47.548
STEP: delete a list of events 12/16/22 14:10:47.552
Dec 16 14:10:47.552: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/16/22 14:10:47.571
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:47.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7897" for this suite. 12/16/22 14:10:47.578
------------------------------
• [0.069 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:47.516
    Dec 16 14:10:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename events 12/16/22 14:10:47.516
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.532
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 12/16/22 14:10:47.535
    STEP: get a list of Events with a label in the current namespace 12/16/22 14:10:47.548
    STEP: delete a list of events 12/16/22 14:10:47.552
    Dec 16 14:10:47.552: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/16/22 14:10:47.571
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:47.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7897" for this suite. 12/16/22 14:10:47.578
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:47.585
Dec 16 14:10:47.585: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:10:47.585
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.599
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:10:47.613
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:10:47.964
STEP: Deploying the webhook pod 12/16/22 14:10:47.974
STEP: Wait for the deployment to be ready 12/16/22 14:10:47.99
Dec 16 14:10:47.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:10:50.011
STEP: Verifying the service has paired with the endpoint 12/16/22 14:10:50.026
Dec 16 14:10:51.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 12/16/22 14:10:51.032
STEP: create a pod 12/16/22 14:10:51.076
Dec 16 14:10:51.081: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3846" to be "running"
Dec 16 14:10:51.086: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105501ms
Dec 16 14:10:53.091: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009920544s
Dec 16 14:10:53.091: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 12/16/22 14:10:53.091
Dec 16 14:10:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=webhook-3846 attach --namespace=webhook-3846 to-be-attached-pod -i -c=container1'
Dec 16 14:10:53.243: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:10:53.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3846" for this suite. 12/16/22 14:10:53.295
STEP: Destroying namespace "webhook-3846-markers" for this suite. 12/16/22 14:10:53.3
------------------------------
• [SLOW TEST] [5.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:47.585
    Dec 16 14:10:47.585: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:10:47.585
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:47.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:47.599
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:10:47.613
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:10:47.964
    STEP: Deploying the webhook pod 12/16/22 14:10:47.974
    STEP: Wait for the deployment to be ready 12/16/22 14:10:47.99
    Dec 16 14:10:47.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:10:50.011
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:10:50.026
    Dec 16 14:10:51.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 12/16/22 14:10:51.032
    STEP: create a pod 12/16/22 14:10:51.076
    Dec 16 14:10:51.081: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3846" to be "running"
    Dec 16 14:10:51.086: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105501ms
    Dec 16 14:10:53.091: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009920544s
    Dec 16 14:10:53.091: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 12/16/22 14:10:53.091
    Dec 16 14:10:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=webhook-3846 attach --namespace=webhook-3846 to-be-attached-pod -i -c=container1'
    Dec 16 14:10:53.243: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:10:53.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3846" for this suite. 12/16/22 14:10:53.295
    STEP: Destroying namespace "webhook-3846-markers" for this suite. 12/16/22 14:10:53.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:10:53.307
Dec 16 14:10:53.307: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename hostport 12/16/22 14:10:53.307
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:53.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:53.321
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/16/22 14:10:53.327
Dec 16 14:10:53.338: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5634" to be "running and ready"
Dec 16 14:10:53.342: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626655ms
Dec 16 14:10:53.342: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:10:55.346: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008018691s
Dec 16 14:10:55.346: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 16 14:10:55.346: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 85.217.161.213 on the node which pod1 resides and expect scheduled 12/16/22 14:10:55.346
Dec 16 14:10:55.352: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5634" to be "running and ready"
Dec 16 14:10:55.356: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045593ms
Dec 16 14:10:55.356: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:10:57.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011121204s
Dec 16 14:10:57.363: INFO: The phase of Pod pod2 is Running (Ready = false)
Dec 16 14:10:59.361: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009098037s
Dec 16 14:10:59.361: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 16 14:10:59.361: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 85.217.161.213 but use UDP protocol on the node which pod2 resides 12/16/22 14:10:59.361
Dec 16 14:10:59.366: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5634" to be "running and ready"
Dec 16 14:10:59.370: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.594615ms
Dec 16 14:10:59.370: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:11:01.375: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008917052s
Dec 16 14:11:01.375: INFO: The phase of Pod pod3 is Running (Ready = true)
Dec 16 14:11:01.375: INFO: Pod "pod3" satisfied condition "running and ready"
Dec 16 14:11:01.382: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5634" to be "running and ready"
Dec 16 14:11:01.385: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004712ms
Dec 16 14:11:01.385: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:11:03.390: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007495895s
Dec 16 14:11:03.390: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Dec 16 14:11:03.390: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/16/22 14:11:03.393
Dec 16 14:11:03.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 85.217.161.213 http://127.0.0.1:54323/hostname] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:11:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:11:03.393: INFO: ExecWithOptions: Clientset creation
Dec 16 14:11:03.393: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+85.217.161.213+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.217.161.213, port: 54323 12/16/22 14:11:03.516
Dec 16 14:11:03.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://85.217.161.213:54323/hostname] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:11:03.516: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:11:03.517: INFO: ExecWithOptions: Clientset creation
Dec 16 14:11:03.517: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F85.217.161.213%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.217.161.213, port: 54323 UDP 12/16/22 14:11:03.631
Dec 16 14:11:03.631: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 85.217.161.213 54323] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:11:03.631: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:11:03.632: INFO: ExecWithOptions: Clientset creation
Dec 16 14:11:03.632: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+85.217.161.213+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Dec 16 14:11:08.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5634" for this suite. 12/16/22 14:11:08.76
------------------------------
• [SLOW TEST] [15.608 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:10:53.307
    Dec 16 14:10:53.307: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename hostport 12/16/22 14:10:53.307
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:10:53.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:10:53.321
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/16/22 14:10:53.327
    Dec 16 14:10:53.338: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5634" to be "running and ready"
    Dec 16 14:10:53.342: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.626655ms
    Dec 16 14:10:53.342: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:10:55.346: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008018691s
    Dec 16 14:10:55.346: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 16 14:10:55.346: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 85.217.161.213 on the node which pod1 resides and expect scheduled 12/16/22 14:10:55.346
    Dec 16 14:10:55.352: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5634" to be "running and ready"
    Dec 16 14:10:55.356: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045593ms
    Dec 16 14:10:55.356: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:10:57.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011121204s
    Dec 16 14:10:57.363: INFO: The phase of Pod pod2 is Running (Ready = false)
    Dec 16 14:10:59.361: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009098037s
    Dec 16 14:10:59.361: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 16 14:10:59.361: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 85.217.161.213 but use UDP protocol on the node which pod2 resides 12/16/22 14:10:59.361
    Dec 16 14:10:59.366: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5634" to be "running and ready"
    Dec 16 14:10:59.370: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.594615ms
    Dec 16 14:10:59.370: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:11:01.375: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008917052s
    Dec 16 14:11:01.375: INFO: The phase of Pod pod3 is Running (Ready = true)
    Dec 16 14:11:01.375: INFO: Pod "pod3" satisfied condition "running and ready"
    Dec 16 14:11:01.382: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5634" to be "running and ready"
    Dec 16 14:11:01.385: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004712ms
    Dec 16 14:11:01.385: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:11:03.390: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007495895s
    Dec 16 14:11:03.390: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Dec 16 14:11:03.390: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/16/22 14:11:03.393
    Dec 16 14:11:03.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 85.217.161.213 http://127.0.0.1:54323/hostname] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:11:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:11:03.393: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:11:03.393: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+85.217.161.213+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.217.161.213, port: 54323 12/16/22 14:11:03.516
    Dec 16 14:11:03.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://85.217.161.213:54323/hostname] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:11:03.516: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:11:03.517: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:11:03.517: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F85.217.161.213%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.217.161.213, port: 54323 UDP 12/16/22 14:11:03.631
    Dec 16 14:11:03.631: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 85.217.161.213 54323] Namespace:hostport-5634 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:11:03.631: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:11:03.632: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:11:03.632: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5634/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+85.217.161.213+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:11:08.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5634" for this suite. 12/16/22 14:11:08.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:11:08.916
Dec 16 14:11:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:11:08.916
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:09.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:09.022
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-2f19312c-959d-4e0a-84ab-96667eda24ba 12/16/22 14:11:09.172
STEP: Creating a pod to test consume secrets 12/16/22 14:11:09.345
Dec 16 14:11:09.613: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693" in namespace "projected-7660" to be "Succeeded or Failed"
Dec 16 14:11:09.617: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307193ms
Dec 16 14:11:11.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809683s
Dec 16 14:11:13.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010577486s
STEP: Saw pod success 12/16/22 14:11:13.623
Dec 16 14:11:13.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693" satisfied condition "Succeeded or Failed"
Dec 16 14:11:13.628: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:11:13.638
Dec 16 14:11:13.650: INFO: Waiting for pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 to disappear
Dec 16 14:11:13.654: INFO: Pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 14:11:13.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7660" for this suite. 12/16/22 14:11:13.658
------------------------------
• [4.749 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:11:08.916
    Dec 16 14:11:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:11:08.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:09.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:09.022
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-2f19312c-959d-4e0a-84ab-96667eda24ba 12/16/22 14:11:09.172
    STEP: Creating a pod to test consume secrets 12/16/22 14:11:09.345
    Dec 16 14:11:09.613: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693" in namespace "projected-7660" to be "Succeeded or Failed"
    Dec 16 14:11:09.617: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307193ms
    Dec 16 14:11:11.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809683s
    Dec 16 14:11:13.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010577486s
    STEP: Saw pod success 12/16/22 14:11:13.623
    Dec 16 14:11:13.623: INFO: Pod "pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693" satisfied condition "Succeeded or Failed"
    Dec 16 14:11:13.628: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:11:13.638
    Dec 16 14:11:13.650: INFO: Waiting for pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 to disappear
    Dec 16 14:11:13.654: INFO: Pod pod-projected-secrets-b10a17e1-b1b4-4ca0-95bd-cb90e2986693 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:11:13.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7660" for this suite. 12/16/22 14:11:13.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:11:13.666
Dec 16 14:11:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 14:11:13.667
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:13.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:13.681
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 12/16/22 14:11:13.684
Dec 16 14:11:13.688: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/16/22 14:11:13.688
Dec 16 14:11:13.693: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/16/22 14:11:13.693
Dec 16 14:11:13.702: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:11:13.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7398" for this suite. 12/16/22 14:11:13.706
------------------------------
• [0.047 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:11:13.666
    Dec 16 14:11:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 14:11:13.667
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:13.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:13.681
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 12/16/22 14:11:13.684
    Dec 16 14:11:13.688: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/16/22 14:11:13.688
    Dec 16 14:11:13.693: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/16/22 14:11:13.693
    Dec 16 14:11:13.702: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:11:13.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7398" for this suite. 12/16/22 14:11:13.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:11:13.713
Dec 16 14:11:13.714: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 14:11:13.714
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:13.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:13.728
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Dec 16 14:11:13.739: INFO: Waiting up to 5m0s for pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e" in namespace "container-probe-5193" to be "running and ready"
Dec 16 14:11:13.743: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364119ms
Dec 16 14:11:13.743: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:11:15.747: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 2.007995453s
Dec 16 14:11:15.747: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:17.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 4.008260202s
Dec 16 14:11:17.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:19.751: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 6.011432936s
Dec 16 14:11:19.751: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:21.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 8.009039562s
Dec 16 14:11:21.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:23.751: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 10.012043391s
Dec 16 14:11:23.751: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:25.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 12.008933753s
Dec 16 14:11:25.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:27.750: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 14.010650789s
Dec 16 14:11:27.750: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:29.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 16.008808435s
Dec 16 14:11:29.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:31.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 18.009050699s
Dec 16 14:11:31.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:33.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 20.008890205s
Dec 16 14:11:33.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
Dec 16 14:11:35.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=true. Elapsed: 22.008752814s
Dec 16 14:11:35.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = true)
Dec 16 14:11:35.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e" satisfied condition "running and ready"
Dec 16 14:11:35.752: INFO: Container started at 2022-12-16 14:11:14 +0000 UTC, pod became ready at 2022-12-16 14:11:34 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 14:11:35.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5193" for this suite. 12/16/22 14:11:35.757
------------------------------
• [SLOW TEST] [22.050 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:11:13.713
    Dec 16 14:11:13.714: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 14:11:13.714
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:13.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:13.728
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Dec 16 14:11:13.739: INFO: Waiting up to 5m0s for pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e" in namespace "container-probe-5193" to be "running and ready"
    Dec 16 14:11:13.743: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364119ms
    Dec 16 14:11:13.743: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:11:15.747: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 2.007995453s
    Dec 16 14:11:15.747: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:17.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 4.008260202s
    Dec 16 14:11:17.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:19.751: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 6.011432936s
    Dec 16 14:11:19.751: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:21.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 8.009039562s
    Dec 16 14:11:21.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:23.751: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 10.012043391s
    Dec 16 14:11:23.751: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:25.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 12.008933753s
    Dec 16 14:11:25.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:27.750: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 14.010650789s
    Dec 16 14:11:27.750: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:29.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 16.008808435s
    Dec 16 14:11:29.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:31.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 18.009050699s
    Dec 16 14:11:31.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:33.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=false. Elapsed: 20.008890205s
    Dec 16 14:11:33.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = false)
    Dec 16 14:11:35.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e": Phase="Running", Reason="", readiness=true. Elapsed: 22.008752814s
    Dec 16 14:11:35.748: INFO: The phase of Pod test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e is Running (Ready = true)
    Dec 16 14:11:35.748: INFO: Pod "test-webserver-1fae882f-de11-4e6b-9fef-621b4574893e" satisfied condition "running and ready"
    Dec 16 14:11:35.752: INFO: Container started at 2022-12-16 14:11:14 +0000 UTC, pod became ready at 2022-12-16 14:11:34 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:11:35.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5193" for this suite. 12/16/22 14:11:35.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:11:35.764
Dec 16 14:11:35.764: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename init-container 12/16/22 14:11:35.765
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:35.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:35.786
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 12/16/22 14:11:35.789
Dec 16 14:11:35.789: INFO: PodSpec: initContainers in spec.initContainers
Dec 16 14:12:23.125: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7a1f0edd-e872-4dd7-adf7-db8889a2472d", GenerateName:"", Namespace:"init-container-3553", SelfLink:"", UID:"1b39739d-3feb-4196-8277-03deb8ad3ceb", ResourceVersion:"680978749", Generation:0, CreationTimestamp:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"789623643"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"86dd8b86f505c5428bcc52d0876cc045ac5354a4f2ff430eabfbdd8f2f193bb8", "cni.projectcalico.org/podIP":"192.168.189.29/32", "cni.projectcalico.org/podIPs":"192.168.189.29/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c0c0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 11, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 12, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c150), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-fqg57", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b7e040), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003d3a240), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pool-a3802-fsxxd", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0051d2000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d3a2d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d3a2f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003d3a2f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003d3a2fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005fc6030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"85.217.161.242", PodIP:"192.168.189.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.189.29"}}, StartTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0051d20e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0051d2150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://6a7d013537ae3fa9c276915cc546f7ed61162c21402676b5ca8bbce180774bd7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b7e0c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b7e0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003d3a374)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:12:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3553" for this suite. 12/16/22 14:12:23.13
------------------------------
• [SLOW TEST] [47.372 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:11:35.764
    Dec 16 14:11:35.764: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename init-container 12/16/22 14:11:35.765
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:11:35.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:11:35.786
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 12/16/22 14:11:35.789
    Dec 16 14:11:35.789: INFO: PodSpec: initContainers in spec.initContainers
    Dec 16 14:12:23.125: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7a1f0edd-e872-4dd7-adf7-db8889a2472d", GenerateName:"", Namespace:"init-container-3553", SelfLink:"", UID:"1b39739d-3feb-4196-8277-03deb8ad3ceb", ResourceVersion:"680978749", Generation:0, CreationTimestamp:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"789623643"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"86dd8b86f505c5428bcc52d0876cc045ac5354a4f2ff430eabfbdd8f2f193bb8", "cni.projectcalico.org/podIP":"192.168.189.29/32", "cni.projectcalico.org/podIPs":"192.168.189.29/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c0c0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 11, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 16, 14, 12, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e2c150), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-fqg57", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b7e040), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fqg57", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003d3a240), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pool-a3802-fsxxd", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0051d2000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d3a2d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003d3a2f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003d3a2f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003d3a2fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005fc6030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"85.217.161.242", PodIP:"192.168.189.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.189.29"}}, StartTime:time.Date(2022, time.December, 16, 14, 11, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0051d20e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0051d2150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://6a7d013537ae3fa9c276915cc546f7ed61162c21402676b5ca8bbce180774bd7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b7e0c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b7e0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003d3a374)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:12:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3553" for this suite. 12/16/22 14:12:23.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:12:23.137
Dec 16 14:12:23.137: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 14:12:23.138
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:23.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:23.155
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 12/16/22 14:12:23.157
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_tcp@PTR;sleep 1; done
 12/16/22 14:12:23.176
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_tcp@PTR;sleep 1; done
 12/16/22 14:12:23.177
STEP: creating a pod to probe DNS 12/16/22 14:12:23.177
STEP: submitting the pod to kubernetes 12/16/22 14:12:23.177
Dec 16 14:12:23.186: INFO: Waiting up to 15m0s for pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba" in namespace "dns-682" to be "running"
Dec 16 14:12:23.189: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973344ms
Dec 16 14:12:25.195: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008350752s
Dec 16 14:12:25.195: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba" satisfied condition "running"
STEP: retrieving the pod 12/16/22 14:12:25.195
STEP: looking for the results for each expected name from probers 12/16/22 14:12:25.198
Dec 16 14:12:25.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
Dec 16 14:12:25.239: INFO: Unable to read wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
Dec 16 14:12:25.273: INFO: Unable to read jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
Dec 16 14:12:25.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
Dec 16 14:12:25.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
Dec 16 14:12:25.320: INFO: Lookups using dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba failed for: [wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local]

Dec 16 14:12:30.422: INFO: DNS probes using dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba succeeded

STEP: deleting the pod 12/16/22 14:12:30.422
STEP: deleting the test service 12/16/22 14:12:30.434
STEP: deleting the test headless service 12/16/22 14:12:30.458
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 14:12:30.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-682" for this suite. 12/16/22 14:12:30.472
------------------------------
• [SLOW TEST] [7.341 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:12:23.137
    Dec 16 14:12:23.137: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 14:12:23.138
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:23.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:23.155
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 12/16/22 14:12:23.157
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_tcp@PTR;sleep 1; done
     12/16/22 14:12:23.176
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local;check="$$(dig +notcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.208.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.208.246_tcp@PTR;sleep 1; done
     12/16/22 14:12:23.177
    STEP: creating a pod to probe DNS 12/16/22 14:12:23.177
    STEP: submitting the pod to kubernetes 12/16/22 14:12:23.177
    Dec 16 14:12:23.186: INFO: Waiting up to 15m0s for pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba" in namespace "dns-682" to be "running"
    Dec 16 14:12:23.189: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973344ms
    Dec 16 14:12:25.195: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008350752s
    Dec 16 14:12:25.195: INFO: Pod "dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba" satisfied condition "running"
    STEP: retrieving the pod 12/16/22 14:12:25.195
    STEP: looking for the results for each expected name from probers 12/16/22 14:12:25.198
    Dec 16 14:12:25.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
    Dec 16 14:12:25.239: INFO: Unable to read wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
    Dec 16 14:12:25.273: INFO: Unable to read jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
    Dec 16 14:12:25.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
    Dec 16 14:12:25.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local from pod dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba: the server could not find the requested resource (get pods dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba)
    Dec 16 14:12:25.320: INFO: Lookups using dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba failed for: [wheezy_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local wheezy_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_udp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_tcp@dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-682.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local]

    Dec 16 14:12:30.422: INFO: DNS probes using dns-682/dns-test-7428150c-42f6-4755-bac3-67769bd7c3ba succeeded

    STEP: deleting the pod 12/16/22 14:12:30.422
    STEP: deleting the test service 12/16/22 14:12:30.434
    STEP: deleting the test headless service 12/16/22 14:12:30.458
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:12:30.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-682" for this suite. 12/16/22 14:12:30.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:12:30.479
Dec 16 14:12:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename controllerrevisions 12/16/22 14:12:30.48
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:30.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:30.501
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-fddx6-daemon-set" 12/16/22 14:12:30.523
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:12:30.529
Dec 16 14:12:30.536: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
Dec 16 14:12:30.536: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:12:31.546: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
Dec 16 14:12:31.546: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:12:32.546: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 3
Dec 16 14:12:32.546: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-fddx6-daemon-set
STEP: Confirm DaemonSet "e2e-fddx6-daemon-set" successfully created with "daemonset-name=e2e-fddx6-daemon-set" label 12/16/22 14:12:32.55
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fddx6-daemon-set" 12/16/22 14:12:32.558
Dec 16 14:12:32.562: INFO: Located ControllerRevision: "e2e-fddx6-daemon-set-7766f559cc"
STEP: Patching ControllerRevision "e2e-fddx6-daemon-set-7766f559cc" 12/16/22 14:12:32.565
Dec 16 14:12:32.572: INFO: e2e-fddx6-daemon-set-7766f559cc has been patched
STEP: Create a new ControllerRevision 12/16/22 14:12:32.572
Dec 16 14:12:32.578: INFO: Created ControllerRevision: e2e-fddx6-daemon-set-5d485c697c
STEP: Confirm that there are two ControllerRevisions 12/16/22 14:12:32.578
Dec 16 14:12:32.578: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 16 14:12:32.581: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-fddx6-daemon-set-7766f559cc" 12/16/22 14:12:32.581
STEP: Confirm that there is only one ControllerRevision 12/16/22 14:12:32.588
Dec 16 14:12:32.588: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 16 14:12:32.590: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-fddx6-daemon-set-5d485c697c" 12/16/22 14:12:32.594
Dec 16 14:12:32.601: INFO: e2e-fddx6-daemon-set-5d485c697c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/16/22 14:12:32.601
W1216 14:12:32.610641      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/16/22 14:12:32.61
Dec 16 14:12:32.610: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 16 14:12:33.614: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 16 14:12:33.618: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fddx6-daemon-set-5d485c697c=updated" 12/16/22 14:12:33.618
STEP: Confirm that there is only one ControllerRevision 12/16/22 14:12:33.626
Dec 16 14:12:33.626: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 16 14:12:33.629: INFO: Found 1 ControllerRevisions
Dec 16 14:12:33.631: INFO: ControllerRevision "e2e-fddx6-daemon-set-585484f8b8" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-fddx6-daemon-set" 12/16/22 14:12:33.634
STEP: deleting DaemonSet.extensions e2e-fddx6-daemon-set in namespace controllerrevisions-4312, will wait for the garbage collector to delete the pods 12/16/22 14:12:33.634
Dec 16 14:12:33.694: INFO: Deleting DaemonSet.extensions e2e-fddx6-daemon-set took: 6.79707ms
Dec 16 14:12:33.794: INFO: Terminating DaemonSet.extensions e2e-fddx6-daemon-set pods took: 100.332598ms
Dec 16 14:12:35.199: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
Dec 16 14:12:35.199: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fddx6-daemon-set
Dec 16 14:12:35.203: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680979208"},"items":null}

Dec 16 14:12:35.207: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680979208"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:12:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4312" for this suite. 12/16/22 14:12:35.227
------------------------------
• [4.755 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:12:30.479
    Dec 16 14:12:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename controllerrevisions 12/16/22 14:12:30.48
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:30.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:30.501
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-fddx6-daemon-set" 12/16/22 14:12:30.523
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:12:30.529
    Dec 16 14:12:30.536: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
    Dec 16 14:12:30.536: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:12:31.546: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
    Dec 16 14:12:31.546: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:12:32.546: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 3
    Dec 16 14:12:32.546: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-fddx6-daemon-set
    STEP: Confirm DaemonSet "e2e-fddx6-daemon-set" successfully created with "daemonset-name=e2e-fddx6-daemon-set" label 12/16/22 14:12:32.55
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fddx6-daemon-set" 12/16/22 14:12:32.558
    Dec 16 14:12:32.562: INFO: Located ControllerRevision: "e2e-fddx6-daemon-set-7766f559cc"
    STEP: Patching ControllerRevision "e2e-fddx6-daemon-set-7766f559cc" 12/16/22 14:12:32.565
    Dec 16 14:12:32.572: INFO: e2e-fddx6-daemon-set-7766f559cc has been patched
    STEP: Create a new ControllerRevision 12/16/22 14:12:32.572
    Dec 16 14:12:32.578: INFO: Created ControllerRevision: e2e-fddx6-daemon-set-5d485c697c
    STEP: Confirm that there are two ControllerRevisions 12/16/22 14:12:32.578
    Dec 16 14:12:32.578: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 16 14:12:32.581: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-fddx6-daemon-set-7766f559cc" 12/16/22 14:12:32.581
    STEP: Confirm that there is only one ControllerRevision 12/16/22 14:12:32.588
    Dec 16 14:12:32.588: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 16 14:12:32.590: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-fddx6-daemon-set-5d485c697c" 12/16/22 14:12:32.594
    Dec 16 14:12:32.601: INFO: e2e-fddx6-daemon-set-5d485c697c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/16/22 14:12:32.601
    W1216 14:12:32.610641      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/16/22 14:12:32.61
    Dec 16 14:12:32.610: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 16 14:12:33.614: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 16 14:12:33.618: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fddx6-daemon-set-5d485c697c=updated" 12/16/22 14:12:33.618
    STEP: Confirm that there is only one ControllerRevision 12/16/22 14:12:33.626
    Dec 16 14:12:33.626: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 16 14:12:33.629: INFO: Found 1 ControllerRevisions
    Dec 16 14:12:33.631: INFO: ControllerRevision "e2e-fddx6-daemon-set-585484f8b8" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-fddx6-daemon-set" 12/16/22 14:12:33.634
    STEP: deleting DaemonSet.extensions e2e-fddx6-daemon-set in namespace controllerrevisions-4312, will wait for the garbage collector to delete the pods 12/16/22 14:12:33.634
    Dec 16 14:12:33.694: INFO: Deleting DaemonSet.extensions e2e-fddx6-daemon-set took: 6.79707ms
    Dec 16 14:12:33.794: INFO: Terminating DaemonSet.extensions e2e-fddx6-daemon-set pods took: 100.332598ms
    Dec 16 14:12:35.199: INFO: Number of nodes with available pods controlled by daemonset e2e-fddx6-daemon-set: 0
    Dec 16 14:12:35.199: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fddx6-daemon-set
    Dec 16 14:12:35.203: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680979208"},"items":null}

    Dec 16 14:12:35.207: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680979208"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:12:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4312" for this suite. 12/16/22 14:12:35.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:12:35.234
Dec 16 14:12:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-runtime 12/16/22 14:12:35.235
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:35.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:35.252
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 12/16/22 14:12:35.255
STEP: wait for the container to reach Succeeded 12/16/22 14:12:35.263
STEP: get the container status 12/16/22 14:12:38.283
STEP: the container should be terminated 12/16/22 14:12:38.287
STEP: the termination message should be set 12/16/22 14:12:38.287
Dec 16 14:12:38.288: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/16/22 14:12:38.288
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 16 14:12:38.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1805" for this suite. 12/16/22 14:12:38.306
------------------------------
• [3.078 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:12:35.234
    Dec 16 14:12:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-runtime 12/16/22 14:12:35.235
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:35.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:35.252
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 12/16/22 14:12:35.255
    STEP: wait for the container to reach Succeeded 12/16/22 14:12:35.263
    STEP: get the container status 12/16/22 14:12:38.283
    STEP: the container should be terminated 12/16/22 14:12:38.287
    STEP: the termination message should be set 12/16/22 14:12:38.287
    Dec 16 14:12:38.288: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/16/22 14:12:38.288
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:12:38.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1805" for this suite. 12/16/22 14:12:38.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:12:38.313
Dec 16 14:12:38.313: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename podtemplate 12/16/22 14:12:38.314
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:38.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:38.329
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 12/16/22 14:12:38.332
Dec 16 14:12:38.338: INFO: created test-podtemplate-1
Dec 16 14:12:38.342: INFO: created test-podtemplate-2
Dec 16 14:12:38.346: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 12/16/22 14:12:38.346
STEP: delete collection of pod templates 12/16/22 14:12:38.35
Dec 16 14:12:38.350: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 12/16/22 14:12:38.365
Dec 16 14:12:38.365: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 16 14:12:38.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8021" for this suite. 12/16/22 14:12:38.372
------------------------------
• [0.065 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:12:38.313
    Dec 16 14:12:38.313: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename podtemplate 12/16/22 14:12:38.314
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:38.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:38.329
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 12/16/22 14:12:38.332
    Dec 16 14:12:38.338: INFO: created test-podtemplate-1
    Dec 16 14:12:38.342: INFO: created test-podtemplate-2
    Dec 16 14:12:38.346: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 12/16/22 14:12:38.346
    STEP: delete collection of pod templates 12/16/22 14:12:38.35
    Dec 16 14:12:38.350: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 12/16/22 14:12:38.365
    Dec 16 14:12:38.365: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:12:38.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8021" for this suite. 12/16/22 14:12:38.372
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:12:38.379
Dec 16 14:12:38.379: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename aggregator 12/16/22 14:12:38.379
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:38.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:38.396
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Dec 16 14:12:38.399: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 12/16/22 14:12:38.4
Dec 16 14:12:38.935: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 16 14:12:40.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:42.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:44.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:46.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:48.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:50.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:52.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:54.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:56.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:12:58.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:13:00.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:13:03.152: INFO: Waited 156.763778ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 12/16/22 14:13:03.205
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/16/22 14:13:03.209
STEP: List APIServices 12/16/22 14:13:03.215
Dec 16 14:13:03.221: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-385" for this suite. 12/16/22 14:13:03.506
------------------------------
• [SLOW TEST] [25.179 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:12:38.379
    Dec 16 14:12:38.379: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename aggregator 12/16/22 14:12:38.379
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:12:38.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:12:38.396
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Dec 16 14:12:38.399: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 12/16/22 14:12:38.4
    Dec 16 14:12:38.935: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Dec 16 14:12:40.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:42.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:44.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:46.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:48.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:50.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:52.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:54.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:56.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:12:58.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:13:00.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 12, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:13:03.152: INFO: Waited 156.763778ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 12/16/22 14:13:03.205
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/16/22 14:13:03.209
    STEP: List APIServices 12/16/22 14:13:03.215
    Dec 16 14:13:03.221: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-385" for this suite. 12/16/22 14:13:03.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:03.558
Dec 16 14:13:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 14:13:03.559
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:03.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:03.576
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 12/16/22 14:13:03.579
STEP: Getting a ResourceQuota 12/16/22 14:13:03.583
STEP: Updating a ResourceQuota 12/16/22 14:13:03.587
STEP: Verifying a ResourceQuota was modified 12/16/22 14:13:03.591
STEP: Deleting a ResourceQuota 12/16/22 14:13:03.595
STEP: Verifying the deleted ResourceQuota 12/16/22 14:13:03.602
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:03.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5135" for this suite. 12/16/22 14:13:03.609
------------------------------
• [0.057 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:03.558
    Dec 16 14:13:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 14:13:03.559
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:03.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:03.576
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 12/16/22 14:13:03.579
    STEP: Getting a ResourceQuota 12/16/22 14:13:03.583
    STEP: Updating a ResourceQuota 12/16/22 14:13:03.587
    STEP: Verifying a ResourceQuota was modified 12/16/22 14:13:03.591
    STEP: Deleting a ResourceQuota 12/16/22 14:13:03.595
    STEP: Verifying the deleted ResourceQuota 12/16/22 14:13:03.602
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:03.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5135" for this suite. 12/16/22 14:13:03.609
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:03.615
Dec 16 14:13:03.615: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename limitrange 12/16/22 14:13:03.616
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:03.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:03.632
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 12/16/22 14:13:03.635
STEP: Setting up watch 12/16/22 14:13:03.635
STEP: Submitting a LimitRange 12/16/22 14:13:03.739
STEP: Verifying LimitRange creation was observed 12/16/22 14:13:03.746
STEP: Fetching the LimitRange to ensure it has proper values 12/16/22 14:13:03.746
Dec 16 14:13:03.749: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 16 14:13:03.749: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 12/16/22 14:13:03.749
STEP: Ensuring Pod has resource requirements applied from LimitRange 12/16/22 14:13:03.754
Dec 16 14:13:03.757: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 16 14:13:03.757: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 12/16/22 14:13:03.757
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/16/22 14:13:03.761
Dec 16 14:13:03.765: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 16 14:13:03.765: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 12/16/22 14:13:03.765
STEP: Failing to create a Pod with more than max resources 12/16/22 14:13:03.766
STEP: Updating a LimitRange 12/16/22 14:13:03.768
STEP: Verifying LimitRange updating is effective 12/16/22 14:13:03.772
STEP: Creating a Pod with less than former min resources 12/16/22 14:13:05.777
STEP: Failing to create a Pod with more than max resources 12/16/22 14:13:05.781
STEP: Deleting a LimitRange 12/16/22 14:13:05.783
STEP: Verifying the LimitRange was deleted 12/16/22 14:13:05.789
Dec 16 14:13:10.794: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 12/16/22 14:13:10.794
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:10.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3115" for this suite. 12/16/22 14:13:10.809
------------------------------
• [SLOW TEST] [7.200 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:03.615
    Dec 16 14:13:03.615: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename limitrange 12/16/22 14:13:03.616
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:03.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:03.632
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 12/16/22 14:13:03.635
    STEP: Setting up watch 12/16/22 14:13:03.635
    STEP: Submitting a LimitRange 12/16/22 14:13:03.739
    STEP: Verifying LimitRange creation was observed 12/16/22 14:13:03.746
    STEP: Fetching the LimitRange to ensure it has proper values 12/16/22 14:13:03.746
    Dec 16 14:13:03.749: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 16 14:13:03.749: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 12/16/22 14:13:03.749
    STEP: Ensuring Pod has resource requirements applied from LimitRange 12/16/22 14:13:03.754
    Dec 16 14:13:03.757: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 16 14:13:03.757: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 12/16/22 14:13:03.757
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/16/22 14:13:03.761
    Dec 16 14:13:03.765: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Dec 16 14:13:03.765: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 12/16/22 14:13:03.765
    STEP: Failing to create a Pod with more than max resources 12/16/22 14:13:03.766
    STEP: Updating a LimitRange 12/16/22 14:13:03.768
    STEP: Verifying LimitRange updating is effective 12/16/22 14:13:03.772
    STEP: Creating a Pod with less than former min resources 12/16/22 14:13:05.777
    STEP: Failing to create a Pod with more than max resources 12/16/22 14:13:05.781
    STEP: Deleting a LimitRange 12/16/22 14:13:05.783
    STEP: Verifying the LimitRange was deleted 12/16/22 14:13:05.789
    Dec 16 14:13:10.794: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 12/16/22 14:13:10.794
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:10.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3115" for this suite. 12/16/22 14:13:10.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:10.815
Dec 16 14:13:10.815: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename dns 12/16/22 14:13:10.816
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:10.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:10.831
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/16/22 14:13:10.834
Dec 16 14:13:10.840: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8603  ce92229d-12d2-4ab5-9d8f-c027fb6bb4d2 680980440 0 2022-12-16 14:13:10 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:10 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drqns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drqns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 16 14:13:10.840: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8603" to be "running and ready"
Dec 16 14:13:10.843: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908664ms
Dec 16 14:13:10.843: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:13:12.849: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492315s
Dec 16 14:13:12.849: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Dec 16 14:13:12.849: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 12/16/22 14:13:12.849
Dec 16 14:13:12.849: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8603 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:13:12.849: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:13:12.849: INFO: ExecWithOptions: Clientset creation
Dec 16 14:13:12.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8603/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 12/16/22 14:13:12.984
Dec 16 14:13:12.984: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8603 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:13:12.984: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:13:12.985: INFO: ExecWithOptions: Clientset creation
Dec 16 14:13:12.985: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8603/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 14:13:13.119: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:13.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8603" for this suite. 12/16/22 14:13:13.137
------------------------------
• [2.330 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:10.815
    Dec 16 14:13:10.815: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename dns 12/16/22 14:13:10.816
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:10.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:10.831
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/16/22 14:13:10.834
    Dec 16 14:13:10.840: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8603  ce92229d-12d2-4ab5-9d8f-c027fb6bb4d2 680980440 0 2022-12-16 14:13:10 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:10 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drqns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drqns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 16 14:13:10.840: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8603" to be "running and ready"
    Dec 16 14:13:10.843: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908664ms
    Dec 16 14:13:10.843: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:13:12.849: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492315s
    Dec 16 14:13:12.849: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Dec 16 14:13:12.849: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 12/16/22 14:13:12.849
    Dec 16 14:13:12.849: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8603 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:13:12.849: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:13:12.849: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:13:12.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8603/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 12/16/22 14:13:12.984
    Dec 16 14:13:12.984: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8603 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:13:12.984: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:13:12.985: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:13:12.985: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8603/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 14:13:13.119: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:13.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8603" for this suite. 12/16/22 14:13:13.137
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:13.146
Dec 16 14:13:13.146: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename watch 12/16/22 14:13:13.147
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:13.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:13.161
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 12/16/22 14:13:13.166
STEP: creating a new configmap 12/16/22 14:13:13.167
STEP: modifying the configmap once 12/16/22 14:13:13.171
STEP: changing the label value of the configmap 12/16/22 14:13:13.178
STEP: Expecting to observe a delete notification for the watched object 12/16/22 14:13:13.185
Dec 16 14:13:13.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980523 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:13:13.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980524 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:13:13.185: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980525 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 12/16/22 14:13:13.185
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/16/22 14:13:13.192
STEP: changing the label value of the configmap back 12/16/22 14:13:23.195
STEP: modifying the configmap a third time 12/16/22 14:13:23.205
STEP: deleting the configmap 12/16/22 14:13:23.213
STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/16/22 14:13:23.219
Dec 16 14:13:23.219: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980902 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:13:23.219: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980903 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:13:23.219: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980904 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:23.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-629" for this suite. 12/16/22 14:13:23.224
------------------------------
• [SLOW TEST] [10.084 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:13.146
    Dec 16 14:13:13.146: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename watch 12/16/22 14:13:13.147
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:13.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:13.161
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 12/16/22 14:13:13.166
    STEP: creating a new configmap 12/16/22 14:13:13.167
    STEP: modifying the configmap once 12/16/22 14:13:13.171
    STEP: changing the label value of the configmap 12/16/22 14:13:13.178
    STEP: Expecting to observe a delete notification for the watched object 12/16/22 14:13:13.185
    Dec 16 14:13:13.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980523 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:13:13.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980524 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:13:13.185: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980525 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 12/16/22 14:13:13.185
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/16/22 14:13:13.192
    STEP: changing the label value of the configmap back 12/16/22 14:13:23.195
    STEP: modifying the configmap a third time 12/16/22 14:13:23.205
    STEP: deleting the configmap 12/16/22 14:13:23.213
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/16/22 14:13:23.219
    Dec 16 14:13:23.219: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980902 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:13:23.219: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980903 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:13:23.219: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-629  0c18eb04-35fd-4f63-be58-b9ae0793b04d 680980904 0 2022-12-16 14:13:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-16 14:13:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:23.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-629" for this suite. 12/16/22 14:13:23.224
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:23.23
Dec 16 14:13:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 14:13:23.231
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:23.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:23.246
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 12/16/22 14:13:23.248
STEP: Wait for the Deployment to create new ReplicaSet 12/16/22 14:13:23.253
STEP: delete the deployment 12/16/22 14:13:23.766
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/16/22 14:13:23.772
STEP: Gathering metrics 12/16/22 14:13:24.297
W1216 14:13:24.305262      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 14:13:24.305: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 14:13:24.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9641" for this suite. 12/16/22 14:13:24.309
------------------------------
• [1.086 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:23.23
    Dec 16 14:13:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 14:13:23.231
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:23.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:23.246
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 12/16/22 14:13:23.248
    STEP: Wait for the Deployment to create new ReplicaSet 12/16/22 14:13:23.253
    STEP: delete the deployment 12/16/22 14:13:23.766
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/16/22 14:13:23.772
    STEP: Gathering metrics 12/16/22 14:13:24.297
    W1216 14:13:24.305262      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 14:13:24.305: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:13:24.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9641" for this suite. 12/16/22 14:13:24.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:13:24.317
Dec 16 14:13:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-watch 12/16/22 14:13:24.318
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:24.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:24.333
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Dec 16 14:13:24.335: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Creating first CR  12/16/22 14:13:26.898
Dec 16 14:13:26.903: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:26Z]] name:name1 resourceVersion:680981035 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 12/16/22 14:13:36.904
Dec 16 14:13:36.910: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:36Z]] name:name2 resourceVersion:680981385 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 12/16/22 14:13:46.911
Dec 16 14:13:46.918: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:46Z]] name:name1 resourceVersion:680981559 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 12/16/22 14:13:56.918
Dec 16 14:13:56.927: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:56Z]] name:name2 resourceVersion:680981691 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 12/16/22 14:14:06.928
Dec 16 14:14:06.935: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:46Z]] name:name1 resourceVersion:680981921 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 12/16/22 14:14:16.936
Dec 16 14:14:16.945: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:56Z]] name:name2 resourceVersion:680982171 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:14:27.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-9086" for this suite. 12/16/22 14:14:27.463
------------------------------
• [SLOW TEST] [63.336 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:13:24.317
    Dec 16 14:13:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-watch 12/16/22 14:13:24.318
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:13:24.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:13:24.333
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Dec 16 14:13:24.335: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Creating first CR  12/16/22 14:13:26.898
    Dec 16 14:13:26.903: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:26Z]] name:name1 resourceVersion:680981035 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 12/16/22 14:13:36.904
    Dec 16 14:13:36.910: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:36Z]] name:name2 resourceVersion:680981385 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 12/16/22 14:13:46.911
    Dec 16 14:13:46.918: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:46Z]] name:name1 resourceVersion:680981559 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 12/16/22 14:13:56.918
    Dec 16 14:13:56.927: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:56Z]] name:name2 resourceVersion:680981691 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 12/16/22 14:14:06.928
    Dec 16 14:14:06.935: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:46Z]] name:name1 resourceVersion:680981921 uid:05d18c72-dde4-4fb9-aa34-b7717c9d3776] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 12/16/22 14:14:16.936
    Dec 16 14:14:16.945: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-16T14:13:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-16T14:13:56Z]] name:name2 resourceVersion:680982171 uid:d65697bb-f359-40ce-b93c-541db88e89e8] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:14:27.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-9086" for this suite. 12/16/22 14:14:27.463
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:14:27.653
Dec 16 14:14:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 14:14:27.654
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:28.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:28.008
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-569 12/16/22 14:14:28.045
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 12/16/22 14:14:28.099
STEP: Creating pod with conflicting port in namespace statefulset-569 12/16/22 14:14:28.104
STEP: Waiting until pod test-pod will start running in namespace statefulset-569 12/16/22 14:14:28.112
Dec 16 14:14:28.112: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-569" to be "running"
Dec 16 14:14:28.118: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574127ms
Dec 16 14:14:30.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012104916s
Dec 16 14:14:30.124: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-569 12/16/22 14:14:30.124
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-569 12/16/22 14:14:30.131
Dec 16 14:14:30.165: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Pending. Waiting for statefulset controller to delete.
Dec 16 14:14:30.201: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Failed. Waiting for statefulset controller to delete.
Dec 16 14:14:30.215: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Failed. Waiting for statefulset controller to delete.
Dec 16 14:14:30.241: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-569
STEP: Removing pod with conflicting port in namespace statefulset-569 12/16/22 14:14:30.241
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-569 and will be in running state 12/16/22 14:14:30.267
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 14:14:32.284: INFO: Deleting all statefulset in ns statefulset-569
Dec 16 14:14:32.288: INFO: Scaling statefulset ss to 0
Dec 16 14:14:42.307: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:14:42.311: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:14:42.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-569" for this suite. 12/16/22 14:14:42.328
------------------------------
• [SLOW TEST] [14.681 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:14:27.653
    Dec 16 14:14:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 14:14:27.654
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:28.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:28.008
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-569 12/16/22 14:14:28.045
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 12/16/22 14:14:28.099
    STEP: Creating pod with conflicting port in namespace statefulset-569 12/16/22 14:14:28.104
    STEP: Waiting until pod test-pod will start running in namespace statefulset-569 12/16/22 14:14:28.112
    Dec 16 14:14:28.112: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-569" to be "running"
    Dec 16 14:14:28.118: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574127ms
    Dec 16 14:14:30.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012104916s
    Dec 16 14:14:30.124: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-569 12/16/22 14:14:30.124
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-569 12/16/22 14:14:30.131
    Dec 16 14:14:30.165: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Pending. Waiting for statefulset controller to delete.
    Dec 16 14:14:30.201: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 16 14:14:30.215: INFO: Observed stateful pod in namespace: statefulset-569, name: ss-0, uid: 97179120-ccd9-4fd9-bfe5-dc1f59ec60ea, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 16 14:14:30.241: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-569
    STEP: Removing pod with conflicting port in namespace statefulset-569 12/16/22 14:14:30.241
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-569 and will be in running state 12/16/22 14:14:30.267
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 14:14:32.284: INFO: Deleting all statefulset in ns statefulset-569
    Dec 16 14:14:32.288: INFO: Scaling statefulset ss to 0
    Dec 16 14:14:42.307: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:14:42.311: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:14:42.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-569" for this suite. 12/16/22 14:14:42.328
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:14:42.334
Dec 16 14:14:42.334: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption 12/16/22 14:14:42.335
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:42.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:42.349
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 12/16/22 14:14:42.356
STEP: Updating PodDisruptionBudget status 12/16/22 14:14:44.365
STEP: Waiting for all pods to be running 12/16/22 14:14:44.373
Dec 16 14:14:44.377: INFO: running pods: 0 < 1
STEP: locating a running pod 12/16/22 14:14:46.383
STEP: Waiting for the pdb to be processed 12/16/22 14:14:46.395
STEP: Patching PodDisruptionBudget status 12/16/22 14:14:46.403
STEP: Waiting for the pdb to be processed 12/16/22 14:14:46.413
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 16 14:14:46.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1656" for this suite. 12/16/22 14:14:46.421
------------------------------
• [4.093 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:14:42.334
    Dec 16 14:14:42.334: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption 12/16/22 14:14:42.335
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:42.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:42.349
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 12/16/22 14:14:42.356
    STEP: Updating PodDisruptionBudget status 12/16/22 14:14:44.365
    STEP: Waiting for all pods to be running 12/16/22 14:14:44.373
    Dec 16 14:14:44.377: INFO: running pods: 0 < 1
    STEP: locating a running pod 12/16/22 14:14:46.383
    STEP: Waiting for the pdb to be processed 12/16/22 14:14:46.395
    STEP: Patching PodDisruptionBudget status 12/16/22 14:14:46.403
    STEP: Waiting for the pdb to be processed 12/16/22 14:14:46.413
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:14:46.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1656" for this suite. 12/16/22 14:14:46.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:14:46.427
Dec 16 14:14:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:14:46.428
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:46.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:46.443
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:14:46.458
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:14:46.601
STEP: Deploying the webhook pod 12/16/22 14:14:46.611
STEP: Wait for the deployment to be ready 12/16/22 14:14:46.622
Dec 16 14:14:46.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:14:48.641
STEP: Verifying the service has paired with the endpoint 12/16/22 14:14:48.654
Dec 16 14:14:49.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/16/22 14:14:49.658
STEP: create a pod that should be updated by the webhook 12/16/22 14:14:49.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:14:49.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8103" for this suite. 12/16/22 14:14:49.788
STEP: Destroying namespace "webhook-8103-markers" for this suite. 12/16/22 14:14:49.794
------------------------------
• [3.373 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:14:46.427
    Dec 16 14:14:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:14:46.428
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:46.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:46.443
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:14:46.458
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:14:46.601
    STEP: Deploying the webhook pod 12/16/22 14:14:46.611
    STEP: Wait for the deployment to be ready 12/16/22 14:14:46.622
    Dec 16 14:14:46.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:14:48.641
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:14:48.654
    Dec 16 14:14:49.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/16/22 14:14:49.658
    STEP: create a pod that should be updated by the webhook 12/16/22 14:14:49.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:14:49.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8103" for this suite. 12/16/22 14:14:49.788
    STEP: Destroying namespace "webhook-8103-markers" for this suite. 12/16/22 14:14:49.794
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:14:49.8
Dec 16 14:14:49.800: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:14:49.801
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:49.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:49.816
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-748bfaf3-39e8-4a94-83c3-867c31ce35c2 12/16/22 14:14:49.818
STEP: Creating a pod to test consume configMaps 12/16/22 14:14:49.824
Dec 16 14:14:49.831: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2" in namespace "projected-627" to be "Succeeded or Failed"
Dec 16 14:14:49.835: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442877ms
Dec 16 14:14:51.839: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007882775s
Dec 16 14:14:53.842: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010647297s
STEP: Saw pod success 12/16/22 14:14:53.842
Dec 16 14:14:53.842: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2" satisfied condition "Succeeded or Failed"
Dec 16 14:14:53.846: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 14:14:53.896
Dec 16 14:14:53.906: INFO: Waiting for pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 to disappear
Dec 16 14:14:53.909: INFO: Pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 14:14:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-627" for this suite. 12/16/22 14:14:53.913
------------------------------
• [4.118 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:14:49.8
    Dec 16 14:14:49.800: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:14:49.801
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:49.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:49.816
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-748bfaf3-39e8-4a94-83c3-867c31ce35c2 12/16/22 14:14:49.818
    STEP: Creating a pod to test consume configMaps 12/16/22 14:14:49.824
    Dec 16 14:14:49.831: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2" in namespace "projected-627" to be "Succeeded or Failed"
    Dec 16 14:14:49.835: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442877ms
    Dec 16 14:14:51.839: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007882775s
    Dec 16 14:14:53.842: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010647297s
    STEP: Saw pod success 12/16/22 14:14:53.842
    Dec 16 14:14:53.842: INFO: Pod "pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2" satisfied condition "Succeeded or Failed"
    Dec 16 14:14:53.846: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 14:14:53.896
    Dec 16 14:14:53.906: INFO: Waiting for pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 to disappear
    Dec 16 14:14:53.909: INFO: Pod pod-projected-configmaps-f01259da-eaed-490d-b468-69bfaf95f8a2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:14:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-627" for this suite. 12/16/22 14:14:53.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:14:53.925
Dec 16 14:14:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:14:53.926
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:53.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:53.942
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6791 12/16/22 14:14:53.945
STEP: creating service affinity-nodeport-transition in namespace services-6791 12/16/22 14:14:53.945
STEP: creating replication controller affinity-nodeport-transition in namespace services-6791 12/16/22 14:14:53.962
I1216 14:14:53.966900      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6791, replica count: 3
I1216 14:14:57.017499      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 14:14:57.029: INFO: Creating new exec pod
Dec 16 14:14:57.033: INFO: Waiting up to 5m0s for pod "execpod-affinityn5q9q" in namespace "services-6791" to be "running"
Dec 16 14:14:57.036: INFO: Pod "execpod-affinityn5q9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926617ms
Dec 16 14:14:59.041: INFO: Pod "execpod-affinityn5q9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007299222s
Dec 16 14:14:59.041: INFO: Pod "execpod-affinityn5q9q" satisfied condition "running"
Dec 16 14:15:00.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Dec 16 14:15:00.233: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 16 14:15:00.233: INFO: stdout: ""
Dec 16 14:15:00.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 10.111.161.82 80'
Dec 16 14:15:00.425: INFO: stderr: "+ nc -v -z -w 2 10.111.161.82 80\nConnection to 10.111.161.82 80 port [tcp/http] succeeded!\n"
Dec 16 14:15:00.425: INFO: stdout: ""
Dec 16 14:15:00.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 30193'
Dec 16 14:15:00.638: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 30193\nConnection to 85.217.161.242 30193 port [tcp/*] succeeded!\n"
Dec 16 14:15:00.638: INFO: stdout: ""
Dec 16 14:15:00.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 85.217.161.222 30193'
Dec 16 14:15:00.828: INFO: stderr: "+ nc -v -z -w 2 85.217.161.222 30193\nConnection to 85.217.161.222 30193 port [tcp/*] succeeded!\n"
Dec 16 14:15:00.828: INFO: stdout: ""
Dec 16 14:15:00.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:30193/ ; done'
Dec 16 14:15:01.115: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n"
Dec 16 14:15:01.115: INFO: stdout: "\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-rs6f6\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-rs6f6"
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-rs6f6
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-rs6f6
Dec 16 14:15:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:30193/ ; done'
Dec 16 14:15:01.399: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n"
Dec 16 14:15:01.399: INFO: stdout: "\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz"
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
Dec 16 14:15:01.399: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6791, will wait for the garbage collector to delete the pods 12/16/22 14:15:01.41
Dec 16 14:15:01.474: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.535334ms
Dec 16 14:15:01.575: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.626155ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:03.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6791" for this suite. 12/16/22 14:15:03.503
------------------------------
• [SLOW TEST] [9.584 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:14:53.925
    Dec 16 14:14:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:14:53.926
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:14:53.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:14:53.942
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6791 12/16/22 14:14:53.945
    STEP: creating service affinity-nodeport-transition in namespace services-6791 12/16/22 14:14:53.945
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6791 12/16/22 14:14:53.962
    I1216 14:14:53.966900      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6791, replica count: 3
    I1216 14:14:57.017499      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 14:14:57.029: INFO: Creating new exec pod
    Dec 16 14:14:57.033: INFO: Waiting up to 5m0s for pod "execpod-affinityn5q9q" in namespace "services-6791" to be "running"
    Dec 16 14:14:57.036: INFO: Pod "execpod-affinityn5q9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926617ms
    Dec 16 14:14:59.041: INFO: Pod "execpod-affinityn5q9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007299222s
    Dec 16 14:14:59.041: INFO: Pod "execpod-affinityn5q9q" satisfied condition "running"
    Dec 16 14:15:00.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Dec 16 14:15:00.233: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Dec 16 14:15:00.233: INFO: stdout: ""
    Dec 16 14:15:00.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 10.111.161.82 80'
    Dec 16 14:15:00.425: INFO: stderr: "+ nc -v -z -w 2 10.111.161.82 80\nConnection to 10.111.161.82 80 port [tcp/http] succeeded!\n"
    Dec 16 14:15:00.425: INFO: stdout: ""
    Dec 16 14:15:00.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 30193'
    Dec 16 14:15:00.638: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 30193\nConnection to 85.217.161.242 30193 port [tcp/*] succeeded!\n"
    Dec 16 14:15:00.638: INFO: stdout: ""
    Dec 16 14:15:00.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c nc -v -z -w 2 85.217.161.222 30193'
    Dec 16 14:15:00.828: INFO: stderr: "+ nc -v -z -w 2 85.217.161.222 30193\nConnection to 85.217.161.222 30193 port [tcp/*] succeeded!\n"
    Dec 16 14:15:00.828: INFO: stdout: ""
    Dec 16 14:15:00.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:30193/ ; done'
    Dec 16 14:15:01.115: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n"
    Dec 16 14:15:01.115: INFO: stdout: "\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-rs6f6\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-jrmbg\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-rs6f6"
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-rs6f6
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-jrmbg
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.115: INFO: Received response from host: affinity-nodeport-transition-rs6f6
    Dec 16 14:15:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-6791 exec execpod-affinityn5q9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:30193/ ; done'
    Dec 16 14:15:01.399: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:30193/\n"
    Dec 16 14:15:01.399: INFO: stdout: "\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz\naffinity-nodeport-transition-cs7hz"
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Received response from host: affinity-nodeport-transition-cs7hz
    Dec 16 14:15:01.399: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6791, will wait for the garbage collector to delete the pods 12/16/22 14:15:01.41
    Dec 16 14:15:01.474: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.535334ms
    Dec 16 14:15:01.575: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.626155ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:03.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6791" for this suite. 12/16/22 14:15:03.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:03.51
Dec 16 14:15:03.510: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename subpath 12/16/22 14:15:03.511
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:03.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:03.538
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/16/22 14:15:03.541
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-bmzp 12/16/22 14:15:03.549
STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:15:03.549
Dec 16 14:15:03.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bmzp" in namespace "subpath-121" to be "Succeeded or Failed"
Dec 16 14:15:03.560: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.178895ms
Dec 16 14:15:05.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009459412s
Dec 16 14:15:08.511: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 4.954604733s
Dec 16 14:15:09.565: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 6.008524695s
Dec 16 14:15:11.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008926s
Dec 16 14:15:13.567: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010510896s
Dec 16 14:15:15.565: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008243579s
Dec 16 14:15:17.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009267221s
Dec 16 14:15:19.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 16.009388225s
Dec 16 14:15:21.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00928076s
Dec 16 14:15:23.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009278268s
Dec 16 14:15:25.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=false. Elapsed: 22.009083117s
Dec 16 14:15:27.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009041681s
STEP: Saw pod success 12/16/22 14:15:27.566
Dec 16 14:15:27.566: INFO: Pod "pod-subpath-test-projected-bmzp" satisfied condition "Succeeded or Failed"
Dec 16 14:15:27.570: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-projected-bmzp container test-container-subpath-projected-bmzp: <nil>
STEP: delete the pod 12/16/22 14:15:27.579
Dec 16 14:15:27.588: INFO: Waiting for pod pod-subpath-test-projected-bmzp to disappear
Dec 16 14:15:27.591: INFO: Pod pod-subpath-test-projected-bmzp no longer exists
STEP: Deleting pod pod-subpath-test-projected-bmzp 12/16/22 14:15:27.591
Dec 16 14:15:27.591: INFO: Deleting pod "pod-subpath-test-projected-bmzp" in namespace "subpath-121"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-121" for this suite. 12/16/22 14:15:27.598
------------------------------
• [SLOW TEST] [24.094 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:03.51
    Dec 16 14:15:03.510: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename subpath 12/16/22 14:15:03.511
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:03.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:03.538
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/16/22 14:15:03.541
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-bmzp 12/16/22 14:15:03.549
    STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:15:03.549
    Dec 16 14:15:03.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bmzp" in namespace "subpath-121" to be "Succeeded or Failed"
    Dec 16 14:15:03.560: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.178895ms
    Dec 16 14:15:05.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009459412s
    Dec 16 14:15:08.511: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 4.954604733s
    Dec 16 14:15:09.565: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 6.008524695s
    Dec 16 14:15:11.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 8.008926s
    Dec 16 14:15:13.567: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010510896s
    Dec 16 14:15:15.565: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008243579s
    Dec 16 14:15:17.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009267221s
    Dec 16 14:15:19.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 16.009388225s
    Dec 16 14:15:21.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00928076s
    Dec 16 14:15:23.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=true. Elapsed: 20.009278268s
    Dec 16 14:15:25.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Running", Reason="", readiness=false. Elapsed: 22.009083117s
    Dec 16 14:15:27.566: INFO: Pod "pod-subpath-test-projected-bmzp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009041681s
    STEP: Saw pod success 12/16/22 14:15:27.566
    Dec 16 14:15:27.566: INFO: Pod "pod-subpath-test-projected-bmzp" satisfied condition "Succeeded or Failed"
    Dec 16 14:15:27.570: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-projected-bmzp container test-container-subpath-projected-bmzp: <nil>
    STEP: delete the pod 12/16/22 14:15:27.579
    Dec 16 14:15:27.588: INFO: Waiting for pod pod-subpath-test-projected-bmzp to disappear
    Dec 16 14:15:27.591: INFO: Pod pod-subpath-test-projected-bmzp no longer exists
    STEP: Deleting pod pod-subpath-test-projected-bmzp 12/16/22 14:15:27.591
    Dec 16 14:15:27.591: INFO: Deleting pod "pod-subpath-test-projected-bmzp" in namespace "subpath-121"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-121" for this suite. 12/16/22 14:15:27.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:27.605
Dec 16 14:15:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:15:27.605
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:27.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:27.619
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Dec 16 14:15:27.621: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 14:15:29.376
Dec 16 14:15:29.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 create -f -'
Dec 16 14:15:30.007: INFO: stderr: ""
Dec 16 14:15:30.007: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 16 14:15:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 delete e2e-test-crd-publish-openapi-1905-crds test-cr'
Dec 16 14:15:30.088: INFO: stderr: ""
Dec 16 14:15:30.088: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 16 14:15:30.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 apply -f -'
Dec 16 14:15:30.776: INFO: stderr: ""
Dec 16 14:15:30.776: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 16 14:15:30.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 delete e2e-test-crd-publish-openapi-1905-crds test-cr'
Dec 16 14:15:30.854: INFO: stderr: ""
Dec 16 14:15:30.855: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/16/22 14:15:30.855
Dec 16 14:15:30.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 explain e2e-test-crd-publish-openapi-1905-crds'
Dec 16 14:15:31.049: INFO: stderr: ""
Dec 16 14:15:31.049: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1905-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:32.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4352" for this suite. 12/16/22 14:15:32.9
------------------------------
• [SLOW TEST] [5.301 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:27.605
    Dec 16 14:15:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:15:27.605
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:27.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:27.619
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Dec 16 14:15:27.621: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 14:15:29.376
    Dec 16 14:15:29.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 create -f -'
    Dec 16 14:15:30.007: INFO: stderr: ""
    Dec 16 14:15:30.007: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 16 14:15:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 delete e2e-test-crd-publish-openapi-1905-crds test-cr'
    Dec 16 14:15:30.088: INFO: stderr: ""
    Dec 16 14:15:30.088: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Dec 16 14:15:30.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 apply -f -'
    Dec 16 14:15:30.776: INFO: stderr: ""
    Dec 16 14:15:30.776: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 16 14:15:30.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 --namespace=crd-publish-openapi-4352 delete e2e-test-crd-publish-openapi-1905-crds test-cr'
    Dec 16 14:15:30.854: INFO: stderr: ""
    Dec 16 14:15:30.855: INFO: stdout: "e2e-test-crd-publish-openapi-1905-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/16/22 14:15:30.855
    Dec 16 14:15:30.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-4352 explain e2e-test-crd-publish-openapi-1905-crds'
    Dec 16 14:15:31.049: INFO: stderr: ""
    Dec 16 14:15:31.049: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1905-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:32.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4352" for this suite. 12/16/22 14:15:32.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:32.906
Dec 16 14:15:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename server-version 12/16/22 14:15:32.907
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:32.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:32.924
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 12/16/22 14:15:32.927
STEP: Confirm major version 12/16/22 14:15:32.928
Dec 16 14:15:32.928: INFO: Major version: 1
STEP: Confirm minor version 12/16/22 14:15:32.928
Dec 16 14:15:32.928: INFO: cleanMinorVersion: 26
Dec 16 14:15:32.928: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:32.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-7298" for this suite. 12/16/22 14:15:32.932
------------------------------
• [0.031 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:32.906
    Dec 16 14:15:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename server-version 12/16/22 14:15:32.907
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:32.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:32.924
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 12/16/22 14:15:32.927
    STEP: Confirm major version 12/16/22 14:15:32.928
    Dec 16 14:15:32.928: INFO: Major version: 1
    STEP: Confirm minor version 12/16/22 14:15:32.928
    Dec 16 14:15:32.928: INFO: cleanMinorVersion: 26
    Dec 16 14:15:32.928: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:32.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-7298" for this suite. 12/16/22 14:15:32.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:32.938
Dec 16 14:15:32.938: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename containers 12/16/22 14:15:32.939
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:32.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:32.956
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 12/16/22 14:15:32.959
Dec 16 14:15:32.965: INFO: Waiting up to 5m0s for pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360" in namespace "containers-6126" to be "Succeeded or Failed"
Dec 16 14:15:32.968: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481856ms
Dec 16 14:15:34.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008419097s
Dec 16 14:15:36.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762681s
STEP: Saw pod success 12/16/22 14:15:36.974
Dec 16 14:15:36.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360" satisfied condition "Succeeded or Failed"
Dec 16 14:15:36.977: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 14:15:37.03
Dec 16 14:15:37.043: INFO: Waiting for pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 to disappear
Dec 16 14:15:37.046: INFO: Pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:37.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6126" for this suite. 12/16/22 14:15:37.051
------------------------------
• [4.119 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:32.938
    Dec 16 14:15:32.938: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename containers 12/16/22 14:15:32.939
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:32.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:32.956
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 12/16/22 14:15:32.959
    Dec 16 14:15:32.965: INFO: Waiting up to 5m0s for pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360" in namespace "containers-6126" to be "Succeeded or Failed"
    Dec 16 14:15:32.968: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481856ms
    Dec 16 14:15:34.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008419097s
    Dec 16 14:15:36.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762681s
    STEP: Saw pod success 12/16/22 14:15:36.974
    Dec 16 14:15:36.974: INFO: Pod "client-containers-f12f1785-47e7-43db-9df9-97f44ca32360" satisfied condition "Succeeded or Failed"
    Dec 16 14:15:36.977: INFO: Trying to get logs from node pool-a3802-fsxxd pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 14:15:37.03
    Dec 16 14:15:37.043: INFO: Waiting for pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 to disappear
    Dec 16 14:15:37.046: INFO: Pod client-containers-f12f1785-47e7-43db-9df9-97f44ca32360 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:37.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6126" for this suite. 12/16/22 14:15:37.051
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:37.058
Dec 16 14:15:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename disruption 12/16/22 14:15:37.059
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:37.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:37.076
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 12/16/22 14:15:37.079
STEP: Waiting for the pdb to be processed 12/16/22 14:15:37.083
STEP: First trying to evict a pod which shouldn't be evictable 12/16/22 14:15:39.098
STEP: Waiting for all pods to be running 12/16/22 14:15:39.098
Dec 16 14:15:39.101: INFO: pods: 0 < 3
STEP: locating a running pod 12/16/22 14:15:41.107
STEP: Updating the pdb to allow a pod to be evicted 12/16/22 14:15:41.119
STEP: Waiting for the pdb to be processed 12/16/22 14:15:41.127
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/16/22 14:15:43.139
STEP: Waiting for all pods to be running 12/16/22 14:15:43.139
STEP: Waiting for the pdb to observed all healthy pods 12/16/22 14:15:43.143
STEP: Patching the pdb to disallow a pod to be evicted 12/16/22 14:15:43.169
STEP: Waiting for the pdb to be processed 12/16/22 14:15:43.182
STEP: Waiting for all pods to be running 12/16/22 14:15:45.19
STEP: locating a running pod 12/16/22 14:15:45.195
STEP: Deleting the pdb to allow a pod to be evicted 12/16/22 14:15:45.205
STEP: Waiting for the pdb to be deleted 12/16/22 14:15:45.21
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/16/22 14:15:45.213
STEP: Waiting for all pods to be running 12/16/22 14:15:45.213
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:45.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1185" for this suite. 12/16/22 14:15:45.236
------------------------------
• [SLOW TEST] [8.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:37.058
    Dec 16 14:15:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename disruption 12/16/22 14:15:37.059
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:37.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:37.076
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 12/16/22 14:15:37.079
    STEP: Waiting for the pdb to be processed 12/16/22 14:15:37.083
    STEP: First trying to evict a pod which shouldn't be evictable 12/16/22 14:15:39.098
    STEP: Waiting for all pods to be running 12/16/22 14:15:39.098
    Dec 16 14:15:39.101: INFO: pods: 0 < 3
    STEP: locating a running pod 12/16/22 14:15:41.107
    STEP: Updating the pdb to allow a pod to be evicted 12/16/22 14:15:41.119
    STEP: Waiting for the pdb to be processed 12/16/22 14:15:41.127
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/16/22 14:15:43.139
    STEP: Waiting for all pods to be running 12/16/22 14:15:43.139
    STEP: Waiting for the pdb to observed all healthy pods 12/16/22 14:15:43.143
    STEP: Patching the pdb to disallow a pod to be evicted 12/16/22 14:15:43.169
    STEP: Waiting for the pdb to be processed 12/16/22 14:15:43.182
    STEP: Waiting for all pods to be running 12/16/22 14:15:45.19
    STEP: locating a running pod 12/16/22 14:15:45.195
    STEP: Deleting the pdb to allow a pod to be evicted 12/16/22 14:15:45.205
    STEP: Waiting for the pdb to be deleted 12/16/22 14:15:45.21
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/16/22 14:15:45.213
    STEP: Waiting for all pods to be running 12/16/22 14:15:45.213
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:45.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1185" for this suite. 12/16/22 14:15:45.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:45.244
Dec 16 14:15:45.244: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-pred 12/16/22 14:15:45.244
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:45.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:45.28
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 16 14:15:45.283: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 14:15:45.290: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 14:15:45.294: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
Dec 16 14:15:45.300: INFO: rs-55fwt from disruption-1185 started at 2022-12-16 14:15:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container donothing ready: false, restart count 0
Dec 16 14:15:45.300: INFO: rs-5rn9s from disruption-1185 started at 2022-12-16 14:15:43 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container donothing ready: true, restart count 0
Dec 16 14:15:45.300: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:15:45.300: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:15:45.300: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:15:45.300: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:15:45.300: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 14:15:45.300: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:15:45.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:15:45.300: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:15:45.300: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
Dec 16 14:15:45.306: INFO: rs-brmwn from disruption-1185 started at 2022-12-16 14:15:39 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.306: INFO: 	Container donothing ready: true, restart count 0
Dec 16 14:15:45.306: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.306: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:15:45.306: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.306: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:15:45.306: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
Dec 16 14:15:45.306: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:15:45.306: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:15:45.306: INFO: webhook-to-be-mutated from webhook-8103 started at 2022-12-16 14:14:49 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.306: INFO: 	Container example ready: false, restart count 0
Dec 16 14:15:45.306: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
Dec 16 14:15:45.313: INFO: rs-cg6t2 from disruption-1185 started at 2022-12-16 14:15:39 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container donothing ready: true, restart count 0
Dec 16 14:15:45.313: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 16 14:15:45.313: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:15:45.313: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:15:45.313: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:15:45.313: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:15:45.313: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container e2e ready: true, restart count 0
Dec 16 14:15:45.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:15:45.313: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:15:45.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:15:45.313: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node pool-a3802-ehprg 12/16/22 14:15:45.33
STEP: verifying the node has the label node pool-a3802-fsxxd 12/16/22 14:15:45.341
STEP: verifying the node has the label node pool-a3802-oewtd 12/16/22 14:15:45.351
Dec 16 14:15:45.362: INFO: Pod rs-55fwt requesting resource cpu=0m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod rs-5rn9s requesting resource cpu=0m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod rs-brmwn requesting resource cpu=0m on Node pool-a3802-fsxxd
Dec 16 14:15:45.362: INFO: Pod rs-cg6t2 requesting resource cpu=0m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod calico-kube-controllers-5f94594857-mnd6f requesting resource cpu=0m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod calico-node-5m9wp requesting resource cpu=250m on Node pool-a3802-fsxxd
Dec 16 14:15:45.362: INFO: Pod calico-node-g9rxj requesting resource cpu=250m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod calico-node-h8rhd requesting resource cpu=250m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod coredns-7cd5b7d6b4-mf5b4 requesting resource cpu=100m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod coredns-7cd5b7d6b4-zz7zw requesting resource cpu=100m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod konnectivity-agent-5f7cbf88d-fpk9f requesting resource cpu=100m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod konnectivity-agent-5f7cbf88d-q4fff requesting resource cpu=100m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod kube-proxy-csksk requesting resource cpu=0m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod kube-proxy-f6k5s requesting resource cpu=0m on Node pool-a3802-fsxxd
Dec 16 14:15:45.362: INFO: Pod kube-proxy-k8cjh requesting resource cpu=0m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod sonobuoy requesting resource cpu=0m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod sonobuoy-e2e-job-779c6bd4e6b34c88 requesting resource cpu=0m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r requesting resource cpu=0m on Node pool-a3802-ehprg
Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv requesting resource cpu=0m on Node pool-a3802-oewtd
Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh requesting resource cpu=0m on Node pool-a3802-fsxxd
Dec 16 14:15:45.362: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node pool-a3802-fsxxd
STEP: Starting Pods to consume most of the cluster CPU. 12/16/22 14:15:45.362
Dec 16 14:15:45.362: INFO: Creating a pod which consumes cpu=2345m on Node pool-a3802-ehprg
Dec 16 14:15:45.373: INFO: Creating a pod which consumes cpu=2485m on Node pool-a3802-fsxxd
Dec 16 14:15:45.379: INFO: Creating a pod which consumes cpu=2345m on Node pool-a3802-oewtd
Dec 16 14:15:45.385: INFO: Waiting up to 5m0s for pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a" in namespace "sched-pred-6271" to be "running"
Dec 16 14:15:45.388: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.661278ms
Dec 16 14:15:47.394: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008760251s
Dec 16 14:15:47.394: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a" satisfied condition "running"
Dec 16 14:15:47.394: INFO: Waiting up to 5m0s for pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2" in namespace "sched-pred-6271" to be "running"
Dec 16 14:15:47.398: INFO: Pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2": Phase="Running", Reason="", readiness=true. Elapsed: 3.55851ms
Dec 16 14:15:47.398: INFO: Pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2" satisfied condition "running"
Dec 16 14:15:47.398: INFO: Waiting up to 5m0s for pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa" in namespace "sched-pred-6271" to be "running"
Dec 16 14:15:47.401: INFO: Pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa": Phase="Running", Reason="", readiness=true. Elapsed: 3.602184ms
Dec 16 14:15:47.401: INFO: Pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/16/22 14:15:47.401
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314bad85194889], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2 to pool-a3802-fsxxd] 12/16/22 14:15:47.405
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badaabb875a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.405
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badab612673], Reason = [Created], Message = [Created container filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2] 12/16/22 14:15:47.405
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badb0a71f39], Reason = [Started], Message = [Started container filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2] 12/16/22 14:15:47.405
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314bad84bdc5c3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a to pool-a3802-ehprg] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badab78614f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badac3da408], Reason = [Created], Message = [Created container filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badb1e811be], Reason = [Started], Message = [Started container filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314bad85753339], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa to pool-a3802-oewtd] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badab3d4334], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badabf75dcc], Reason = [Created], Message = [Created container filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badb019f990], Reason = [Started], Message = [Started container filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa] 12/16/22 14:15:47.406
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17314badfde8cc7b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 12/16/22 14:15:47.416
STEP: removing the label node off the node pool-a3802-ehprg 12/16/22 14:15:48.418
STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.43
STEP: removing the label node off the node pool-a3802-fsxxd 12/16/22 14:15:48.433
STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.445
STEP: removing the label node off the node pool-a3802-oewtd 12/16/22 14:15:48.448
STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.457
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:15:48.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6271" for this suite. 12/16/22 14:15:48.463
------------------------------
• [3.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:45.244
    Dec 16 14:15:45.244: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-pred 12/16/22 14:15:45.244
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:45.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:45.28
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 16 14:15:45.283: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 16 14:15:45.290: INFO: Waiting for terminating namespaces to be deleted...
    Dec 16 14:15:45.294: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
    Dec 16 14:15:45.300: INFO: rs-55fwt from disruption-1185 started at 2022-12-16 14:15:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container donothing ready: false, restart count 0
    Dec 16 14:15:45.300: INFO: rs-5rn9s from disruption-1185 started at 2022-12-16 14:15:43 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container donothing ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:15:45.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:15:45.300: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
    Dec 16 14:15:45.306: INFO: rs-brmwn from disruption-1185 started at 2022-12-16 14:15:39 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.306: INFO: 	Container donothing ready: true, restart count 0
    Dec 16 14:15:45.306: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.306: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:15:45.306: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.306: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:15:45.306: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
    Dec 16 14:15:45.306: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:15:45.306: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:15:45.306: INFO: webhook-to-be-mutated from webhook-8103 started at 2022-12-16 14:14:49 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.306: INFO: 	Container example ready: false, restart count 0
    Dec 16 14:15:45.306: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
    Dec 16 14:15:45.313: INFO: rs-cg6t2 from disruption-1185 started at 2022-12-16 14:15:39 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container donothing ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container e2e ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:15:45.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:15:45.313: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node pool-a3802-ehprg 12/16/22 14:15:45.33
    STEP: verifying the node has the label node pool-a3802-fsxxd 12/16/22 14:15:45.341
    STEP: verifying the node has the label node pool-a3802-oewtd 12/16/22 14:15:45.351
    Dec 16 14:15:45.362: INFO: Pod rs-55fwt requesting resource cpu=0m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod rs-5rn9s requesting resource cpu=0m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod rs-brmwn requesting resource cpu=0m on Node pool-a3802-fsxxd
    Dec 16 14:15:45.362: INFO: Pod rs-cg6t2 requesting resource cpu=0m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod calico-kube-controllers-5f94594857-mnd6f requesting resource cpu=0m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod calico-node-5m9wp requesting resource cpu=250m on Node pool-a3802-fsxxd
    Dec 16 14:15:45.362: INFO: Pod calico-node-g9rxj requesting resource cpu=250m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod calico-node-h8rhd requesting resource cpu=250m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod coredns-7cd5b7d6b4-mf5b4 requesting resource cpu=100m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod coredns-7cd5b7d6b4-zz7zw requesting resource cpu=100m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod konnectivity-agent-5f7cbf88d-fpk9f requesting resource cpu=100m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod konnectivity-agent-5f7cbf88d-q4fff requesting resource cpu=100m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod kube-proxy-csksk requesting resource cpu=0m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod kube-proxy-f6k5s requesting resource cpu=0m on Node pool-a3802-fsxxd
    Dec 16 14:15:45.362: INFO: Pod kube-proxy-k8cjh requesting resource cpu=0m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod sonobuoy requesting resource cpu=0m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod sonobuoy-e2e-job-779c6bd4e6b34c88 requesting resource cpu=0m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r requesting resource cpu=0m on Node pool-a3802-ehprg
    Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv requesting resource cpu=0m on Node pool-a3802-oewtd
    Dec 16 14:15:45.362: INFO: Pod sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh requesting resource cpu=0m on Node pool-a3802-fsxxd
    Dec 16 14:15:45.362: INFO: Pod webhook-to-be-mutated requesting resource cpu=0m on Node pool-a3802-fsxxd
    STEP: Starting Pods to consume most of the cluster CPU. 12/16/22 14:15:45.362
    Dec 16 14:15:45.362: INFO: Creating a pod which consumes cpu=2345m on Node pool-a3802-ehprg
    Dec 16 14:15:45.373: INFO: Creating a pod which consumes cpu=2485m on Node pool-a3802-fsxxd
    Dec 16 14:15:45.379: INFO: Creating a pod which consumes cpu=2345m on Node pool-a3802-oewtd
    Dec 16 14:15:45.385: INFO: Waiting up to 5m0s for pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a" in namespace "sched-pred-6271" to be "running"
    Dec 16 14:15:45.388: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.661278ms
    Dec 16 14:15:47.394: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008760251s
    Dec 16 14:15:47.394: INFO: Pod "filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a" satisfied condition "running"
    Dec 16 14:15:47.394: INFO: Waiting up to 5m0s for pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2" in namespace "sched-pred-6271" to be "running"
    Dec 16 14:15:47.398: INFO: Pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2": Phase="Running", Reason="", readiness=true. Elapsed: 3.55851ms
    Dec 16 14:15:47.398: INFO: Pod "filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2" satisfied condition "running"
    Dec 16 14:15:47.398: INFO: Waiting up to 5m0s for pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa" in namespace "sched-pred-6271" to be "running"
    Dec 16 14:15:47.401: INFO: Pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa": Phase="Running", Reason="", readiness=true. Elapsed: 3.602184ms
    Dec 16 14:15:47.401: INFO: Pod "filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/16/22 14:15:47.401
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314bad85194889], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2 to pool-a3802-fsxxd] 12/16/22 14:15:47.405
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badaabb875a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.405
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badab612673], Reason = [Created], Message = [Created container filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2] 12/16/22 14:15:47.405
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2.17314badb0a71f39], Reason = [Started], Message = [Started container filler-pod-0f7d0e12-a098-4b19-8512-66288869eca2] 12/16/22 14:15:47.405
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314bad84bdc5c3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a to pool-a3802-ehprg] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badab78614f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badac3da408], Reason = [Created], Message = [Created container filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a.17314badb1e811be], Reason = [Started], Message = [Started container filler-pod-b042b77e-1b83-411d-be3e-9b4fd585e75a] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314bad85753339], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6271/filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa to pool-a3802-oewtd] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badab3d4334], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badabf75dcc], Reason = [Created], Message = [Created container filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa.17314badb019f990], Reason = [Started], Message = [Started container filler-pod-b9e94cdb-c25b-43fc-95cb-c9b7e00d37aa] 12/16/22 14:15:47.406
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17314badfde8cc7b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 12/16/22 14:15:47.416
    STEP: removing the label node off the node pool-a3802-ehprg 12/16/22 14:15:48.418
    STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.43
    STEP: removing the label node off the node pool-a3802-fsxxd 12/16/22 14:15:48.433
    STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.445
    STEP: removing the label node off the node pool-a3802-oewtd 12/16/22 14:15:48.448
    STEP: verifying the node doesn't have the label node 12/16/22 14:15:48.457
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:15:48.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6271" for this suite. 12/16/22 14:15:48.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:15:48.469
Dec 16 14:15:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 14:15:48.47
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:48.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:48.494
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c in namespace container-probe-5918 12/16/22 14:15:48.496
Dec 16 14:15:48.504: INFO: Waiting up to 5m0s for pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c" in namespace "container-probe-5918" to be "not pending"
Dec 16 14:15:48.507: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.160312ms
Dec 16 14:15:50.512: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007629529s
Dec 16 14:15:50.512: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c" satisfied condition "not pending"
Dec 16 14:15:50.512: INFO: Started pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c in namespace container-probe-5918
STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:15:50.512
Dec 16 14:15:50.515: INFO: Initial restart count of pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c is 0
STEP: deleting the pod 12/16/22 14:19:51.331
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 14:19:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5918" for this suite. 12/16/22 14:19:51.349
------------------------------
• [SLOW TEST] [242.885 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:15:48.469
    Dec 16 14:15:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 14:15:48.47
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:15:48.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:15:48.494
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c in namespace container-probe-5918 12/16/22 14:15:48.496
    Dec 16 14:15:48.504: INFO: Waiting up to 5m0s for pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c" in namespace "container-probe-5918" to be "not pending"
    Dec 16 14:15:48.507: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.160312ms
    Dec 16 14:15:50.512: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007629529s
    Dec 16 14:15:50.512: INFO: Pod "busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c" satisfied condition "not pending"
    Dec 16 14:15:50.512: INFO: Started pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c in namespace container-probe-5918
    STEP: checking the pod's current state and verifying that restartCount is present 12/16/22 14:15:50.512
    Dec 16 14:15:50.515: INFO: Initial restart count of pod busybox-d626cf0c-44ac-4c1f-b26d-5ce6318b0a1c is 0
    STEP: deleting the pod 12/16/22 14:19:51.331
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:19:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5918" for this suite. 12/16/22 14:19:51.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:19:51.355
Dec 16 14:19:51.355: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename endpointslice 12/16/22 14:19:51.355
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:19:51.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:19:51.373
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 16 14:19:53.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5660" for this suite. 12/16/22 14:19:53.431
------------------------------
• [2.081 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:19:51.355
    Dec 16 14:19:51.355: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename endpointslice 12/16/22 14:19:51.355
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:19:51.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:19:51.373
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:19:53.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5660" for this suite. 12/16/22 14:19:53.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:19:53.437
Dec 16 14:19:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename cronjob 12/16/22 14:19:53.438
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:19:53.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:19:53.453
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 12/16/22 14:19:53.456
STEP: Ensuring more than one job is running at a time 12/16/22 14:19:53.461
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/16/22 14:21:01.466
STEP: Removing cronjob 12/16/22 14:21:01.47
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:01.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2605" for this suite. 12/16/22 14:21:01.482
------------------------------
• [SLOW TEST] [68.052 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:19:53.437
    Dec 16 14:19:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename cronjob 12/16/22 14:19:53.438
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:19:53.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:19:53.453
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 12/16/22 14:19:53.456
    STEP: Ensuring more than one job is running at a time 12/16/22 14:19:53.461
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/16/22 14:21:01.466
    STEP: Removing cronjob 12/16/22 14:21:01.47
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:01.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2605" for this suite. 12/16/22 14:21:01.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:01.489
Dec 16 14:21:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:21:01.49
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:01.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:01.51
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:21:01.525
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:21:01.956
STEP: Deploying the webhook pod 12/16/22 14:21:01.966
STEP: Wait for the deployment to be ready 12/16/22 14:21:01.979
Dec 16 14:21:01.987: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:21:03.999
STEP: Verifying the service has paired with the endpoint 12/16/22 14:21:04.019
Dec 16 14:21:05.019: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 12/16/22 14:21:05.024
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/16/22 14:21:05.025
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/16/22 14:21:05.025
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/16/22 14:21:05.025
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/16/22 14:21:05.026
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/16/22 14:21:05.026
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/16/22 14:21:05.027
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:05.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8983" for this suite. 12/16/22 14:21:05.072
STEP: Destroying namespace "webhook-8983-markers" for this suite. 12/16/22 14:21:05.079
------------------------------
• [3.596 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:01.489
    Dec 16 14:21:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:21:01.49
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:01.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:01.51
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:21:01.525
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:21:01.956
    STEP: Deploying the webhook pod 12/16/22 14:21:01.966
    STEP: Wait for the deployment to be ready 12/16/22 14:21:01.979
    Dec 16 14:21:01.987: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:21:03.999
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:21:04.019
    Dec 16 14:21:05.019: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 12/16/22 14:21:05.024
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/16/22 14:21:05.025
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/16/22 14:21:05.025
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/16/22 14:21:05.025
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/16/22 14:21:05.026
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/16/22 14:21:05.026
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/16/22 14:21:05.027
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:05.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8983" for this suite. 12/16/22 14:21:05.072
    STEP: Destroying namespace "webhook-8983-markers" for this suite. 12/16/22 14:21:05.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:05.086
Dec 16 14:21:05.086: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename events 12/16/22 14:21:05.087
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.103
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 12/16/22 14:21:05.105
STEP: listing events in all namespaces 12/16/22 14:21:05.112
STEP: listing events in test namespace 12/16/22 14:21:05.116
STEP: listing events with field selection filtering on source 12/16/22 14:21:05.119
STEP: listing events with field selection filtering on reportingController 12/16/22 14:21:05.123
STEP: getting the test event 12/16/22 14:21:05.126
STEP: patching the test event 12/16/22 14:21:05.129
STEP: getting the test event 12/16/22 14:21:05.138
STEP: updating the test event 12/16/22 14:21:05.141
STEP: getting the test event 12/16/22 14:21:05.147
STEP: deleting the test event 12/16/22 14:21:05.15
STEP: listing events in all namespaces 12/16/22 14:21:05.157
STEP: listing events in test namespace 12/16/22 14:21:05.16
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5670" for this suite. 12/16/22 14:21:05.168
------------------------------
• [0.088 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:05.086
    Dec 16 14:21:05.086: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename events 12/16/22 14:21:05.087
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.103
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 12/16/22 14:21:05.105
    STEP: listing events in all namespaces 12/16/22 14:21:05.112
    STEP: listing events in test namespace 12/16/22 14:21:05.116
    STEP: listing events with field selection filtering on source 12/16/22 14:21:05.119
    STEP: listing events with field selection filtering on reportingController 12/16/22 14:21:05.123
    STEP: getting the test event 12/16/22 14:21:05.126
    STEP: patching the test event 12/16/22 14:21:05.129
    STEP: getting the test event 12/16/22 14:21:05.138
    STEP: updating the test event 12/16/22 14:21:05.141
    STEP: getting the test event 12/16/22 14:21:05.147
    STEP: deleting the test event 12/16/22 14:21:05.15
    STEP: listing events in all namespaces 12/16/22 14:21:05.157
    STEP: listing events in test namespace 12/16/22 14:21:05.16
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5670" for this suite. 12/16/22 14:21:05.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:05.174
Dec 16 14:21:05.174: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 14:21:05.175
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.192
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 12/16/22 14:21:05.194
STEP: patching the Namespace 12/16/22 14:21:05.21
STEP: get the Namespace and ensuring it has the label 12/16/22 14:21:05.215
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:05.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7096" for this suite. 12/16/22 14:21:05.223
STEP: Destroying namespace "nspatchtest-a19a8885-48fe-44e5-94f6-29409c69449a-9655" for this suite. 12/16/22 14:21:05.229
------------------------------
• [0.061 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:05.174
    Dec 16 14:21:05.174: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 14:21:05.175
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.192
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 12/16/22 14:21:05.194
    STEP: patching the Namespace 12/16/22 14:21:05.21
    STEP: get the Namespace and ensuring it has the label 12/16/22 14:21:05.215
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:05.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7096" for this suite. 12/16/22 14:21:05.223
    STEP: Destroying namespace "nspatchtest-a19a8885-48fe-44e5-94f6-29409c69449a-9655" for this suite. 12/16/22 14:21:05.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:05.237
Dec 16 14:21:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename endpointslicemirroring 12/16/22 14:21:05.237
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.255
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 12/16/22 14:21:05.275
Dec 16 14:21:05.287: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 12/16/22 14:21:07.297
Dec 16 14:21:07.308: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 12/16/22 14:21:09.312
Dec 16 14:21:09.320: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7169" for this suite. 12/16/22 14:21:11.328
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:05.237
    Dec 16 14:21:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename endpointslicemirroring 12/16/22 14:21:05.237
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:05.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:05.255
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 12/16/22 14:21:05.275
    Dec 16 14:21:05.287: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 12/16/22 14:21:07.297
    Dec 16 14:21:07.308: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 12/16/22 14:21:09.312
    Dec 16 14:21:09.320: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7169" for this suite. 12/16/22 14:21:11.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:11.337
Dec 16 14:21:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:21:11.338
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:11.356
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-580-delete-me 12/16/22 14:21:11.363
STEP: Waiting for the RuntimeClass to disappear 12/16/22 14:21:11.369
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:11.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-580" for this suite. 12/16/22 14:21:11.383
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:11.337
    Dec 16 14:21:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:21:11.338
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:11.356
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-580-delete-me 12/16/22 14:21:11.363
    STEP: Waiting for the RuntimeClass to disappear 12/16/22 14:21:11.369
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:11.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-580" for this suite. 12/16/22 14:21:11.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:11.39
Dec 16 14:21:11.390: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename namespaces 12/16/22 14:21:11.391
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:11.408
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 12/16/22 14:21:11.41
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.425
STEP: Creating a pod in the namespace 12/16/22 14:21:11.427
STEP: Waiting for the pod to have running status 12/16/22 14:21:11.434
Dec 16 14:21:11.435: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1646" to be "running"
Dec 16 14:21:11.438: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408957ms
Dec 16 14:21:13.443: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00882809s
Dec 16 14:21:13.443: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/16/22 14:21:13.443
STEP: Waiting for the namespace to be removed. 12/16/22 14:21:13.451
STEP: Recreating the namespace 12/16/22 14:21:24.455
STEP: Verifying there are no pods in the namespace 12/16/22 14:21:24.471
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:24.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7810" for this suite. 12/16/22 14:21:24.479
STEP: Destroying namespace "nsdeletetest-1646" for this suite. 12/16/22 14:21:24.484
Dec 16 14:21:24.487: INFO: Namespace nsdeletetest-1646 was already deleted
STEP: Destroying namespace "nsdeletetest-7991" for this suite. 12/16/22 14:21:24.487
------------------------------
• [SLOW TEST] [13.102 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:11.39
    Dec 16 14:21:11.390: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename namespaces 12/16/22 14:21:11.391
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:11.408
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 12/16/22 14:21:11.41
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:11.425
    STEP: Creating a pod in the namespace 12/16/22 14:21:11.427
    STEP: Waiting for the pod to have running status 12/16/22 14:21:11.434
    Dec 16 14:21:11.435: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1646" to be "running"
    Dec 16 14:21:11.438: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408957ms
    Dec 16 14:21:13.443: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00882809s
    Dec 16 14:21:13.443: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/16/22 14:21:13.443
    STEP: Waiting for the namespace to be removed. 12/16/22 14:21:13.451
    STEP: Recreating the namespace 12/16/22 14:21:24.455
    STEP: Verifying there are no pods in the namespace 12/16/22 14:21:24.471
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:24.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7810" for this suite. 12/16/22 14:21:24.479
    STEP: Destroying namespace "nsdeletetest-1646" for this suite. 12/16/22 14:21:24.484
    Dec 16 14:21:24.487: INFO: Namespace nsdeletetest-1646 was already deleted
    STEP: Destroying namespace "nsdeletetest-7991" for this suite. 12/16/22 14:21:24.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:24.493
Dec 16 14:21:24.493: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replication-controller 12/16/22 14:21:24.494
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:24.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:24.511
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 12/16/22 14:21:24.517
STEP: waiting for RC to be added 12/16/22 14:21:24.523
STEP: waiting for available Replicas 12/16/22 14:21:24.523
STEP: patching ReplicationController 12/16/22 14:21:26.19
STEP: waiting for RC to be modified 12/16/22 14:21:26.199
STEP: patching ReplicationController status 12/16/22 14:21:26.199
STEP: waiting for RC to be modified 12/16/22 14:21:26.204
STEP: waiting for available Replicas 12/16/22 14:21:26.204
STEP: fetching ReplicationController status 12/16/22 14:21:26.209
STEP: patching ReplicationController scale 12/16/22 14:21:26.212
STEP: waiting for RC to be modified 12/16/22 14:21:26.218
STEP: waiting for ReplicationController's scale to be the max amount 12/16/22 14:21:26.218
STEP: fetching ReplicationController; ensuring that it's patched 12/16/22 14:21:27.671
STEP: updating ReplicationController status 12/16/22 14:21:27.675
STEP: waiting for RC to be modified 12/16/22 14:21:27.681
STEP: listing all ReplicationControllers 12/16/22 14:21:27.681
STEP: checking that ReplicationController has expected values 12/16/22 14:21:27.685
STEP: deleting ReplicationControllers by collection 12/16/22 14:21:27.685
STEP: waiting for ReplicationController to have a DELETED watchEvent 12/16/22 14:21:27.692
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:27.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1002" for this suite. 12/16/22 14:21:27.735
------------------------------
• [3.248 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:24.493
    Dec 16 14:21:24.493: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replication-controller 12/16/22 14:21:24.494
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:24.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:24.511
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 12/16/22 14:21:24.517
    STEP: waiting for RC to be added 12/16/22 14:21:24.523
    STEP: waiting for available Replicas 12/16/22 14:21:24.523
    STEP: patching ReplicationController 12/16/22 14:21:26.19
    STEP: waiting for RC to be modified 12/16/22 14:21:26.199
    STEP: patching ReplicationController status 12/16/22 14:21:26.199
    STEP: waiting for RC to be modified 12/16/22 14:21:26.204
    STEP: waiting for available Replicas 12/16/22 14:21:26.204
    STEP: fetching ReplicationController status 12/16/22 14:21:26.209
    STEP: patching ReplicationController scale 12/16/22 14:21:26.212
    STEP: waiting for RC to be modified 12/16/22 14:21:26.218
    STEP: waiting for ReplicationController's scale to be the max amount 12/16/22 14:21:26.218
    STEP: fetching ReplicationController; ensuring that it's patched 12/16/22 14:21:27.671
    STEP: updating ReplicationController status 12/16/22 14:21:27.675
    STEP: waiting for RC to be modified 12/16/22 14:21:27.681
    STEP: listing all ReplicationControllers 12/16/22 14:21:27.681
    STEP: checking that ReplicationController has expected values 12/16/22 14:21:27.685
    STEP: deleting ReplicationControllers by collection 12/16/22 14:21:27.685
    STEP: waiting for ReplicationController to have a DELETED watchEvent 12/16/22 14:21:27.692
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:27.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1002" for this suite. 12/16/22 14:21:27.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:27.742
Dec 16 14:21:27.742: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename subpath 12/16/22 14:21:27.743
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:27.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:27.761
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/16/22 14:21:27.763
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-m45c 12/16/22 14:21:27.772
STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:21:27.772
Dec 16 14:21:27.780: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-m45c" in namespace "subpath-943" to be "Succeeded or Failed"
Dec 16 14:21:27.783: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587052ms
Dec 16 14:21:29.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008845594s
Dec 16 14:21:31.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 4.008709677s
Dec 16 14:21:33.788: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 6.007854983s
Dec 16 14:21:35.788: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 8.007991426s
Dec 16 14:21:37.790: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010011011s
Dec 16 14:21:39.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 12.009055006s
Dec 16 14:21:41.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 14.008285949s
Dec 16 14:21:43.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 16.009070097s
Dec 16 14:21:45.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 18.008453976s
Dec 16 14:21:47.790: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 20.009799564s
Dec 16 14:21:49.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=false. Elapsed: 22.009013892s
Dec 16 14:21:51.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009127613s
STEP: Saw pod success 12/16/22 14:21:51.789
Dec 16 14:21:51.790: INFO: Pod "pod-subpath-test-secret-m45c" satisfied condition "Succeeded or Failed"
Dec 16 14:21:51.794: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-secret-m45c container test-container-subpath-secret-m45c: <nil>
STEP: delete the pod 12/16/22 14:21:51.849
Dec 16 14:21:51.864: INFO: Waiting for pod pod-subpath-test-secret-m45c to disappear
Dec 16 14:21:51.867: INFO: Pod pod-subpath-test-secret-m45c no longer exists
STEP: Deleting pod pod-subpath-test-secret-m45c 12/16/22 14:21:51.867
Dec 16 14:21:51.867: INFO: Deleting pod "pod-subpath-test-secret-m45c" in namespace "subpath-943"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-943" for this suite. 12/16/22 14:21:51.876
------------------------------
• [SLOW TEST] [24.141 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:27.742
    Dec 16 14:21:27.742: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename subpath 12/16/22 14:21:27.743
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:27.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:27.761
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/16/22 14:21:27.763
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-m45c 12/16/22 14:21:27.772
    STEP: Creating a pod to test atomic-volume-subpath 12/16/22 14:21:27.772
    Dec 16 14:21:27.780: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-m45c" in namespace "subpath-943" to be "Succeeded or Failed"
    Dec 16 14:21:27.783: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587052ms
    Dec 16 14:21:29.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008845594s
    Dec 16 14:21:31.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 4.008709677s
    Dec 16 14:21:33.788: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 6.007854983s
    Dec 16 14:21:35.788: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 8.007991426s
    Dec 16 14:21:37.790: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010011011s
    Dec 16 14:21:39.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 12.009055006s
    Dec 16 14:21:41.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 14.008285949s
    Dec 16 14:21:43.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 16.009070097s
    Dec 16 14:21:45.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 18.008453976s
    Dec 16 14:21:47.790: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=true. Elapsed: 20.009799564s
    Dec 16 14:21:49.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Running", Reason="", readiness=false. Elapsed: 22.009013892s
    Dec 16 14:21:51.789: INFO: Pod "pod-subpath-test-secret-m45c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009127613s
    STEP: Saw pod success 12/16/22 14:21:51.789
    Dec 16 14:21:51.790: INFO: Pod "pod-subpath-test-secret-m45c" satisfied condition "Succeeded or Failed"
    Dec 16 14:21:51.794: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-subpath-test-secret-m45c container test-container-subpath-secret-m45c: <nil>
    STEP: delete the pod 12/16/22 14:21:51.849
    Dec 16 14:21:51.864: INFO: Waiting for pod pod-subpath-test-secret-m45c to disappear
    Dec 16 14:21:51.867: INFO: Pod pod-subpath-test-secret-m45c no longer exists
    STEP: Deleting pod pod-subpath-test-secret-m45c 12/16/22 14:21:51.867
    Dec 16 14:21:51.867: INFO: Deleting pod "pod-subpath-test-secret-m45c" in namespace "subpath-943"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-943" for this suite. 12/16/22 14:21:51.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:51.884
Dec 16 14:21:51.884: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 14:21:51.885
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:51.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:51.905
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 12/16/22 14:21:51.908
Dec 16 14:21:51.919: INFO: Waiting up to 5m0s for pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537" in namespace "emptydir-7096" to be "Succeeded or Failed"
Dec 16 14:21:51.922: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103099ms
Dec 16 14:21:53.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008538947s
Dec 16 14:21:55.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008259256s
STEP: Saw pod success 12/16/22 14:21:55.927
Dec 16 14:21:55.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537" satisfied condition "Succeeded or Failed"
Dec 16 14:21:55.931: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 container test-container: <nil>
STEP: delete the pod 12/16/22 14:21:55.94
Dec 16 14:21:55.953: INFO: Waiting for pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 to disappear
Dec 16 14:21:55.956: INFO: Pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 14:21:55.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7096" for this suite. 12/16/22 14:21:55.961
------------------------------
• [4.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:51.884
    Dec 16 14:21:51.884: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 14:21:51.885
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:51.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:51.905
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 12/16/22 14:21:51.908
    Dec 16 14:21:51.919: INFO: Waiting up to 5m0s for pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537" in namespace "emptydir-7096" to be "Succeeded or Failed"
    Dec 16 14:21:51.922: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103099ms
    Dec 16 14:21:53.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008538947s
    Dec 16 14:21:55.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008259256s
    STEP: Saw pod success 12/16/22 14:21:55.927
    Dec 16 14:21:55.927: INFO: Pod "pod-897ef34c-d485-4694-a7c0-65b5922ec537" satisfied condition "Succeeded or Failed"
    Dec 16 14:21:55.931: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 container test-container: <nil>
    STEP: delete the pod 12/16/22 14:21:55.94
    Dec 16 14:21:55.953: INFO: Waiting for pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 to disappear
    Dec 16 14:21:55.956: INFO: Pod pod-897ef34c-d485-4694-a7c0-65b5922ec537 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:21:55.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7096" for this suite. 12/16/22 14:21:55.961
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:21:55.968
Dec 16 14:21:55.968: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:21:55.969
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:55.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:55.99
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5178 12/16/22 14:21:55.993
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[] 12/16/22 14:21:56.009
Dec 16 14:21:56.013: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 16 14:21:57.023: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5178 12/16/22 14:21:57.023
Dec 16 14:21:57.032: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5178" to be "running and ready"
Dec 16 14:21:57.035: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054811ms
Dec 16 14:21:57.035: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:21:59.042: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00969506s
Dec 16 14:21:59.042: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 16 14:21:59.042: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod1:[80]] 12/16/22 14:21:59.046
Dec 16 14:21:59.059: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 12/16/22 14:21:59.059
Dec 16 14:21:59.059: INFO: Creating new exec pod
Dec 16 14:21:59.065: INFO: Waiting up to 5m0s for pod "execpodz6j78" in namespace "services-5178" to be "running"
Dec 16 14:21:59.069: INFO: Pod "execpodz6j78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.57395ms
Dec 16 14:22:01.074: INFO: Pod "execpodz6j78": Phase="Running", Reason="", readiness=true. Elapsed: 2.009301478s
Dec 16 14:22:01.074: INFO: Pod "execpodz6j78" satisfied condition "running"
Dec 16 14:22:02.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 16 14:22:02.282: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:02.282: INFO: stdout: ""
Dec 16 14:22:02.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
Dec 16 14:22:02.476: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:02.476: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5178 12/16/22 14:22:02.476
Dec 16 14:22:02.484: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5178" to be "running and ready"
Dec 16 14:22:02.488: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45062ms
Dec 16 14:22:02.488: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:22:04.493: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008244041s
Dec 16 14:22:04.493: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 16 14:22:04.493: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod1:[80] pod2:[80]] 12/16/22 14:22:04.496
Dec 16 14:22:04.511: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 12/16/22 14:22:04.511
Dec 16 14:22:05.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 16 14:22:05.697: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:05.697: INFO: stdout: ""
Dec 16 14:22:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
Dec 16 14:22:05.885: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:05.885: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5178 12/16/22 14:22:05.885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod2:[80]] 12/16/22 14:22:05.899
Dec 16 14:22:05.909: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 12/16/22 14:22:05.909
Dec 16 14:22:06.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 16 14:22:07.105: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:07.105: INFO: stdout: ""
Dec 16 14:22:07.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
Dec 16 14:22:07.295: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
Dec 16 14:22:07.295: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5178 12/16/22 14:22:07.295
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[] 12/16/22 14:22:07.307
Dec 16 14:22:07.313: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:22:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5178" for this suite. 12/16/22 14:22:07.337
------------------------------
• [SLOW TEST] [11.375 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:21:55.968
    Dec 16 14:21:55.968: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:21:55.969
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:21:55.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:21:55.99
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5178 12/16/22 14:21:55.993
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[] 12/16/22 14:21:56.009
    Dec 16 14:21:56.013: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Dec 16 14:21:57.023: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5178 12/16/22 14:21:57.023
    Dec 16 14:21:57.032: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5178" to be "running and ready"
    Dec 16 14:21:57.035: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054811ms
    Dec 16 14:21:57.035: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:21:59.042: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00969506s
    Dec 16 14:21:59.042: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 16 14:21:59.042: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod1:[80]] 12/16/22 14:21:59.046
    Dec 16 14:21:59.059: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 12/16/22 14:21:59.059
    Dec 16 14:21:59.059: INFO: Creating new exec pod
    Dec 16 14:21:59.065: INFO: Waiting up to 5m0s for pod "execpodz6j78" in namespace "services-5178" to be "running"
    Dec 16 14:21:59.069: INFO: Pod "execpodz6j78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.57395ms
    Dec 16 14:22:01.074: INFO: Pod "execpodz6j78": Phase="Running", Reason="", readiness=true. Elapsed: 2.009301478s
    Dec 16 14:22:01.074: INFO: Pod "execpodz6j78" satisfied condition "running"
    Dec 16 14:22:02.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 16 14:22:02.282: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:02.282: INFO: stdout: ""
    Dec 16 14:22:02.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
    Dec 16 14:22:02.476: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:02.476: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5178 12/16/22 14:22:02.476
    Dec 16 14:22:02.484: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5178" to be "running and ready"
    Dec 16 14:22:02.488: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45062ms
    Dec 16 14:22:02.488: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:22:04.493: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008244041s
    Dec 16 14:22:04.493: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 16 14:22:04.493: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod1:[80] pod2:[80]] 12/16/22 14:22:04.496
    Dec 16 14:22:04.511: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 12/16/22 14:22:04.511
    Dec 16 14:22:05.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 16 14:22:05.697: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:05.697: INFO: stdout: ""
    Dec 16 14:22:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
    Dec 16 14:22:05.885: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:05.885: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5178 12/16/22 14:22:05.885
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[pod2:[80]] 12/16/22 14:22:05.899
    Dec 16 14:22:05.909: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 12/16/22 14:22:05.909
    Dec 16 14:22:06.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 16 14:22:07.105: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:07.105: INFO: stdout: ""
    Dec 16 14:22:07.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-5178 exec execpodz6j78 -- /bin/sh -x -c nc -v -z -w 2 10.96.147.38 80'
    Dec 16 14:22:07.295: INFO: stderr: "+ nc -v -z -w 2 10.96.147.38 80\nConnection to 10.96.147.38 80 port [tcp/http] succeeded!\n"
    Dec 16 14:22:07.295: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5178 12/16/22 14:22:07.295
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5178 to expose endpoints map[] 12/16/22 14:22:07.307
    Dec 16 14:22:07.313: INFO: successfully validated that service endpoint-test2 in namespace services-5178 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:22:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5178" for this suite. 12/16/22 14:22:07.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:22:07.344
Dec 16 14:22:07.344: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 14:22:07.344
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:07.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:07.361
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 12/16/22 14:22:07.367
STEP: create the rc2 12/16/22 14:22:07.372
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/16/22 14:22:12.383
STEP: delete the rc simpletest-rc-to-be-deleted 12/16/22 14:22:12.745
STEP: wait for the rc to be deleted 12/16/22 14:22:12.754
Dec 16 14:22:17.769: INFO: 69 pods remaining
Dec 16 14:22:17.769: INFO: 69 pods has nil DeletionTimestamp
Dec 16 14:22:17.769: INFO: 
STEP: Gathering metrics 12/16/22 14:22:22.779
W1216 14:22:22.786244      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 14:22:22.786: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 16 14:22:22.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-25wql" in namespace "gc-6979"
Dec 16 14:22:22.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-27jzd" in namespace "gc-6979"
Dec 16 14:22:22.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kfpz" in namespace "gc-6979"
Dec 16 14:22:22.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p7jq" in namespace "gc-6979"
Dec 16 14:22:22.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xqkq" in namespace "gc-6979"
Dec 16 14:22:22.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-44cjp" in namespace "gc-6979"
Dec 16 14:22:22.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-48qkl" in namespace "gc-6979"
Dec 16 14:22:22.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s5mz" in namespace "gc-6979"
Dec 16 14:22:22.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zn75" in namespace "gc-6979"
Dec 16 14:22:22.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-65kpg" in namespace "gc-6979"
Dec 16 14:22:22.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-69jg4" in namespace "gc-6979"
Dec 16 14:22:22.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gzn2" in namespace "gc-6979"
Dec 16 14:22:22.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zxd5" in namespace "gc-6979"
Dec 16 14:22:22.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-75rzt" in namespace "gc-6979"
Dec 16 14:22:22.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-79hb4" in namespace "gc-6979"
Dec 16 14:22:22.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-79lxx" in namespace "gc-6979"
Dec 16 14:22:22.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zph4" in namespace "gc-6979"
Dec 16 14:22:22.979: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d4js" in namespace "gc-6979"
Dec 16 14:22:22.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t7mv" in namespace "gc-6979"
Dec 16 14:22:23.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wqkw" in namespace "gc-6979"
Dec 16 14:22:23.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-97vh7" in namespace "gc-6979"
Dec 16 14:22:23.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bb9z" in namespace "gc-6979"
Dec 16 14:22:23.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hjfb" in namespace "gc-6979"
Dec 16 14:22:23.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pcdx" in namespace "gc-6979"
Dec 16 14:22:23.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wh7c" in namespace "gc-6979"
Dec 16 14:22:23.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf765" in namespace "gc-6979"
Dec 16 14:22:23.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkfrz" in namespace "gc-6979"
Dec 16 14:22:23.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpgfc" in namespace "gc-6979"
Dec 16 14:22:23.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjz8h" in namespace "gc-6979"
Dec 16 14:22:23.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckscc" in namespace "gc-6979"
Dec 16 14:22:23.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxrxk" in namespace "gc-6979"
Dec 16 14:22:23.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-d97pc" in namespace "gc-6979"
Dec 16 14:22:23.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9kjm" in namespace "gc-6979"
Dec 16 14:22:23.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-djxr4" in namespace "gc-6979"
Dec 16 14:22:23.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzmt6" in namespace "gc-6979"
Dec 16 14:22:23.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2d5k" in namespace "gc-6979"
Dec 16 14:22:23.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4swj" in namespace "gc-6979"
Dec 16 14:22:23.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8m74" in namespace "gc-6979"
Dec 16 14:22:23.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjqqb" in namespace "gc-6979"
Dec 16 14:22:23.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-frsp8" in namespace "gc-6979"
Dec 16 14:22:23.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftbml" in namespace "gc-6979"
Dec 16 14:22:23.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2ts9" in namespace "gc-6979"
Dec 16 14:22:23.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm2fw" in namespace "gc-6979"
Dec 16 14:22:23.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt5dz" in namespace "gc-6979"
Dec 16 14:22:23.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-h26cj" in namespace "gc-6979"
Dec 16 14:22:23.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5d2p" in namespace "gc-6979"
Dec 16 14:22:23.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8zdq" in namespace "gc-6979"
Dec 16 14:22:23.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9hm4" in namespace "gc-6979"
Dec 16 14:22:23.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkjkm" in namespace "gc-6979"
Dec 16 14:22:23.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2ptw" in namespace "gc-6979"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 14:22:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6979" for this suite. 12/16/22 14:22:23.467
------------------------------
• [SLOW TEST] [16.173 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:22:07.344
    Dec 16 14:22:07.344: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 14:22:07.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:07.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:07.361
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 12/16/22 14:22:07.367
    STEP: create the rc2 12/16/22 14:22:07.372
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/16/22 14:22:12.383
    STEP: delete the rc simpletest-rc-to-be-deleted 12/16/22 14:22:12.745
    STEP: wait for the rc to be deleted 12/16/22 14:22:12.754
    Dec 16 14:22:17.769: INFO: 69 pods remaining
    Dec 16 14:22:17.769: INFO: 69 pods has nil DeletionTimestamp
    Dec 16 14:22:17.769: INFO: 
    STEP: Gathering metrics 12/16/22 14:22:22.779
    W1216 14:22:22.786244      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 14:22:22.786: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 16 14:22:22.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-25wql" in namespace "gc-6979"
    Dec 16 14:22:22.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-27jzd" in namespace "gc-6979"
    Dec 16 14:22:22.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kfpz" in namespace "gc-6979"
    Dec 16 14:22:22.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p7jq" in namespace "gc-6979"
    Dec 16 14:22:22.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xqkq" in namespace "gc-6979"
    Dec 16 14:22:22.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-44cjp" in namespace "gc-6979"
    Dec 16 14:22:22.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-48qkl" in namespace "gc-6979"
    Dec 16 14:22:22.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s5mz" in namespace "gc-6979"
    Dec 16 14:22:22.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zn75" in namespace "gc-6979"
    Dec 16 14:22:22.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-65kpg" in namespace "gc-6979"
    Dec 16 14:22:22.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-69jg4" in namespace "gc-6979"
    Dec 16 14:22:22.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gzn2" in namespace "gc-6979"
    Dec 16 14:22:22.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zxd5" in namespace "gc-6979"
    Dec 16 14:22:22.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-75rzt" in namespace "gc-6979"
    Dec 16 14:22:22.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-79hb4" in namespace "gc-6979"
    Dec 16 14:22:22.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-79lxx" in namespace "gc-6979"
    Dec 16 14:22:22.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zph4" in namespace "gc-6979"
    Dec 16 14:22:22.979: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d4js" in namespace "gc-6979"
    Dec 16 14:22:22.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t7mv" in namespace "gc-6979"
    Dec 16 14:22:23.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wqkw" in namespace "gc-6979"
    Dec 16 14:22:23.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-97vh7" in namespace "gc-6979"
    Dec 16 14:22:23.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bb9z" in namespace "gc-6979"
    Dec 16 14:22:23.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hjfb" in namespace "gc-6979"
    Dec 16 14:22:23.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pcdx" in namespace "gc-6979"
    Dec 16 14:22:23.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wh7c" in namespace "gc-6979"
    Dec 16 14:22:23.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf765" in namespace "gc-6979"
    Dec 16 14:22:23.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkfrz" in namespace "gc-6979"
    Dec 16 14:22:23.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpgfc" in namespace "gc-6979"
    Dec 16 14:22:23.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjz8h" in namespace "gc-6979"
    Dec 16 14:22:23.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckscc" in namespace "gc-6979"
    Dec 16 14:22:23.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxrxk" in namespace "gc-6979"
    Dec 16 14:22:23.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-d97pc" in namespace "gc-6979"
    Dec 16 14:22:23.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9kjm" in namespace "gc-6979"
    Dec 16 14:22:23.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-djxr4" in namespace "gc-6979"
    Dec 16 14:22:23.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzmt6" in namespace "gc-6979"
    Dec 16 14:22:23.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2d5k" in namespace "gc-6979"
    Dec 16 14:22:23.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4swj" in namespace "gc-6979"
    Dec 16 14:22:23.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8m74" in namespace "gc-6979"
    Dec 16 14:22:23.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjqqb" in namespace "gc-6979"
    Dec 16 14:22:23.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-frsp8" in namespace "gc-6979"
    Dec 16 14:22:23.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftbml" in namespace "gc-6979"
    Dec 16 14:22:23.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2ts9" in namespace "gc-6979"
    Dec 16 14:22:23.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm2fw" in namespace "gc-6979"
    Dec 16 14:22:23.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt5dz" in namespace "gc-6979"
    Dec 16 14:22:23.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-h26cj" in namespace "gc-6979"
    Dec 16 14:22:23.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5d2p" in namespace "gc-6979"
    Dec 16 14:22:23.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8zdq" in namespace "gc-6979"
    Dec 16 14:22:23.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9hm4" in namespace "gc-6979"
    Dec 16 14:22:23.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkjkm" in namespace "gc-6979"
    Dec 16 14:22:23.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2ptw" in namespace "gc-6979"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:22:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6979" for this suite. 12/16/22 14:22:23.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:22:23.518
Dec 16 14:22:23.518: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 14:22:23.519
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:23.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:23.589
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:22:23.592
Dec 16 14:22:23.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c" in namespace "downward-api-3417" to be "Succeeded or Failed"
Dec 16 14:22:23.607: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455041ms
Dec 16 14:22:25.611: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008537019s
Dec 16 14:22:27.621: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018305923s
Dec 16 14:22:29.613: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010443216s
STEP: Saw pod success 12/16/22 14:22:29.613
Dec 16 14:22:29.613: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c" satisfied condition "Succeeded or Failed"
Dec 16 14:22:29.616: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c container client-container: <nil>
STEP: delete the pod 12/16/22 14:22:29.624
Dec 16 14:22:29.635: INFO: Waiting for pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c to disappear
Dec 16 14:22:29.638: INFO: Pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 14:22:29.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3417" for this suite. 12/16/22 14:22:29.643
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:22:23.518
    Dec 16 14:22:23.518: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 14:22:23.519
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:23.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:23.589
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:22:23.592
    Dec 16 14:22:23.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c" in namespace "downward-api-3417" to be "Succeeded or Failed"
    Dec 16 14:22:23.607: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455041ms
    Dec 16 14:22:25.611: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008537019s
    Dec 16 14:22:27.621: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018305923s
    Dec 16 14:22:29.613: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010443216s
    STEP: Saw pod success 12/16/22 14:22:29.613
    Dec 16 14:22:29.613: INFO: Pod "downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c" satisfied condition "Succeeded or Failed"
    Dec 16 14:22:29.616: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c container client-container: <nil>
    STEP: delete the pod 12/16/22 14:22:29.624
    Dec 16 14:22:29.635: INFO: Waiting for pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c to disappear
    Dec 16 14:22:29.638: INFO: Pod downwardapi-volume-b449b508-ce75-4a10-be5c-614dd82ee79c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:22:29.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3417" for this suite. 12/16/22 14:22:29.643
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:22:29.649
Dec 16 14:22:29.649: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 14:22:29.649
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:29.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:29.669
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 12/16/22 14:22:29.672
STEP: Creating a ResourceQuota 12/16/22 14:22:34.676
STEP: Ensuring resource quota status is calculated 12/16/22 14:22:34.682
STEP: Creating a ReplicationController 12/16/22 14:22:36.686
STEP: Ensuring resource quota status captures replication controller creation 12/16/22 14:22:36.697
STEP: Deleting a ReplicationController 12/16/22 14:22:38.703
STEP: Ensuring resource quota status released usage 12/16/22 14:22:38.709
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 14:22:40.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8853" for this suite. 12/16/22 14:22:40.721
------------------------------
• [SLOW TEST] [11.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:22:29.649
    Dec 16 14:22:29.649: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 14:22:29.649
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:29.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:29.669
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 12/16/22 14:22:29.672
    STEP: Creating a ResourceQuota 12/16/22 14:22:34.676
    STEP: Ensuring resource quota status is calculated 12/16/22 14:22:34.682
    STEP: Creating a ReplicationController 12/16/22 14:22:36.686
    STEP: Ensuring resource quota status captures replication controller creation 12/16/22 14:22:36.697
    STEP: Deleting a ReplicationController 12/16/22 14:22:38.703
    STEP: Ensuring resource quota status released usage 12/16/22 14:22:38.709
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:22:40.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8853" for this suite. 12/16/22 14:22:40.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:22:40.728
Dec 16 14:22:40.728: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-pred 12/16/22 14:22:40.729
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:40.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:40.745
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 16 14:22:40.747: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 14:22:40.754: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 14:22:40.757: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
Dec 16 14:22:40.763: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:22:40.763: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:22:40.763: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:22:40.763: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:22:40.763: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 14:22:40.763: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:22:40.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:22:40.763: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:22:40.763: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
Dec 16 14:22:40.768: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.769: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:22:40.769: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.769: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:22:40.769: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
Dec 16 14:22:40.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:22:40.769: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:22:40.769: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
Dec 16 14:22:40.775: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 16 14:22:40.775: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:22:40.775: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:22:40.775: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:22:40.775: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:22:40.775: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container e2e ready: true, restart count 0
Dec 16 14:22:40.775: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:22:40.775: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:22:40.775: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:22:40.775: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/16/22 14:22:40.775
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17314c0e3daeed91], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/16/22 14:22:40.798
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:22:41.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8535" for this suite. 12/16/22 14:22:41.802
------------------------------
• [1.079 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:22:40.728
    Dec 16 14:22:40.728: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-pred 12/16/22 14:22:40.729
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:40.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:40.745
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 16 14:22:40.747: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 16 14:22:40.754: INFO: Waiting for terminating namespaces to be deleted...
    Dec 16 14:22:40.757: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
    Dec 16 14:22:40.763: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:22:40.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:22:40.763: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
    Dec 16 14:22:40.768: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.769: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:22:40.769: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.769: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:22:40.769: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
    Dec 16 14:22:40.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:22:40.769: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:22:40.769: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
    Dec 16 14:22:40.775: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container e2e ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:22:40.775: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:22:40.775: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/16/22 14:22:40.775
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17314c0e3daeed91], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/16/22 14:22:40.798
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:22:41.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8535" for this suite. 12/16/22 14:22:41.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:22:41.807
Dec 16 14:22:41.807: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 14:22:41.808
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:41.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:41.825
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Dec 16 14:22:41.836: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 16 14:22:46.841: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 14:22:46.841
Dec 16 14:22:46.842: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 16 14:22:48.846: INFO: Creating deployment "test-rollover-deployment"
Dec 16 14:22:48.855: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 16 14:22:50.864: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 16 14:22:50.870: INFO: Ensure that both replica sets have 1 created replica
Dec 16 14:22:50.877: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 16 14:22:50.887: INFO: Updating deployment test-rollover-deployment
Dec 16 14:22:50.887: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 16 14:22:52.896: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 16 14:22:52.904: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 16 14:22:52.910: INFO: all replica sets need to contain the pod-template-hash label
Dec 16 14:22:52.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:22:54.918: INFO: all replica sets need to contain the pod-template-hash label
Dec 16 14:22:54.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:22:56.920: INFO: all replica sets need to contain the pod-template-hash label
Dec 16 14:22:56.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:22:58.918: INFO: all replica sets need to contain the pod-template-hash label
Dec 16 14:22:58.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:23:00.919: INFO: all replica sets need to contain the pod-template-hash label
Dec 16 14:23:00.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 16 14:23:02.931: INFO: 
Dec 16 14:23:02.932: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 14:23:02.942: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-821  cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 680999047 2 2022-12-16 14:22:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efc718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 14:22:48 +0000 UTC,LastTransitionTime:2022-12-16 14:22:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-16 14:23:02 +0000 UTC,LastTransitionTime:2022-12-16 14:22:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 16 14:23:02.946: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-821  7bdd12a7-e155-4c56-847c-9f04a234bc78 680999035 2 2022-12-16 14:22:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc000a5f757 0xc000a5f758}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a5ff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:23:02.946: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 16 14:23:02.946: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-821  f8958da9-96a0-4d9b-9cb4-d17758422f5a 680999046 2 2022-12-16 14:22:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc000a5f2f7 0xc000a5f2f8}] [] [{e2e.test Update apps/v1 2022-12-16 14:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a5f5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:23:02.946: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-821  306fa8f9-1fab-41a6-a5f6-7f6627ffb43e 680998660 2 2022-12-16 14:22:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc0047f0027 0xc0047f0028}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047f00d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:23:02.950: INFO: Pod "test-rollover-deployment-6c6df9974f-77727" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-77727 test-rollover-deployment-6c6df9974f- deployment-821  43ad413c-84a3-4b21-9855-70cfaa19cc11 680998785 0 2022-12-16 14:22:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:86bda3adbfa6c0ecd5dae694b0a67308a577b17b35e563b17f6f7496f331eabe cni.projectcalico.org/podIP:192.168.189.32/32 cni.projectcalico.org/podIPs:192.168.189.32/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7bdd12a7-e155-4c56-847c-9f04a234bc78 0xc0032823d7 0xc0032823d8}] [] [{kube-controller-manager Update v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bdd12a7-e155-4c56-847c-9f04a234bc78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:22:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzkmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzkmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.32,StartTime:2022-12-16 14:22:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:22:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cc17003400fe13a3a0231a5b9957407efaf7d1aecf7776d4f186edd4d0d757b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:02.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-821" for this suite. 12/16/22 14:23:02.955
------------------------------
• [SLOW TEST] [21.155 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:22:41.807
    Dec 16 14:22:41.807: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 14:22:41.808
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:22:41.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:22:41.825
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Dec 16 14:22:41.836: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Dec 16 14:22:46.841: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 14:22:46.841
    Dec 16 14:22:46.842: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Dec 16 14:22:48.846: INFO: Creating deployment "test-rollover-deployment"
    Dec 16 14:22:48.855: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Dec 16 14:22:50.864: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Dec 16 14:22:50.870: INFO: Ensure that both replica sets have 1 created replica
    Dec 16 14:22:50.877: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Dec 16 14:22:50.887: INFO: Updating deployment test-rollover-deployment
    Dec 16 14:22:50.887: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Dec 16 14:22:52.896: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Dec 16 14:22:52.904: INFO: Make sure deployment "test-rollover-deployment" is complete
    Dec 16 14:22:52.910: INFO: all replica sets need to contain the pod-template-hash label
    Dec 16 14:22:52.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:22:54.918: INFO: all replica sets need to contain the pod-template-hash label
    Dec 16 14:22:54.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:22:56.920: INFO: all replica sets need to contain the pod-template-hash label
    Dec 16 14:22:56.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:22:58.918: INFO: all replica sets need to contain the pod-template-hash label
    Dec 16 14:22:58.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:23:00.919: INFO: all replica sets need to contain the pod-template-hash label
    Dec 16 14:23:00.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 16, 14, 22, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 16, 14, 22, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 16 14:23:02.931: INFO: 
    Dec 16 14:23:02.932: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 14:23:02.942: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-821  cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 680999047 2 2022-12-16 14:22:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efc718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-16 14:22:48 +0000 UTC,LastTransitionTime:2022-12-16 14:22:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-16 14:23:02 +0000 UTC,LastTransitionTime:2022-12-16 14:22:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 16 14:23:02.946: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-821  7bdd12a7-e155-4c56-847c-9f04a234bc78 680999035 2 2022-12-16 14:22:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc000a5f757 0xc000a5f758}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a5ff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:23:02.946: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Dec 16 14:23:02.946: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-821  f8958da9-96a0-4d9b-9cb4-d17758422f5a 680999046 2 2022-12-16 14:22:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc000a5f2f7 0xc000a5f2f8}] [] [{e2e.test Update apps/v1 2022-12-16 14:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:23:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a5f5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:23:02.946: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-821  306fa8f9-1fab-41a6-a5f6-7f6627ffb43e 680998660 2 2022-12-16 14:22:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8 0xc0047f0027 0xc0047f0028}] [] [{kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbdf6219-3b83-4829-b3a8-3c19d3ff9ae8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047f00d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:23:02.950: INFO: Pod "test-rollover-deployment-6c6df9974f-77727" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-77727 test-rollover-deployment-6c6df9974f- deployment-821  43ad413c-84a3-4b21-9855-70cfaa19cc11 680998785 0 2022-12-16 14:22:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:86bda3adbfa6c0ecd5dae694b0a67308a577b17b35e563b17f6f7496f331eabe cni.projectcalico.org/podIP:192.168.189.32/32 cni.projectcalico.org/podIPs:192.168.189.32/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7bdd12a7-e155-4c56-847c-9f04a234bc78 0xc0032823d7 0xc0032823d8}] [] [{kube-controller-manager Update v1 2022-12-16 14:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bdd12a7-e155-4c56-847c-9f04a234bc78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:22:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzkmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzkmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.32,StartTime:2022-12-16 14:22:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:22:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cc17003400fe13a3a0231a5b9957407efaf7d1aecf7776d4f186edd4d0d757b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:02.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-821" for this suite. 12/16/22 14:23:02.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:02.963
Dec 16 14:23:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:23:02.964
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:02.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:02.981
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:23:02.984
Dec 16 14:23:02.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879" in namespace "projected-6839" to be "Succeeded or Failed"
Dec 16 14:23:02.996: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.621297ms
Dec 16 14:23:05.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550831s
Dec 16 14:23:07.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008662409s
STEP: Saw pod success 12/16/22 14:23:07.002
Dec 16 14:23:07.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879" satisfied condition "Succeeded or Failed"
Dec 16 14:23:07.006: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 container client-container: <nil>
STEP: delete the pod 12/16/22 14:23:07.015
Dec 16 14:23:07.027: INFO: Waiting for pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 to disappear
Dec 16 14:23:07.030: INFO: Pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:07.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6839" for this suite. 12/16/22 14:23:07.035
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:02.963
    Dec 16 14:23:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:23:02.964
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:02.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:02.981
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:23:02.984
    Dec 16 14:23:02.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879" in namespace "projected-6839" to be "Succeeded or Failed"
    Dec 16 14:23:02.996: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.621297ms
    Dec 16 14:23:05.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550831s
    Dec 16 14:23:07.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008662409s
    STEP: Saw pod success 12/16/22 14:23:07.002
    Dec 16 14:23:07.002: INFO: Pod "downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879" satisfied condition "Succeeded or Failed"
    Dec 16 14:23:07.006: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 container client-container: <nil>
    STEP: delete the pod 12/16/22 14:23:07.015
    Dec 16 14:23:07.027: INFO: Waiting for pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 to disappear
    Dec 16 14:23:07.030: INFO: Pod downwardapi-volume-092481a9-be4c-4fbf-bb8a-a0194939c879 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:07.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6839" for this suite. 12/16/22 14:23:07.035
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:07.042
Dec 16 14:23:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 14:23:07.043
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:07.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:07.061
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 12/16/22 14:23:07.083
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:23:07.089
Dec 16 14:23:07.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:23:07.095: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:23:08.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:23:08.110: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:23:09.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 14:23:09.925: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/16/22 14:23:10.747
Dec 16 14:23:11.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:23:11.342: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 14:23:12.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:23:12.350: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
Dec 16 14:23:13.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 14:23:13.353: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/16/22 14:23:13.353
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:23:13.361
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1226, will wait for the garbage collector to delete the pods 12/16/22 14:23:13.361
Dec 16 14:23:13.433: INFO: Deleting DaemonSet.extensions daemon-set took: 6.747768ms
Dec 16 14:23:13.533: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358379ms
Dec 16 14:23:15.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:23:15.840: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 14:23:15.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680999573"},"items":null}

Dec 16 14:23:15.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680999573"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:15.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1226" for this suite. 12/16/22 14:23:15.866
------------------------------
• [SLOW TEST] [8.830 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:07.042
    Dec 16 14:23:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 14:23:07.043
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:07.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:07.061
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 12/16/22 14:23:07.083
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:23:07.089
    Dec 16 14:23:07.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:23:07.095: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:23:08.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:23:08.110: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:23:09.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 14:23:09.925: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/16/22 14:23:10.747
    Dec 16 14:23:11.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:23:11.342: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 14:23:12.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:23:12.350: INFO: Node pool-a3802-fsxxd is running 0 daemon pod, expected 1
    Dec 16 14:23:13.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 14:23:13.353: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/16/22 14:23:13.353
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:23:13.361
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1226, will wait for the garbage collector to delete the pods 12/16/22 14:23:13.361
    Dec 16 14:23:13.433: INFO: Deleting DaemonSet.extensions daemon-set took: 6.747768ms
    Dec 16 14:23:13.533: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358379ms
    Dec 16 14:23:15.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:23:15.840: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 14:23:15.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"680999573"},"items":null}

    Dec 16 14:23:15.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"680999573"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:15.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1226" for this suite. 12/16/22 14:23:15.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:15.873
Dec 16 14:23:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:23:15.874
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:15.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:15.893
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1018 12/16/22 14:23:15.895
STEP: changing the ExternalName service to type=ClusterIP 12/16/22 14:23:15.9
STEP: creating replication controller externalname-service in namespace services-1018 12/16/22 14:23:15.972
I1216 14:23:15.981889      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1018, replica count: 2
I1216 14:23:19.034203      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 14:23:19.034: INFO: Creating new exec pod
Dec 16 14:23:19.042: INFO: Waiting up to 5m0s for pod "execpodjzm9q" in namespace "services-1018" to be "running"
Dec 16 14:23:19.046: INFO: Pod "execpodjzm9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21148ms
Dec 16 14:23:21.050: INFO: Pod "execpodjzm9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007594527s
Dec 16 14:23:21.050: INFO: Pod "execpodjzm9q" satisfied condition "running"
Dec 16 14:23:22.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1018 exec execpodjzm9q -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec 16 14:23:22.253: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 16 14:23:22.253: INFO: stdout: ""
Dec 16 14:23:22.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1018 exec execpodjzm9q -- /bin/sh -x -c nc -v -z -w 2 10.106.227.184 80'
Dec 16 14:23:22.451: INFO: stderr: "+ nc -v -z -w 2 10.106.227.184 80\nConnection to 10.106.227.184 80 port [tcp/http] succeeded!\n"
Dec 16 14:23:22.451: INFO: stdout: ""
Dec 16 14:23:22.451: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:22.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1018" for this suite. 12/16/22 14:23:22.477
------------------------------
• [SLOW TEST] [6.610 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:15.873
    Dec 16 14:23:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:23:15.874
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:15.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:15.893
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1018 12/16/22 14:23:15.895
    STEP: changing the ExternalName service to type=ClusterIP 12/16/22 14:23:15.9
    STEP: creating replication controller externalname-service in namespace services-1018 12/16/22 14:23:15.972
    I1216 14:23:15.981889      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1018, replica count: 2
    I1216 14:23:19.034203      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 14:23:19.034: INFO: Creating new exec pod
    Dec 16 14:23:19.042: INFO: Waiting up to 5m0s for pod "execpodjzm9q" in namespace "services-1018" to be "running"
    Dec 16 14:23:19.046: INFO: Pod "execpodjzm9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21148ms
    Dec 16 14:23:21.050: INFO: Pod "execpodjzm9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007594527s
    Dec 16 14:23:21.050: INFO: Pod "execpodjzm9q" satisfied condition "running"
    Dec 16 14:23:22.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1018 exec execpodjzm9q -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec 16 14:23:22.253: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 16 14:23:22.253: INFO: stdout: ""
    Dec 16 14:23:22.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1018 exec execpodjzm9q -- /bin/sh -x -c nc -v -z -w 2 10.106.227.184 80'
    Dec 16 14:23:22.451: INFO: stderr: "+ nc -v -z -w 2 10.106.227.184 80\nConnection to 10.106.227.184 80 port [tcp/http] succeeded!\n"
    Dec 16 14:23:22.451: INFO: stdout: ""
    Dec 16 14:23:22.451: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:22.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1018" for this suite. 12/16/22 14:23:22.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:22.483
Dec 16 14:23:22.483: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 14:23:22.484
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:22.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:22.501
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 12/16/22 14:23:22.504
STEP: delete the rc 12/16/22 14:23:27.514
STEP: wait for all pods to be garbage collected 12/16/22 14:23:27.52
STEP: Gathering metrics 12/16/22 14:23:32.528
W1216 14:23:32.536908      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 14:23:32.536: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:32.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8340" for this suite. 12/16/22 14:23:32.54
------------------------------
• [SLOW TEST] [10.062 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:22.483
    Dec 16 14:23:22.483: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 14:23:22.484
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:22.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:22.501
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 12/16/22 14:23:22.504
    STEP: delete the rc 12/16/22 14:23:27.514
    STEP: wait for all pods to be garbage collected 12/16/22 14:23:27.52
    STEP: Gathering metrics 12/16/22 14:23:32.528
    W1216 14:23:32.536908      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 14:23:32.536: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:32.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8340" for this suite. 12/16/22 14:23:32.54
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:32.546
Dec 16 14:23:32.546: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-runtime 12/16/22 14:23:32.546
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:32.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:32.564
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/16/22 14:23:32.575
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/16/22 14:23:49.669
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/16/22 14:23:49.673
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/16/22 14:23:49.68
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/16/22 14:23:49.68
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/16/22 14:23:49.698
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/16/22 14:23:52.717
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/16/22 14:23:54.731
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/16/22 14:23:54.738
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/16/22 14:23:54.738
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/16/22 14:23:54.758
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/16/22 14:23:55.771
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/16/22 14:23:58.797
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/16/22 14:23:58.805
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/16/22 14:23:58.805
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 16 14:23:58.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5389" for this suite. 12/16/22 14:23:58.834
------------------------------
• [SLOW TEST] [26.294 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:32.546
    Dec 16 14:23:32.546: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-runtime 12/16/22 14:23:32.546
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:32.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:32.564
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/16/22 14:23:32.575
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/16/22 14:23:49.669
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/16/22 14:23:49.673
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/16/22 14:23:49.68
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/16/22 14:23:49.68
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/16/22 14:23:49.698
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/16/22 14:23:52.717
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/16/22 14:23:54.731
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/16/22 14:23:54.738
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/16/22 14:23:54.738
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/16/22 14:23:54.758
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/16/22 14:23:55.771
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/16/22 14:23:58.797
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/16/22 14:23:58.805
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/16/22 14:23:58.805
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:23:58.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5389" for this suite. 12/16/22 14:23:58.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:23:58.841
Dec 16 14:23:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 14:23:58.841
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:58.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:58.88
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 12/16/22 14:23:58.883
STEP: Ensuring active pods == parallelism 12/16/22 14:23:58.89
STEP: Orphaning one of the Job's Pods 12/16/22 14:24:00.895
Dec 16 14:24:01.415: INFO: Successfully updated pod "adopt-release-gd4ht"
STEP: Checking that the Job readopts the Pod 12/16/22 14:24:01.415
Dec 16 14:24:01.415: INFO: Waiting up to 15m0s for pod "adopt-release-gd4ht" in namespace "job-9290" to be "adopted"
Dec 16 14:24:01.418: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 3.332232ms
Dec 16 14:24:03.423: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.007826724s
Dec 16 14:24:03.423: INFO: Pod "adopt-release-gd4ht" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 12/16/22 14:24:03.423
Dec 16 14:24:03.935: INFO: Successfully updated pod "adopt-release-gd4ht"
STEP: Checking that the Job releases the Pod 12/16/22 14:24:03.935
Dec 16 14:24:03.935: INFO: Waiting up to 15m0s for pod "adopt-release-gd4ht" in namespace "job-9290" to be "released"
Dec 16 14:24:03.939: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 3.33907ms
Dec 16 14:24:05.944: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.008945873s
Dec 16 14:24:05.944: INFO: Pod "adopt-release-gd4ht" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:05.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9290" for this suite. 12/16/22 14:24:05.95
------------------------------
• [SLOW TEST] [7.115 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:23:58.841
    Dec 16 14:23:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 14:23:58.841
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:23:58.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:23:58.88
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 12/16/22 14:23:58.883
    STEP: Ensuring active pods == parallelism 12/16/22 14:23:58.89
    STEP: Orphaning one of the Job's Pods 12/16/22 14:24:00.895
    Dec 16 14:24:01.415: INFO: Successfully updated pod "adopt-release-gd4ht"
    STEP: Checking that the Job readopts the Pod 12/16/22 14:24:01.415
    Dec 16 14:24:01.415: INFO: Waiting up to 15m0s for pod "adopt-release-gd4ht" in namespace "job-9290" to be "adopted"
    Dec 16 14:24:01.418: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 3.332232ms
    Dec 16 14:24:03.423: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.007826724s
    Dec 16 14:24:03.423: INFO: Pod "adopt-release-gd4ht" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 12/16/22 14:24:03.423
    Dec 16 14:24:03.935: INFO: Successfully updated pod "adopt-release-gd4ht"
    STEP: Checking that the Job releases the Pod 12/16/22 14:24:03.935
    Dec 16 14:24:03.935: INFO: Waiting up to 15m0s for pod "adopt-release-gd4ht" in namespace "job-9290" to be "released"
    Dec 16 14:24:03.939: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 3.33907ms
    Dec 16 14:24:05.944: INFO: Pod "adopt-release-gd4ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.008945873s
    Dec 16 14:24:05.944: INFO: Pod "adopt-release-gd4ht" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:05.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9290" for this suite. 12/16/22 14:24:05.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:05.958
Dec 16 14:24:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 14:24:05.958
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:05.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:05.978
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 12/16/22 14:24:05.98
Dec 16 14:24:05.989: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8" in namespace "emptydir-8585" to be "running"
Dec 16 14:24:05.993: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579176ms
Dec 16 14:24:07.998: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.009369573s
Dec 16 14:24:07.998: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8" satisfied condition "running"
STEP: Reading file content from the nginx-container 12/16/22 14:24:07.998
Dec 16 14:24:07.998: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8585 PodName:pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:24:07.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:24:07.999: INFO: ExecWithOptions: Clientset creation
Dec 16 14:24:07.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8585/pods/pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec 16 14:24:08.115: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:08.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8585" for this suite. 12/16/22 14:24:08.12
------------------------------
• [2.169 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:05.958
    Dec 16 14:24:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 14:24:05.958
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:05.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:05.978
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 12/16/22 14:24:05.98
    Dec 16 14:24:05.989: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8" in namespace "emptydir-8585" to be "running"
    Dec 16 14:24:05.993: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579176ms
    Dec 16 14:24:07.998: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.009369573s
    Dec 16 14:24:07.998: INFO: Pod "pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 12/16/22 14:24:07.998
    Dec 16 14:24:07.998: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8585 PodName:pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:24:07.998: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:24:07.999: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:24:07.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8585/pods/pod-sharedvolume-2f721daa-ec97-4e8c-ab3e-12ff4b23e9d8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Dec 16 14:24:08.115: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:08.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8585" for this suite. 12/16/22 14:24:08.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:08.13
Dec 16 14:24:08.130: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename var-expansion 12/16/22 14:24:08.13
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:08.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:08.149
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Dec 16 14:24:08.158: INFO: Waiting up to 2m0s for pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" in namespace "var-expansion-8380" to be "container 0 failed with reason CreateContainerConfigError"
Dec 16 14:24:08.161: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961995ms
Dec 16 14:24:10.167: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008188117s
Dec 16 14:24:10.167: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 16 14:24:10.167: INFO: Deleting pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" in namespace "var-expansion-8380"
Dec 16 14:24:10.174: INFO: Wait up to 5m0s for pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:14.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8380" for this suite. 12/16/22 14:24:14.188
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:08.13
    Dec 16 14:24:08.130: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename var-expansion 12/16/22 14:24:08.13
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:08.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:08.149
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Dec 16 14:24:08.158: INFO: Waiting up to 2m0s for pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" in namespace "var-expansion-8380" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 16 14:24:08.161: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961995ms
    Dec 16 14:24:10.167: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008188117s
    Dec 16 14:24:10.167: INFO: Pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 16 14:24:10.167: INFO: Deleting pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" in namespace "var-expansion-8380"
    Dec 16 14:24:10.174: INFO: Wait up to 5m0s for pod "var-expansion-f8068ae1-dac8-4323-9f78-8cae7908e477" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:14.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8380" for this suite. 12/16/22 14:24:14.188
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:14.195
Dec 16 14:24:14.195: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename ephemeral-containers-test 12/16/22 14:24:14.195
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:14.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:14.213
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 12/16/22 14:24:14.215
Dec 16 14:24:14.224: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5723" to be "running and ready"
Dec 16 14:24:14.227: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988518ms
Dec 16 14:24:14.227: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:24:16.232: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008377965s
Dec 16 14:24:16.232: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Dec 16 14:24:16.232: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 12/16/22 14:24:16.236
Dec 16 14:24:16.247: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5723" to be "container debugger running"
Dec 16 14:24:16.250: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.338328ms
Dec 16 14:24:18.257: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009670021s
Dec 16 14:24:20.256: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008969727s
Dec 16 14:24:20.256: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 12/16/22 14:24:20.256
Dec 16 14:24:20.256: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5723 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:24:20.256: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:24:20.257: INFO: ExecWithOptions: Clientset creation
Dec 16 14:24:20.257: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-5723/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Dec 16 14:24:20.374: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:20.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-5723" for this suite. 12/16/22 14:24:20.388
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:14.195
    Dec 16 14:24:14.195: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename ephemeral-containers-test 12/16/22 14:24:14.195
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:14.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:14.213
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 12/16/22 14:24:14.215
    Dec 16 14:24:14.224: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5723" to be "running and ready"
    Dec 16 14:24:14.227: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988518ms
    Dec 16 14:24:14.227: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:24:16.232: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008377965s
    Dec 16 14:24:16.232: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Dec 16 14:24:16.232: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 12/16/22 14:24:16.236
    Dec 16 14:24:16.247: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5723" to be "container debugger running"
    Dec 16 14:24:16.250: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.338328ms
    Dec 16 14:24:18.257: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009670021s
    Dec 16 14:24:20.256: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008969727s
    Dec 16 14:24:20.256: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 12/16/22 14:24:20.256
    Dec 16 14:24:20.256: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5723 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:24:20.256: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:24:20.257: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:24:20.257: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-5723/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Dec 16 14:24:20.374: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:20.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-5723" for this suite. 12/16/22 14:24:20.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:20.395
Dec 16 14:24:20.395: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:24:20.396
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:20.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:20.414
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 12/16/22 14:24:20.417
STEP: getting /apis/node.k8s.io 12/16/22 14:24:20.419
STEP: getting /apis/node.k8s.io/v1 12/16/22 14:24:20.42
STEP: creating 12/16/22 14:24:20.421
STEP: watching 12/16/22 14:24:20.438
Dec 16 14:24:20.438: INFO: starting watch
STEP: getting 12/16/22 14:24:20.444
STEP: listing 12/16/22 14:24:20.447
STEP: patching 12/16/22 14:24:20.45
STEP: updating 12/16/22 14:24:20.455
Dec 16 14:24:20.464: INFO: waiting for watch events with expected annotations
STEP: deleting 12/16/22 14:24:20.464
STEP: deleting a collection 12/16/22 14:24:20.476
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:20.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1123" for this suite. 12/16/22 14:24:20.494
------------------------------
• [0.106 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:20.395
    Dec 16 14:24:20.395: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:24:20.396
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:20.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:20.414
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 12/16/22 14:24:20.417
    STEP: getting /apis/node.k8s.io 12/16/22 14:24:20.419
    STEP: getting /apis/node.k8s.io/v1 12/16/22 14:24:20.42
    STEP: creating 12/16/22 14:24:20.421
    STEP: watching 12/16/22 14:24:20.438
    Dec 16 14:24:20.438: INFO: starting watch
    STEP: getting 12/16/22 14:24:20.444
    STEP: listing 12/16/22 14:24:20.447
    STEP: patching 12/16/22 14:24:20.45
    STEP: updating 12/16/22 14:24:20.455
    Dec 16 14:24:20.464: INFO: waiting for watch events with expected annotations
    STEP: deleting 12/16/22 14:24:20.464
    STEP: deleting a collection 12/16/22 14:24:20.476
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:20.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1123" for this suite. 12/16/22 14:24:20.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:20.503
Dec 16 14:24:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:24:20.504
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:20.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:20.521
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-3416 12/16/22 14:24:20.524
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[] 12/16/22 14:24:20.538
Dec 16 14:24:20.541: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 16 14:24:21.552: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3416 12/16/22 14:24:21.552
Dec 16 14:24:21.560: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3416" to be "running and ready"
Dec 16 14:24:21.563: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.984428ms
Dec 16 14:24:21.563: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:24:23.569: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009309364s
Dec 16 14:24:23.569: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 16 14:24:23.569: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod1:[100]] 12/16/22 14:24:23.573
Dec 16 14:24:23.584: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3416 12/16/22 14:24:23.584
Dec 16 14:24:23.590: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3416" to be "running and ready"
Dec 16 14:24:23.593: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.496443ms
Dec 16 14:24:23.593: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:24:25.598: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007940582s
Dec 16 14:24:25.598: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 16 14:24:25.598: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod1:[100] pod2:[101]] 12/16/22 14:24:25.601
Dec 16 14:24:25.615: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 12/16/22 14:24:25.615
Dec 16 14:24:25.615: INFO: Creating new exec pod
Dec 16 14:24:25.620: INFO: Waiting up to 5m0s for pod "execpodk8m8q" in namespace "services-3416" to be "running"
Dec 16 14:24:25.624: INFO: Pod "execpodk8m8q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007537ms
Dec 16 14:24:27.629: INFO: Pod "execpodk8m8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.009654833s
Dec 16 14:24:27.629: INFO: Pod "execpodk8m8q" satisfied condition "running"
Dec 16 14:24:28.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Dec 16 14:24:28.831: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec 16 14:24:28.831: INFO: stdout: ""
Dec 16 14:24:28.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 10.103.15.96 80'
Dec 16 14:24:29.010: INFO: stderr: "+ nc -v -z -w 2 10.103.15.96 80\nConnection to 10.103.15.96 80 port [tcp/http] succeeded!\n"
Dec 16 14:24:29.010: INFO: stdout: ""
Dec 16 14:24:29.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Dec 16 14:24:29.365: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec 16 14:24:29.365: INFO: stdout: ""
Dec 16 14:24:29.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 10.103.15.96 81'
Dec 16 14:24:29.830: INFO: stderr: "+ nc -v -z -w 2 10.103.15.96 81\nConnection to 10.103.15.96 81 port [tcp/*] succeeded!\n"
Dec 16 14:24:29.830: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3416 12/16/22 14:24:29.83
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod2:[101]] 12/16/22 14:24:29.958
Dec 16 14:24:30.181: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3416 12/16/22 14:24:30.181
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[] 12/16/22 14:24:30.325
Dec 16 14:24:31.338: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:31.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3416" for this suite. 12/16/22 14:24:31.507
------------------------------
• [SLOW TEST] [11.011 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:20.503
    Dec 16 14:24:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:24:20.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:20.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:20.521
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-3416 12/16/22 14:24:20.524
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[] 12/16/22 14:24:20.538
    Dec 16 14:24:20.541: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Dec 16 14:24:21.552: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3416 12/16/22 14:24:21.552
    Dec 16 14:24:21.560: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3416" to be "running and ready"
    Dec 16 14:24:21.563: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.984428ms
    Dec 16 14:24:21.563: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:24:23.569: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009309364s
    Dec 16 14:24:23.569: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 16 14:24:23.569: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod1:[100]] 12/16/22 14:24:23.573
    Dec 16 14:24:23.584: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3416 12/16/22 14:24:23.584
    Dec 16 14:24:23.590: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3416" to be "running and ready"
    Dec 16 14:24:23.593: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.496443ms
    Dec 16 14:24:23.593: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:24:25.598: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007940582s
    Dec 16 14:24:25.598: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 16 14:24:25.598: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod1:[100] pod2:[101]] 12/16/22 14:24:25.601
    Dec 16 14:24:25.615: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 12/16/22 14:24:25.615
    Dec 16 14:24:25.615: INFO: Creating new exec pod
    Dec 16 14:24:25.620: INFO: Waiting up to 5m0s for pod "execpodk8m8q" in namespace "services-3416" to be "running"
    Dec 16 14:24:25.624: INFO: Pod "execpodk8m8q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007537ms
    Dec 16 14:24:27.629: INFO: Pod "execpodk8m8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.009654833s
    Dec 16 14:24:27.629: INFO: Pod "execpodk8m8q" satisfied condition "running"
    Dec 16 14:24:28.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Dec 16 14:24:28.831: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Dec 16 14:24:28.831: INFO: stdout: ""
    Dec 16 14:24:28.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 10.103.15.96 80'
    Dec 16 14:24:29.010: INFO: stderr: "+ nc -v -z -w 2 10.103.15.96 80\nConnection to 10.103.15.96 80 port [tcp/http] succeeded!\n"
    Dec 16 14:24:29.010: INFO: stdout: ""
    Dec 16 14:24:29.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Dec 16 14:24:29.365: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Dec 16 14:24:29.365: INFO: stdout: ""
    Dec 16 14:24:29.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-3416 exec execpodk8m8q -- /bin/sh -x -c nc -v -z -w 2 10.103.15.96 81'
    Dec 16 14:24:29.830: INFO: stderr: "+ nc -v -z -w 2 10.103.15.96 81\nConnection to 10.103.15.96 81 port [tcp/*] succeeded!\n"
    Dec 16 14:24:29.830: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3416 12/16/22 14:24:29.83
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[pod2:[101]] 12/16/22 14:24:29.958
    Dec 16 14:24:30.181: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3416 12/16/22 14:24:30.181
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3416 to expose endpoints map[] 12/16/22 14:24:30.325
    Dec 16 14:24:31.338: INFO: successfully validated that service multi-endpoint-test in namespace services-3416 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:31.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3416" for this suite. 12/16/22 14:24:31.507
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:31.514
Dec 16 14:24:31.514: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 14:24:31.515
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:31.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:31.695
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/16/22 14:24:31.698
Dec 16 14:24:31.797: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5266" to be "running and ready"
Dec 16 14:24:31.801: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369464ms
Dec 16 14:24:31.801: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:24:33.807: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009504142s
Dec 16 14:24:33.807: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Dec 16 14:24:33.807: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 12/16/22 14:24:33.811
STEP: Then the orphan pod is adopted 12/16/22 14:24:33.816
STEP: When the matched label of one of its pods change 12/16/22 14:24:34.824
Dec 16 14:24:34.828: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 12/16/22 14:24:34.861
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:35.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5266" for this suite. 12/16/22 14:24:35.983
------------------------------
• [4.476 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:31.514
    Dec 16 14:24:31.514: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 14:24:31.515
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:31.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:31.695
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/16/22 14:24:31.698
    Dec 16 14:24:31.797: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5266" to be "running and ready"
    Dec 16 14:24:31.801: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369464ms
    Dec 16 14:24:31.801: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:24:33.807: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009504142s
    Dec 16 14:24:33.807: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Dec 16 14:24:33.807: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 12/16/22 14:24:33.811
    STEP: Then the orphan pod is adopted 12/16/22 14:24:33.816
    STEP: When the matched label of one of its pods change 12/16/22 14:24:34.824
    Dec 16 14:24:34.828: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/16/22 14:24:34.861
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:35.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5266" for this suite. 12/16/22 14:24:35.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:35.99
Dec 16 14:24:35.990: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:24:35.991
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:36.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:36.008
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:24:36.01
Dec 16 14:24:36.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b" in namespace "projected-7627" to be "Succeeded or Failed"
Dec 16 14:24:36.022: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624847ms
Dec 16 14:24:38.027: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00886339s
Dec 16 14:24:40.029: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010499045s
STEP: Saw pod success 12/16/22 14:24:40.029
Dec 16 14:24:40.029: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b" satisfied condition "Succeeded or Failed"
Dec 16 14:24:40.032: INFO: Trying to get logs from node pool-a3802-ehprg pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b container client-container: <nil>
STEP: delete the pod 12/16/22 14:24:40.087
Dec 16 14:24:40.100: INFO: Waiting for pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b to disappear
Dec 16 14:24:40.103: INFO: Pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:24:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7627" for this suite. 12/16/22 14:24:40.107
------------------------------
• [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:35.99
    Dec 16 14:24:35.990: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:24:35.991
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:36.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:36.008
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:24:36.01
    Dec 16 14:24:36.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b" in namespace "projected-7627" to be "Succeeded or Failed"
    Dec 16 14:24:36.022: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624847ms
    Dec 16 14:24:38.027: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00886339s
    Dec 16 14:24:40.029: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010499045s
    STEP: Saw pod success 12/16/22 14:24:40.029
    Dec 16 14:24:40.029: INFO: Pod "downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b" satisfied condition "Succeeded or Failed"
    Dec 16 14:24:40.032: INFO: Trying to get logs from node pool-a3802-ehprg pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b container client-container: <nil>
    STEP: delete the pod 12/16/22 14:24:40.087
    Dec 16 14:24:40.100: INFO: Waiting for pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b to disappear
    Dec 16 14:24:40.103: INFO: Pod downwardapi-volume-57b3f59d-75cf-4fb4-8ee2-ec2ec0ced40b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:24:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7627" for this suite. 12/16/22 14:24:40.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:24:40.113
Dec 16 14:24:40.113: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 14:24:40.113
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:40.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:40.131
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5800 12/16/22 14:24:40.134
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-5800 12/16/22 14:24:40.139
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5800 12/16/22 14:24:40.145
Dec 16 14:24:40.147: INFO: Found 0 stateful pods, waiting for 1
Dec 16 14:24:50.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/16/22 14:24:50.157
Dec 16 14:24:50.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:24:50.364: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:24:50.364: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:24:50.364: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 14:24:50.369: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 16 14:25:00.377: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 14:25:00.377: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:25:00.396: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Dec 16 14:25:00.396: INFO: ss-0  pool-a3802-fsxxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  }]
Dec 16 14:25:00.396: INFO: 
Dec 16 14:25:00.396: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 16 14:25:01.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995507489s
Dec 16 14:25:02.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989841327s
Dec 16 14:25:03.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985086239s
Dec 16 14:25:04.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978793455s
Dec 16 14:25:05.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973058892s
Dec 16 14:25:06.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967209785s
Dec 16 14:25:07.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961842574s
Dec 16 14:25:08.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955660116s
Dec 16 14:25:09.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.797115ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5800 12/16/22 14:25:10.447
Dec 16 14:25:10.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 14:25:10.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 16 14:25:10.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 14:25:10.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 14:25:10.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 14:25:10.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 16 14:25:10.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 14:25:10.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 14:25:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 16 14:25:11.021: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 16 14:25:11.021: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 16 14:25:11.021: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 16 14:25:11.025: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 16 14:25:21.031: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 14:25:21.031: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 16 14:25:21.031: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 12/16/22 14:25:21.031
Dec 16 14:25:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:25:21.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:25:21.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:25:21.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 14:25:21.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:25:21.410: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:25:21.410: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:25:21.410: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 14:25:21.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 16 14:25:21.590: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 16 14:25:21.590: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 16 14:25:21.590: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 16 14:25:21.590: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:25:21.595: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 16 14:25:31.605: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 14:25:31.605: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 14:25:31.605: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 16 14:25:31.618: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Dec 16 14:25:31.618: INFO: ss-0  pool-a3802-fsxxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  }]
Dec 16 14:25:31.618: INFO: ss-1  pool-a3802-ehprg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
Dec 16 14:25:31.618: INFO: ss-2  pool-a3802-oewtd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
Dec 16 14:25:31.618: INFO: 
Dec 16 14:25:31.618: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 16 14:25:32.624: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Dec 16 14:25:32.624: INFO: ss-2  pool-a3802-oewtd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
Dec 16 14:25:32.624: INFO: 
Dec 16 14:25:32.624: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 16 14:25:33.629: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990774826s
Dec 16 14:25:34.634: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98556283s
Dec 16 14:25:35.639: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980394506s
Dec 16 14:25:36.643: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975867556s
Dec 16 14:25:37.648: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971377256s
Dec 16 14:25:38.654: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966407679s
Dec 16 14:25:39.659: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960849225s
Dec 16 14:25:40.663: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.793567ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5800 12/16/22 14:25:41.663
Dec 16 14:25:41.669: INFO: Scaling statefulset ss to 0
Dec 16 14:25:41.680: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 14:25:41.684: INFO: Deleting all statefulset in ns statefulset-5800
Dec 16 14:25:41.687: INFO: Scaling statefulset ss to 0
Dec 16 14:25:41.698: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:25:41.701: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:25:41.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5800" for this suite. 12/16/22 14:25:41.72
------------------------------
• [SLOW TEST] [61.613 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:24:40.113
    Dec 16 14:24:40.113: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 14:24:40.113
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:24:40.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:24:40.131
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5800 12/16/22 14:24:40.134
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-5800 12/16/22 14:24:40.139
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5800 12/16/22 14:24:40.145
    Dec 16 14:24:40.147: INFO: Found 0 stateful pods, waiting for 1
    Dec 16 14:24:50.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/16/22 14:24:50.157
    Dec 16 14:24:50.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:24:50.364: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:24:50.364: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:24:50.364: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 14:24:50.369: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 16 14:25:00.377: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 14:25:00.377: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:25:00.396: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Dec 16 14:25:00.396: INFO: ss-0  pool-a3802-fsxxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  }]
    Dec 16 14:25:00.396: INFO: 
    Dec 16 14:25:00.396: INFO: StatefulSet ss has not reached scale 3, at 1
    Dec 16 14:25:01.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995507489s
    Dec 16 14:25:02.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989841327s
    Dec 16 14:25:03.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985086239s
    Dec 16 14:25:04.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978793455s
    Dec 16 14:25:05.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973058892s
    Dec 16 14:25:06.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967209785s
    Dec 16 14:25:07.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961842574s
    Dec 16 14:25:08.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955660116s
    Dec 16 14:25:09.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.797115ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5800 12/16/22 14:25:10.447
    Dec 16 14:25:10.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 14:25:10.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 16 14:25:10.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 14:25:10.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 14:25:10.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 14:25:10.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 16 14:25:10.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 14:25:10.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 14:25:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 16 14:25:11.021: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 16 14:25:11.021: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 16 14:25:11.021: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 16 14:25:11.025: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Dec 16 14:25:21.031: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 14:25:21.031: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 16 14:25:21.031: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 12/16/22 14:25:21.031
    Dec 16 14:25:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:25:21.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:25:21.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:25:21.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 14:25:21.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:25:21.410: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:25:21.410: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:25:21.410: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 14:25:21.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=statefulset-5800 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 16 14:25:21.590: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 16 14:25:21.590: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 16 14:25:21.590: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 16 14:25:21.590: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:25:21.595: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Dec 16 14:25:31.605: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 14:25:31.605: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 14:25:31.605: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 16 14:25:31.618: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Dec 16 14:25:31.618: INFO: ss-0  pool-a3802-fsxxd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:24:40 +0000 UTC  }]
    Dec 16 14:25:31.618: INFO: ss-1  pool-a3802-ehprg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
    Dec 16 14:25:31.618: INFO: ss-2  pool-a3802-oewtd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
    Dec 16 14:25:31.618: INFO: 
    Dec 16 14:25:31.618: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec 16 14:25:32.624: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Dec 16 14:25:32.624: INFO: ss-2  pool-a3802-oewtd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-16 14:25:00 +0000 UTC  }]
    Dec 16 14:25:32.624: INFO: 
    Dec 16 14:25:32.624: INFO: StatefulSet ss has not reached scale 0, at 1
    Dec 16 14:25:33.629: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990774826s
    Dec 16 14:25:34.634: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98556283s
    Dec 16 14:25:35.639: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980394506s
    Dec 16 14:25:36.643: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975867556s
    Dec 16 14:25:37.648: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971377256s
    Dec 16 14:25:38.654: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966407679s
    Dec 16 14:25:39.659: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960849225s
    Dec 16 14:25:40.663: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.793567ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5800 12/16/22 14:25:41.663
    Dec 16 14:25:41.669: INFO: Scaling statefulset ss to 0
    Dec 16 14:25:41.680: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 14:25:41.684: INFO: Deleting all statefulset in ns statefulset-5800
    Dec 16 14:25:41.687: INFO: Scaling statefulset ss to 0
    Dec 16 14:25:41.698: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:25:41.701: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:25:41.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5800" for this suite. 12/16/22 14:25:41.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:25:41.727
Dec 16 14:25:41.727: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 14:25:41.727
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:41.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:41.744
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 12/16/22 14:25:41.75
STEP: Patching the Job 12/16/22 14:25:41.756
STEP: Watching for Job to be patched 12/16/22 14:25:41.777
Dec 16 14:25:41.778: INFO: Event ADDED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 16 14:25:41.778: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 16 14:25:41.778: INFO: Event MODIFIED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 12/16/22 14:25:41.778
STEP: Watching for Job to be updated 12/16/22 14:25:41.788
Dec 16 14:25:41.789: INFO: Event MODIFIED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:41.789: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 12/16/22 14:25:41.789
Dec 16 14:25:41.792: INFO: Job: e2e-tmcdg as labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched]
STEP: Waiting for job to complete 12/16/22 14:25:41.792
STEP: Delete a job collection with a labelselector 12/16/22 14:25:49.799
STEP: Watching for Job to be deleted 12/16/22 14:25:49.808
Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 16 14:25:49.810: INFO: Event DELETED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 12/16/22 14:25:49.81
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 14:25:49.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2140" for this suite. 12/16/22 14:25:49.817
------------------------------
• [SLOW TEST] [8.106 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:25:41.727
    Dec 16 14:25:41.727: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 14:25:41.727
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:41.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:41.744
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 12/16/22 14:25:41.75
    STEP: Patching the Job 12/16/22 14:25:41.756
    STEP: Watching for Job to be patched 12/16/22 14:25:41.777
    Dec 16 14:25:41.778: INFO: Event ADDED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 16 14:25:41.778: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 16 14:25:41.778: INFO: Event MODIFIED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 12/16/22 14:25:41.778
    STEP: Watching for Job to be updated 12/16/22 14:25:41.788
    Dec 16 14:25:41.789: INFO: Event MODIFIED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:41.789: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 12/16/22 14:25:41.789
    Dec 16 14:25:41.792: INFO: Job: e2e-tmcdg as labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched]
    STEP: Waiting for job to complete 12/16/22 14:25:41.792
    STEP: Delete a job collection with a labelselector 12/16/22 14:25:49.799
    STEP: Watching for Job to be deleted 12/16/22 14:25:49.808
    Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:49.810: INFO: Event MODIFIED observed for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 16 14:25:49.810: INFO: Event DELETED found for Job e2e-tmcdg in namespace job-2140 with labels: map[e2e-job-label:e2e-tmcdg e2e-tmcdg:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 12/16/22 14:25:49.81
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:25:49.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2140" for this suite. 12/16/22 14:25:49.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:25:49.833
Dec 16 14:25:49.833: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:25:49.833
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:49.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:49.85
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Dec 16 14:25:49.855: INFO: Got root ca configmap in namespace "svcaccounts-7665"
Dec 16 14:25:49.861: INFO: Deleted root ca configmap in namespace "svcaccounts-7665"
STEP: waiting for a new root ca configmap created 12/16/22 14:25:50.362
Dec 16 14:25:50.367: INFO: Recreated root ca configmap in namespace "svcaccounts-7665"
Dec 16 14:25:50.372: INFO: Updated root ca configmap in namespace "svcaccounts-7665"
STEP: waiting for the root ca configmap reconciled 12/16/22 14:25:50.872
Dec 16 14:25:50.877: INFO: Reconciled root ca configmap in namespace "svcaccounts-7665"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 14:25:50.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7665" for this suite. 12/16/22 14:25:50.882
------------------------------
• [1.056 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:25:49.833
    Dec 16 14:25:49.833: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:25:49.833
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:49.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:49.85
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Dec 16 14:25:49.855: INFO: Got root ca configmap in namespace "svcaccounts-7665"
    Dec 16 14:25:49.861: INFO: Deleted root ca configmap in namespace "svcaccounts-7665"
    STEP: waiting for a new root ca configmap created 12/16/22 14:25:50.362
    Dec 16 14:25:50.367: INFO: Recreated root ca configmap in namespace "svcaccounts-7665"
    Dec 16 14:25:50.372: INFO: Updated root ca configmap in namespace "svcaccounts-7665"
    STEP: waiting for the root ca configmap reconciled 12/16/22 14:25:50.872
    Dec 16 14:25:50.877: INFO: Reconciled root ca configmap in namespace "svcaccounts-7665"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:25:50.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7665" for this suite. 12/16/22 14:25:50.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:25:50.889
Dec 16 14:25:50.889: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 14:25:50.889
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:50.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:50.907
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 12/16/22 14:25:50.91
Dec 16 14:25:50.917: INFO: Waiting up to 5m0s for pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4" in namespace "downward-api-4406" to be "running and ready"
Dec 16 14:25:50.920: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.057257ms
Dec 16 14:25:50.920: INFO: The phase of Pod labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:25:52.926: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00933418s
Dec 16 14:25:52.926: INFO: The phase of Pod labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4 is Running (Ready = true)
Dec 16 14:25:52.926: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4" satisfied condition "running and ready"
Dec 16 14:25:53.496: INFO: Successfully updated pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 16 14:25:57.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4406" for this suite. 12/16/22 14:25:57.529
------------------------------
• [SLOW TEST] [6.648 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:25:50.889
    Dec 16 14:25:50.889: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 14:25:50.889
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:50.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:50.907
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 12/16/22 14:25:50.91
    Dec 16 14:25:50.917: INFO: Waiting up to 5m0s for pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4" in namespace "downward-api-4406" to be "running and ready"
    Dec 16 14:25:50.920: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.057257ms
    Dec 16 14:25:50.920: INFO: The phase of Pod labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:25:52.926: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00933418s
    Dec 16 14:25:52.926: INFO: The phase of Pod labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4 is Running (Ready = true)
    Dec 16 14:25:52.926: INFO: Pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4" satisfied condition "running and ready"
    Dec 16 14:25:53.496: INFO: Successfully updated pod "labelsupdate4dcd3880-caf7-45ec-ab7c-225aba20c7b4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:25:57.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4406" for this suite. 12/16/22 14:25:57.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:25:57.537
Dec 16 14:25:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename security-context-test 12/16/22 14:25:57.538
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:57.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:57.555
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Dec 16 14:25:57.566: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84" in namespace "security-context-test-7974" to be "Succeeded or Failed"
Dec 16 14:25:57.568: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831093ms
Dec 16 14:25:59.575: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00950716s
Dec 16 14:26:01.573: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007426717s
Dec 16 14:26:01.573: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84" satisfied condition "Succeeded or Failed"
Dec 16 14:26:01.582: INFO: Got logs for pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:01.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7974" for this suite. 12/16/22 14:26:01.588
------------------------------
• [4.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:25:57.537
    Dec 16 14:25:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename security-context-test 12/16/22 14:25:57.538
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:25:57.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:25:57.555
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Dec 16 14:25:57.566: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84" in namespace "security-context-test-7974" to be "Succeeded or Failed"
    Dec 16 14:25:57.568: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831093ms
    Dec 16 14:25:59.575: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00950716s
    Dec 16 14:26:01.573: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007426717s
    Dec 16 14:26:01.573: INFO: Pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84" satisfied condition "Succeeded or Failed"
    Dec 16 14:26:01.582: INFO: Got logs for pod "busybox-privileged-false-d91e636b-0301-4d02-9e2c-c67912cb3d84": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:01.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7974" for this suite. 12/16/22 14:26:01.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:01.595
Dec 16 14:26:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename limitrange 12/16/22 14:26:01.595
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:01.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:01.613
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-dl486" in namespace "limitrange-68" 12/16/22 14:26:01.616
STEP: Creating another limitRange in another namespace 12/16/22 14:26:01.622
Dec 16 14:26:01.636: INFO: Namespace "e2e-limitrange-dl486-9601" created
Dec 16 14:26:01.636: INFO: Creating LimitRange "e2e-limitrange-dl486" in namespace "e2e-limitrange-dl486-9601"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dl486" 12/16/22 14:26:01.641
Dec 16 14:26:01.645: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-dl486" in "limitrange-68" namespace 12/16/22 14:26:01.645
Dec 16 14:26:01.652: INFO: LimitRange "e2e-limitrange-dl486" has been patched
STEP: Delete LimitRange "e2e-limitrange-dl486" by Collection with labelSelector: "e2e-limitrange-dl486=patched" 12/16/22 14:26:01.652
STEP: Confirm that the limitRange "e2e-limitrange-dl486" has been deleted 12/16/22 14:26:01.663
Dec 16 14:26:01.664: INFO: Requesting list of LimitRange to confirm quantity
Dec 16 14:26:01.667: INFO: Found 0 LimitRange with label "e2e-limitrange-dl486=patched"
Dec 16 14:26:01.667: INFO: LimitRange "e2e-limitrange-dl486" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dl486" 12/16/22 14:26:01.667
Dec 16 14:26:01.671: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:01.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-68" for this suite. 12/16/22 14:26:01.675
STEP: Destroying namespace "e2e-limitrange-dl486-9601" for this suite. 12/16/22 14:26:01.682
------------------------------
• [0.093 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:01.595
    Dec 16 14:26:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename limitrange 12/16/22 14:26:01.595
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:01.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:01.613
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-dl486" in namespace "limitrange-68" 12/16/22 14:26:01.616
    STEP: Creating another limitRange in another namespace 12/16/22 14:26:01.622
    Dec 16 14:26:01.636: INFO: Namespace "e2e-limitrange-dl486-9601" created
    Dec 16 14:26:01.636: INFO: Creating LimitRange "e2e-limitrange-dl486" in namespace "e2e-limitrange-dl486-9601"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dl486" 12/16/22 14:26:01.641
    Dec 16 14:26:01.645: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-dl486" in "limitrange-68" namespace 12/16/22 14:26:01.645
    Dec 16 14:26:01.652: INFO: LimitRange "e2e-limitrange-dl486" has been patched
    STEP: Delete LimitRange "e2e-limitrange-dl486" by Collection with labelSelector: "e2e-limitrange-dl486=patched" 12/16/22 14:26:01.652
    STEP: Confirm that the limitRange "e2e-limitrange-dl486" has been deleted 12/16/22 14:26:01.663
    Dec 16 14:26:01.664: INFO: Requesting list of LimitRange to confirm quantity
    Dec 16 14:26:01.667: INFO: Found 0 LimitRange with label "e2e-limitrange-dl486=patched"
    Dec 16 14:26:01.667: INFO: LimitRange "e2e-limitrange-dl486" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dl486" 12/16/22 14:26:01.667
    Dec 16 14:26:01.671: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:01.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-68" for this suite. 12/16/22 14:26:01.675
    STEP: Destroying namespace "e2e-limitrange-dl486-9601" for this suite. 12/16/22 14:26:01.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:01.688
Dec 16 14:26:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 14:26:01.689
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:01.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:01.707
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/16/22 14:26:01.714
Dec 16 14:26:01.722: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1129" to be "running and ready"
Dec 16 14:26:01.725: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092836ms
Dec 16 14:26:01.725: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:26:03.731: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009018534s
Dec 16 14:26:03.731: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 16 14:26:03.731: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 12/16/22 14:26:03.735
Dec 16 14:26:03.742: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1129" to be "running and ready"
Dec 16 14:26:03.744: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922764ms
Dec 16 14:26:03.744: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:26:05.751: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009028531s
Dec 16 14:26:05.751: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Dec 16 14:26:05.751: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/16/22 14:26:05.754
Dec 16 14:26:05.761: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 14:26:05.765: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 14:26:07.766: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 14:26:07.771: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 16 14:26:09.766: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 16 14:26:09.771: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 12/16/22 14:26:09.771
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:09.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1129" for this suite. 12/16/22 14:26:09.785
------------------------------
• [SLOW TEST] [8.104 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:01.688
    Dec 16 14:26:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 14:26:01.689
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:01.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:01.707
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/16/22 14:26:01.714
    Dec 16 14:26:01.722: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1129" to be "running and ready"
    Dec 16 14:26:01.725: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092836ms
    Dec 16 14:26:01.725: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:26:03.731: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009018534s
    Dec 16 14:26:03.731: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 16 14:26:03.731: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 12/16/22 14:26:03.735
    Dec 16 14:26:03.742: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1129" to be "running and ready"
    Dec 16 14:26:03.744: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922764ms
    Dec 16 14:26:03.744: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:26:05.751: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009028531s
    Dec 16 14:26:05.751: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Dec 16 14:26:05.751: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/16/22 14:26:05.754
    Dec 16 14:26:05.761: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 16 14:26:05.765: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 16 14:26:07.766: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 16 14:26:07.771: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 16 14:26:09.766: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 16 14:26:09.771: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 12/16/22 14:26:09.771
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:09.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1129" for this suite. 12/16/22 14:26:09.785
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:09.792
Dec 16 14:26:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename watch 12/16/22 14:26:09.793
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:09.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:09.811
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 12/16/22 14:26:09.813
STEP: modifying the configmap once 12/16/22 14:26:09.817
STEP: modifying the configmap a second time 12/16/22 14:26:09.824
STEP: deleting the configmap 12/16/22 14:26:09.831
STEP: creating a watch on configmaps from the resource version returned by the first update 12/16/22 14:26:09.837
STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/16/22 14:26:09.838
Dec 16 14:26:09.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4042  df38034b-7f8e-4381-807a-d7c3b034a602 681004329 0 2022-12-16 14:26:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-16 14:26:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:26:09.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4042  df38034b-7f8e-4381-807a-d7c3b034a602 681004330 0 2022-12-16 14:26:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-16 14:26:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:09.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4042" for this suite. 12/16/22 14:26:09.842
------------------------------
• [0.055 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:09.792
    Dec 16 14:26:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename watch 12/16/22 14:26:09.793
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:09.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:09.811
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 12/16/22 14:26:09.813
    STEP: modifying the configmap once 12/16/22 14:26:09.817
    STEP: modifying the configmap a second time 12/16/22 14:26:09.824
    STEP: deleting the configmap 12/16/22 14:26:09.831
    STEP: creating a watch on configmaps from the resource version returned by the first update 12/16/22 14:26:09.837
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/16/22 14:26:09.838
    Dec 16 14:26:09.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4042  df38034b-7f8e-4381-807a-d7c3b034a602 681004329 0 2022-12-16 14:26:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-16 14:26:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:26:09.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4042  df38034b-7f8e-4381-807a-d7c3b034a602 681004330 0 2022-12-16 14:26:09 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-16 14:26:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:09.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4042" for this suite. 12/16/22 14:26:09.842
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:09.848
Dec 16 14:26:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename replicaset 12/16/22 14:26:09.849
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:09.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:09.867
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Dec 16 14:26:09.881: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 16 14:26:14.885: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 14:26:14.886
STEP: Scaling up "test-rs" replicaset  12/16/22 14:26:14.886
Dec 16 14:26:14.895: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 12/16/22 14:26:14.895
W1216 14:26:14.902146      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 16 14:26:14.903: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
Dec 16 14:26:14.920: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
Dec 16 14:26:14.932: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
Dec 16 14:26:14.943: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
Dec 16 14:26:15.860: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 2, AvailableReplicas 2
Dec 16 14:26:16.600: INFO: observed Replicaset test-rs in namespace replicaset-6140 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:16.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6140" for this suite. 12/16/22 14:26:16.606
------------------------------
• [SLOW TEST] [6.763 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:09.848
    Dec 16 14:26:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename replicaset 12/16/22 14:26:09.849
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:09.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:09.867
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Dec 16 14:26:09.881: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 16 14:26:14.885: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 14:26:14.886
    STEP: Scaling up "test-rs" replicaset  12/16/22 14:26:14.886
    Dec 16 14:26:14.895: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 12/16/22 14:26:14.895
    W1216 14:26:14.902146      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 16 14:26:14.903: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
    Dec 16 14:26:14.920: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
    Dec 16 14:26:14.932: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
    Dec 16 14:26:14.943: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 1, AvailableReplicas 1
    Dec 16 14:26:15.860: INFO: observed ReplicaSet test-rs in namespace replicaset-6140 with ReadyReplicas 2, AvailableReplicas 2
    Dec 16 14:26:16.600: INFO: observed Replicaset test-rs in namespace replicaset-6140 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:16.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6140" for this suite. 12/16/22 14:26:16.606
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:16.612
Dec 16 14:26:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename csiinlinevolumes 12/16/22 14:26:16.612
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:16.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:16.631
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 12/16/22 14:26:16.633
STEP: getting 12/16/22 14:26:16.648
STEP: listing in namespace 12/16/22 14:26:16.653
STEP: patching 12/16/22 14:26:16.655
STEP: deleting 12/16/22 14:26:16.668
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:16.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3619" for this suite. 12/16/22 14:26:16.685
------------------------------
• [0.078 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:16.612
    Dec 16 14:26:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename csiinlinevolumes 12/16/22 14:26:16.612
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:16.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:16.631
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 12/16/22 14:26:16.633
    STEP: getting 12/16/22 14:26:16.648
    STEP: listing in namespace 12/16/22 14:26:16.653
    STEP: patching 12/16/22 14:26:16.655
    STEP: deleting 12/16/22 14:26:16.668
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:16.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3619" for this suite. 12/16/22 14:26:16.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:16.691
Dec 16 14:26:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename emptydir 12/16/22 14:26:16.691
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:16.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:16.709
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/16/22 14:26:16.712
Dec 16 14:26:16.720: INFO: Waiting up to 5m0s for pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc" in namespace "emptydir-2591" to be "Succeeded or Failed"
Dec 16 14:26:16.723: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052044ms
Dec 16 14:26:18.729: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550686s
Dec 16 14:26:20.728: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008445439s
STEP: Saw pod success 12/16/22 14:26:20.728
Dec 16 14:26:20.729: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc" satisfied condition "Succeeded or Failed"
Dec 16 14:26:20.732: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc container test-container: <nil>
STEP: delete the pod 12/16/22 14:26:20.742
Dec 16 14:26:20.754: INFO: Waiting for pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc to disappear
Dec 16 14:26:20.757: INFO: Pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2591" for this suite. 12/16/22 14:26:20.762
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:16.691
    Dec 16 14:26:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename emptydir 12/16/22 14:26:16.691
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:16.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:16.709
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/16/22 14:26:16.712
    Dec 16 14:26:16.720: INFO: Waiting up to 5m0s for pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc" in namespace "emptydir-2591" to be "Succeeded or Failed"
    Dec 16 14:26:16.723: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052044ms
    Dec 16 14:26:18.729: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008550686s
    Dec 16 14:26:20.728: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008445439s
    STEP: Saw pod success 12/16/22 14:26:20.728
    Dec 16 14:26:20.729: INFO: Pod "pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc" satisfied condition "Succeeded or Failed"
    Dec 16 14:26:20.732: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc container test-container: <nil>
    STEP: delete the pod 12/16/22 14:26:20.742
    Dec 16 14:26:20.754: INFO: Waiting for pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc to disappear
    Dec 16 14:26:20.757: INFO: Pod pod-8a01ce0c-33a8-4b65-abd8-4b4838963bcc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2591" for this suite. 12/16/22 14:26:20.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:20.768
Dec 16 14:26:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:26:20.769
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:20.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:20.786
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:26:20.799
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:26:21.071
STEP: Deploying the webhook pod 12/16/22 14:26:21.08
STEP: Wait for the deployment to be ready 12/16/22 14:26:21.091
Dec 16 14:26:21.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:26:23.112
STEP: Verifying the service has paired with the endpoint 12/16/22 14:26:23.132
Dec 16 14:26:24.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 12/16/22 14:26:24.137
STEP: Creating a custom resource definition that should be denied by the webhook 12/16/22 14:26:24.181
Dec 16 14:26:24.181: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:24.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7120" for this suite. 12/16/22 14:26:24.27
STEP: Destroying namespace "webhook-7120-markers" for this suite. 12/16/22 14:26:24.275
------------------------------
• [3.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:20.768
    Dec 16 14:26:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:26:20.769
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:20.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:20.786
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:26:20.799
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:26:21.071
    STEP: Deploying the webhook pod 12/16/22 14:26:21.08
    STEP: Wait for the deployment to be ready 12/16/22 14:26:21.091
    Dec 16 14:26:21.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:26:23.112
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:26:23.132
    Dec 16 14:26:24.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 12/16/22 14:26:24.137
    STEP: Creating a custom resource definition that should be denied by the webhook 12/16/22 14:26:24.181
    Dec 16 14:26:24.181: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:24.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7120" for this suite. 12/16/22 14:26:24.27
    STEP: Destroying namespace "webhook-7120-markers" for this suite. 12/16/22 14:26:24.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:24.283
Dec 16 14:26:24.283: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:26:24.284
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:24.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:24.302
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/16/22 14:26:24.304
Dec 16 14:26:24.304: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:26:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4427" for this suite. 12/16/22 14:26:33.039
------------------------------
• [SLOW TEST] [8.764 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:24.283
    Dec 16 14:26:24.283: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:26:24.284
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:24.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:24.302
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/16/22 14:26:24.304
    Dec 16 14:26:24.304: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:26:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4427" for this suite. 12/16/22 14:26:33.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:33.047
Dec 16 14:26:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:26:33.048
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:33.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:33.068
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:26:33.083
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:26:33.363
STEP: Deploying the webhook pod 12/16/22 14:26:33.37
STEP: Wait for the deployment to be ready 12/16/22 14:26:33.38
Dec 16 14:26:33.388: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/16/22 14:26:35.401
STEP: Verifying the service has paired with the endpoint 12/16/22 14:26:35.415
Dec 16 14:26:36.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Dec 16 14:26:36.420: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4101-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 14:26:36.931
STEP: Creating a custom resource while v1 is storage version 12/16/22 14:26:36.975
STEP: Patching Custom Resource Definition to set v2 as storage 12/16/22 14:26:39.086
STEP: Patching the custom resource while v2 is storage version 12/16/22 14:26:39.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:39.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-32" for this suite. 12/16/22 14:26:39.733
STEP: Destroying namespace "webhook-32-markers" for this suite. 12/16/22 14:26:39.738
------------------------------
• [SLOW TEST] [6.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:33.047
    Dec 16 14:26:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:26:33.048
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:33.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:33.068
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:26:33.083
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:26:33.363
    STEP: Deploying the webhook pod 12/16/22 14:26:33.37
    STEP: Wait for the deployment to be ready 12/16/22 14:26:33.38
    Dec 16 14:26:33.388: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/16/22 14:26:35.401
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:26:35.415
    Dec 16 14:26:36.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Dec 16 14:26:36.420: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4101-crds.webhook.example.com via the AdmissionRegistration API 12/16/22 14:26:36.931
    STEP: Creating a custom resource while v1 is storage version 12/16/22 14:26:36.975
    STEP: Patching Custom Resource Definition to set v2 as storage 12/16/22 14:26:39.086
    STEP: Patching the custom resource while v2 is storage version 12/16/22 14:26:39.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:39.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-32" for this suite. 12/16/22 14:26:39.733
    STEP: Destroying namespace "webhook-32-markers" for this suite. 12/16/22 14:26:39.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:39.746
Dec 16 14:26:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 14:26:39.747
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:39.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:39.763
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Dec 16 14:26:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9455" for this suite. 12/16/22 14:26:42.933
------------------------------
• [3.193 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:39.746
    Dec 16 14:26:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 14:26:39.747
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:39.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:39.763
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Dec 16 14:26:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9455" for this suite. 12/16/22 14:26:42.933
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:42.939
Dec 16 14:26:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename deployment 12/16/22 14:26:42.94
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:42.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:42.957
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Dec 16 14:26:42.967: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 16 14:26:47.975: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/16/22 14:26:47.975
Dec 16 14:26:47.975: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/16/22 14:26:47.984
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 16 14:26:47.995: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6760  b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8 681005330 1 2022-12-16 14:26:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-16 14:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b344f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 16 14:26:47.998: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec 16 14:26:47.998: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 16 14:26:47.998: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6760  eb60f0b8-e7d9-47a5-9eab-90b08f452b19 681005331 1 2022-12-16 14:26:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8 0xc004b348e7 0xc004b348e8}] [] [{e2e.test Update apps/v1 2022-12-16 14:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:26:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-16 14:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b349c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 16 14:26:48.001: INFO: Pod "test-cleanup-controller-msjk7" is available:
&Pod{ObjectMeta:{test-cleanup-controller-msjk7 test-cleanup-controller- deployment-6760  b1a8d8c1-1de3-44e5-b3cc-ca78ff317a05 681005261 0 2022-12-16 14:26:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0c5d78b27570b63d00316cce311dfc97bafe1a312569b97651ed10544bbeb274 cni.projectcalico.org/podIP:192.168.189.51/32 cni.projectcalico.org/podIPs:192.168.189.51/32] [{apps/v1 ReplicaSet test-cleanup-controller eb60f0b8-e7d9-47a5-9eab-90b08f452b19 0xc004b34d57 0xc004b34d58}] [] [{kube-controller-manager Update v1 2022-12-16 14:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb60f0b8-e7d9-47a5-9eab-90b08f452b19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:26:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcxn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcxn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.51,StartTime:2022-12-16 14:26:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:26:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fc077c0500e0728d33fce7ed36404cd076bde5828381af34a1fc2358c9b3930,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:48.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6760" for this suite. 12/16/22 14:26:48.006
------------------------------
• [SLOW TEST] [5.073 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:42.939
    Dec 16 14:26:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename deployment 12/16/22 14:26:42.94
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:42.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:42.957
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Dec 16 14:26:42.967: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Dec 16 14:26:47.975: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/16/22 14:26:47.975
    Dec 16 14:26:47.975: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/16/22 14:26:47.984
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 16 14:26:47.995: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6760  b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8 681005330 1 2022-12-16 14:26:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-16 14:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b344f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 16 14:26:47.998: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Dec 16 14:26:47.998: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Dec 16 14:26:47.998: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6760  eb60f0b8-e7d9-47a5-9eab-90b08f452b19 681005331 1 2022-12-16 14:26:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8 0xc004b348e7 0xc004b348e8}] [] [{e2e.test Update apps/v1 2022-12-16 14:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-16 14:26:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-16 14:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b6e14a2c-4d45-4d9e-9085-ffbbe8bf76f8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b349c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 16 14:26:48.001: INFO: Pod "test-cleanup-controller-msjk7" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-msjk7 test-cleanup-controller- deployment-6760  b1a8d8c1-1de3-44e5-b3cc-ca78ff317a05 681005261 0 2022-12-16 14:26:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0c5d78b27570b63d00316cce311dfc97bafe1a312569b97651ed10544bbeb274 cni.projectcalico.org/podIP:192.168.189.51/32 cni.projectcalico.org/podIPs:192.168.189.51/32] [{apps/v1 ReplicaSet test-cleanup-controller eb60f0b8-e7d9-47a5-9eab-90b08f452b19 0xc004b34d57 0xc004b34d58}] [] [{kube-controller-manager Update v1 2022-12-16 14:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb60f0b8-e7d9-47a5-9eab-90b08f452b19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-16 14:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-16 14:26:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.189.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcxn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcxn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-a3802-fsxxd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-16 14:26:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.217.161.242,PodIP:192.168.189.51,StartTime:2022-12-16 14:26:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-16 14:26:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fc077c0500e0728d33fce7ed36404cd076bde5828381af34a1fc2358c9b3930,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.189.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:48.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6760" for this suite. 12/16/22 14:26:48.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:48.014
Dec 16 14:26:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename daemonsets 12/16/22 14:26:48.014
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:48.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:48.03
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 12/16/22 14:26:48.05
STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:26:48.055
Dec 16 14:26:48.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:26:48.061: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:49.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 16 14:26:49.070: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:50.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 14:26:50.070: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/16/22 14:26:50.073
Dec 16 14:26:50.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:26:50.090: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:51.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:26:51.100: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:52.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:26:52.099: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:53.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 16 14:26:53.098: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
Dec 16 14:26:54.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 16 14:26:54.099: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:26:54.102
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-162, will wait for the garbage collector to delete the pods 12/16/22 14:26:54.102
Dec 16 14:26:54.162: INFO: Deleting DaemonSet.extensions daemon-set took: 6.422727ms
Dec 16 14:26:54.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.718979ms
Dec 16 14:26:56.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 16 14:26:56.767: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 16 14:26:56.771: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"681005578"},"items":null}

Dec 16 14:26:56.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"681005578"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:26:56.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-162" for this suite. 12/16/22 14:26:56.791
------------------------------
• [SLOW TEST] [8.784 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:48.014
    Dec 16 14:26:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename daemonsets 12/16/22 14:26:48.014
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:48.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:48.03
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 12/16/22 14:26:48.05
    STEP: Check that daemon pods launch on every node of the cluster. 12/16/22 14:26:48.055
    Dec 16 14:26:48.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:26:48.061: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:49.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 16 14:26:49.070: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:50.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 14:26:50.070: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/16/22 14:26:50.073
    Dec 16 14:26:50.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:26:50.090: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:51.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:26:51.100: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:52.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:26:52.099: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:53.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 16 14:26:53.098: INFO: Node pool-a3802-ehprg is running 0 daemon pod, expected 1
    Dec 16 14:26:54.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 16 14:26:54.099: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/16/22 14:26:54.102
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-162, will wait for the garbage collector to delete the pods 12/16/22 14:26:54.102
    Dec 16 14:26:54.162: INFO: Deleting DaemonSet.extensions daemon-set took: 6.422727ms
    Dec 16 14:26:54.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.718979ms
    Dec 16 14:26:56.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 16 14:26:56.767: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 16 14:26:56.771: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"681005578"},"items":null}

    Dec 16 14:26:56.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"681005578"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:26:56.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-162" for this suite. 12/16/22 14:26:56.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:26:56.798
Dec 16 14:26:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename sched-pred 12/16/22 14:26:56.798
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:56.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:56.817
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 16 14:26:56.819: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 16 14:26:56.827: INFO: Waiting for terminating namespaces to be deleted...
Dec 16 14:26:56.830: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
Dec 16 14:26:56.836: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:26:56.836: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:26:56.836: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:26:56.836: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:26:56.836: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 16 14:26:56.836: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:26:56.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:26:56.836: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:26:56.836: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
Dec 16 14:26:56.842: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.842: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:26:56.842: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.842: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:26:56.842: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
Dec 16 14:26:56.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:26:56.842: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 16 14:26:56.842: INFO: 
Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
Dec 16 14:26:56.848: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 16 14:26:56.848: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container calico-node ready: true, restart count 0
Dec 16 14:26:56.848: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container coredns ready: true, restart count 0
Dec 16 14:26:56.848: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec 16 14:26:56.848: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 16 14:26:56.848: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container e2e ready: true, restart count 0
Dec 16 14:26:56.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:26:56.848: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
Dec 16 14:26:56.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 16 14:26:56.848: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 14:26:56.848
Dec 16 14:26:56.856: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6451" to be "running"
Dec 16 14:26:56.860: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521268ms
Dec 16 14:26:58.866: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009974676s
Dec 16 14:26:58.866: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 14:26:58.87
STEP: Trying to apply a random label on the found node. 12/16/22 14:26:58.883
STEP: verifying the node has the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c 95 12/16/22 14:26:58.891
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/16/22 14:26:58.894
Dec 16 14:26:58.899: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6451" to be "not pending"
Dec 16 14:26:58.904: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392787ms
Dec 16 14:27:00.909: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00974881s
Dec 16 14:27:00.909: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 85.217.161.242 on the node which pod4 resides and expect not scheduled 12/16/22 14:27:00.909
Dec 16 14:27:00.914: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6451" to be "not pending"
Dec 16 14:27:00.922: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.926011ms
Dec 16 14:27:02.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124545s
Dec 16 14:27:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012723538s
Dec 16 14:27:06.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012648614s
Dec 16 14:27:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013773769s
Dec 16 14:27:10.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012592481s
Dec 16 14:27:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014371776s
Dec 16 14:27:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012271039s
Dec 16 14:27:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012920607s
Dec 16 14:27:18.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014018081s
Dec 16 14:27:20.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01269826s
Dec 16 14:27:22.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013189574s
Dec 16 14:27:24.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014413015s
Dec 16 14:27:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013586231s
Dec 16 14:27:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012944037s
Dec 16 14:27:30.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012306395s
Dec 16 14:27:32.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014341247s
Dec 16 14:27:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012610242s
Dec 16 14:27:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013199073s
Dec 16 14:27:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014347101s
Dec 16 14:27:40.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013139395s
Dec 16 14:27:42.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012960021s
Dec 16 14:27:44.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012645927s
Dec 16 14:27:46.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013530627s
Dec 16 14:27:48.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015028148s
Dec 16 14:27:50.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012897101s
Dec 16 14:27:53.040: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.12565243s
Dec 16 14:27:54.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012894629s
Dec 16 14:27:56.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014235555s
Dec 16 14:27:58.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0143674s
Dec 16 14:28:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013950258s
Dec 16 14:28:02.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020542523s
Dec 16 14:28:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012254568s
Dec 16 14:28:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013415361s
Dec 16 14:28:08.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014237002s
Dec 16 14:28:10.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012073139s
Dec 16 14:28:12.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01319245s
Dec 16 14:28:14.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017885577s
Dec 16 14:28:16.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014270344s
Dec 16 14:28:18.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013166754s
Dec 16 14:28:20.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012947593s
Dec 16 14:28:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014270719s
Dec 16 14:28:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012583826s
Dec 16 14:28:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013224665s
Dec 16 14:28:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012218694s
Dec 16 14:28:30.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014050115s
Dec 16 14:28:32.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011786202s
Dec 16 14:28:34.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013180645s
Dec 16 14:28:36.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014129642s
Dec 16 14:28:38.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013403065s
Dec 16 14:28:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012457685s
Dec 16 14:28:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01343862s
Dec 16 14:28:44.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011975862s
Dec 16 14:28:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012749974s
Dec 16 14:28:48.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012692993s
Dec 16 14:28:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013601194s
Dec 16 14:28:52.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012771781s
Dec 16 14:28:54.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011803226s
Dec 16 14:28:56.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012116712s
Dec 16 14:28:58.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014918047s
Dec 16 14:29:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013399033s
Dec 16 14:29:02.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013477868s
Dec 16 14:29:04.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013635076s
Dec 16 14:29:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013430143s
Dec 16 14:29:08.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.014427689s
Dec 16 14:29:10.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012979959s
Dec 16 14:29:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.0147311s
Dec 16 14:29:14.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.013839623s
Dec 16 14:29:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.012565647s
Dec 16 14:29:18.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.014945712s
Dec 16 14:29:20.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012092842s
Dec 16 14:29:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014261737s
Dec 16 14:29:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012906861s
Dec 16 14:29:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013856739s
Dec 16 14:29:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012829576s
Dec 16 14:29:30.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012712783s
Dec 16 14:29:32.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.013961557s
Dec 16 14:29:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012728877s
Dec 16 14:29:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013314158s
Dec 16 14:29:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.014723995s
Dec 16 14:29:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.013047277s
Dec 16 14:29:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.013747396s
Dec 16 14:29:44.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012578252s
Dec 16 14:29:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012757943s
Dec 16 14:29:48.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01345063s
Dec 16 14:29:50.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012072369s
Dec 16 14:29:52.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.0140404s
Dec 16 14:29:54.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.01229376s
Dec 16 14:29:56.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012809577s
Dec 16 14:29:58.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013759464s
Dec 16 14:30:00.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012344794s
Dec 16 14:30:02.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01310396s
Dec 16 14:30:04.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.013283115s
Dec 16 14:30:06.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012589507s
Dec 16 14:30:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013723292s
Dec 16 14:30:10.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013754695s
Dec 16 14:30:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.01434851s
Dec 16 14:30:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012791142s
Dec 16 14:30:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012819169s
Dec 16 14:30:18.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.01270032s
Dec 16 14:30:20.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013682442s
Dec 16 14:30:22.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012893444s
Dec 16 14:30:24.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013673624s
Dec 16 14:30:26.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012710967s
Dec 16 14:30:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.012561112s
Dec 16 14:30:30.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.014236072s
Dec 16 14:30:32.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012073716s
Dec 16 14:30:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013054922s
Dec 16 14:30:36.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.013050904s
Dec 16 14:30:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.014879547s
Dec 16 14:30:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012917652s
Dec 16 14:30:42.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.015413182s
Dec 16 14:30:44.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.01352801s
Dec 16 14:30:46.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.013790806s
Dec 16 14:30:48.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012600156s
Dec 16 14:30:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01405813s
Dec 16 14:30:52.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.015354325s
Dec 16 14:30:54.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01424402s
Dec 16 14:30:56.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.011776137s
Dec 16 14:30:58.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014059675s
Dec 16 14:31:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013315707s
Dec 16 14:31:02.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.014224149s
Dec 16 14:31:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01227532s
Dec 16 14:31:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.01368639s
Dec 16 14:31:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.014063432s
Dec 16 14:31:11.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.615580706s
Dec 16 14:31:12.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012849993s
Dec 16 14:31:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012380165s
Dec 16 14:31:16.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.014535243s
Dec 16 14:31:18.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.014687369s
Dec 16 14:31:20.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013498519s
Dec 16 14:31:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01503448s
Dec 16 14:31:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.01242177s
Dec 16 14:31:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013848714s
Dec 16 14:31:28.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014838928s
Dec 16 14:31:30.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.014276184s
Dec 16 14:31:32.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013230625s
Dec 16 14:31:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012384313s
Dec 16 14:31:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013790053s
Dec 16 14:31:38.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015172304s
Dec 16 14:31:40.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.014206518s
Dec 16 14:31:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013987455s
Dec 16 14:31:44.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012007213s
Dec 16 14:31:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012313049s
Dec 16 14:31:48.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.014599534s
Dec 16 14:31:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.013929763s
Dec 16 14:31:53.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.562412719s
Dec 16 14:31:54.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013933411s
Dec 16 14:31:56.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013104474s
Dec 16 14:31:58.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012036495s
Dec 16 14:32:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013610704s
Dec 16 14:32:00.931: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016882168s
STEP: removing the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c off the node pool-a3802-fsxxd 12/16/22 14:32:00.931
STEP: verifying the node doesn't have the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c 12/16/22 14:32:00.943
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:00.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6451" for this suite. 12/16/22 14:32:00.95
------------------------------
• [SLOW TEST] [304.157 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:26:56.798
    Dec 16 14:26:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename sched-pred 12/16/22 14:26:56.798
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:26:56.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:26:56.817
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 16 14:26:56.819: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 16 14:26:56.827: INFO: Waiting for terminating namespaces to be deleted...
    Dec 16 14:26:56.830: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-ehprg before test
    Dec 16 14:26:56.836: INFO: calico-node-g9rxj from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: coredns-7cd5b7d6b4-zz7zw from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: konnectivity-agent-5f7cbf88d-fpk9f from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: kube-proxy-csksk from kube-system started at 2022-12-16 13:10:12 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: sonobuoy from sonobuoy started at 2022-12-16 13:10:48 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-6lf7r from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:26:56.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:26:56.836: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-fsxxd before test
    Dec 16 14:26:56.842: INFO: calico-node-5m9wp from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.842: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:26:56.842: INFO: kube-proxy-f6k5s from kube-system started at 2022-12-16 13:10:45 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.842: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:26:56.842: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-wnvqh from sonobuoy started at 2022-12-16 13:10:51 +0000 UTC (2 container statuses recorded)
    Dec 16 14:26:56.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:26:56.842: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 16 14:26:56.842: INFO: 
    Logging pods the apiserver thinks is on node pool-a3802-oewtd before test
    Dec 16 14:26:56.848: INFO: calico-kube-controllers-5f94594857-mnd6f from kube-system started at 2022-12-16 13:10:14 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: calico-node-h8rhd from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container calico-node ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: coredns-7cd5b7d6b4-mf5b4 from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container coredns ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: konnectivity-agent-5f7cbf88d-q4fff from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: kube-proxy-k8cjh from kube-system started at 2022-12-16 13:10:01 +0000 UTC (1 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: sonobuoy-e2e-job-779c6bd4e6b34c88 from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container e2e ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: sonobuoy-systemd-logs-daemon-set-86eb72ffbe2540cb-jtqdv from sonobuoy started at 2022-12-16 13:10:52 +0000 UTC (2 container statuses recorded)
    Dec 16 14:26:56.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 16 14:26:56.848: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/16/22 14:26:56.848
    Dec 16 14:26:56.856: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6451" to be "running"
    Dec 16 14:26:56.860: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521268ms
    Dec 16 14:26:58.866: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009974676s
    Dec 16 14:26:58.866: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/16/22 14:26:58.87
    STEP: Trying to apply a random label on the found node. 12/16/22 14:26:58.883
    STEP: verifying the node has the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c 95 12/16/22 14:26:58.891
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/16/22 14:26:58.894
    Dec 16 14:26:58.899: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6451" to be "not pending"
    Dec 16 14:26:58.904: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392787ms
    Dec 16 14:27:00.909: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00974881s
    Dec 16 14:27:00.909: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 85.217.161.242 on the node which pod4 resides and expect not scheduled 12/16/22 14:27:00.909
    Dec 16 14:27:00.914: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6451" to be "not pending"
    Dec 16 14:27:00.922: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.926011ms
    Dec 16 14:27:02.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124545s
    Dec 16 14:27:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012723538s
    Dec 16 14:27:06.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012648614s
    Dec 16 14:27:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013773769s
    Dec 16 14:27:10.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012592481s
    Dec 16 14:27:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014371776s
    Dec 16 14:27:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012271039s
    Dec 16 14:27:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012920607s
    Dec 16 14:27:18.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014018081s
    Dec 16 14:27:20.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01269826s
    Dec 16 14:27:22.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013189574s
    Dec 16 14:27:24.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014413015s
    Dec 16 14:27:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013586231s
    Dec 16 14:27:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012944037s
    Dec 16 14:27:30.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012306395s
    Dec 16 14:27:32.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014341247s
    Dec 16 14:27:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012610242s
    Dec 16 14:27:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013199073s
    Dec 16 14:27:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014347101s
    Dec 16 14:27:40.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013139395s
    Dec 16 14:27:42.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012960021s
    Dec 16 14:27:44.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012645927s
    Dec 16 14:27:46.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013530627s
    Dec 16 14:27:48.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015028148s
    Dec 16 14:27:50.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012897101s
    Dec 16 14:27:53.040: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.12565243s
    Dec 16 14:27:54.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012894629s
    Dec 16 14:27:56.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014235555s
    Dec 16 14:27:58.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0143674s
    Dec 16 14:28:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013950258s
    Dec 16 14:28:02.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020542523s
    Dec 16 14:28:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012254568s
    Dec 16 14:28:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013415361s
    Dec 16 14:28:08.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014237002s
    Dec 16 14:28:10.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012073139s
    Dec 16 14:28:12.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01319245s
    Dec 16 14:28:14.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017885577s
    Dec 16 14:28:16.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014270344s
    Dec 16 14:28:18.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013166754s
    Dec 16 14:28:20.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012947593s
    Dec 16 14:28:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014270719s
    Dec 16 14:28:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012583826s
    Dec 16 14:28:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013224665s
    Dec 16 14:28:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012218694s
    Dec 16 14:28:30.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014050115s
    Dec 16 14:28:32.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011786202s
    Dec 16 14:28:34.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013180645s
    Dec 16 14:28:36.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014129642s
    Dec 16 14:28:38.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013403065s
    Dec 16 14:28:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012457685s
    Dec 16 14:28:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01343862s
    Dec 16 14:28:44.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011975862s
    Dec 16 14:28:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012749974s
    Dec 16 14:28:48.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012692993s
    Dec 16 14:28:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013601194s
    Dec 16 14:28:52.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012771781s
    Dec 16 14:28:54.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011803226s
    Dec 16 14:28:56.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012116712s
    Dec 16 14:28:58.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014918047s
    Dec 16 14:29:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013399033s
    Dec 16 14:29:02.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013477868s
    Dec 16 14:29:04.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013635076s
    Dec 16 14:29:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013430143s
    Dec 16 14:29:08.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.014427689s
    Dec 16 14:29:10.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012979959s
    Dec 16 14:29:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.0147311s
    Dec 16 14:29:14.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.013839623s
    Dec 16 14:29:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.012565647s
    Dec 16 14:29:18.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.014945712s
    Dec 16 14:29:20.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012092842s
    Dec 16 14:29:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014261737s
    Dec 16 14:29:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012906861s
    Dec 16 14:29:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013856739s
    Dec 16 14:29:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012829576s
    Dec 16 14:29:30.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012712783s
    Dec 16 14:29:32.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.013961557s
    Dec 16 14:29:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012728877s
    Dec 16 14:29:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013314158s
    Dec 16 14:29:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.014723995s
    Dec 16 14:29:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.013047277s
    Dec 16 14:29:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.013747396s
    Dec 16 14:29:44.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012578252s
    Dec 16 14:29:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012757943s
    Dec 16 14:29:48.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01345063s
    Dec 16 14:29:50.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012072369s
    Dec 16 14:29:52.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.0140404s
    Dec 16 14:29:54.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.01229376s
    Dec 16 14:29:56.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012809577s
    Dec 16 14:29:58.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013759464s
    Dec 16 14:30:00.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012344794s
    Dec 16 14:30:02.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01310396s
    Dec 16 14:30:04.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.013283115s
    Dec 16 14:30:06.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012589507s
    Dec 16 14:30:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013723292s
    Dec 16 14:30:10.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013754695s
    Dec 16 14:30:12.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.01434851s
    Dec 16 14:30:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012791142s
    Dec 16 14:30:16.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012819169s
    Dec 16 14:30:18.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.01270032s
    Dec 16 14:30:20.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013682442s
    Dec 16 14:30:22.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012893444s
    Dec 16 14:30:24.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013673624s
    Dec 16 14:30:26.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012710967s
    Dec 16 14:30:28.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.012561112s
    Dec 16 14:30:30.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.014236072s
    Dec 16 14:30:32.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012073716s
    Dec 16 14:30:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013054922s
    Dec 16 14:30:36.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.013050904s
    Dec 16 14:30:38.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.014879547s
    Dec 16 14:30:40.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012917652s
    Dec 16 14:30:42.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.015413182s
    Dec 16 14:30:44.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.01352801s
    Dec 16 14:30:46.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.013790806s
    Dec 16 14:30:48.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012600156s
    Dec 16 14:30:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01405813s
    Dec 16 14:30:52.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.015354325s
    Dec 16 14:30:54.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01424402s
    Dec 16 14:30:56.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.011776137s
    Dec 16 14:30:58.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014059675s
    Dec 16 14:31:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013315707s
    Dec 16 14:31:02.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.014224149s
    Dec 16 14:31:04.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01227532s
    Dec 16 14:31:06.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.01368639s
    Dec 16 14:31:08.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.014063432s
    Dec 16 14:31:11.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.615580706s
    Dec 16 14:31:12.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012849993s
    Dec 16 14:31:14.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012380165s
    Dec 16 14:31:16.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.014535243s
    Dec 16 14:31:18.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.014687369s
    Dec 16 14:31:20.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013498519s
    Dec 16 14:31:22.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01503448s
    Dec 16 14:31:24.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.01242177s
    Dec 16 14:31:26.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013848714s
    Dec 16 14:31:28.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014838928s
    Dec 16 14:31:30.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.014276184s
    Dec 16 14:31:32.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013230625s
    Dec 16 14:31:34.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012384313s
    Dec 16 14:31:36.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013790053s
    Dec 16 14:31:38.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015172304s
    Dec 16 14:31:40.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.014206518s
    Dec 16 14:31:42.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013987455s
    Dec 16 14:31:44.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012007213s
    Dec 16 14:31:46.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012313049s
    Dec 16 14:31:48.929: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.014599534s
    Dec 16 14:31:50.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.013929763s
    Dec 16 14:31:53.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.562412719s
    Dec 16 14:31:54.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013933411s
    Dec 16 14:31:56.927: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013104474s
    Dec 16 14:31:58.926: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012036495s
    Dec 16 14:32:00.928: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013610704s
    Dec 16 14:32:00.931: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016882168s
    STEP: removing the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c off the node pool-a3802-fsxxd 12/16/22 14:32:00.931
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-42672cee-afdc-40f1-85ed-9ea89033e22c 12/16/22 14:32:00.943
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:00.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6451" for this suite. 12/16/22 14:32:00.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:00.956
Dec 16 14:32:00.956: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename events 12/16/22 14:32:00.956
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:00.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:00.974
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 12/16/22 14:32:00.977
Dec 16 14:32:00.983: INFO: created test-event-1
Dec 16 14:32:00.986: INFO: created test-event-2
Dec 16 14:32:00.990: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 12/16/22 14:32:00.99
STEP: delete collection of events 12/16/22 14:32:00.992
Dec 16 14:32:00.992: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/16/22 14:32:01.009
Dec 16 14:32:01.009: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:01.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1386" for this suite. 12/16/22 14:32:01.015
------------------------------
• [0.065 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:00.956
    Dec 16 14:32:00.956: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename events 12/16/22 14:32:00.956
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:00.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:00.974
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 12/16/22 14:32:00.977
    Dec 16 14:32:00.983: INFO: created test-event-1
    Dec 16 14:32:00.986: INFO: created test-event-2
    Dec 16 14:32:00.990: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 12/16/22 14:32:00.99
    STEP: delete collection of events 12/16/22 14:32:00.992
    Dec 16 14:32:00.992: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/16/22 14:32:01.009
    Dec 16 14:32:01.009: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:01.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1386" for this suite. 12/16/22 14:32:01.015
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:01.021
Dec 16 14:32:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 14:32:01.021
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:01.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:01.038
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Dec 16 14:32:01.040: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:01.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8737" for this suite. 12/16/22 14:32:01.579
------------------------------
• [0.563 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:01.021
    Dec 16 14:32:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename custom-resource-definition 12/16/22 14:32:01.021
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:01.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:01.038
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Dec 16 14:32:01.040: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:01.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8737" for this suite. 12/16/22 14:32:01.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:01.584
Dec 16 14:32:01.585: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:32:01.585
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:01.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:01.602
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9708 12/16/22 14:32:01.604
STEP: creating replication controller nodeport-test in namespace services-9708 12/16/22 14:32:01.637
I1216 14:32:01.644360      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9708, replica count: 2
I1216 14:32:04.695002      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 14:32:04.695: INFO: Creating new exec pod
Dec 16 14:32:04.702: INFO: Waiting up to 5m0s for pod "execpodvmwn9" in namespace "services-9708" to be "running"
Dec 16 14:32:04.706: INFO: Pod "execpodvmwn9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434585ms
Dec 16 14:32:06.711: INFO: Pod "execpodvmwn9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008387698s
Dec 16 14:32:06.711: INFO: Pod "execpodvmwn9" satisfied condition "running"
Dec 16 14:32:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Dec 16 14:32:07.911: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 16 14:32:07.911: INFO: stdout: ""
Dec 16 14:32:07.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 10.97.37.113 80'
Dec 16 14:32:08.101: INFO: stderr: "+ nc -v -z -w 2 10.97.37.113 80\nConnection to 10.97.37.113 80 port [tcp/http] succeeded!\n"
Dec 16 14:32:08.101: INFO: stdout: ""
Dec 16 14:32:08.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 30396'
Dec 16 14:32:08.297: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 30396\nConnection to 85.217.161.242 30396 port [tcp/*] succeeded!\n"
Dec 16 14:32:08.297: INFO: stdout: ""
Dec 16 14:32:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 85.217.161.213 30396'
Dec 16 14:32:08.494: INFO: stderr: "+ nc -v -z -w 2 85.217.161.213 30396\nConnection to 85.217.161.213 30396 port [tcp/*] succeeded!\n"
Dec 16 14:32:08.494: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:08.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9708" for this suite. 12/16/22 14:32:08.499
------------------------------
• [SLOW TEST] [6.921 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:01.584
    Dec 16 14:32:01.585: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:32:01.585
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:01.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:01.602
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9708 12/16/22 14:32:01.604
    STEP: creating replication controller nodeport-test in namespace services-9708 12/16/22 14:32:01.637
    I1216 14:32:01.644360      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9708, replica count: 2
    I1216 14:32:04.695002      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 14:32:04.695: INFO: Creating new exec pod
    Dec 16 14:32:04.702: INFO: Waiting up to 5m0s for pod "execpodvmwn9" in namespace "services-9708" to be "running"
    Dec 16 14:32:04.706: INFO: Pod "execpodvmwn9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434585ms
    Dec 16 14:32:06.711: INFO: Pod "execpodvmwn9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008387698s
    Dec 16 14:32:06.711: INFO: Pod "execpodvmwn9" satisfied condition "running"
    Dec 16 14:32:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Dec 16 14:32:07.911: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Dec 16 14:32:07.911: INFO: stdout: ""
    Dec 16 14:32:07.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 10.97.37.113 80'
    Dec 16 14:32:08.101: INFO: stderr: "+ nc -v -z -w 2 10.97.37.113 80\nConnection to 10.97.37.113 80 port [tcp/http] succeeded!\n"
    Dec 16 14:32:08.101: INFO: stdout: ""
    Dec 16 14:32:08.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 30396'
    Dec 16 14:32:08.297: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 30396\nConnection to 85.217.161.242 30396 port [tcp/*] succeeded!\n"
    Dec 16 14:32:08.297: INFO: stdout: ""
    Dec 16 14:32:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-9708 exec execpodvmwn9 -- /bin/sh -x -c nc -v -z -w 2 85.217.161.213 30396'
    Dec 16 14:32:08.494: INFO: stderr: "+ nc -v -z -w 2 85.217.161.213 30396\nConnection to 85.217.161.213 30396 port [tcp/*] succeeded!\n"
    Dec 16 14:32:08.494: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:08.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9708" for this suite. 12/16/22 14:32:08.499
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:08.506
Dec 16 14:32:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename endpointslice 12/16/22 14:32:08.506
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:08.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:08.523
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 12/16/22 14:32:13.593
STEP: referencing matching pods with named port 12/16/22 14:32:18.602
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/16/22 14:32:23.615
STEP: recreating EndpointSlices after they've been deleted 12/16/22 14:32:28.629
Dec 16 14:32:28.647: INFO: EndpointSlice for Service endpointslice-4320/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:38.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4320" for this suite. 12/16/22 14:32:38.662
------------------------------
• [SLOW TEST] [30.163 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:08.506
    Dec 16 14:32:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename endpointslice 12/16/22 14:32:08.506
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:08.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:08.523
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 12/16/22 14:32:13.593
    STEP: referencing matching pods with named port 12/16/22 14:32:18.602
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/16/22 14:32:23.615
    STEP: recreating EndpointSlices after they've been deleted 12/16/22 14:32:28.629
    Dec 16 14:32:28.647: INFO: EndpointSlice for Service endpointslice-4320/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:38.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4320" for this suite. 12/16/22 14:32:38.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:38.669
Dec 16 14:32:38.669: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:32:38.67
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:38.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:38.689
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:32:38.703
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:32:39.073
STEP: Deploying the webhook pod 12/16/22 14:32:39.083
STEP: Wait for the deployment to be ready 12/16/22 14:32:39.094
Dec 16 14:32:39.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:32:41.112
STEP: Verifying the service has paired with the endpoint 12/16/22 14:32:41.126
Dec 16 14:32:42.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Dec 16 14:32:42.131: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/16/22 14:32:42.64
STEP: Creating a custom resource that should be denied by the webhook 12/16/22 14:32:42.683
STEP: Creating a custom resource whose deletion would be denied by the webhook 12/16/22 14:32:44.76
STEP: Updating the custom resource with disallowed data should be denied 12/16/22 14:32:44.768
STEP: Deleting the custom resource should be denied 12/16/22 14:32:44.779
STEP: Remove the offending key and value from the custom resource data 12/16/22 14:32:44.788
STEP: Deleting the updated custom resource should be successful 12/16/22 14:32:44.798
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3151" for this suite. 12/16/22 14:32:45.383
STEP: Destroying namespace "webhook-3151-markers" for this suite. 12/16/22 14:32:45.389
------------------------------
• [SLOW TEST] [6.725 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:38.669
    Dec 16 14:32:38.669: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:32:38.67
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:38.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:38.689
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:32:38.703
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:32:39.073
    STEP: Deploying the webhook pod 12/16/22 14:32:39.083
    STEP: Wait for the deployment to be ready 12/16/22 14:32:39.094
    Dec 16 14:32:39.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:32:41.112
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:32:41.126
    Dec 16 14:32:42.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Dec 16 14:32:42.131: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/16/22 14:32:42.64
    STEP: Creating a custom resource that should be denied by the webhook 12/16/22 14:32:42.683
    STEP: Creating a custom resource whose deletion would be denied by the webhook 12/16/22 14:32:44.76
    STEP: Updating the custom resource with disallowed data should be denied 12/16/22 14:32:44.768
    STEP: Deleting the custom resource should be denied 12/16/22 14:32:44.779
    STEP: Remove the offending key and value from the custom resource data 12/16/22 14:32:44.788
    STEP: Deleting the updated custom resource should be successful 12/16/22 14:32:44.798
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3151" for this suite. 12/16/22 14:32:45.383
    STEP: Destroying namespace "webhook-3151-markers" for this suite. 12/16/22 14:32:45.389
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:45.394
Dec 16 14:32:45.394: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename resourcequota 12/16/22 14:32:45.395
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:45.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:45.411
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 12/16/22 14:32:45.414
STEP: Creating a ResourceQuota 12/16/22 14:32:50.418
STEP: Ensuring resource quota status is calculated 12/16/22 14:32:50.422
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:52.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-184" for this suite. 12/16/22 14:32:52.432
------------------------------
• [SLOW TEST] [7.045 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:45.394
    Dec 16 14:32:45.394: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename resourcequota 12/16/22 14:32:45.395
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:45.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:45.411
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 12/16/22 14:32:45.414
    STEP: Creating a ResourceQuota 12/16/22 14:32:50.418
    STEP: Ensuring resource quota status is calculated 12/16/22 14:32:50.422
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:52.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-184" for this suite. 12/16/22 14:32:52.432
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:52.44
Dec 16 14:32:52.440: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename configmap 12/16/22 14:32:52.44
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:52.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:52.458
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-ad1947cd-2b0a-432b-b391-a9e3f9987f48 12/16/22 14:32:52.46
STEP: Creating a pod to test consume configMaps 12/16/22 14:32:52.464
Dec 16 14:32:52.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8" in namespace "configmap-7790" to be "Succeeded or Failed"
Dec 16 14:32:52.474: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685947ms
Dec 16 14:32:54.479: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007668258s
Dec 16 14:32:56.481: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00967553s
STEP: Saw pod success 12/16/22 14:32:56.481
Dec 16 14:32:56.481: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8" satisfied condition "Succeeded or Failed"
Dec 16 14:32:56.486: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 14:32:56.538
Dec 16 14:32:56.551: INFO: Waiting for pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 to disappear
Dec 16 14:32:56.554: INFO: Pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 16 14:32:56.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7790" for this suite. 12/16/22 14:32:56.558
------------------------------
• [4.124 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:52.44
    Dec 16 14:32:52.440: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename configmap 12/16/22 14:32:52.44
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:52.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:52.458
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-ad1947cd-2b0a-432b-b391-a9e3f9987f48 12/16/22 14:32:52.46
    STEP: Creating a pod to test consume configMaps 12/16/22 14:32:52.464
    Dec 16 14:32:52.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8" in namespace "configmap-7790" to be "Succeeded or Failed"
    Dec 16 14:32:52.474: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685947ms
    Dec 16 14:32:54.479: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007668258s
    Dec 16 14:32:56.481: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00967553s
    STEP: Saw pod success 12/16/22 14:32:56.481
    Dec 16 14:32:56.481: INFO: Pod "pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8" satisfied condition "Succeeded or Failed"
    Dec 16 14:32:56.486: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 14:32:56.538
    Dec 16 14:32:56.551: INFO: Waiting for pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 to disappear
    Dec 16 14:32:56.554: INFO: Pod pod-configmaps-6626e164-7e2a-4b62-a3e9-e14e0d2431d8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:32:56.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7790" for this suite. 12/16/22 14:32:56.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:32:56.565
Dec 16 14:32:56.565: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:32:56.566
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:56.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:56.583
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Dec 16 14:32:56.595: INFO: created pod
Dec 16 14:32:56.595: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4677" to be "Succeeded or Failed"
Dec 16 14:32:56.598: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610381ms
Dec 16 14:32:58.602: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006868243s
Dec 16 14:33:00.603: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007820067s
STEP: Saw pod success 12/16/22 14:33:00.603
Dec 16 14:33:00.603: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 16 14:33:30.607: INFO: polling logs
Dec 16 14:33:30.619: INFO: Pod logs: 
I1216 14:32:57.307404       1 log.go:198] OK: Got token
I1216 14:32:57.307440       1 log.go:198] validating with in-cluster discovery
I1216 14:32:57.307820       1 log.go:198] OK: got issuer https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local
I1216 14:32:57.307856       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local", Subject:"system:serviceaccount:svcaccounts-4677:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671201776, NotBefore:1671201176, IssuedAt:1671201176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4677", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6668bd84-f427-47e9-b8b5-188685471704"}}}
I1216 14:32:57.325356       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local
I1216 14:32:57.343088       1 log.go:198] OK: Validated signature on JWT
I1216 14:32:57.343150       1 log.go:198] OK: Got valid claims from token!
I1216 14:32:57.343175       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local", Subject:"system:serviceaccount:svcaccounts-4677:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671201776, NotBefore:1671201176, IssuedAt:1671201176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4677", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6668bd84-f427-47e9-b8b5-188685471704"}}}

Dec 16 14:33:30.619: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 16 14:33:30.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4677" for this suite. 12/16/22 14:33:30.633
------------------------------
• [SLOW TEST] [34.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:32:56.565
    Dec 16 14:32:56.565: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename svcaccounts 12/16/22 14:32:56.566
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:32:56.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:32:56.583
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Dec 16 14:32:56.595: INFO: created pod
    Dec 16 14:32:56.595: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4677" to be "Succeeded or Failed"
    Dec 16 14:32:56.598: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610381ms
    Dec 16 14:32:58.602: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006868243s
    Dec 16 14:33:00.603: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007820067s
    STEP: Saw pod success 12/16/22 14:33:00.603
    Dec 16 14:33:00.603: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Dec 16 14:33:30.607: INFO: polling logs
    Dec 16 14:33:30.619: INFO: Pod logs: 
    I1216 14:32:57.307404       1 log.go:198] OK: Got token
    I1216 14:32:57.307440       1 log.go:198] validating with in-cluster discovery
    I1216 14:32:57.307820       1 log.go:198] OK: got issuer https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local
    I1216 14:32:57.307856       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local", Subject:"system:serviceaccount:svcaccounts-4677:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671201776, NotBefore:1671201176, IssuedAt:1671201176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4677", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6668bd84-f427-47e9-b8b5-188685471704"}}}
    I1216 14:32:57.325356       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local
    I1216 14:32:57.343088       1 log.go:198] OK: Validated signature on JWT
    I1216 14:32:57.343150       1 log.go:198] OK: Got valid claims from token!
    I1216 14:32:57.343175       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.730ad6a8-2dcb-4ab2-8b79-442c07800cca.cluster.local", Subject:"system:serviceaccount:svcaccounts-4677:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671201776, NotBefore:1671201176, IssuedAt:1671201176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4677", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"6668bd84-f427-47e9-b8b5-188685471704"}}}

    Dec 16 14:33:30.619: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:33:30.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4677" for this suite. 12/16/22 14:33:30.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:33:30.639
Dec 16 14:33:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename watch 12/16/22 14:33:30.64
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:33:30.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:33:30.658
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 12/16/22 14:33:30.66
STEP: creating a watch on configmaps with label B 12/16/22 14:33:30.661
STEP: creating a watch on configmaps with label A or B 12/16/22 14:33:30.662
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.663
Dec 16 14:33:30.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016531 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:30.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016531 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.668
Dec 16 14:33:30.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016532 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:30.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016532 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/16/22 14:33:30.676
Dec 16 14:33:30.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016533 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:30.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016533 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.684
Dec 16 14:33:30.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016534 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:30.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016534 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/16/22 14:33:30.689
Dec 16 14:33:30.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016535 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:30.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016535 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/16/22 14:33:40.695
Dec 16 14:33:40.703: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016733 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 16 14:33:40.703: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016733 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 16 14:33:50.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2042" for this suite. 12/16/22 14:33:50.711
------------------------------
• [SLOW TEST] [20.078 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:33:30.639
    Dec 16 14:33:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename watch 12/16/22 14:33:30.64
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:33:30.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:33:30.658
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 12/16/22 14:33:30.66
    STEP: creating a watch on configmaps with label B 12/16/22 14:33:30.661
    STEP: creating a watch on configmaps with label A or B 12/16/22 14:33:30.662
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.663
    Dec 16 14:33:30.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016531 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:30.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016531 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.668
    Dec 16 14:33:30.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016532 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:30.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016532 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/16/22 14:33:30.676
    Dec 16 14:33:30.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016533 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:30.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016533 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/16/22 14:33:30.684
    Dec 16 14:33:30.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016534 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:30.689: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2042  b64c9dc2-7c20-4281-980c-477e0203f9a0 681016534 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/16/22 14:33:30.689
    Dec 16 14:33:30.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016535 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:30.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016535 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/16/22 14:33:40.695
    Dec 16 14:33:40.703: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016733 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 16 14:33:40.703: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2042  b2ccd09e-b8ae-4ad8-a009-778590a5e3e5 681016733 0 2022-12-16 14:33:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-16 14:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:33:50.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2042" for this suite. 12/16/22 14:33:50.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:33:50.719
Dec 16 14:33:50.719: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pod-network-test 12/16/22 14:33:50.719
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:33:50.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:33:50.748
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2787 12/16/22 14:33:50.75
STEP: creating a selector 12/16/22 14:33:50.75
STEP: Creating the service pods in kubernetes 12/16/22 14:33:50.75
Dec 16 14:33:50.750: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 16 14:33:50.788: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2787" to be "running and ready"
Dec 16 14:33:50.792: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227102ms
Dec 16 14:33:50.792: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:33:52.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010108734s
Dec 16 14:33:52.798: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 14:33:54.799: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011007596s
Dec 16 14:33:54.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 14:33:56.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008944003s
Dec 16 14:33:56.797: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 14:33:58.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009093281s
Dec 16 14:33:58.797: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 14:34:00.799: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010801145s
Dec 16 14:34:00.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 16 14:34:02.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010488801s
Dec 16 14:34:02.798: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 16 14:34:02.798: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 16 14:34:02.802: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2787" to be "running and ready"
Dec 16 14:34:02.806: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.759423ms
Dec 16 14:34:02.806: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 16 14:34:02.806: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 16 14:34:02.809: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2787" to be "running and ready"
Dec 16 14:34:02.813: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.826576ms
Dec 16 14:34:02.813: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 16 14:34:02.813: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/16/22 14:34:02.817
Dec 16 14:34:02.829: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2787" to be "running"
Dec 16 14:34:02.835: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983158ms
Dec 16 14:34:04.840: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011208883s
Dec 16 14:34:04.840: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 16 14:34:04.843: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2787" to be "running"
Dec 16 14:34:04.847: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.462632ms
Dec 16 14:34:04.847: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 16 14:34:04.850: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 16 14:34:04.850: INFO: Going to poll 192.168.156.156 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 16 14:34:04.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.156.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:34:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:34:04.854: INFO: ExecWithOptions: Clientset creation
Dec 16 14:34:04.854: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.156.156%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 14:34:04.992: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 16 14:34:04.992: INFO: Going to poll 192.168.189.22 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 16 14:34:04.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.189.22:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:34:04.997: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:34:04.998: INFO: ExecWithOptions: Clientset creation
Dec 16 14:34:04.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.189.22%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 14:34:05.117: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 16 14:34:05.117: INFO: Going to poll 192.168.189.199 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 16 14:34:05.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.189.199:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 16 14:34:05.121: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
Dec 16 14:34:05.122: INFO: ExecWithOptions: Clientset creation
Dec 16 14:34:05.122: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.189.199%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 16 14:34:05.251: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 16 14:34:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2787" for this suite. 12/16/22 14:34:05.256
------------------------------
• [SLOW TEST] [14.544 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:33:50.719
    Dec 16 14:33:50.719: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pod-network-test 12/16/22 14:33:50.719
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:33:50.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:33:50.748
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2787 12/16/22 14:33:50.75
    STEP: creating a selector 12/16/22 14:33:50.75
    STEP: Creating the service pods in kubernetes 12/16/22 14:33:50.75
    Dec 16 14:33:50.750: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 16 14:33:50.788: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2787" to be "running and ready"
    Dec 16 14:33:50.792: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227102ms
    Dec 16 14:33:50.792: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:33:52.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010108734s
    Dec 16 14:33:52.798: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 14:33:54.799: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011007596s
    Dec 16 14:33:54.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 14:33:56.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008944003s
    Dec 16 14:33:56.797: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 14:33:58.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009093281s
    Dec 16 14:33:58.797: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 14:34:00.799: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010801145s
    Dec 16 14:34:00.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 16 14:34:02.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010488801s
    Dec 16 14:34:02.798: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 16 14:34:02.798: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 16 14:34:02.802: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2787" to be "running and ready"
    Dec 16 14:34:02.806: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.759423ms
    Dec 16 14:34:02.806: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 16 14:34:02.806: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 16 14:34:02.809: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2787" to be "running and ready"
    Dec 16 14:34:02.813: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.826576ms
    Dec 16 14:34:02.813: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 16 14:34:02.813: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/16/22 14:34:02.817
    Dec 16 14:34:02.829: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2787" to be "running"
    Dec 16 14:34:02.835: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983158ms
    Dec 16 14:34:04.840: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011208883s
    Dec 16 14:34:04.840: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 16 14:34:04.843: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2787" to be "running"
    Dec 16 14:34:04.847: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.462632ms
    Dec 16 14:34:04.847: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 16 14:34:04.850: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 16 14:34:04.850: INFO: Going to poll 192.168.156.156 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 14:34:04.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.156.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:34:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:34:04.854: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:34:04.854: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.156.156%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 14:34:04.992: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 16 14:34:04.992: INFO: Going to poll 192.168.189.22 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 14:34:04.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.189.22:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:34:04.997: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:34:04.998: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:34:04.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.189.22%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 14:34:05.117: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 16 14:34:05.117: INFO: Going to poll 192.168.189.199 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 16 14:34:05.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.189.199:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 16 14:34:05.121: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    Dec 16 14:34:05.122: INFO: ExecWithOptions: Clientset creation
    Dec 16 14:34:05.122: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.189.199%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 16 14:34:05.251: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:34:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2787" for this suite. 12/16/22 14:34:05.256
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:34:05.263
Dec 16 14:34:05.263: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:34:05.263
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:05.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:05.281
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 12/16/22 14:34:05.283
Dec 16 14:34:05.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f" in namespace "projected-1330" to be "Succeeded or Failed"
Dec 16 14:34:05.321: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195138ms
Dec 16 14:34:07.326: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010168775s
Dec 16 14:34:09.327: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011685524s
STEP: Saw pod success 12/16/22 14:34:09.327
Dec 16 14:34:09.327: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f" satisfied condition "Succeeded or Failed"
Dec 16 14:34:09.331: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f container client-container: <nil>
STEP: delete the pod 12/16/22 14:34:09.34
Dec 16 14:34:09.356: INFO: Waiting for pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f to disappear
Dec 16 14:34:09.359: INFO: Pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 16 14:34:09.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1330" for this suite. 12/16/22 14:34:09.364
------------------------------
• [4.108 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:34:05.263
    Dec 16 14:34:05.263: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:34:05.263
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:05.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:05.281
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 12/16/22 14:34:05.283
    Dec 16 14:34:05.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f" in namespace "projected-1330" to be "Succeeded or Failed"
    Dec 16 14:34:05.321: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195138ms
    Dec 16 14:34:07.326: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010168775s
    Dec 16 14:34:09.327: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011685524s
    STEP: Saw pod success 12/16/22 14:34:09.327
    Dec 16 14:34:09.327: INFO: Pod "downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f" satisfied condition "Succeeded or Failed"
    Dec 16 14:34:09.331: INFO: Trying to get logs from node pool-a3802-fsxxd pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f container client-container: <nil>
    STEP: delete the pod 12/16/22 14:34:09.34
    Dec 16 14:34:09.356: INFO: Waiting for pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f to disappear
    Dec 16 14:34:09.359: INFO: Pod downwardapi-volume-e787c3a3-1c45-4871-b4ff-74776d03134f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:34:09.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1330" for this suite. 12/16/22 14:34:09.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:34:09.373
Dec 16 14:34:09.373: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 14:34:09.373
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:09.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:09.393
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/16/22 14:34:09.399
Dec 16 14:34:09.409: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9382" to be "running and ready"
Dec 16 14:34:09.412: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.464149ms
Dec 16 14:34:09.412: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:34:11.418: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008667926s
Dec 16 14:34:11.418: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 16 14:34:11.418: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 12/16/22 14:34:11.422
Dec 16 14:34:11.428: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9382" to be "running and ready"
Dec 16 14:34:11.431: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487676ms
Dec 16 14:34:11.431: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:34:13.436: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343076s
Dec 16 14:34:13.436: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Dec 16 14:34:13.436: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/16/22 14:34:13.44
STEP: delete the pod with lifecycle hook 12/16/22 14:34:13.497
Dec 16 14:34:13.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 16 14:34:13.510: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 16 14:34:15.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 16 14:34:15.515: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 16 14:34:17.512: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 16 14:34:17.517: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 16 14:34:17.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9382" for this suite. 12/16/22 14:34:17.522
------------------------------
• [SLOW TEST] [8.156 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:34:09.373
    Dec 16 14:34:09.373: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/16/22 14:34:09.373
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:09.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:09.393
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/16/22 14:34:09.399
    Dec 16 14:34:09.409: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9382" to be "running and ready"
    Dec 16 14:34:09.412: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.464149ms
    Dec 16 14:34:09.412: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:34:11.418: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008667926s
    Dec 16 14:34:11.418: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 16 14:34:11.418: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 12/16/22 14:34:11.422
    Dec 16 14:34:11.428: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9382" to be "running and ready"
    Dec 16 14:34:11.431: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487676ms
    Dec 16 14:34:11.431: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:34:13.436: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343076s
    Dec 16 14:34:13.436: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Dec 16 14:34:13.436: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/16/22 14:34:13.44
    STEP: delete the pod with lifecycle hook 12/16/22 14:34:13.497
    Dec 16 14:34:13.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 16 14:34:13.510: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 16 14:34:15.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 16 14:34:15.515: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 16 14:34:17.512: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 16 14:34:17.517: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:34:17.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9382" for this suite. 12/16/22 14:34:17.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:34:17.529
Dec 16 14:34:17.529: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename webhook 12/16/22 14:34:17.53
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:17.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:17.548
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/16/22 14:34:17.561
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:34:17.922
STEP: Deploying the webhook pod 12/16/22 14:34:17.93
STEP: Wait for the deployment to be ready 12/16/22 14:34:17.942
Dec 16 14:34:17.948: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/16/22 14:34:19.961
STEP: Verifying the service has paired with the endpoint 12/16/22 14:34:19.978
Dec 16 14:34:20.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 12/16/22 14:34:20.983
STEP: create a pod that should be denied by the webhook 12/16/22 14:34:26.028
STEP: create a pod that causes the webhook to hang 12/16/22 14:34:26.07
STEP: create a configmap that should be denied by the webhook 12/16/22 14:34:36.077
STEP: create a configmap that should be admitted by the webhook 12/16/22 14:34:36.127
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/16/22 14:34:36.167
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/16/22 14:34:36.177
STEP: create a namespace that bypass the webhook 12/16/22 14:34:36.185
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/16/22 14:34:36.193
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:34:36.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9267" for this suite. 12/16/22 14:34:36.414
STEP: Destroying namespace "webhook-9267-markers" for this suite. 12/16/22 14:34:36.59
------------------------------
• [SLOW TEST] [19.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:34:17.529
    Dec 16 14:34:17.529: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename webhook 12/16/22 14:34:17.53
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:17.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:17.548
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/16/22 14:34:17.561
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/16/22 14:34:17.922
    STEP: Deploying the webhook pod 12/16/22 14:34:17.93
    STEP: Wait for the deployment to be ready 12/16/22 14:34:17.942
    Dec 16 14:34:17.948: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/16/22 14:34:19.961
    STEP: Verifying the service has paired with the endpoint 12/16/22 14:34:19.978
    Dec 16 14:34:20.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 12/16/22 14:34:20.983
    STEP: create a pod that should be denied by the webhook 12/16/22 14:34:26.028
    STEP: create a pod that causes the webhook to hang 12/16/22 14:34:26.07
    STEP: create a configmap that should be denied by the webhook 12/16/22 14:34:36.077
    STEP: create a configmap that should be admitted by the webhook 12/16/22 14:34:36.127
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/16/22 14:34:36.167
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/16/22 14:34:36.177
    STEP: create a namespace that bypass the webhook 12/16/22 14:34:36.185
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/16/22 14:34:36.193
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:34:36.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9267" for this suite. 12/16/22 14:34:36.414
    STEP: Destroying namespace "webhook-9267-markers" for this suite. 12/16/22 14:34:36.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:34:36.6
Dec 16 14:34:36.600: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename gc 12/16/22 14:34:36.601
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:36.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:36.943
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 12/16/22 14:34:36.949
STEP: delete the rc 12/16/22 14:34:42.002
STEP: wait for the rc to be deleted 12/16/22 14:34:42.009
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/16/22 14:34:47.014
STEP: Gathering metrics 12/16/22 14:35:17.028
W1216 14:35:17.035486      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 16 14:35:17.035: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 16 14:35:17.035: INFO: Deleting pod "simpletest.rc-26kg6" in namespace "gc-6043"
Dec 16 14:35:17.049: INFO: Deleting pod "simpletest.rc-29nnf" in namespace "gc-6043"
Dec 16 14:35:17.059: INFO: Deleting pod "simpletest.rc-2pktc" in namespace "gc-6043"
Dec 16 14:35:17.076: INFO: Deleting pod "simpletest.rc-2v6hd" in namespace "gc-6043"
Dec 16 14:35:17.089: INFO: Deleting pod "simpletest.rc-4ghfw" in namespace "gc-6043"
Dec 16 14:35:17.102: INFO: Deleting pod "simpletest.rc-4knkm" in namespace "gc-6043"
Dec 16 14:35:17.110: INFO: Deleting pod "simpletest.rc-4qgbp" in namespace "gc-6043"
Dec 16 14:35:17.123: INFO: Deleting pod "simpletest.rc-5bxzs" in namespace "gc-6043"
Dec 16 14:35:17.135: INFO: Deleting pod "simpletest.rc-5j5lh" in namespace "gc-6043"
Dec 16 14:35:17.147: INFO: Deleting pod "simpletest.rc-68lpv" in namespace "gc-6043"
Dec 16 14:35:17.158: INFO: Deleting pod "simpletest.rc-69fss" in namespace "gc-6043"
Dec 16 14:35:17.167: INFO: Deleting pod "simpletest.rc-6wpms" in namespace "gc-6043"
Dec 16 14:35:17.177: INFO: Deleting pod "simpletest.rc-6zj9s" in namespace "gc-6043"
Dec 16 14:35:17.186: INFO: Deleting pod "simpletest.rc-79b4l" in namespace "gc-6043"
Dec 16 14:35:17.198: INFO: Deleting pod "simpletest.rc-7jk9t" in namespace "gc-6043"
Dec 16 14:35:17.210: INFO: Deleting pod "simpletest.rc-7sh95" in namespace "gc-6043"
Dec 16 14:35:17.221: INFO: Deleting pod "simpletest.rc-89tgx" in namespace "gc-6043"
Dec 16 14:35:17.231: INFO: Deleting pod "simpletest.rc-8nbz7" in namespace "gc-6043"
Dec 16 14:35:17.244: INFO: Deleting pod "simpletest.rc-8txst" in namespace "gc-6043"
Dec 16 14:35:17.255: INFO: Deleting pod "simpletest.rc-8z66s" in namespace "gc-6043"
Dec 16 14:35:17.267: INFO: Deleting pod "simpletest.rc-8z7c7" in namespace "gc-6043"
Dec 16 14:35:17.278: INFO: Deleting pod "simpletest.rc-9kddq" in namespace "gc-6043"
Dec 16 14:35:17.286: INFO: Deleting pod "simpletest.rc-9pd9k" in namespace "gc-6043"
Dec 16 14:35:17.301: INFO: Deleting pod "simpletest.rc-b6glt" in namespace "gc-6043"
Dec 16 14:35:17.313: INFO: Deleting pod "simpletest.rc-b7rr2" in namespace "gc-6043"
Dec 16 14:35:17.324: INFO: Deleting pod "simpletest.rc-bdgkv" in namespace "gc-6043"
Dec 16 14:35:17.334: INFO: Deleting pod "simpletest.rc-c9jv4" in namespace "gc-6043"
Dec 16 14:35:17.345: INFO: Deleting pod "simpletest.rc-cdf47" in namespace "gc-6043"
Dec 16 14:35:17.356: INFO: Deleting pod "simpletest.rc-cmsms" in namespace "gc-6043"
Dec 16 14:35:17.367: INFO: Deleting pod "simpletest.rc-cwqwc" in namespace "gc-6043"
Dec 16 14:35:17.378: INFO: Deleting pod "simpletest.rc-dtrsx" in namespace "gc-6043"
Dec 16 14:35:17.389: INFO: Deleting pod "simpletest.rc-ffrg5" in namespace "gc-6043"
Dec 16 14:35:17.411: INFO: Deleting pod "simpletest.rc-fjdqv" in namespace "gc-6043"
Dec 16 14:35:17.427: INFO: Deleting pod "simpletest.rc-fkhk5" in namespace "gc-6043"
Dec 16 14:35:17.437: INFO: Deleting pod "simpletest.rc-frgrg" in namespace "gc-6043"
Dec 16 14:35:17.448: INFO: Deleting pod "simpletest.rc-fwgln" in namespace "gc-6043"
Dec 16 14:35:17.460: INFO: Deleting pod "simpletest.rc-g2cqp" in namespace "gc-6043"
Dec 16 14:35:17.476: INFO: Deleting pod "simpletest.rc-g8g24" in namespace "gc-6043"
Dec 16 14:35:17.485: INFO: Deleting pod "simpletest.rc-gbrkm" in namespace "gc-6043"
Dec 16 14:35:17.496: INFO: Deleting pod "simpletest.rc-gdrfh" in namespace "gc-6043"
Dec 16 14:35:17.511: INFO: Deleting pod "simpletest.rc-gqq8g" in namespace "gc-6043"
Dec 16 14:35:17.523: INFO: Deleting pod "simpletest.rc-h4mjg" in namespace "gc-6043"
Dec 16 14:35:17.533: INFO: Deleting pod "simpletest.rc-j64bm" in namespace "gc-6043"
Dec 16 14:35:17.544: INFO: Deleting pod "simpletest.rc-j69wr" in namespace "gc-6043"
Dec 16 14:35:17.554: INFO: Deleting pod "simpletest.rc-jgshg" in namespace "gc-6043"
Dec 16 14:35:17.562: INFO: Deleting pod "simpletest.rc-jkl9r" in namespace "gc-6043"
Dec 16 14:35:17.572: INFO: Deleting pod "simpletest.rc-k756j" in namespace "gc-6043"
Dec 16 14:35:17.584: INFO: Deleting pod "simpletest.rc-khjh8" in namespace "gc-6043"
Dec 16 14:35:17.594: INFO: Deleting pod "simpletest.rc-kjt5c" in namespace "gc-6043"
Dec 16 14:35:17.606: INFO: Deleting pod "simpletest.rc-lm4kv" in namespace "gc-6043"
Dec 16 14:35:17.616: INFO: Deleting pod "simpletest.rc-lx44d" in namespace "gc-6043"
Dec 16 14:35:17.626: INFO: Deleting pod "simpletest.rc-m42x9" in namespace "gc-6043"
Dec 16 14:35:17.638: INFO: Deleting pod "simpletest.rc-m5zbb" in namespace "gc-6043"
Dec 16 14:35:17.650: INFO: Deleting pod "simpletest.rc-m9fc2" in namespace "gc-6043"
Dec 16 14:35:17.660: INFO: Deleting pod "simpletest.rc-mkkkp" in namespace "gc-6043"
Dec 16 14:35:17.673: INFO: Deleting pod "simpletest.rc-mm5sr" in namespace "gc-6043"
Dec 16 14:35:17.683: INFO: Deleting pod "simpletest.rc-mtk8r" in namespace "gc-6043"
Dec 16 14:35:17.693: INFO: Deleting pod "simpletest.rc-n5bx8" in namespace "gc-6043"
Dec 16 14:35:17.704: INFO: Deleting pod "simpletest.rc-n76j5" in namespace "gc-6043"
Dec 16 14:35:17.712: INFO: Deleting pod "simpletest.rc-n7qb9" in namespace "gc-6043"
Dec 16 14:35:17.725: INFO: Deleting pod "simpletest.rc-njlx6" in namespace "gc-6043"
Dec 16 14:35:17.735: INFO: Deleting pod "simpletest.rc-nwpft" in namespace "gc-6043"
Dec 16 14:35:17.747: INFO: Deleting pod "simpletest.rc-nxpk6" in namespace "gc-6043"
Dec 16 14:35:17.779: INFO: Deleting pod "simpletest.rc-p2l4n" in namespace "gc-6043"
Dec 16 14:35:17.831: INFO: Deleting pod "simpletest.rc-p5wrg" in namespace "gc-6043"
Dec 16 14:35:17.878: INFO: Deleting pod "simpletest.rc-pfvkw" in namespace "gc-6043"
Dec 16 14:35:17.934: INFO: Deleting pod "simpletest.rc-pqwcw" in namespace "gc-6043"
Dec 16 14:35:17.978: INFO: Deleting pod "simpletest.rc-q98zn" in namespace "gc-6043"
Dec 16 14:35:18.028: INFO: Deleting pod "simpletest.rc-qmx75" in namespace "gc-6043"
Dec 16 14:35:18.091: INFO: Deleting pod "simpletest.rc-qrjjq" in namespace "gc-6043"
Dec 16 14:35:18.131: INFO: Deleting pod "simpletest.rc-qtlll" in namespace "gc-6043"
Dec 16 14:35:18.179: INFO: Deleting pod "simpletest.rc-qxnv2" in namespace "gc-6043"
Dec 16 14:35:18.226: INFO: Deleting pod "simpletest.rc-r5w67" in namespace "gc-6043"
Dec 16 14:35:18.276: INFO: Deleting pod "simpletest.rc-rhl8p" in namespace "gc-6043"
Dec 16 14:35:18.340: INFO: Deleting pod "simpletest.rc-rkhbt" in namespace "gc-6043"
Dec 16 14:35:18.391: INFO: Deleting pod "simpletest.rc-rvdmb" in namespace "gc-6043"
Dec 16 14:35:18.428: INFO: Deleting pod "simpletest.rc-rw698" in namespace "gc-6043"
Dec 16 14:35:18.474: INFO: Deleting pod "simpletest.rc-rxdw9" in namespace "gc-6043"
Dec 16 14:35:18.539: INFO: Deleting pod "simpletest.rc-st4c9" in namespace "gc-6043"
Dec 16 14:35:18.576: INFO: Deleting pod "simpletest.rc-sx2sh" in namespace "gc-6043"
Dec 16 14:35:18.628: INFO: Deleting pod "simpletest.rc-tm6v4" in namespace "gc-6043"
Dec 16 14:35:18.678: INFO: Deleting pod "simpletest.rc-v27rp" in namespace "gc-6043"
Dec 16 14:35:18.727: INFO: Deleting pod "simpletest.rc-vjmj6" in namespace "gc-6043"
Dec 16 14:35:18.776: INFO: Deleting pod "simpletest.rc-vvqhb" in namespace "gc-6043"
Dec 16 14:35:18.831: INFO: Deleting pod "simpletest.rc-vwkx6" in namespace "gc-6043"
Dec 16 14:35:18.878: INFO: Deleting pod "simpletest.rc-w8kcv" in namespace "gc-6043"
Dec 16 14:35:18.929: INFO: Deleting pod "simpletest.rc-whsjt" in namespace "gc-6043"
Dec 16 14:35:18.996: INFO: Deleting pod "simpletest.rc-wlz5q" in namespace "gc-6043"
Dec 16 14:35:19.030: INFO: Deleting pod "simpletest.rc-ws567" in namespace "gc-6043"
Dec 16 14:35:19.087: INFO: Deleting pod "simpletest.rc-xhzn5" in namespace "gc-6043"
Dec 16 14:35:19.129: INFO: Deleting pod "simpletest.rc-xlbhq" in namespace "gc-6043"
Dec 16 14:35:19.187: INFO: Deleting pod "simpletest.rc-xlnx2" in namespace "gc-6043"
Dec 16 14:35:19.234: INFO: Deleting pod "simpletest.rc-xpffl" in namespace "gc-6043"
Dec 16 14:35:19.279: INFO: Deleting pod "simpletest.rc-xq2jq" in namespace "gc-6043"
Dec 16 14:35:19.334: INFO: Deleting pod "simpletest.rc-xstfd" in namespace "gc-6043"
Dec 16 14:35:19.376: INFO: Deleting pod "simpletest.rc-xt9zf" in namespace "gc-6043"
Dec 16 14:35:19.424: INFO: Deleting pod "simpletest.rc-zhpbv" in namespace "gc-6043"
Dec 16 14:35:19.476: INFO: Deleting pod "simpletest.rc-zlfgg" in namespace "gc-6043"
Dec 16 14:35:19.539: INFO: Deleting pod "simpletest.rc-zvhng" in namespace "gc-6043"
Dec 16 14:35:19.574: INFO: Deleting pod "simpletest.rc-zx6l4" in namespace "gc-6043"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 16 14:35:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6043" for this suite. 12/16/22 14:35:19.67
------------------------------
• [SLOW TEST] [43.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:34:36.6
    Dec 16 14:34:36.600: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename gc 12/16/22 14:34:36.601
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:34:36.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:34:36.943
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 12/16/22 14:34:36.949
    STEP: delete the rc 12/16/22 14:34:42.002
    STEP: wait for the rc to be deleted 12/16/22 14:34:42.009
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/16/22 14:34:47.014
    STEP: Gathering metrics 12/16/22 14:35:17.028
    W1216 14:35:17.035486      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Dec 16 14:35:17.035: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 16 14:35:17.035: INFO: Deleting pod "simpletest.rc-26kg6" in namespace "gc-6043"
    Dec 16 14:35:17.049: INFO: Deleting pod "simpletest.rc-29nnf" in namespace "gc-6043"
    Dec 16 14:35:17.059: INFO: Deleting pod "simpletest.rc-2pktc" in namespace "gc-6043"
    Dec 16 14:35:17.076: INFO: Deleting pod "simpletest.rc-2v6hd" in namespace "gc-6043"
    Dec 16 14:35:17.089: INFO: Deleting pod "simpletest.rc-4ghfw" in namespace "gc-6043"
    Dec 16 14:35:17.102: INFO: Deleting pod "simpletest.rc-4knkm" in namespace "gc-6043"
    Dec 16 14:35:17.110: INFO: Deleting pod "simpletest.rc-4qgbp" in namespace "gc-6043"
    Dec 16 14:35:17.123: INFO: Deleting pod "simpletest.rc-5bxzs" in namespace "gc-6043"
    Dec 16 14:35:17.135: INFO: Deleting pod "simpletest.rc-5j5lh" in namespace "gc-6043"
    Dec 16 14:35:17.147: INFO: Deleting pod "simpletest.rc-68lpv" in namespace "gc-6043"
    Dec 16 14:35:17.158: INFO: Deleting pod "simpletest.rc-69fss" in namespace "gc-6043"
    Dec 16 14:35:17.167: INFO: Deleting pod "simpletest.rc-6wpms" in namespace "gc-6043"
    Dec 16 14:35:17.177: INFO: Deleting pod "simpletest.rc-6zj9s" in namespace "gc-6043"
    Dec 16 14:35:17.186: INFO: Deleting pod "simpletest.rc-79b4l" in namespace "gc-6043"
    Dec 16 14:35:17.198: INFO: Deleting pod "simpletest.rc-7jk9t" in namespace "gc-6043"
    Dec 16 14:35:17.210: INFO: Deleting pod "simpletest.rc-7sh95" in namespace "gc-6043"
    Dec 16 14:35:17.221: INFO: Deleting pod "simpletest.rc-89tgx" in namespace "gc-6043"
    Dec 16 14:35:17.231: INFO: Deleting pod "simpletest.rc-8nbz7" in namespace "gc-6043"
    Dec 16 14:35:17.244: INFO: Deleting pod "simpletest.rc-8txst" in namespace "gc-6043"
    Dec 16 14:35:17.255: INFO: Deleting pod "simpletest.rc-8z66s" in namespace "gc-6043"
    Dec 16 14:35:17.267: INFO: Deleting pod "simpletest.rc-8z7c7" in namespace "gc-6043"
    Dec 16 14:35:17.278: INFO: Deleting pod "simpletest.rc-9kddq" in namespace "gc-6043"
    Dec 16 14:35:17.286: INFO: Deleting pod "simpletest.rc-9pd9k" in namespace "gc-6043"
    Dec 16 14:35:17.301: INFO: Deleting pod "simpletest.rc-b6glt" in namespace "gc-6043"
    Dec 16 14:35:17.313: INFO: Deleting pod "simpletest.rc-b7rr2" in namespace "gc-6043"
    Dec 16 14:35:17.324: INFO: Deleting pod "simpletest.rc-bdgkv" in namespace "gc-6043"
    Dec 16 14:35:17.334: INFO: Deleting pod "simpletest.rc-c9jv4" in namespace "gc-6043"
    Dec 16 14:35:17.345: INFO: Deleting pod "simpletest.rc-cdf47" in namespace "gc-6043"
    Dec 16 14:35:17.356: INFO: Deleting pod "simpletest.rc-cmsms" in namespace "gc-6043"
    Dec 16 14:35:17.367: INFO: Deleting pod "simpletest.rc-cwqwc" in namespace "gc-6043"
    Dec 16 14:35:17.378: INFO: Deleting pod "simpletest.rc-dtrsx" in namespace "gc-6043"
    Dec 16 14:35:17.389: INFO: Deleting pod "simpletest.rc-ffrg5" in namespace "gc-6043"
    Dec 16 14:35:17.411: INFO: Deleting pod "simpletest.rc-fjdqv" in namespace "gc-6043"
    Dec 16 14:35:17.427: INFO: Deleting pod "simpletest.rc-fkhk5" in namespace "gc-6043"
    Dec 16 14:35:17.437: INFO: Deleting pod "simpletest.rc-frgrg" in namespace "gc-6043"
    Dec 16 14:35:17.448: INFO: Deleting pod "simpletest.rc-fwgln" in namespace "gc-6043"
    Dec 16 14:35:17.460: INFO: Deleting pod "simpletest.rc-g2cqp" in namespace "gc-6043"
    Dec 16 14:35:17.476: INFO: Deleting pod "simpletest.rc-g8g24" in namespace "gc-6043"
    Dec 16 14:35:17.485: INFO: Deleting pod "simpletest.rc-gbrkm" in namespace "gc-6043"
    Dec 16 14:35:17.496: INFO: Deleting pod "simpletest.rc-gdrfh" in namespace "gc-6043"
    Dec 16 14:35:17.511: INFO: Deleting pod "simpletest.rc-gqq8g" in namespace "gc-6043"
    Dec 16 14:35:17.523: INFO: Deleting pod "simpletest.rc-h4mjg" in namespace "gc-6043"
    Dec 16 14:35:17.533: INFO: Deleting pod "simpletest.rc-j64bm" in namespace "gc-6043"
    Dec 16 14:35:17.544: INFO: Deleting pod "simpletest.rc-j69wr" in namespace "gc-6043"
    Dec 16 14:35:17.554: INFO: Deleting pod "simpletest.rc-jgshg" in namespace "gc-6043"
    Dec 16 14:35:17.562: INFO: Deleting pod "simpletest.rc-jkl9r" in namespace "gc-6043"
    Dec 16 14:35:17.572: INFO: Deleting pod "simpletest.rc-k756j" in namespace "gc-6043"
    Dec 16 14:35:17.584: INFO: Deleting pod "simpletest.rc-khjh8" in namespace "gc-6043"
    Dec 16 14:35:17.594: INFO: Deleting pod "simpletest.rc-kjt5c" in namespace "gc-6043"
    Dec 16 14:35:17.606: INFO: Deleting pod "simpletest.rc-lm4kv" in namespace "gc-6043"
    Dec 16 14:35:17.616: INFO: Deleting pod "simpletest.rc-lx44d" in namespace "gc-6043"
    Dec 16 14:35:17.626: INFO: Deleting pod "simpletest.rc-m42x9" in namespace "gc-6043"
    Dec 16 14:35:17.638: INFO: Deleting pod "simpletest.rc-m5zbb" in namespace "gc-6043"
    Dec 16 14:35:17.650: INFO: Deleting pod "simpletest.rc-m9fc2" in namespace "gc-6043"
    Dec 16 14:35:17.660: INFO: Deleting pod "simpletest.rc-mkkkp" in namespace "gc-6043"
    Dec 16 14:35:17.673: INFO: Deleting pod "simpletest.rc-mm5sr" in namespace "gc-6043"
    Dec 16 14:35:17.683: INFO: Deleting pod "simpletest.rc-mtk8r" in namespace "gc-6043"
    Dec 16 14:35:17.693: INFO: Deleting pod "simpletest.rc-n5bx8" in namespace "gc-6043"
    Dec 16 14:35:17.704: INFO: Deleting pod "simpletest.rc-n76j5" in namespace "gc-6043"
    Dec 16 14:35:17.712: INFO: Deleting pod "simpletest.rc-n7qb9" in namespace "gc-6043"
    Dec 16 14:35:17.725: INFO: Deleting pod "simpletest.rc-njlx6" in namespace "gc-6043"
    Dec 16 14:35:17.735: INFO: Deleting pod "simpletest.rc-nwpft" in namespace "gc-6043"
    Dec 16 14:35:17.747: INFO: Deleting pod "simpletest.rc-nxpk6" in namespace "gc-6043"
    Dec 16 14:35:17.779: INFO: Deleting pod "simpletest.rc-p2l4n" in namespace "gc-6043"
    Dec 16 14:35:17.831: INFO: Deleting pod "simpletest.rc-p5wrg" in namespace "gc-6043"
    Dec 16 14:35:17.878: INFO: Deleting pod "simpletest.rc-pfvkw" in namespace "gc-6043"
    Dec 16 14:35:17.934: INFO: Deleting pod "simpletest.rc-pqwcw" in namespace "gc-6043"
    Dec 16 14:35:17.978: INFO: Deleting pod "simpletest.rc-q98zn" in namespace "gc-6043"
    Dec 16 14:35:18.028: INFO: Deleting pod "simpletest.rc-qmx75" in namespace "gc-6043"
    Dec 16 14:35:18.091: INFO: Deleting pod "simpletest.rc-qrjjq" in namespace "gc-6043"
    Dec 16 14:35:18.131: INFO: Deleting pod "simpletest.rc-qtlll" in namespace "gc-6043"
    Dec 16 14:35:18.179: INFO: Deleting pod "simpletest.rc-qxnv2" in namespace "gc-6043"
    Dec 16 14:35:18.226: INFO: Deleting pod "simpletest.rc-r5w67" in namespace "gc-6043"
    Dec 16 14:35:18.276: INFO: Deleting pod "simpletest.rc-rhl8p" in namespace "gc-6043"
    Dec 16 14:35:18.340: INFO: Deleting pod "simpletest.rc-rkhbt" in namespace "gc-6043"
    Dec 16 14:35:18.391: INFO: Deleting pod "simpletest.rc-rvdmb" in namespace "gc-6043"
    Dec 16 14:35:18.428: INFO: Deleting pod "simpletest.rc-rw698" in namespace "gc-6043"
    Dec 16 14:35:18.474: INFO: Deleting pod "simpletest.rc-rxdw9" in namespace "gc-6043"
    Dec 16 14:35:18.539: INFO: Deleting pod "simpletest.rc-st4c9" in namespace "gc-6043"
    Dec 16 14:35:18.576: INFO: Deleting pod "simpletest.rc-sx2sh" in namespace "gc-6043"
    Dec 16 14:35:18.628: INFO: Deleting pod "simpletest.rc-tm6v4" in namespace "gc-6043"
    Dec 16 14:35:18.678: INFO: Deleting pod "simpletest.rc-v27rp" in namespace "gc-6043"
    Dec 16 14:35:18.727: INFO: Deleting pod "simpletest.rc-vjmj6" in namespace "gc-6043"
    Dec 16 14:35:18.776: INFO: Deleting pod "simpletest.rc-vvqhb" in namespace "gc-6043"
    Dec 16 14:35:18.831: INFO: Deleting pod "simpletest.rc-vwkx6" in namespace "gc-6043"
    Dec 16 14:35:18.878: INFO: Deleting pod "simpletest.rc-w8kcv" in namespace "gc-6043"
    Dec 16 14:35:18.929: INFO: Deleting pod "simpletest.rc-whsjt" in namespace "gc-6043"
    Dec 16 14:35:18.996: INFO: Deleting pod "simpletest.rc-wlz5q" in namespace "gc-6043"
    Dec 16 14:35:19.030: INFO: Deleting pod "simpletest.rc-ws567" in namespace "gc-6043"
    Dec 16 14:35:19.087: INFO: Deleting pod "simpletest.rc-xhzn5" in namespace "gc-6043"
    Dec 16 14:35:19.129: INFO: Deleting pod "simpletest.rc-xlbhq" in namespace "gc-6043"
    Dec 16 14:35:19.187: INFO: Deleting pod "simpletest.rc-xlnx2" in namespace "gc-6043"
    Dec 16 14:35:19.234: INFO: Deleting pod "simpletest.rc-xpffl" in namespace "gc-6043"
    Dec 16 14:35:19.279: INFO: Deleting pod "simpletest.rc-xq2jq" in namespace "gc-6043"
    Dec 16 14:35:19.334: INFO: Deleting pod "simpletest.rc-xstfd" in namespace "gc-6043"
    Dec 16 14:35:19.376: INFO: Deleting pod "simpletest.rc-xt9zf" in namespace "gc-6043"
    Dec 16 14:35:19.424: INFO: Deleting pod "simpletest.rc-zhpbv" in namespace "gc-6043"
    Dec 16 14:35:19.476: INFO: Deleting pod "simpletest.rc-zlfgg" in namespace "gc-6043"
    Dec 16 14:35:19.539: INFO: Deleting pod "simpletest.rc-zvhng" in namespace "gc-6043"
    Dec 16 14:35:19.574: INFO: Deleting pod "simpletest.rc-zx6l4" in namespace "gc-6043"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:35:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6043" for this suite. 12/16/22 14:35:19.67
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:35:19.722
Dec 16 14:35:19.722: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename container-probe 12/16/22 14:35:19.723
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:35:19.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:35:19.741
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:19.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-391" for this suite. 12/16/22 14:36:19.763
------------------------------
• [SLOW TEST] [60.048 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:35:19.722
    Dec 16 14:35:19.722: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename container-probe 12/16/22 14:35:19.723
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:35:19.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:35:19.741
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:19.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-391" for this suite. 12/16/22 14:36:19.763
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:19.77
Dec 16 14:36:19.770: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:36:19.77
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:19.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:19.788
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Dec 16 14:36:19.790: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 14:36:21.554
Dec 16 14:36:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 create -f -'
Dec 16 14:36:22.186: INFO: stderr: ""
Dec 16 14:36:22.186: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 16 14:36:22.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 delete e2e-test-crd-publish-openapi-297-crds test-cr'
Dec 16 14:36:22.260: INFO: stderr: ""
Dec 16 14:36:22.260: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 16 14:36:22.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 apply -f -'
Dec 16 14:36:22.835: INFO: stderr: ""
Dec 16 14:36:22.835: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 16 14:36:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 delete e2e-test-crd-publish-openapi-297-crds test-cr'
Dec 16 14:36:22.905: INFO: stderr: ""
Dec 16 14:36:22.905: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 12/16/22 14:36:22.905
Dec 16 14:36:22.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 explain e2e-test-crd-publish-openapi-297-crds'
Dec 16 14:36:23.107: INFO: stderr: ""
Dec 16 14:36:23.107: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-297-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5522" for this suite. 12/16/22 14:36:24.927
------------------------------
• [SLOW TEST] [5.164 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:19.77
    Dec 16 14:36:19.770: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename crd-publish-openapi 12/16/22 14:36:19.77
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:19.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:19.788
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Dec 16 14:36:19.790: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/16/22 14:36:21.554
    Dec 16 14:36:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 create -f -'
    Dec 16 14:36:22.186: INFO: stderr: ""
    Dec 16 14:36:22.186: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 16 14:36:22.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 delete e2e-test-crd-publish-openapi-297-crds test-cr'
    Dec 16 14:36:22.260: INFO: stderr: ""
    Dec 16 14:36:22.260: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Dec 16 14:36:22.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 apply -f -'
    Dec 16 14:36:22.835: INFO: stderr: ""
    Dec 16 14:36:22.835: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 16 14:36:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 --namespace=crd-publish-openapi-5522 delete e2e-test-crd-publish-openapi-297-crds test-cr'
    Dec 16 14:36:22.905: INFO: stderr: ""
    Dec 16 14:36:22.905: INFO: stdout: "e2e-test-crd-publish-openapi-297-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 12/16/22 14:36:22.905
    Dec 16 14:36:22.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=crd-publish-openapi-5522 explain e2e-test-crd-publish-openapi-297-crds'
    Dec 16 14:36:23.107: INFO: stderr: ""
    Dec 16 14:36:23.107: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-297-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5522" for this suite. 12/16/22 14:36:24.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:24.934
Dec 16 14:36:24.934: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:36:24.935
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:24.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:24.953
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Dec 16 14:36:24.966: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5156 to be scheduled
Dec 16 14:36:24.969: INFO: 1 pods are not scheduled: [runtimeclass-5156/test-runtimeclass-runtimeclass-5156-preconfigured-handler-lkb8b(da167546-d4fc-4d71-a83b-d7d0784de56e)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:26.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5156" for this suite. 12/16/22 14:36:26.992
------------------------------
• [2.071 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:24.934
    Dec 16 14:36:24.934: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename runtimeclass 12/16/22 14:36:24.935
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:24.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:24.953
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Dec 16 14:36:24.966: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5156 to be scheduled
    Dec 16 14:36:24.969: INFO: 1 pods are not scheduled: [runtimeclass-5156/test-runtimeclass-runtimeclass-5156-preconfigured-handler-lkb8b(da167546-d4fc-4d71-a83b-d7d0784de56e)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:26.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5156" for this suite. 12/16/22 14:36:26.992
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:27.006
Dec 16 14:36:27.006: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:36:27.006
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:27.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:27.027
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-589cdef1-0c5d-4843-8971-90782ce81d2f 12/16/22 14:36:27.029
STEP: Creating a pod to test consume secrets 12/16/22 14:36:27.037
Dec 16 14:36:27.045: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb" in namespace "projected-982" to be "Succeeded or Failed"
Dec 16 14:36:27.050: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464869ms
Dec 16 14:36:29.055: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Running", Reason="", readiness=false. Elapsed: 2.009812975s
Dec 16 14:36:31.057: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011334688s
STEP: Saw pod success 12/16/22 14:36:31.057
Dec 16 14:36:31.057: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb" satisfied condition "Succeeded or Failed"
Dec 16 14:36:31.061: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb container projected-secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:36:31.111
Dec 16 14:36:31.124: INFO: Waiting for pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb to disappear
Dec 16 14:36:31.127: INFO: Pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:31.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-982" for this suite. 12/16/22 14:36:31.132
------------------------------
• [4.132 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:27.006
    Dec 16 14:36:27.006: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:36:27.006
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:27.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:27.027
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-589cdef1-0c5d-4843-8971-90782ce81d2f 12/16/22 14:36:27.029
    STEP: Creating a pod to test consume secrets 12/16/22 14:36:27.037
    Dec 16 14:36:27.045: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb" in namespace "projected-982" to be "Succeeded or Failed"
    Dec 16 14:36:27.050: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464869ms
    Dec 16 14:36:29.055: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Running", Reason="", readiness=false. Elapsed: 2.009812975s
    Dec 16 14:36:31.057: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011334688s
    STEP: Saw pod success 12/16/22 14:36:31.057
    Dec 16 14:36:31.057: INFO: Pod "pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb" satisfied condition "Succeeded or Failed"
    Dec 16 14:36:31.061: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:36:31.111
    Dec 16 14:36:31.124: INFO: Waiting for pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb to disappear
    Dec 16 14:36:31.127: INFO: Pod pod-projected-secrets-9a63380e-0aca-4a4f-95f1-4f2c94bc1feb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:31.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-982" for this suite. 12/16/22 14:36:31.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:31.138
Dec 16 14:36:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename secrets 12/16/22 14:36:31.139
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:31.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:31.16
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-eedac128-b1e3-46bb-9e42-eb834d83eba7 12/16/22 14:36:31.162
STEP: Creating a pod to test consume secrets 12/16/22 14:36:31.166
Dec 16 14:36:31.173: INFO: Waiting up to 5m0s for pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b" in namespace "secrets-1406" to be "Succeeded or Failed"
Dec 16 14:36:31.177: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236047ms
Dec 16 14:36:33.181: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007414195s
Dec 16 14:36:35.183: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009642822s
STEP: Saw pod success 12/16/22 14:36:35.183
Dec 16 14:36:35.183: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b" satisfied condition "Succeeded or Failed"
Dec 16 14:36:35.187: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b container secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:36:35.197
Dec 16 14:36:35.210: INFO: Waiting for pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b to disappear
Dec 16 14:36:35.213: INFO: Pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:35.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1406" for this suite. 12/16/22 14:36:35.218
------------------------------
• [4.088 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:31.138
    Dec 16 14:36:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename secrets 12/16/22 14:36:31.139
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:31.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:31.16
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-eedac128-b1e3-46bb-9e42-eb834d83eba7 12/16/22 14:36:31.162
    STEP: Creating a pod to test consume secrets 12/16/22 14:36:31.166
    Dec 16 14:36:31.173: INFO: Waiting up to 5m0s for pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b" in namespace "secrets-1406" to be "Succeeded or Failed"
    Dec 16 14:36:31.177: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236047ms
    Dec 16 14:36:33.181: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007414195s
    Dec 16 14:36:35.183: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009642822s
    STEP: Saw pod success 12/16/22 14:36:35.183
    Dec 16 14:36:35.183: INFO: Pod "pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b" satisfied condition "Succeeded or Failed"
    Dec 16 14:36:35.187: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b container secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:36:35.197
    Dec 16 14:36:35.210: INFO: Waiting for pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b to disappear
    Dec 16 14:36:35.213: INFO: Pod pod-secrets-93eb08a5-a170-4730-9138-7f00fb6acf3b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:35.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1406" for this suite. 12/16/22 14:36:35.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:35.227
Dec 16 14:36:35.227: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:36:35.228
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:35.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:35.247
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-c8e43a86-a7f0-4dee-bd19-f471a6c3461d 12/16/22 14:36:35.25
STEP: Creating a pod to test consume secrets 12/16/22 14:36:35.255
Dec 16 14:36:35.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd" in namespace "projected-2082" to be "Succeeded or Failed"
Dec 16 14:36:35.267: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533038ms
Dec 16 14:36:37.272: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008938931s
Dec 16 14:36:39.273: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009395541s
STEP: Saw pod success 12/16/22 14:36:39.273
Dec 16 14:36:39.273: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd" satisfied condition "Succeeded or Failed"
Dec 16 14:36:39.276: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd container projected-secret-volume-test: <nil>
STEP: delete the pod 12/16/22 14:36:39.284
Dec 16 14:36:39.296: INFO: Waiting for pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd to disappear
Dec 16 14:36:39.300: INFO: Pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:39.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2082" for this suite. 12/16/22 14:36:39.304
------------------------------
• [4.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:35.227
    Dec 16 14:36:35.227: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:36:35.228
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:35.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:35.247
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-c8e43a86-a7f0-4dee-bd19-f471a6c3461d 12/16/22 14:36:35.25
    STEP: Creating a pod to test consume secrets 12/16/22 14:36:35.255
    Dec 16 14:36:35.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd" in namespace "projected-2082" to be "Succeeded or Failed"
    Dec 16 14:36:35.267: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533038ms
    Dec 16 14:36:37.272: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008938931s
    Dec 16 14:36:39.273: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009395541s
    STEP: Saw pod success 12/16/22 14:36:39.273
    Dec 16 14:36:39.273: INFO: Pod "pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd" satisfied condition "Succeeded or Failed"
    Dec 16 14:36:39.276: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/16/22 14:36:39.284
    Dec 16 14:36:39.296: INFO: Waiting for pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd to disappear
    Dec 16 14:36:39.300: INFO: Pod pod-projected-secrets-1cce1633-9aad-4804-8272-73ac4712b2fd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:39.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2082" for this suite. 12/16/22 14:36:39.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:39.311
Dec 16 14:36:39.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename services 12/16/22 14:36:39.312
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:39.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:39.33
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1956 12/16/22 14:36:39.333
STEP: creating service affinity-nodeport in namespace services-1956 12/16/22 14:36:39.333
STEP: creating replication controller affinity-nodeport in namespace services-1956 12/16/22 14:36:39.351
I1216 14:36:39.357221      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1956, replica count: 3
I1216 14:36:42.407858      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 16 14:36:42.420: INFO: Creating new exec pod
Dec 16 14:36:42.429: INFO: Waiting up to 5m0s for pod "execpod-affinitykl4fk" in namespace "services-1956" to be "running"
Dec 16 14:36:42.432: INFO: Pod "execpod-affinitykl4fk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119028ms
Dec 16 14:36:44.437: INFO: Pod "execpod-affinitykl4fk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007893482s
Dec 16 14:36:44.437: INFO: Pod "execpod-affinitykl4fk" satisfied condition "running"
Dec 16 14:36:45.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Dec 16 14:36:45.630: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 16 14:36:45.630: INFO: stdout: ""
Dec 16 14:36:45.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 10.101.54.28 80'
Dec 16 14:36:45.817: INFO: stderr: "+ nc -v -z -w 2 10.101.54.28 80\nConnection to 10.101.54.28 80 port [tcp/http] succeeded!\n"
Dec 16 14:36:45.817: INFO: stdout: ""
Dec 16 14:36:45.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 85.217.161.213 31663'
Dec 16 14:36:45.994: INFO: stderr: "+ nc -v -z -w 2 85.217.161.213 31663\nConnection to 85.217.161.213 31663 port [tcp/*] succeeded!\n"
Dec 16 14:36:45.994: INFO: stdout: ""
Dec 16 14:36:45.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 31663'
Dec 16 14:36:46.194: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 31663\nConnection to 85.217.161.242 31663 port [tcp/*] succeeded!\n"
Dec 16 14:36:46.194: INFO: stdout: ""
Dec 16 14:36:46.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:31663/ ; done'
Dec 16 14:36:46.465: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n"
Dec 16 14:36:46.465: INFO: stdout: "\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds"
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
Dec 16 14:36:46.465: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1956, will wait for the garbage collector to delete the pods 12/16/22 14:36:46.476
Dec 16 14:36:46.536: INFO: Deleting ReplicationController affinity-nodeport took: 6.604677ms
Dec 16 14:36:46.636: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.115788ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:48.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1956" for this suite. 12/16/22 14:36:48.971
------------------------------
• [SLOW TEST] [9.666 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:39.311
    Dec 16 14:36:39.311: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename services 12/16/22 14:36:39.312
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:39.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:39.33
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1956 12/16/22 14:36:39.333
    STEP: creating service affinity-nodeport in namespace services-1956 12/16/22 14:36:39.333
    STEP: creating replication controller affinity-nodeport in namespace services-1956 12/16/22 14:36:39.351
    I1216 14:36:39.357221      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1956, replica count: 3
    I1216 14:36:42.407858      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 16 14:36:42.420: INFO: Creating new exec pod
    Dec 16 14:36:42.429: INFO: Waiting up to 5m0s for pod "execpod-affinitykl4fk" in namespace "services-1956" to be "running"
    Dec 16 14:36:42.432: INFO: Pod "execpod-affinitykl4fk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119028ms
    Dec 16 14:36:44.437: INFO: Pod "execpod-affinitykl4fk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007893482s
    Dec 16 14:36:44.437: INFO: Pod "execpod-affinitykl4fk" satisfied condition "running"
    Dec 16 14:36:45.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Dec 16 14:36:45.630: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Dec 16 14:36:45.630: INFO: stdout: ""
    Dec 16 14:36:45.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 10.101.54.28 80'
    Dec 16 14:36:45.817: INFO: stderr: "+ nc -v -z -w 2 10.101.54.28 80\nConnection to 10.101.54.28 80 port [tcp/http] succeeded!\n"
    Dec 16 14:36:45.817: INFO: stdout: ""
    Dec 16 14:36:45.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 85.217.161.213 31663'
    Dec 16 14:36:45.994: INFO: stderr: "+ nc -v -z -w 2 85.217.161.213 31663\nConnection to 85.217.161.213 31663 port [tcp/*] succeeded!\n"
    Dec 16 14:36:45.994: INFO: stdout: ""
    Dec 16 14:36:45.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c nc -v -z -w 2 85.217.161.242 31663'
    Dec 16 14:36:46.194: INFO: stderr: "+ nc -v -z -w 2 85.217.161.242 31663\nConnection to 85.217.161.242 31663 port [tcp/*] succeeded!\n"
    Dec 16 14:36:46.194: INFO: stdout: ""
    Dec 16 14:36:46.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-401541609 --namespace=services-1956 exec execpod-affinitykl4fk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://85.217.161.222:31663/ ; done'
    Dec 16 14:36:46.465: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n+ echo\n+ curl -q -s --connect-timeout 2 http://85.217.161.222:31663/\n"
    Dec 16 14:36:46.465: INFO: stdout: "\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds\naffinity-nodeport-pjvds"
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Received response from host: affinity-nodeport-pjvds
    Dec 16 14:36:46.465: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1956, will wait for the garbage collector to delete the pods 12/16/22 14:36:46.476
    Dec 16 14:36:46.536: INFO: Deleting ReplicationController affinity-nodeport took: 6.604677ms
    Dec 16 14:36:46.636: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.115788ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:48.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1956" for this suite. 12/16/22 14:36:48.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:48.978
Dec 16 14:36:48.978: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 14:36:48.978
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:48.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:48.994
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Dec 16 14:36:48.996: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: creating the pod 12/16/22 14:36:48.997
STEP: submitting the pod to kubernetes 12/16/22 14:36:48.997
Dec 16 14:36:49.004: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c" in namespace "pods-4274" to be "running and ready"
Dec 16 14:36:49.007: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578266ms
Dec 16 14:36:49.007: INFO: The phase of Pod pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:36:51.012: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307772s
Dec 16 14:36:51.012: INFO: The phase of Pod pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c is Running (Ready = true)
Dec 16 14:36:51.012: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:51.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4274" for this suite. 12/16/22 14:36:51.154
------------------------------
• [2.182 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:48.978
    Dec 16 14:36:48.978: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 14:36:48.978
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:48.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:48.994
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Dec 16 14:36:48.996: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: creating the pod 12/16/22 14:36:48.997
    STEP: submitting the pod to kubernetes 12/16/22 14:36:48.997
    Dec 16 14:36:49.004: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c" in namespace "pods-4274" to be "running and ready"
    Dec 16 14:36:49.007: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578266ms
    Dec 16 14:36:49.007: INFO: The phase of Pod pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:36:51.012: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307772s
    Dec 16 14:36:51.012: INFO: The phase of Pod pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c is Running (Ready = true)
    Dec 16 14:36:51.012: INFO: Pod "pod-exec-websocket-8ca37ef9-335f-4377-97a5-52b95893d90c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:51.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4274" for this suite. 12/16/22 14:36:51.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:51.161
Dec 16 14:36:51.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename job 12/16/22 14:36:51.161
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:51.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:51.181
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 12/16/22 14:36:51.184
STEP: Ensure pods equal to parallelism count is attached to the job 12/16/22 14:36:51.19
STEP: patching /status 12/16/22 14:36:53.195
STEP: updating /status 12/16/22 14:36:53.203
STEP: get /status 12/16/22 14:36:53.234
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:53.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5271" for this suite. 12/16/22 14:36:53.242
------------------------------
• [2.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:51.161
    Dec 16 14:36:51.161: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename job 12/16/22 14:36:51.161
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:51.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:51.181
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 12/16/22 14:36:51.184
    STEP: Ensure pods equal to parallelism count is attached to the job 12/16/22 14:36:51.19
    STEP: patching /status 12/16/22 14:36:53.195
    STEP: updating /status 12/16/22 14:36:53.203
    STEP: get /status 12/16/22 14:36:53.234
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:53.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5271" for this suite. 12/16/22 14:36:53.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:53.248
Dec 16 14:36:53.249: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename projected 12/16/22 14:36:53.249
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:53.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:53.267
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-f03e6131-9ca7-45ad-a49c-de55fa75ef3e 12/16/22 14:36:53.269
STEP: Creating a pod to test consume configMaps 12/16/22 14:36:53.274
Dec 16 14:36:53.281: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0" in namespace "projected-6972" to be "Succeeded or Failed"
Dec 16 14:36:53.284: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927041ms
Dec 16 14:36:55.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008058455s
Dec 16 14:36:57.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007902081s
STEP: Saw pod success 12/16/22 14:36:57.289
Dec 16 14:36:57.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0" satisfied condition "Succeeded or Failed"
Dec 16 14:36:57.294: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 container agnhost-container: <nil>
STEP: delete the pod 12/16/22 14:36:57.304
Dec 16 14:36:57.316: INFO: Waiting for pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 to disappear
Dec 16 14:36:57.319: INFO: Pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 16 14:36:57.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6972" for this suite. 12/16/22 14:36:57.323
------------------------------
• [4.081 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:53.248
    Dec 16 14:36:53.249: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename projected 12/16/22 14:36:53.249
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:53.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:53.267
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-f03e6131-9ca7-45ad-a49c-de55fa75ef3e 12/16/22 14:36:53.269
    STEP: Creating a pod to test consume configMaps 12/16/22 14:36:53.274
    Dec 16 14:36:53.281: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0" in namespace "projected-6972" to be "Succeeded or Failed"
    Dec 16 14:36:53.284: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927041ms
    Dec 16 14:36:55.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008058455s
    Dec 16 14:36:57.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007902081s
    STEP: Saw pod success 12/16/22 14:36:57.289
    Dec 16 14:36:57.289: INFO: Pod "pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0" satisfied condition "Succeeded or Failed"
    Dec 16 14:36:57.294: INFO: Trying to get logs from node pool-a3802-fsxxd pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 container agnhost-container: <nil>
    STEP: delete the pod 12/16/22 14:36:57.304
    Dec 16 14:36:57.316: INFO: Waiting for pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 to disappear
    Dec 16 14:36:57.319: INFO: Pod pod-projected-configmaps-1cec269e-870f-4977-b474-d16989fdb2e0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:36:57.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6972" for this suite. 12/16/22 14:36:57.323
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:36:57.329
Dec 16 14:36:57.330: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename statefulset 12/16/22 14:36:57.33
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:57.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:57.349
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9 12/16/22 14:36:57.351
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9 12/16/22 14:36:57.36
Dec 16 14:36:57.369: INFO: Found 0 stateful pods, waiting for 1
Dec 16 14:37:08.710: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 12/16/22 14:37:08.723
STEP: Getting /status 12/16/22 14:37:08.73
Dec 16 14:37:08.734: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 12/16/22 14:37:08.734
Dec 16 14:37:08.742: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 12/16/22 14:37:08.742
Dec 16 14:37:08.744: INFO: Observed &StatefulSet event: ADDED
Dec 16 14:37:08.744: INFO: Found Statefulset ss in namespace statefulset-9 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 16 14:37:08.744: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 12/16/22 14:37:08.744
Dec 16 14:37:08.744: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 16 14:37:08.750: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 12/16/22 14:37:08.75
Dec 16 14:37:08.752: INFO: Observed &StatefulSet event: ADDED
Dec 16 14:37:08.752: INFO: Observed Statefulset ss in namespace statefulset-9 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 16 14:37:08.752: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 16 14:37:08.752: INFO: Deleting all statefulset in ns statefulset-9
Dec 16 14:37:08.755: INFO: Scaling statefulset ss to 0
Dec 16 14:37:18.776: INFO: Waiting for statefulset status.replicas updated to 0
Dec 16 14:37:18.779: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 16 14:37:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9" for this suite. 12/16/22 14:37:18.796
------------------------------
• [SLOW TEST] [21.474 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:36:57.329
    Dec 16 14:36:57.330: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename statefulset 12/16/22 14:36:57.33
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:36:57.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:36:57.349
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9 12/16/22 14:36:57.351
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9 12/16/22 14:36:57.36
    Dec 16 14:36:57.369: INFO: Found 0 stateful pods, waiting for 1
    Dec 16 14:37:08.710: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 12/16/22 14:37:08.723
    STEP: Getting /status 12/16/22 14:37:08.73
    Dec 16 14:37:08.734: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 12/16/22 14:37:08.734
    Dec 16 14:37:08.742: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 12/16/22 14:37:08.742
    Dec 16 14:37:08.744: INFO: Observed &StatefulSet event: ADDED
    Dec 16 14:37:08.744: INFO: Found Statefulset ss in namespace statefulset-9 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 16 14:37:08.744: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 12/16/22 14:37:08.744
    Dec 16 14:37:08.744: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 16 14:37:08.750: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 12/16/22 14:37:08.75
    Dec 16 14:37:08.752: INFO: Observed &StatefulSet event: ADDED
    Dec 16 14:37:08.752: INFO: Observed Statefulset ss in namespace statefulset-9 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 16 14:37:08.752: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 16 14:37:08.752: INFO: Deleting all statefulset in ns statefulset-9
    Dec 16 14:37:08.755: INFO: Scaling statefulset ss to 0
    Dec 16 14:37:18.776: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 16 14:37:18.779: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:37:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9" for this suite. 12/16/22 14:37:18.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:37:18.81
Dec 16 14:37:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename csistoragecapacity 12/16/22 14:37:18.811
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:18.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:18.831
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 12/16/22 14:37:18.833
STEP: getting /apis/storage.k8s.io 12/16/22 14:37:18.836
STEP: getting /apis/storage.k8s.io/v1 12/16/22 14:37:18.837
STEP: creating 12/16/22 14:37:18.838
STEP: watching 12/16/22 14:37:18.855
Dec 16 14:37:18.855: INFO: starting watch
STEP: getting 12/16/22 14:37:18.862
STEP: listing in namespace 12/16/22 14:37:18.864
STEP: listing across namespaces 12/16/22 14:37:18.867
STEP: patching 12/16/22 14:37:18.87
STEP: updating 12/16/22 14:37:18.875
Dec 16 14:37:18.879: INFO: waiting for watch events with expected annotations in namespace
Dec 16 14:37:18.879: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 12/16/22 14:37:18.88
STEP: deleting a collection 12/16/22 14:37:18.89
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Dec 16 14:37:18.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-5285" for this suite. 12/16/22 14:37:18.91
------------------------------
• [0.104 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:37:18.81
    Dec 16 14:37:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename csistoragecapacity 12/16/22 14:37:18.811
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:18.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:18.831
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 12/16/22 14:37:18.833
    STEP: getting /apis/storage.k8s.io 12/16/22 14:37:18.836
    STEP: getting /apis/storage.k8s.io/v1 12/16/22 14:37:18.837
    STEP: creating 12/16/22 14:37:18.838
    STEP: watching 12/16/22 14:37:18.855
    Dec 16 14:37:18.855: INFO: starting watch
    STEP: getting 12/16/22 14:37:18.862
    STEP: listing in namespace 12/16/22 14:37:18.864
    STEP: listing across namespaces 12/16/22 14:37:18.867
    STEP: patching 12/16/22 14:37:18.87
    STEP: updating 12/16/22 14:37:18.875
    Dec 16 14:37:18.879: INFO: waiting for watch events with expected annotations in namespace
    Dec 16 14:37:18.879: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 12/16/22 14:37:18.88
    STEP: deleting a collection 12/16/22 14:37:18.89
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:37:18.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-5285" for this suite. 12/16/22 14:37:18.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:37:18.915
Dec 16 14:37:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename downward-api 12/16/22 14:37:18.916
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:18.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:18.933
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 12/16/22 14:37:18.936
Dec 16 14:37:18.944: INFO: Waiting up to 5m0s for pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72" in namespace "downward-api-9698" to be "Succeeded or Failed"
Dec 16 14:37:18.947: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558528ms
Dec 16 14:37:20.953: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008942006s
Dec 16 14:37:22.952: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007655692s
STEP: Saw pod success 12/16/22 14:37:22.952
Dec 16 14:37:22.952: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72" satisfied condition "Succeeded or Failed"
Dec 16 14:37:22.955: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 container dapi-container: <nil>
STEP: delete the pod 12/16/22 14:37:22.964
Dec 16 14:37:22.976: INFO: Waiting for pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 to disappear
Dec 16 14:37:22.979: INFO: Pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 16 14:37:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9698" for this suite. 12/16/22 14:37:22.982
------------------------------
• [4.073 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:37:18.915
    Dec 16 14:37:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename downward-api 12/16/22 14:37:18.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:18.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:18.933
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 12/16/22 14:37:18.936
    Dec 16 14:37:18.944: INFO: Waiting up to 5m0s for pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72" in namespace "downward-api-9698" to be "Succeeded or Failed"
    Dec 16 14:37:18.947: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558528ms
    Dec 16 14:37:20.953: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008942006s
    Dec 16 14:37:22.952: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007655692s
    STEP: Saw pod success 12/16/22 14:37:22.952
    Dec 16 14:37:22.952: INFO: Pod "downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72" satisfied condition "Succeeded or Failed"
    Dec 16 14:37:22.955: INFO: Trying to get logs from node pool-a3802-fsxxd pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 container dapi-container: <nil>
    STEP: delete the pod 12/16/22 14:37:22.964
    Dec 16 14:37:22.976: INFO: Waiting for pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 to disappear
    Dec 16 14:37:22.979: INFO: Pod downward-api-1f54acc3-a46e-4cb3-9fe5-9345b1b8fe72 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:37:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9698" for this suite. 12/16/22 14:37:22.982
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:37:22.989
Dec 16 14:37:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename pods 12/16/22 14:37:22.99
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:23.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:23.008
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 12/16/22 14:37:23.01
Dec 16 14:37:23.019: INFO: Waiting up to 5m0s for pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e" in namespace "pods-2115" to be "running and ready"
Dec 16 14:37:23.023: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541777ms
Dec 16 14:37:23.023: INFO: The phase of Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e is Pending, waiting for it to be Running (with Ready = true)
Dec 16 14:37:25.028: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008277196s
Dec 16 14:37:25.028: INFO: The phase of Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e is Running (Ready = true)
Dec 16 14:37:25.028: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e" satisfied condition "running and ready"
Dec 16 14:37:25.034: INFO: Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e has hostIP: 85.217.161.242
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 16 14:37:25.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2115" for this suite. 12/16/22 14:37:25.039
------------------------------
• [2.056 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:37:22.989
    Dec 16 14:37:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename pods 12/16/22 14:37:22.99
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:23.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:23.008
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 12/16/22 14:37:23.01
    Dec 16 14:37:23.019: INFO: Waiting up to 5m0s for pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e" in namespace "pods-2115" to be "running and ready"
    Dec 16 14:37:23.023: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541777ms
    Dec 16 14:37:23.023: INFO: The phase of Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e is Pending, waiting for it to be Running (with Ready = true)
    Dec 16 14:37:25.028: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008277196s
    Dec 16 14:37:25.028: INFO: The phase of Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e is Running (Ready = true)
    Dec 16 14:37:25.028: INFO: Pod "pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e" satisfied condition "running and ready"
    Dec 16 14:37:25.034: INFO: Pod pod-hostip-8acf8ba3-42b0-4999-86e2-7cd976bb258e has hostIP: 85.217.161.242
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:37:25.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2115" for this suite. 12/16/22 14:37:25.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/16/22 14:37:25.045
Dec 16 14:37:25.045: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
STEP: Building a namespace api object, basename cronjob 12/16/22 14:37:25.046
STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:25.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:25.063
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 12/16/22 14:37:25.066
STEP: Ensuring a job is scheduled 12/16/22 14:37:25.07
STEP: Ensuring exactly one is scheduled 12/16/22 14:38:01.075
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/16/22 14:38:01.082
STEP: Ensuring no more jobs are scheduled 12/16/22 14:38:01.085
STEP: Removing cronjob 12/16/22 14:43:01.095
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 16 14:43:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9685" for this suite. 12/16/22 14:43:01.106
------------------------------
• [SLOW TEST] [336.067 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/16/22 14:37:25.045
    Dec 16 14:37:25.045: INFO: >>> kubeConfig: /tmp/kubeconfig-401541609
    STEP: Building a namespace api object, basename cronjob 12/16/22 14:37:25.046
    STEP: Waiting for a default service account to be provisioned in namespace 12/16/22 14:37:25.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/16/22 14:37:25.063
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 12/16/22 14:37:25.066
    STEP: Ensuring a job is scheduled 12/16/22 14:37:25.07
    STEP: Ensuring exactly one is scheduled 12/16/22 14:38:01.075
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/16/22 14:38:01.082
    STEP: Ensuring no more jobs are scheduled 12/16/22 14:38:01.085
    STEP: Removing cronjob 12/16/22 14:43:01.095
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 16 14:43:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9685" for this suite. 12/16/22 14:43:01.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Dec 16 14:43:01.114: INFO: Running AfterSuite actions on node 1
Dec 16 14:43:01.114: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Dec 16 14:43:01.114: INFO: Running AfterSuite actions on node 1
    Dec 16 14:43:01.114: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.071 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5515.331 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h31m55.629369273s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

